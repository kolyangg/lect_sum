[Chunk start 0.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 25.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 50.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 75.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 100.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 125.00s]: В этом году мы должны поддержать и поддержать общую связь между нами и между нами.
[Chunk start 150.00s]: В этом году мы должны поддержать и поддержать общую связь между нами и между нами.
[Chunk start 175.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 200.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 225.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 250.00s]: Всем привет!
[Chunk start 275.00s]: Всем привет! Давайте подождем и начнем.
[Chunk start 300.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 325.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 350.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.
[Chunk start 375.00s]: 
[Chunk start 400.00s]: Да, давайте начнем.
[Chunk start 425.00s]: Нас сегодня ждет не то чтобы сильно большая, поэтому, скорее всего, мы проведем ее без перерыва. Думаю, мы успеем где-то за час максимум все это дело рассказать. Но при этом заключительная лекция в блоке первом лекции, связанной с лоломками, такими именно больше коры историями про модальности и про то, как их обучают, какие там способы используют.
[Chunk start 450.00s]: про модальности и про то, как их обучают, какие там способы используют. И сегодня мы, по большей части, как раз поговорим про какие-то оставшиеся модальности, которые не успели затронуть на предыдущей лекции, и заодно немного поговорим о тех методах, которые используют для того, чтобы вообще сервис на базе ламп какой-то свой сделать, почему эти методы вдруг актуальны и зачем их используют, И подведем некоторые вообще в целом итоги того, что мы видим.
[Chunk start 475.00s]: И подведем некоторые вообще в целом итоги того, что мы там изучили на первом блоке и зачем мы это все дело проходили. Да, мы в прошлой лекции разбрали Vision модальность. Она такая достаточно большая и интересная. Сегодня будем рассматривать с вами и код, и аудио. И начнем с кода. Код как модальность, она в целом достаточно уникальная, Потому что ее даже вводил модальность в статистическую.
[Chunk start 500.00s]: Код как модальность в целом достаточно уникальная, потому что ее даже в отдел модальности в целом не всегда выносят. Однако код сам по себе имеет достаточно много особенностей. Во-первых, он написан на привычном для любой лоломки тексте, так или иначе, но при этом, естественно, не представляет собой естественный язык. Для чего вообще нужен код как модальность? Ну, во-первых, любой разработчик в целом...
[Chunk start 525.00s]: Во-первых, любой разработчик, который особенно разрабатывает подобные модали, он так или иначе хотел иметь какого-то универсального помощника, который поможет достаточно быстро решать какие-то задачи, связанные с кодом, которые возникают в целом практически всегда, если вы разрабатываете или занимаетесь дата-сайенсом или что-то подобное. И таких задач достаточно много, которые возникают у вас из раза в раз в вашей жизни. Иногда...
[Chunk start 550.00s]: Раз в раз в вашей жизни иногда нужно посмотреть, где находится какой-то баг. Причем этот баг нужно найти и потом еще и понять, каким образом нам надо сделать так, чтобы этот баг не работал. Точнее, исправить этот баг. Нам нужно найти что-то в коде, либо в своем, либо в чужом. Code-to-code retrieval достаточно часто тоже история, которая позволяет нам решать кодовую модальность. Ну, естественно, это не значит, что мы не будем делать это.
[Chunk start 575.00s]: 
[Chunk start 600.00s]: Причем кодовые сервисы обычно сочетают в себе не просто какую-то одну модельку, так они в целом сочетают множество моделей. Сейчас секундочку. Они сочетают множество моделей, начиная от single line модели, это как отдельная модель, где требуется завершить только одну маленькую строчку кода, При этом, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате,
[Chunk start 625.00s]: 
[Chunk start 650.00s]: 
[Chunk start 675.00s]: 
[Chunk start 700.00s]: 
[Chunk start 725.00s]: Там порядка 900 миллиардов токенов. Как вы помните, в российском интернете нам в целом такое даже сниться не может на естественном языке, да даже на англоязычном на самом деле. Не то чтобы там прям имелись такие какие-то датасеты, где столько много токенов было бы представлено. Весит, конечно, эта махина достаточно мощно, но даже на этом, на самом деле, разработчики не осознают, что это не так.
[Chunk start 750.00s]: Но даже на этом, на самом деле, разработчики не останавливаются, потому что у кода в целом, как у отдельной модальности, есть такое свойство, что мы всегда можем проверить правильность кода, который так или иначе нам встретится как кусок на какую-то истинность.
[Chunk start 775.00s]: И добавить такой синтетический код у нас в обучение. Сейчас тяжело даже назвать толком долю синтетических данных внутри современных моделей, связанных с модальностью кода, однако их достаточно много.
[Chunk start 800.00s]: 
[Chunk start 825.00s]: Как маленькие модели, так и в целом достаточно большие некоторые представлены, но они эффективно справляются с задачей генерации кода. Причем генерация кода у них может быть трех типов. Это либо какая-то self-instruct история, когда мы даем самой какой-то ломке, который нам будет этот код генерировать для тех или иных задач, какое-то задание, она нам на основании этого задания генерит что-то. Это может быть какой-то evolution instruction, когда мы хотим дать какую-то проблему,
[Chunk start 850.00s]: 
[Chunk start 875.00s]: 
[Chunk start 900.00s]: Либо это может быть OS-инструкция, когда у нас есть какой-то сниппет кода, и на базе такого сниппета нам может сгенерироваться целое множество проблем, которые до этого не встречались особо сильно, извините, пожалуйста. Да, сейчас.
[Chunk start 925.00s]: 
[Chunk start 950.00s]: История, связанная с дедупликацией данных, которые нам позволяют активно как-то отсеять нежелательные какие-то вещи в притрении, ровно потому что мы умеем хорошо как раз-таки некоторые вещи заосерсить, некоторые вещи мы семантически можем выискивать в наших данных, чтобы понять, насколько код этот был похож, потому что либо по выходам каких-то кусков кода мы можем проводить,
[Chunk start 975.00s]: Либо по выходам каких-то кусков кода мы можем проводить как раз-таки similarity search, для того чтобы эффективно дублицировать данные. И самое важное, на самом деле, в плане обучения кода является контроль лицензий. Если мы видим какую-то копирует лицензию, мы, к сожалению, такое обычно должны фильтровать и не использовать в нашем обучении, потому что чревато какими-то последствиями. С токенизацией тут тоже ничего нового нет. Определенную токенизацию
[Chunk start 1000.00s]: С токенизацией тут тоже ничего нового нет. Определенную токенизацию проводят по коду в рамках обучения модели. Здесь ровно такие же проблемы, как у обычных LLAM. Тут ничего в целом нового нет. Каким-то образом дезаблюцируют код, используют ровно те же техники, которые для естественного языка. В целом все понятно. Что можно поменять в обучении LLAM? На самом деле можно не менять ровным счетом ничего. Просто взяв большой пайл...
[Chunk start 1025.00s]: 
[Chunk start 1050.00s]: И все эти методы, они достаточно стандартные в целом для обучения простых LLM, просто новых задач на естественном языке. И при этом они достаточно эффективны для кода, как для модальности. И мы тут как раз-таки сейчас каких-то определенных проблем не встречаем. Хотя хотели бы именно, чтобы мы могли использовать их в нашем обучении.
[Chunk start 1075.00s]: И мы тут как раз-таки сейчас каких-то определенных проблем не встречаем, хотя хотели бы иметь какие-то определенные структуры, архитектуры моделей, которые могут эффективно на нас работать с кодом, об этом еще поговорим.
[Chunk start 1100.00s]: Собственно, для модели так или иначе, из-за того, что мы можем как раз-таки проводить этап эволюции кода, который сгенерирует наша моделька, мы можем давать фидбэк самой модели. Это такой у нас получается бесплатный RL, где модельку как раз-таки можно достаточно эффективно обучить, чтобы она лишилась каких-то своих ошибок. Причем можно давать...
[Chunk start 1125.00s]: 
[Chunk start 1150.00s]: 
[Chunk start 1175.00s]: 
[Chunk start 1200.00s]: Можно потерять какие-то или слить, по крайней мере, данные. В кусках кода нередко, как минимум, находятся какие-то сниппеты или примеры, которые могут содержаться в комментариях тех или иных персональных данных. Зачастую это тоже может являться большой проблемой. Мы должны уметь фильтровать такие куски кода. Плюс весь код, который...
[Chunk start 1225.00s]: 
[Chunk start 1250.00s]: Либо изначально мы должны иметь требования к тем кускам кода, которые мы должны будет рисовывать в притрейн. Да, множество всего, короче, может быть в целом связано с рисками, начиная от каких-то плохих, которые могут привести к нашим каким-то галлюцинациям или нежелательным потерям, заканчивая какой-то легальной историей. Что касается бичмарков, из-за того, что мы эффективно умеем генерировать в целом куски кода, у нас тут проблем вообще нет.
[Chunk start 1275.00s]: Из-за того, что мы эффективно умеем генерировать в целом куски кода, у нас тут проблем вообще никаких нет. Бенчмаркам завались. Единственное, что на текущий момент, наверное, плохо, то, что нет сложных бенчмарков, потому что на текущий момент не то чтобы все решения, несмотря на то, что у них кода достаточно много, не умеют в сложные задачи, связанные с кодингом. Об этом тоже обсудим. И, наверное, из всех...
[Chunk start 1300.00s]: Об этом тоже обсудим. И, наверное, из всех бичмарков, даже которые сейчас здесь перечислены, и которые развиваются даже на текущий момент, там самым оптимальным, самым лучшим решением является просто посмотреть, как на биткомп-бенче у нас, какая у нас там метрика, посмотреть, может быть, какие-то более специфичные истории, типа, если мы хотим помочь когда-то сиентиста обучить, то на DS1000, Если мы хотим, чтобы у нас там много языков знало, то...
[Chunk start 1325.00s]: Если мы хотим, чтобы у нас там много языков знало, то что-то еще проверить, типа Multilingual Humanival. Ну да, сконцентрироваться на каких-то локальных проверках и какой-то общий бенчмарк вообще в целом среди всех моделей взять за основу там BitCodeBench. Руки у нас поросли, на самом деле, из всех бенчмарков, начиная как раз таки с Human Evala, который вышел достаточно давно, где-то четыре года назад, по-моему, его сделали.
[Chunk start 1350.00s]: 
[Chunk start 1375.00s]: 
[Chunk start 1400.00s]: В зависимости от того, за какое количество попыток нас устроит, что наша какая-то лампка пройдет тот или иной тест. Здесь мы хотим как раз-таки учитывать вариативность нашей лампки. Желательно, конечно, смотреть на метрику Passed-1. То есть нас интересует первая генерация, и она должна быть суперидеальной. Но иногда смотрят на это и не понимают, что это за шутка.
[Chunk start 1425.00s]: Первая генерация, и она должна быть суперидеальной, но иногда смотрят и пассет 10 обычно, пассет 5. То есть мы в целом допускаем, что часть генерации может быть плохая, но нас интересует хотя бы, чтобы один раз из пяти было хорошо. Понятное дело, что бичмаркам сейчас особо сильно не следят. Не так давно сам HumanEval под названием кодовал.
[Chunk start 1450.00s]: Не так давно сам Human Eval под названием Code Eval был адаптирован для русского языка, поэтому сейчас в России у нас за этим следят. Но сейчас бичмарк сам по себе устарел, на нем выбиваются какие-то уже невероятные значения, плюс сам бичмарк особо не обновляется. То есть там как GPT, точнее какая-то там агентура над GPT победила в этом бичмарке, так там сейчас только одни GPT, в целом такой бичмарк особо не смотрят.
[Chunk start 1475.00s]: 
[Chunk start 1500.00s]: 
[Chunk start 1525.00s]: Есть какие-то параметры, которые тоже важно и правильно как-то задать, либо прочитать с другого контекста, к примеру, с наших импортов, что это за параметры вообще могут быть. Нам надо что-то вернуть, какую-то ошибку, возможно, поднять. У нас есть определенные реквайрменты. Мы можем немножко зафишотить нашу всю историю. Ну и, естественно, как-то это все дело проверить с какими-то...
[Chunk start 1550.00s]: Ну и, естественно, как-то это все дело проверить какими-то как раз таки ассертами, которые мы делаем. Причем все примеры, которые в биткоин-бенче так или иначе находятся, они верифицируются трехэтапно. Изначально генерируются вообще все эти примеры с помощью каких-то методов, связанных с генерацией кода. Затем все это дело перепрогоняется как людьми, так и какими-то более мощными моделями.
[Chunk start 1575.00s]: как людьми, так и какими-то более мощными моделями. И затем еще раз дополнительная проверка с помощью каких-то экспертов в области программирования, какие-то кросс-чеки. Ну, так или иначе, да, под людьми все курировано, так что там какие-то суперидеальные примеры. Таких примеров может быть не очень много, 1140 всего, однако они хорошо разбиты по доменам, они задействуют так или иначе большинство библиотек, которые
[Chunk start 1600.00s]: Они задействуют так или иначе большинство библиотек, которые завязаны на деятельность разработчиков и с чем они чаще всего встречаются. Однако, как вы могли заметить, все это дело только на питоне на текущий момент. По-моему, да. Если я не ошибаюсь, это пока что только питон. Другие языки, сейчас даже посмотрим, по-моему, он где-то...
[Chunk start 1625.00s]: Другие языки. Сейчас даже посмотрим, по-моему, где-то тут есть. Да, да, он только на питоне. Это огорчает, потому что зачастую мы хотим от помощника-разработчика, чтобы он нам не только на питоне помогал, однако на текущий момент это действительно самый лучший бичмарк, хотя трехэтапный ступенчатый какой-то анализ тех тасок, он дорогой,
[Chunk start 1650.00s]: 
[Chunk start 1675.00s]: И мы должны на основании как раз-таки этой задачи заняться генерацией кода на основании какого-то кодового контекста и привести какое-то решение. И в дальнейшем его как-то засертить, что-то провести.
[Chunk start 1700.00s]: Ну и, да, современные, собственно, модели, которые там побивают все это, естественно, GPT-Cho, Cloud, DeepSeq. В целом ничего удивительного тут нет.
[Chunk start 1725.00s]: 
[Chunk start 1750.00s]: Девяносто тысяч пиаров было проанализировано так или иначе, иши с ним были все просмотрены. Иши обычно идут с какими-то кусками кода, и мы должны на основании как раз-таки всех данных, которые приведены в том или ином иши, сгенерировать либо код, который сможет помочь решить это ишью, и потом, собственно, сгенерировать тесты, ну, точнее, да, провести тесты под него, потому что в этом бенчмарке они заранее все известны.
[Chunk start 1775.00s]: Потому что в этом бенчмарке они заранее все известны. Да, и мы должны сгенерировать решение. Это конец. На этом бенчмарке тоже не особо большие какие-то результаты. И достаточно затюденные, как будто бы только под код модели, но выбиваются вперед. То есть там не встретишь просто как там, не знаю, в DS1000 GbT.
[Chunk start 1800.00s]: То есть там не встретишь просто, как там, не знаю, в DS1000 GPT-чо, клауд. Нет, тут встретишь конкретно какие-то вещи типа CodeAct, которые как-то запромптировали клоуд. Там SVE-агент, опять же, на клауде. Что-то связанное с GPT, я не знаю название этого приложения, но так или иначе. То есть созданы там людьми специальный промпт, какая-то инструкция, Которая позволяет хорошо с этим печем как-то справляться.
[Chunk start 1825.00s]: Продолжение следует.
[Chunk start 1850.00s]: Именно такой пример использования. Да, на бичмарках по коду все. Мы хотим в целом в будущем от подобных помощников добиться как раз-таки генерации сложного кода. Мультилайн это в целом уже достаточно сложная история. Это не просто закончить предложение, это еще и как-то продолжить его, написать несколько строчек кода. Мы хотим, чтобы такие помощники дали нам возможность.
[Chunk start 1875.00s]: продолжить его, написать несколько строчек кода, но мы хотим, чтобы такие помощники думали куда дальше. Возможно, написали как и целый скрипт, который поможет решить проблему внутри какого-то кода, либо как-то оптимизирует целую библиотеку, либо вообще сгенерирует целое репо под какую-то задачу, которую можно будет из коробки запускать. Пока таких решений, к сожалению, на рынке либо крайне мало, и они неэффективны, В этом году мы должны поддержать и поддержать общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общую общ
[Chunk start 1900.00s]: Таких решений, к сожалению, на рынке либо крайне мало и они неэффективны, либо вообще в целом нет. Мы хотим изобрести, естественно, специальную архитектуру, адаптированную под код, ровно такую же, как мы это видели, к примеру, в модальности по видео, когда мы делали какой-то проектор на токены естественного языка, тот же проектор, к примеру, для кода. В целом, возможно, был бы хорошей идеей. Я таких исследований еще не видел и не находил. А.
[Chunk start 1925.00s]: Я таких исследований еще не видел и не находил. Возможно, мы хотим более умный способ работы с данными, иметь связанных с кодом. Как-то лучше использовать какой-то фильтринг, понимать, какой код у нас может нести действительно большую ценность для обучения модели. Мы хотим изобрести крутые бенчмарки, связанные с
[Chunk start 1950.00s]: Мы хотим изобрести крутые бенчмарки, связанные с код-геном, потому что все текущие бенчмарки, они так или иначе связаны на достаточно простые проблемы для текущих помощников. Помимо колд-бенча, там, SVE-бенча, мало что суперсложного можно найти. Мы хотим поддержку иметь не только там бетон как языка, мы хотим вообще в целом все языки, в том числе и низкоуровневые, которые представляют...
[Chunk start 1975.00s]: Вообще, в целом, все языки, в том числе низкоуровневые, которые представляются в текущем ландшафте, очень редко, очень мало. Но так или иначе, как будто бы лампка умеет ходить в грамматике, поэтому why not? Почему бы не обучить какие-то сложные, тяжелые языки, почему бы не синтезировать данные, которые могут быть для подобного языка.
[Chunk start 2000.00s]: Мы хотим, естественно, иметь поддержку Continuous Learning. Тут супер сильно важно, напоминаю, что такое это, когда мы продолжаем обучаться даже после тренинга какой-то нашей очередной модельке. Мы хотим понимать, какие кодовые фреймворки сейчас актуальны, что нам нужно сейчас обязательно, чтобы наша модель умела, чтобы она могла работать.
[Chunk start 2025.00s]: Что нам нужно сейчас обязательно, чтобы наша модель умела, чтобы она подстраивалась под текущие какие-то обстоятельства. К примеру, выходит новая статья про какую-то оптимизацию, и мы хотим, чтобы если она революционная была, то наш универсальный помощник поддерживал подобный алгоритм, который были бы реализованы в этом новом алгоритме.
[Chunk start 2050.00s]: 
[Chunk start 2075.00s]: И как-то нивелировать это, это тоже будущий вызов, пока естественное решение особо нет. Теперь поговорим про аудио. Олег, а можно вопрос про кодовую модальность? У моделей будет такое же свойство, например, если, ну вот, когда обучают LLM, большая часть данных на английском языке, добавляют немного языков, например, русский, китайский, и модель начинает, ну, с меньшим количеством данных понимает уже другие языки. Такое же свойство есть на данном этапе.
[Chunk start 2100.00s]: 
[Chunk start 2125.00s]: На самом деле, все данные, которые вот тут были рассмотрены на самом первом слайде, они и так или иначе используются в притринах, но при этом, почему у них такой большой размер? Не всегда это код. Во время процесса дедубликации порой мы фильтрируем все комментарии, однако чаще всего, даже при разработке гигакода нашего российского, мы эти комментарии все оставляем.
[Chunk start 2150.00s]: Мы эти комментарии все оставляем. В комментариях у нас находится куча интересной информации, которая связана, во-первых, с другими какими-то языками программирования зачастую, потому что вставляют, к примеру, докстринговый сниппет кода на C++, перепиши это на Python или что-нибудь подобное. Так и в целом комментарии несут очень много технической информации, которая содержит как русский язык, так и китайский язык, так и английский, какие-то такие большие представленности.
[Chunk start 2175.00s]: 
[Chunk start 2200.00s]: Может быть, там 6% от всего, от всех данных для обучения, но все равно у нас ломки достаточно хорошо запоминают подобные данные и могут спокойно потом генерировать данные, там, с Kotlin связанные, даже несмотря на то, что там достаточно мало было примеров. Единственное, что проблема будет это хорошо как-то грамотно протестировать. У нас из всех бенчмарков, которые вообще в целом есть там для кода, у нас только парочка связанных.
[Chunk start 2225.00s]: 
[Chunk start 2250.00s]: Но если мы разработаем какие-то определенные наши тесты, которые нам нужны для проверки эффективности работы на том или ином языке, нам, возможно, этого будет достаточно. Нужно ждать появления каких-то новых бенчмарков, которые нам помогут рассказать, типа, мультилингвулу, бигконд-бенч, к примеру, о качествах подобных моделей, которые будут ориентированы не только на Python, и было бы вообще суперславно, а так в целом все современные помощники,
[Chunk start 2275.00s]: И было бы вообще суперславно, а так в целом все современные помощники, они так или иначе поддерживают практически все языки программирования, как и любая в целом белоламка. Какую ни спроси, все в целом на каких-то даже около мертвых языках умеют говорить. Но, понятное дело, с не самым большим качеством. Да, вот как ты развернуто ответил. Надеюсь, ответил. Да, спасибо.
[Chunk start 2300.00s]: Да, спасибо.
[Chunk start 2325.00s]: И нам на выходе получился просто какой-то текст-инпут. Зачастую мы хотим, вот самую последнюю историю, которая реализована на картинке, это аудио-инпут и текст-промптинг. И на выходе мы получаем, зачастую нам даже текст-аутпут не нужен, мы хотим тоже получить аудио. Но про это рассказывать можно очень долго, там много подходов. По большей части сегодня сосредоточить просто на аудио как модальности. То есть мы добавляем, к примеру, какое-то аудио на вход, Вот мы добавляем текст и получаем темноту.
[Chunk start 2350.00s]: То есть мы добавляем, к примеру, какое-то аудио на вход, мы добавляем текст и получаем текст на выход. Добавление вукодеров несет определенные и какие-то добавочные применения похожих модальностей. К примеру, мы можем генерировать музыку достаточно эффективно, причем неплохо делают современные модели. Но в основном мы хотим просуммаризировать какое-то видео.
[Chunk start 2375.00s]: Мы хотим просуммаризировать какое-то видео, к примеру, на YouTube по аудиотранскрипции. Здесь как раз нам помогают speech плюс текст, то текст истории. Что там нужно нам как-то поменять? Но на самом деле я не стал сильно растягивать историю, связанную с этой модальностью. Ровно почему? Потому что она сильно не отличается от Vision истории от слова совсем. Мы единственное, что подменяем, это...
[Chunk start 2400.00s]: 
[Chunk start 2425.00s]: 
[Chunk start 2450.00s]: Это суперинтересный процесс. Про это можно найти тысячи и тысячи, одну реализацию и статью в интернете, поэтому тут тоже говорить об этом сильно не буду. Мы достаточно эффективно умеем это делать. Единственное, что об этом нужно знать, что когда мы подаем все в фьюзированный эмбеддинг-спейс, у нас могут быть разные значения у аудиотокенов и текст-токенов. Желательно, чтобы они были равно распределены.
[Chunk start 2475.00s]: 
[Chunk start 2500.00s]: На самом деле, мы проверяем любую модель, которая так или иначе затрагивает у нас аудиоинпут, как обычно текстовую модель. Аудиоинпут порой тестируем отдельно, просто за инструктив модель на задачу автоматик спич рекогнишена. То есть у нас есть какое-то голосовое сообщение, которым мы подаем модель, мы промтим ее, чтобы...
[Chunk start 2525.00s]: который мы подаем модель, мы промтим ее, чтобы она написала нам транскрипцию этого звука, модель выдает транскрипцию звука, и мы меряем по всем тем же как раз таки методам, которые современные люди меряют в Automatic Speech Recognition Service. Основной метрикой во всех этих сервисах является Word Error Rate, есть модификации Character Error Rate, есть Sentence Error Rate и так далее.
[Chunk start 2550.00s]: Есть модификации, там, character rate, есть sentence rate и так далее, и так далее, но в основном ver. Ver меряется просто как количество замен, количество вставок и количество удаления тех или иных символов на общее число символов в оригинальном как раз-таки сообщении. То есть у нас есть, к примеру, вот оригинал, у нас слово, да, и транскрипция, у нас слово quick поменялось на brown, а точнее quick вообще убралось, осталось только вот это.
[Chunk start 2575.00s]: 
[Chunk start 2600.00s]: Порядка 3%. То есть в целом очень редкие какие-то ошибки. И зачастую мы никак не хотим учитывать аудиосоставляющую таких моделей, потому что ошибка там маленькая. Это не картиночная история, где ошибки могут быть достаточно большие, и там вообще нет какой-то определенной истины. Для аудио данных в аудиоэнкодерах в любом случае можно использовать
[Chunk start 2625.00s]: Для аудио-данных в аудио-энкодерах в любом случае решил привести, и конкретно для российского рынка, для нероссийского рынка можно найти где угодно и в большем количестве часов записи. Основным височником данных является OpenCT на русском. Там порядка 20 тысяч часов записи продиктованного текста в абсолютно различных доменах. Э-э, это голос, э-э, от Сбера, э-э.
[Chunk start 2650.00s]: Это голос от Сбера. Тоже очень-очень много часов, порядка 18 тысяч. Транскрибации происходят обычно с каких-то радиоэфиров, потому что очень хороший источник данных для нас, там постоянно люди говорят, поэтому давайте запишем большое количество эфиров, заставим людей все это транскрибировать, и потом на этом все деле обучимся.
[Chunk start 2675.00s]: На этом все обучаемся. Паблик спичей, ютуба, аудиокниг, звонков и прочие истории. И есть еще небольшой либриспич на 98 часов записи, но его зачастую используют как тестовую выборку для проверки навыков. Как раз просто посчитать веру. Единственным исключением из правил больше бенчей вы не найдете.
[Chunk start 2700.00s]: Больше бенчей вы не найдете. Наш любимый и знакомый BigBench, только теперь с приставкой аудио. Тоже тысяча аудио каких-то вопросов. Мы хотим посмотреть на какой-то спичи-ризонинг, связать аудио наше сообщение с сообщением на текстовом языке, что-то померить. Но больших отличий от любого BigBench на натуральном языке на самом деле нет. Это тот же BigBench, только вместе с голосовыми сообщениями.
[Chunk start 2725.00s]: Это тот же биг бенч, только вместе с голосовыми сообщениями, поэтому он достаточно сильно там урезанный. Но при этом учитывается несколько задач, как текст-то текст, так спич-то спич, текст-то спич и спич-то текст для моделей, потому что там есть такая вот модификация у этого бенчмарка, так или иначе. Да, закончили на самом деле с модальностями, про аудио больше углубляться не будем, про код рассказали, про вижн тоже.
[Chunk start 2750.00s]: 
[Chunk start 2775.00s]: 
[Chunk start 2800.00s]: В этом случае, мы должны поддерживать все необходимые правила и стабильность.
[Chunk start 2825.00s]: Зачастую сервисы вообще делятся на два типа. Я тут презентацию не представил, поэтому голос там расскажу. И у каждого есть какая-то своя метрика, которая является основной. Так, к примеру, если мы делаем сервис по типу Нейра, который Яндексовская, Мы на самом деле не хотим добиться от нее...
[Chunk start 2850.00s]: мы не хотим чтобы потому что она очень часто занимается там к примеру кем-то задачами связаны про суммаризируем не это видео в это самый популярный запрос там у нейро берутся какой-то видео там на ютюбе мы хотим чтобы она просуммаризировалась нам не суть важно тут tokens per second на самом деле нам здесь очень будет важно там как раз таки пропускная способность
[Chunk start 2875.00s]: Нам здесь очень будет важна пропускная способность. То есть мы хотим, чтобы при очень большой нагрузке наш сервис все равно стабильно работал, выдавая тот же токен сперсекунд, который он выдает в обычном режиме, чтобы это сильно не страдало. Когда в каких-то онлайн-ассистентах, к примеру, если вы зайдете сейчас в...
[Chunk start 2900.00s]: К примеру, если вы зайдете сейчас в Telegram, напишите гигачату что-то. Нам тут в целом будет важно, конечно, токен с Persecon super важно, но нам будет важен и Time-to-First токен. Это самый первый шаг для любой лоломки, потому что нужно заниматься контекст-декодингом, нужно простоять какое-то время в очереди на запрос, потому что там пропускная способность может не позволить.
[Chunk start 2925.00s]: В общем, это очень важный момент, который мы должны рассматривать.
[Chunk start 2950.00s]: Не учитывается порой первый токен, его отбрасывают, потому что это как отдельная метрика у нас. Хотя можно и не отбрасывать в целом, тогда будут там чуть другие, но очень схожие все равно значения. Современные модельки как-то распределены на этом графике по как раз таки двум этим метрикам. Самый идеальный квадрант у нас очень маленькое время на Time to First Token.
[Chunk start 2975.00s]: У нас очень маленькое время на Time-to-First токен и очень большое количество токенов per секунд. Понятное дело, такого достигают обычно какие-то маленькие модельки, либо модельки с приставкой Flash. Ну, а какие-то очень мощные модели, у них достаточно мощный какой-то декодинг происходит, типа DPSC, у них очень большой обычно time-to-first токен, но при этом может...
[Chunk start 3000.00s]: У них очень большой обычно time-to-first токен, но при этом может сильно отличаться в зависимости от задач, которые ставятся перед моделькой по tokens-per-sequence. В tokens-per-sequence очень важно в целом учитывать то, с чем мы работаем. Так у нас может быть история связана с тем, что нам важно максимизировать ТПС по питону, по математике.
[Chunk start 3025.00s]: В этом случае, мы можем заметить, что токены по разным доменам сильно разные, поэтому нужно обращать на это внимание. Иногда производится замер по ТПСу сильно, как сказать, смещенный, ввиду того, что, не знаю, мы там меряем ТПС GPT-ЧО и меряем...
[Chunk start 3050.00s]: Мы там меряем TPS GPT-CHA и меряем TPS какого-то помощника, как раз разработчика. И мы знаем, что там фертильность токенизации, то есть в среднем размер токена какой-то, он на коде сильно выше, чем на русском языке. Так мы можем наблюдать здесь, что у нас там здесь видно. Средняя длина токена в GPT-4, она там четыре с чем-то на русском языке.
[Chunk start 3075.00s]: Средняя длина токена в GPT-4, она там четыре с чем-то на русском языке, это два и там два. То есть сгенерировать такой токен, сгенерировать другой на самом деле требует разных скоростей. И о чем очень хочется сильно поговорить, что вообще так или иначе повлияет на риски использования LLAM, это промерч методы оптимизации, которые в сервисах используют. Вдруг мы очень быстро...
[Chunk start 3100.00s]: которые в сервисах используют. Вдруг мы очень быстро на самом деле начинаем понимать, что все наши модели, которые мы разработали, безумно долгие и требуют очень много памяти, излишней памяти. Мы это все можем сильно оптимизировать, при этом никак не потеряв качество. Зачастую оптимизация добавляет... Самый легкий способ это просто добавить новых железяк. Понятное дело, можно купить там самые современные видеокарточки и заложить их в сервис.
[Chunk start 3125.00s]: Понятное дело, можно купить там самые современные видеокарточки и забыть о нашей текущей проблеме, но на самом деле мы можем куда лучше. Мы иногда можем заменить какие-то архитектурные способности модели, там, маешки в целом, микшеров экспорта, они быстрее работают, чем обычные, там, ДЭНС, да, нейросети. Но зачастую объединяют методы, связанные с какими-то архитектурными решениями внутри модели.
[Chunk start 3150.00s]: 
[Chunk start 3175.00s]: Видео-памятью. Так что у нас в целом все начнет считаться в несколько раз быстрее. При этом это никак не потеряет в качестве, потому что мы сохраняем ровно ту же логику работы. Так, к примеру, самый эффективный способ вообще по оптимизации любого LLM-сервиса, мы его подробно сегодня разберем, таковый кэш. Continuous batching. Идея в нем суперпроста. Она на картинке, мне кажется, даже сильно объяснять как-то это не надо. Мы хотим улучшить развитие экономики.
[Chunk start 3200.00s]: И мне кажется, даже сильно объяснять как-то это не надо. Мы хотим, когда у нас вот такой вот бач, при этом он там заканчивается достаточно рано, мы хотим в оставшуюся часть бача, потому что она у нас просто западенная, вставить какой-то другой бач, который был там сильно меньше всего нашего сэмпла. И мы таких бачей, скорее всего, найдем. Так вот, к примеру, в S1, вот эту всю историю, вместился бач S6, который состоял всего из двух токенов условно.
[Chunk start 3225.00s]: Вместился бач S6, который состоял всего из двух токенов условно. Здесь вместился S5, который тоже из двух токенов состоял, при этом из трех. Это из двух, наверное, а тут просто падинг. И мы таким образом сильно скомпонуем наше пространство бачей, которые мы подаем нашу ломку, и сильно быстрее посчитаем. При этом мы будем очень хорошо знать, где у нас что заканчивается, потому что мы специальные токены будем использовать, которые end-of-sentence являются. А это флешмобильный флешмобиль.
[Chunk start 3250.00s]: Мы специальные токены будем использовать, которые end-of-sentence являются. Это Flash Attention, их целое семейство, тоже сегодня подробно о них поговорим. Это Page Detention, он ничего общего с словом Attention вообще не имеет, но подход очень интересный. Я отдельно статью тут оставил на эту всю историю, но мы смотреть на него не будем. Там квантизованные лоры, интересные методы квантизации ламок на Int8.
[Chunk start 3275.00s]: 
[Chunk start 3300.00s]: Но зачастую это используется, когда несколько слоев у нас просто по мощности обменяются условно в один, и быстрее считаться начинает. И есть целые фреймворки для оптимизации. Это там Petals и Swarm. Но давайте начнем с Flash Attention. Здесь не сильно устали, но не так много осталось. Нет, это тот бач, который мы подаем на вход Alalam. Поэтому мы тут просто определенным образом...
[Chunk start 3325.00s]: Поэтому мы тут просто определенным образом... Синие это паддинги, насколько я понимаю, либо какие-то специальные токены. Красные это end of sentence, желтые это собственно те данные, которые мы хотим подать. Просто это как пример компоновки бача, когда мы вместо... Если у нас, к примеру, весь наш бач заканчивается, то мы можем подавать все данные, которые мы хотим подать.
[Chunk start 3350.00s]: Весь наш батч заканчивается, к примеру, на S6, то есть шестое предложение. Мы укомпоновали, на самом деле, всего в четыре. Хотя их было изначально шесть, просто было очень много паддингов, пробелов, которые нам особо не нужны. И мы это все дело скомпоновали. Continuous Patching здесь хорошо работает. Начнем с Flash Attention. Это такая супер база для всех LLM.
[Chunk start 3375.00s]: Это такая супер-база для всех LLM-ок, сервисов и так далее, которые используются. Она по своим результатам в целом сильно меняет тренинг тайм, она меняет инференс тайм в том числе. Какие-то модельки, обученные Flash Attention, обычно показывают рост производительности в 3,5 раза. С чем это вдруг связано?
[Chunk start 3400.00s]: 
[Chunk start 3425.00s]: На самом деле, почет softmax и матричные приложения, которые первые в наших коверах, очень затратные. Обычно, как оно делается во всех фреймворках, они пихают так называемую «Hardband with Memory» внутри GPU. Нашу GPU можно представить как какую-то consistent память, как будто бы жесткий динамик.
[Chunk start 3450.00s]: какую-то consistent память, наш какой-то как будто бы жесткий диск внутри GPU, и есть какая-то оперативка внутри GPU. Несмотря на то, что полтора терабайта в секунду, кажется, для GPU-хи это все равно, это вау, какие скорости, но на самом деле на огромных там пайлах данных и так далее это сильно замедляет процесс там и обучения и всего остального, когда у нас при этом есть очень маленькая, потому что там всего 20 гигабайт обычно, Но при этом очень важно, чтобы мы не были в состоянии,
[Chunk start 3475.00s]: У нас при этом есть очень маленькая, потому что там всего 20 гигабайт обычно, но при этом очень пропускная по своей способности виртуальная память внутри нашей GPU. И Flash Attention, они очень эффективно научились работать с этим небольшим кусочком, как раз таки связанным с видеооперативной памятью. Они, вся основная суть, почему вдруг этот кусочек стал использоваться,
[Chunk start 3500.00s]: Вся основная суть, почему вдруг этот кусочек стал использоваться, ведь на него вроде не положишь целую матрицу, это в рамках самой первой реализации Flash Attention, в рамках перемножения матриц, это использование тайлинга, когда мы вместо подсчетов, когда мы считаем Q на K, мы обычно матрицу перемножаем по х.
[Chunk start 3525.00s]: Когда мы считаем кью на к, мы обычно матрицу перемножаем, как мы знаем, как мы их перемножаем. Там используется просто более эффективный метод, который резко сокращает количество операций, но это окей. А в рамках подсчета как раз таки Softmax используется так называемый онлайн Softmax, который считается у нас рекуррентным.
[Chunk start 3550.00s]: 
[Chunk start 3575.00s]: Во-первых, не просто быстро считать, так еще и производить вычисления на одном ядре GPU при всем при этом. Прошлые все наши вычисления, они слабо параллелизовались, а сейчас это параллелизуется просто прекрасно. Так что вот на одном ядре GPU это все дело считается. И ввиду как раз-таки этого мы заметили там сильный прирост Flash Attention 2, Flash Attention 3 и по...
[Chunk start 3600.00s]: Flash Attention 2, Flash Attention 3 и прочие какие-то модернизации подобных флешей, они так или иначе продолжают идею авторов изначального Flash Attention. Но Flash Attention сейчас в той или иной реализации нет ни одной, наверное, лампки, которая бы не использовала его. Не знаю, правда. Поэтому он сейчас везде. И при этом все мы не теряем качество от слова совсем никак.
[Chunk start 3625.00s]: И при этом все мы не теряем качество от слова совсем никак, потому что мы получаем ровно тот же результат. То есть это действительно очень хорошая оптимизация работы алгоритма, причем не по какой-то computational cost, то есть мы в целом имеем ту же сложность алгоритма, которая была у нас до этого. Мы очень сильно имеем ниже требования по памяти, которые требуются нам для подсчета как раз-таки attention score.
[Chunk start 3650.00s]: Ну и теперь KVCache, стараюсь быстренько по нему пройти. Идея очень простая. Как у нас работает LLM? У нас есть изначальный какой-то запрос, это, к примеру, 2 плюс 2. Оно отсылается к LLM, LLM генерит будет, теперь 2 плюс 2 будет равно 2 плюс 2 равно 4. Ввиду того, что LLM у нас обычно это декод.
[Chunk start 3675.00s]: 
[Chunk start 3700.00s]: Точнее, даже не так. Мы вот эти все куски, которые у нас были там в качестве запроса, в качестве ответа, мы можем очень эффективно где-то хранить. Уже не дурно как идея. А теперь развеем эту идею совсем до крайностей. А почему бы нам просто вот эти куски кода не хранить в каком-то кэше, который у нас будет постоянно обновляться, а именно этот кэш, который у нас всегда заложен как значение,
[Chunk start 3725.00s]: 
[Chunk start 3750.00s]: 
[Chunk start 3775.00s]: 
[Chunk start 3800.00s]: Что вдруг научились его очень хорошо квантизовывать и оптимизировать. У нас была проблема, что изначально веса модели при маленьких контекстах, то есть при маленьком КВ-кэше, они у нас занимали большую часть памяти, понятное дело. Но этот КВ-кэш, он при больших контекстах сильно растет, так что у нас не хватает там памяти на эффективное хранение весов. И поэтому научились кавыкаш-квантификации.
[Chunk start 3825.00s]: Поэтому научились квантизовать KVCache. С помощью квантизации KVCache можно сохранить те же результаты по модели, которая используется, но при этом сильно сократив потребление памяти на хранение KVCache. И текущий как раз-таки реализации квантизации КВ-кваша, там КВ-квант просто и называется,
[Chunk start 3850.00s]: реализации квантизации KV-ковша, KV-квант просто и называется, они позволяют достичь как раз-таки ломков на продакшене контекстного окна в 10 миллионов токенов. Ровно потому, что вот этот оптимизированный KV-кэш, он хорошо поместится на любую железяку. И последнее, что мы рассмотрим. На каждом шаге мы просто конкатим в конец добавляем
[Chunk start 3875.00s]: Да, все просто, очень просто. Все так. То есть тут нет какого-то суперношества или гениальной идеи, просто обратили внимание, что у нас в целом какие-то куски текста, они постоянно повторяются, их решили отдельно в каком-то каше хранить, ключ значения по ним и все. Тут ничего супер особенного нету, вот этот кавыкеш, он просто отвечает за все, что мы делаем.
[Chunk start 3900.00s]: Тут ничего супер-особенного нету. Вот этот KVCash, он просто отвечает за какой-то контекст, который на каждом шаге модель сама себе дает. Вот она может к нему очень эффективно быстро обратиться. Там с помощью квантизации он еще и весить мало начинает. Да, и последняя история тоже, она просто более умная квантизация, нежели чем там просто заквантить весы и найти какой-то фактор квантизации. Если это, там, LLMint8, то это будет очень удобно. В общем, это очень удобно.
[Chunk start 3925.00s]: И найти какой-то фактор квантизации. Это там LLMint8. Там прям так называется. Что делают? Смотрят на значения любой на самом деле матрицы. Находят как и какие-то условные обычные значения внутри матрицы, которые как-то равномерно распределены между самими собой. Но в матрице мы также еще и чаще всего находим какие-то аутлайеры. Чаще всего из-за того, что у нас...
[Chunk start 3950.00s]: В этой матрице мы также еще и чаще всего находим какие-то аутлайеры. Чаще всего из-за того, что у нас достаточно спорсированные порой бывают эмбеддинги, у нас эти аутлайеры действительно хранят какую-то суперключевую информацию для ломки. Поэтому было предложено, давайте мы на самом деле наши аутлайеры, ввиду того, что они несут достаточно большую смысловую нагрузку для наших моделей, не будем никак трогать в рамках квантизации.
[Chunk start 3975.00s]: Не будем никак трогать в рамках квантизации, потому что из-за квантизации подобных параметров сильно потом может пострадать качество. Мы оставим их как есть. И действительно, можно эти вещи оставить как есть, но при этом заняться квантизацией не аутлайеров, а просто каких-то значений, которые так или иначе в нашем алгоритме распределены. По стандартному алгоритму квантизации. И потом все это дело...
[Chunk start 4000.00s]: По стандартному алгоритму квантизации и потом все это дело уметь эффективно объединять. Подробнее про алгоритм, то, как он выбирает аутлайеры, то, как он выбирает регулярные значения, тут решил не касаться. Но идея тоже достаточно простая, интуитивная. И на самом деле очень весомая, потому что зачастую все современные реализации, которые можно скачать с Hugging Face, они поддерживают LMN-8. В этом случае мы должны поддержать и поддержать общую связь между этими инициативами.
[Chunk start 4025.00s]: И это дает еще меньшую просадку по качеству, чем при обычной квантизации полноценной. И при этом все позволяет сильно меньше памяти потреблять модели на инференции, на обучении и так далее. Нам осталось поговорить по поводу фреймворка.
[Chunk start 4050.00s]: Нам осталось поговорить по поводу фреймворков, на которых работают разработчики, которые выводят модели в прод. У нас есть несколько фреймворков, которые точно хотелось бы затронуть, не суперподробно, но просто хотя бы рассказать, которые все опенсорсники так или иначе используют.
[Chunk start 4075.00s]: Это больше такой продакшн, на самом деле, фреймворк. Эти фреймворки все в целом зачем-то нужны. Они объединяют все то, что мы обсудили до этого, то есть какие-то методы оптимизации, какие-то ускорения, инференсы, поддержка стабильности и функциональности нашего сервиса как раз-таки внутри себя имеют хорошую поддержку каком-то
[Chunk start 4100.00s]: Имеют хорошую поддержку на каком-то более низком уровне, то есть по общению с железяками. К примеру, TensorRT, это непосредственно разработано NVIDIA, оно поддерживает самые современные ядра. Если выкатывается драйвер на какой-нибудь H100 GPU, на который вы будете учить свою модель, то, скорее всего, TensorRT обновится сиюсекундно, и у вас будет самая современная поддержка без багов и так далее.В отличие от всех других фреймворков, мы можем использовать и другие фреймворки, которые мы используем. В этом случае мы можем использовать фреймворк, который мы используем для того, чтобы выделить фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который будет доступен в нашем обществе. В этом случае мы можем использовать фреймворк, который
[Chunk start 4125.00s]: И у вас будет самая современная поддержка без багов и так далее, в отличие от всех других фреймворков, потому что они зачастую просто реализованы китайцами, у которых хоть и есть какие-то свои GPU, они не так распространены, и все равно все метятся в как раз-таки использовании GPU от NVIDIA.
[Chunk start 4150.00s]: порог входа. Не каждая лаборатория может позволить себе вдруг взять специалиста по этому фреймву и как-то на нем работать, продолжать. Зачастую самым популярным это VLLM используется. У него очень простой сам по себе. У него хорошая скорость, как раз-таки, и инференс модели после определенных операций, которые этот фреймворк делает.
[Chunk start 4175.00s]: там после определенных операций которые этот фреймворк делает авторы вы как раз реализовали по патч тэншин патча тэншин нам очень эффективно позволяет работать с кого кэшом в удивление вот блочно как-то реализуют там какую-то структуру там по хранению этого кого кэша в не особо разбираюсь честно вот очень популярный это можно по звездочкам увидеть его
[Chunk start 4200.00s]: Очень популярный, это можно по звездочкам увидеть, его зачастую используют почти все современные LLM, которые не супер большие. Они там на VLLM так или иначе написаны. Есть еще LM Deploy, первые ребята, которые там запустили LLM1, LLM2, сделали там от автора Continuous Batching, тоже простой, но гениальной идеи. Тоже какой-то фреймворк. То вот они есть.
[Chunk start 4225.00s]: И, заключительно, хотелось бы сказать, а именно сделать какой-то определенный рекап, зачем мы это вообще вдруг все прошли на протяжении всех этих пяти лекций, ведь что нас дальше ждет. Мы в целом поговорили на самой первой лекции, что генеративный искусственный интеллект — это круто, но есть определенные понятные дела о грехе.
[Chunk start 4250.00s]: 
[Chunk start 4275.00s]: Такой самый большой, наверное, организации, которая там так или иначе занимается тем, что подсвечивает какие-то риски мировые именно, к чему все прислушиваются, там самые большие компании и так далее, то есть это очень авторитетный источник. Да, и эти риски, связанные с галлюцинациями, считают сильно, не просто галлюцинациями, но и дезинформацией считают очень опасными. И очень важно нам добросовестно и очень качественно мерить в таком случае аллалай.
[Chunk start 4300.00s]: И очень важно нам добросовестно и очень качественно мерить в таком случае LLM. Мы поговорили на второй лекции про то, что используют вообще в рамках обучения LLM, и какие модификации делают над LLM-ками, которые так или иначе влияют на работу самой LLM, и их нужно учитывать. Вообще в целом, что нужно уметь все правильно измерять, нужно не просто вслепую бросаться на первый попавшийся бенчмарк, но уметь все правильно измерять.
[Chunk start 4325.00s]: Нужно не просто вслепую бросаться на первый попавшийся бичмарк, но уметь как-то его оценить, оценить особенности нашей LLM, что она может, что не может, не использовать ее там, где она не может, обучить ее тоже определенным образом. И да, рассмотрели модальности как следующий шаг развития вообще в целом всех LLM. Некоторые нюансы, связанные с их обучением, о том, что модальность это не просто какой-то блокбокс.
[Chunk start 4350.00s]: 
[Chunk start 4375.00s]: И которые могут нам так или иначе повлиять на картину, что мы можем увидеть, что там HF-модельки, которые мы загружаем там с Hagen-Faith, они могут отличаться от того, что мы можем увидеть на сервисе. И поэтому от этого нам, собственно, правильно надо строить наше тестирование. Все дальнейшие лекции проведет Ваня Подпружников и Степан Пономарев, мои коллеги.
[Chunk start 4400.00s]: 
[Chunk start 4425.00s]: Проговорим про какие-то реальные истории жизни. Степан, как раз-таки по большей части сконцентрированный на диффузионных моделях, расскажет ровно про них. Скорее всего, самый современный доклад будет связан с текстом видео, той вещью, которая развивается меньше года. Посмотрим на методы оценки подобных моделей, какие риски там тоже могут быть, что с этим делать.
[Chunk start 4450.00s]: 
[Chunk start 4475.00s]: А есть какое-то понимание, про что домашка-то будет? Да, есть. Обычно домашка будет состоять из... У вас будет, скорее всего, инференция ллмки. Достаточно быстрой, очень надеюсь на это. И нужно будет эту ллмку уметь прогнать на бенчмарках, которые мы обсуждали как раз-таки в рамках этих лекций.
[Chunk start 4500.00s]: Спасибо.
[Chunk start 4525.00s]: И тогда всем спасибо. Получается, Ваня начнёт с четверга, а по поводу первого домашнего задания сброшу как раз-таки на неделю информацию. Всем хорошего вечера. Спасибо, пока.
[Chunk start 4550.00s]: В этом году мы должны поддержать и поддержать общую связь между нами.