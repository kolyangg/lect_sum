{
  "10400": "",
  "10425": "нас сегодня ждет не то чтобы сильно большая поэтому скорее всего мы проведем ее без перерыва думаю мы Секунду. про то, как их обучают, какие там способы используют. Старый мир, который не может быть разрушен, не может быть разрушен.",
  "10750": "успеем где-то за час максимум все это дело рассказать но при этом заключительная лекция в блоке первом лекции связанных с ломками такими именно больше коры историями там про модальности историями про модальности и про то, как их обучают, какие там способы используют. И сегодня мы, по большей части, как раз поговорим про какие-то оставшиеся модальности, которые не успели затронуть на предыдущей лекции, и заодно немного поговорим о тех методах, которые используют для того, чтобы вообще сервис на базе ламп какой-то свой сделать, почему эти методы вдруг актуальны и зачем их используют, и подвинем некоторые вообще в целом итоги того, зачем их используют и подведем некоторые вообще в целом итоге того что мы там изучили на первом с. блоке и зачем это сидел проходили да на прошлой лекции разобрали по вижу модальность такая достаточно большая интересная сегодня будем рассматривать с вами и код и аудио и начнем с кода вот как модальность она в целом достаточно уникальная потому что ее даже в отделу модальности Код как модальность, она в целом достаточно уникальная, потому что ее даже в отдел модальности в целом не всегда выносят.",
  "12100": "Ну, во-первых, любой разработчик в целом может сделать это. Однако код сам по себе имеет достаточно много особенностей. Во-первых, он написан на привычном для любой лоломки тексте, так или иначе, но при этом, естественно, не представляет собой естественный язык. Для чего вообще нужен код как модальность? Ну, во-первых, любой разработчик в целом, который особенно разрабатывает подобные модальности, Иногда, когда мы говорим о том, что мы не можем сделать он так или иначе хотел иметь какого-то универсального помощника, который поможет достаточно быстро решать какие-то задачи, связанные с кодом, которые возникают в целом практически всегда, если вы разрабатываете или занимаетесь дата-сайенсом или что-то подобное. И таких задач достаточно много, которые возникают у вас там из раза в раз в вашей жизни. у вас там из раза в раз в вашей жизни. Ну, естественно, мы должны поддержать эту работу. Иногда нужно посмотреть, где находится какой-то баг, причем этот баг нужно найти и потом еще и понять, каким образом нам надо сделать так, чтобы этот баг не работал, точнее, исправить этот баг. Нам нужно найти что-то в коде, либо своем, либо чужом. Code-to-code retrieval достаточно часто тоже история, история, которая позволяет нам решать кодовую модальность. Естественно, самая главная вещь, которая позволяет нам решать кодовую модальность. Потом кодовые сервисы обычно, они сочетаются с кодовыми сервисами. для чего кодовая модальность нужна, это генерация кода того или иного. Это может быть полностью как рерайтинг кода, который написали, либо завершение кода, то есть какие-то универсальные помощники, которые позволяют на основании тех данных, которых они обучились, завершить ту или иную вещь. Причем Причем кодовые сервисы обычно сочетают в себе не просто какую-то одну модельку, так они в целом сочетают множество моделей. при этом, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, Сейчас секундочку. Они сочетают множество моделей, начиная от single line модели, это как отдельная модель, где требуется завершить только одну маленькую строчку кода, как отдельная модель, где требуется завершить только одну маленькую строчку кода, и может провести с вами диалог, как раз-таки связанный при этом контекст у нее должен быть связан либо с одной строчкой кода, либо как раз таки с кодом предыдущим каким-то, который был написан. Это может быть мультилайн история, когда мы хотим несколько строчек кода сгенерить, которые нам помогут выполнить ту или иную функцию. Либо это должен быть какой-то ассистент, который понимает код какой-то ассистент который понимает код и может провести с вами диалог как раз таки связаны с тем как вы должны там либо код свой построить либо поговорить в целом о коде либо дать задачку обучения здесь как раз таки всей кодовой модальности безумно сильно повезло потому что года комнаты кодовому ассистенту на там работу сингла и мультилай модели вот что касается данных для всей кодовой модальности безумно сильно повезло потому что кода очень много почти весь в так или для подобных моделей, они редусят с помощью дедубликации кода данных.",
  "16750": "иначе представлены в интернете коде он полезный несмотря на достаточно большое количество дубликатов в коде обычно все бенчмарки ой бен датасеты которые там являются при троеном для редусит с помощью дедубликации кода данных буквально свой размер раза в два но при этом как 900 миллиардов токенов, как в области, как в области. все этих данных сильно больше чем на естественном языке на удивление и самый такой распространенный к примеру там при тройного их дата сета для кода это стак 2 недавно вышедший там порядка вышедший. Там порядка 900 миллиардов токенов. Как вы помните, там в российском интернете нам в целом такое даже сниться там не может на естественном языке. Да даже на англоязычном на самом деле. Не то чтобы там прям имелись такие какие-то датасеты, где столько много токенов было бы представлено. Весит, конечно, эта махина достаточно мощно, но даже на этом, на самом деле, разработчики не осознают, что это не так. Но даже на этом, на самом деле, разработчики не останавливаются, потому что у кода в целом, как у отдельной модальности, есть такое свойство, что мы всегда можем проверить правильность кода, который так или иначе нам встретится как кусок на какую-то истинность.",
  "18950": "Раз мы можем проводить такие проверки, мы можем написать какое-то определенное задание на сервис, связанный с генерацией подобных данных. определенные задания на сервисы, связанные с генерацией подобных данных, и заасертить какую-то историю так, чтобы мы хотели, чтобы все асерты проходили по данным, которые нам сгенерировали тайная или иная модель, и добавить такой синтетический код у нас в обучение. Сейчас тяжело даже назвать толком долю синтетических данных внутри современных моделей, моделей, связанные с модальностью кода. но они эффективно не могут быть внести в работу. связанных с модальностью кода, однако их достаточно много. Однако их достаточно много, потому что любая такая синтетика, она так или иначе достаточно качественная. Причем есть несколько подходов, связанные с тем, как подобные данные можно генерить. Есть модели достаточно эффективные, вот их тут список представленный для генерации кода конкретно. Это как маленькие модели, так и в целом достаточно большие некоторые представлены, Как маленькие модели, так и в целом достаточно большие некоторые представлены, но они эффективно справляются с задачей генерации кода. Причем генерация кода у них может быть трех типов. Это либо какая-то self-instruct история, когда мы даем самой какой-то ломке, который нам будет этот код генерировать для тех или иных задач, какое-то задание, она нам на основании этого задания генерит что-то. Это может быть какой-то evolution instruction, когда мы хотим дать какую-то проблему. это evolution instruction, когда мы хотим дать какую-то проблему, И это открывает нам новые возможности. которая нам не кажется достаточно серьезной или сложной, и мы хотим каким-то образом с помощью какой-то инструкции для ломки ее усложнить. Причем это может быть достаточно интеративный процесс. Мы еще поговорим о усложнении задач, связанных с кодом, но это позволяет нам сильно увеличить сложность данных, коли мы можем контролировать на самом деле сложность данных в обучении подобных модальностей. можем контролировать на самом деле сложность данных в обучении подобных модальностей. Это открывает по когортам сложности. Либо это может быть OS instruction, нам достаточно большой простор в плане построения каких-то эффективных методов обучения нашей ломки, когда у нас есть... которая будет связана с кодом. Можно использовать тот же Siriculum Learning, который дает достаточно большую эффективность и страдает от того, что как раз у нас недостаточно данных, которые разбиты либо это может быть о с instruction когда у нас есть какой-то сниппет кода и на базе такого сниппета изначального решения. на может генерироваться целое множество проблем которые до этого не встречалось особо сильно извините пожалуйста да сейчас вот которые доселе нам не встречалось либо на котором у нас там селе нам не встречалось, либо на которой у нас изначального Данных, которые были предоставлены в Казахстане, были предоставлены в Казахстане. решения не было, но при этом модель хорошо умеет генерировать подобные примеры. В целом, у нас достаточно хорошо развиты алгоритмы, связанные с фильтрацией подобного, точнее, какого-то плохого хода, который мы можем встретить в нашем притрейне, ровно как и история, связанная с дедупликацией как и истории связанные с дедупликацией данных которые нам позволяют активно как-то отсеять дико. нежелательные какие-то вещи в при трене ровно потому что мы умеем хорошо как раз таки некоторые вещи заострить некоторые вещи мы семантически можем выискивать в наших данных чтобы понять насколько код этот был похож потому что либо по выходам какого каких-то кусков кода мы можем проводить что либо по выходам на какого каких-то кусков кода мы можем проводить как раз таки симулярии search какими-то последствиями. С токенизацией тут тоже ничего нового нет. Определенную токенизацию для того чтобы эффективно там где дублицировать данные ну и самое важное на самом деле для его плане обучения кода является контроль лицензий если мы видим какой-то копирают лицензию мы к сожалению такое обычно должны фильтровать и не использовать нашем обучении потому что чреват С токенизацией тут тоже ничего нового нет. Просто взяв большой пайл... Определенную токенизацию проводят по коду в рамках обучения модели. Здесь ровно такие же проблемы, как у обычных LLAM. Тут ничего в целом нового нет. Каким-то образом дезаблюцируют код, используют ровно те же техники, которые для естественного языка. В целом все понятно. Что можно поменять в обучении LLAM?",
  "25550": "На самом деле можно не менять ровным счетом ничего. не менять ровным счетом ничего просто взяв большой пайл там естественного языка для того чтобы у нас И на коде тоже проблема. модель понимал инструкции и взять тоже кучу данных связанных с кодом все это дело вместе как-то обучить подружить и у нас появится неплохой такой помощник но можно понятное дело взять какую-то готовую лампу которую у нас обучена на естественном языке разморозить несколько слоев также обучить естественном языке, разморозить несколько слоев, также обучить на коде, тоже проблем не возникнет, появляющимися в мире. Хотя хотели бы иметь в виду и другие, но не могли. В этом случае, в основном, мы должны поддерживать и поддерживать, и поддерживать, и поддерживать, и поддерживать и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, можно добавить какую-то голову, можно заадаптить под какую-то нашу историю, которая нам интересна. и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и поддерживать, и И все эти методы, они достаточно стандартные в целом для обучения простых LLM, просто новых задач на естественном языке, и при этом они достаточно эффективны для кода, как для модальности, и мы тут как раз-таки сейчас каких-то определенных проблем не встречаем, И мы тут как раз-таки сейчас каких-то определенных проблем не встречаем, хотя хотели бы иметь какие-то определенные структуры, архитектуры моделей, которые могут эффективно на нас работать с кодом, об этом еще поговорим. Но важной вещью также является возможность селф-импрува таких моделей, как отдельный просто процесс обучения на кормежке данных, собственно, для модельки, так или иначе, из-за того, что мы... данных собственно для модельки так или иначе из за того что мы можем как раз таки проводить этап она лишилась каких-то своих ошибок причем можно давать эволюция кода которая сгенерирует наша моделька мы можем давать фидбэк самой модели это такое у нас получается бесплатный рель где модельку как раз таки можно достаточно эффективно обучить чтобы чтобы она лишилась каких-то своих ошибок. потому что Причем можно давать ассерты к написанию либо как вручную, так и какой-то другой кодовой лампе, которая выигрывает сейчас всех на бенчмарках. Таким образом, достаточно быстро сходить в хорошем качестве для моделей. У нас может быть много связанных рисков, рисков связанных с обучением модели потому что зачастую в любых про скрапленных данных даже в связанных с обучением модели, Это нехороший ход, который мог быть даже выложен в рамках просто...",
  "28700": "стеке 2 встречаются какие-то куски кода которые могут быть связаны с лицензией а это нарушение каких-то авторских прав достаточно много всяких маловаров то есть там вирусов либо какой-то ход который мог быть даже выложен в рамках просто ознакомительных внутри там гитхаба в качестве там есть, я точно не знаю, но риск точно имеется. Можно потерять какие-то или слить. учебного материала и так далее которые так или иначе может привести к тому что там помощник там разработчика на его же мощностях может запустить какой-то вирус такие истории наверное Можно потерять какие-то или слить, по крайней мере, данные. В кусках кода нередко, как минимум, такие куски хода, плюс весь код, который так... находятся какие-то сниппеты или примеры, которые могут содержаться в комментариях. Тех или иных персональных данных зачастую тоже может являться большой проблемой. Мы должны уметь фильтровать куски хода плюс весь код который так или иначе пошел обучение не факт что является очень находятся, но это дорого. Либо изначально мы должны иметь требования к тем кускам кода, которые мы должны иметь. эффективным и таким образом у нас появляется проблема связанная с тем что у нас в лампка может научиться на неэффективных куска хода эта история опять же у нас должна фикситься либо какой-то другой более мощные лампы который будет проверять есть код который у нас при тренинге либо изначально мы должны иметь требования к тем кускам кода, которые мы должны будем рисовывать в целом куске кода, у нас тут проблем вообще нет. в притрейн. Да, множество всего, короче, может быть в целом связано с рисками, начиная от каких-то плохих, которые могут привести к нашим каким-то галлюцинациям или нежелательным потерям, заканчивая какой-то легальной историей. Что касается бичмарков, из-за того, что мы эффективно умеем генерировать Из-за того, что мы эффективно умеем генерировать в целом куски кода, у нас тут проблем вообще никаких нет. Бенчмаркам завались. Единственное, что на текущий момент, наверное, плохо, то, что нет сложных бенчмарков, потому что на текущий момент не то чтобы все решения, несмотря на то, что у них кода достаточно много, не умеют в сложной задаче, связанной с кодингом. Об этом тоже обсудим.",
  "31850": "И, наверное, из всех... кодинга в этом тоже обсудим и наверное из всех бичмарков даже которые сейчас здесь перечислены Если мы хотим, чтобы у нас там много языков знало, то, конечно, мы можем. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, и которые развиваются даже на текущий момент там самого оптимальным самым лучшим решением является потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, просто посмотреть как на битком пенча у нас какая у нас там метрика посмотреть может быть какие-то потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, потому что это не так просто. Мы можем сделать это, но мы не можем сделать это, более специфичные истории типа если мы хотим помочь когда циентист обучить 1дс тысячу если обучить тот на ds-1000 если мы хотим чтобы у нас там много языков знала то что-то еще проверить давно, где-то 4 года назад, по-моему, его сделали. типа мультилингах и уменовал ну да сконцентрироваться на каких-то локальных проверках и какой какой-то общий бенчмарк вообще в целом среди всех моделей взять то за основу там биткотт бенч руки у нас поросли на самом деле всех бенчмарков начиная с как раз таки human ивала который вышел достаточно вышел достаточно давно, где-то четыре года назад, по-моему, его сделали как раз-таки ребята из OpenAE. за то, что мы не можем нести в мире. Задача достаточно простая. У нас есть какой-то код, чаще всего это просто функция, причем внутри функции описан док-стрингом примерно, как это должно выглядеть, либо данные куски кода других функций, может быть, даже законченных, может быть даже законченных, либо внутри самой функции уже несколько строк реализована. либо внутри самой функции уже несколько строк реализовано. Задача бенчмарка достаточно простая — продолжить код. В зависимости от того, за какое количество денег По какому-то определенному контексту связаны там либо с этой функцией, либо с несколькими функциями одновременно. И на основании того кода, который, собственно, сгенерится, строится метрика под названием там path, это что-то там. В зависимости от того, за какое количество попыток нас устроит, но иногда смотрят что наша какая-то лампка пройдет тот или иной тест. Здесь мы хотим как раз-таки учитывать вариативность нашей лампки. Желательно, конечно, смотреть на метрику Passed-1. То есть нас интересует первая генерация, и она должна быть суперидеальной. интересует первая генерация и она должна быть супер идеальный но иногда смотрит и пасы т.н. обычно сильно не следят. Не так давно там сам Humanival под названием Codeval. пасы т.п. то есть мы в целом допускаем что часть там генерации может быть плохая но у нас интересует хотя бы чтобы в один раз из пяти было хорошо вот понятное дело что там бичмарком сейчас особо следят не так давно там сам humanовал под названием кодовал был адаптирован для русского языка поэтому пока ни Gbt, ни в целом такой бенчмарк особо не смотрят. сейчас в россии у нас за этим следят но сейчас бичмарк сам по себе устарел на нем выбиваются какие-то уже невероятные значения плюсом бичмарк особо не обновляется то есть там как джипе тишка точнее какая-то там агентура на джипе тишка она победила в этом бичмарке так там сейчас только победила в этом бичмарке, так там сейчас только одни GPT, Метрики на нем присутствуют. в целом такой бичмарк особо не смотрят. В отличие от бигкот-бенча, который является таким наследником Humanvala, они об этом напрямую пишут в своей статье, вот как раз-таки бигкот-бенч является достаточно интересным и достаточно сложным бичмарком, причем есть сразу несколько версий, как и просто фулл бигкот-бенч, так и хард бигкот-бенч.",
  "37025": "full биткотт бенч так и hard биткотт бенч метрики на нем пока что даже у самых классных моделей они параметры не сильно большие это не может не радовать значит есть куда расти и значит задачи действительно сложные суть такого бенча заключается в том что у нас опять же есть какая-то функция нам эту функцию как-то закончить но при этом все наши докстринги они оформлены определенным образом есть какие-то оформлены определенным образом, есть какие-то параметры, которые тоже важно и правильно как-то задать, Ну и естественно как-то это все дело проверить с какими-то... либо прочитать с другого контекста, к примеру, с наших импортов, что это за параметры вообще могут быть. Нам надо что-то вернуть, какую-то ошибку, возможно, поднять. У нас есть определенные, да, реквайрменты, мы можем немножко зафишотить нашу всю историю. Ну и, естественно, как-то это все дело проверить какими-то как раз таки ассертами, которые мы делаем. Затем все это дело перепрогоняется как людьми, так и какими-то более мощными моделями. Причем все примеры, которые в биткот-бенче так или иначе находятся, они верифицируются трехэтапно. Изначально генерируются вообще все эти примеры с помощью каких-то методов, связанных с генерацией кода. как людьми, так и библиотек, которые там какими-то более мощными моделями. И затем еще раз дополнительная проверка с помощью каких-то экспертов в области программирования, какие-то кросс-чеки. Ну, так или иначе, да, под людьми все курировано, так что там какие-то суперидеальные примеры. Таких примеров может быть не очень много, 1140 всего, однако они хорошо разбиты по доменам, они разбиты по доменам, они задействуют так или иначе большинство библиотек, которые завязаны на Он был в пустыне, и он не мог спать. Он не мог спать. задействуют так или иначе большинство Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. измерениях, ну, завязаны на деятельность, там, разработчиков, и с чем они чаще всего встречаются. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Однако, как вы могли заметить, все это дело только на питоне на текущий момент. По-моему, Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. Он не мог спать. да. Да, если я не ошибаюсь, это пока что только питон. Другие языки, сейчас даже посмотрим, Он не мог спать. Он не мог спать. Он не мог спать. что только питон другие языки и даже посмотрим по моему где-то тут есть да да он только на питоне Он не мог спать. Он не мог спать.",
  "40700": "Он не мог спать. Хотя трехэтапный ступенчатый какой-то анализ тех тасок, он дорогой. это огорчает потому что зачастую мы хотим от помощника разработчика чтобы она мне только на питоне помогал однако на текущий момент это действительно сам лучший бенчмарк хотя ступенчатый какой-то анализ тех таз и кону дорогой и только адаптируется на текущий момент под какие-то мы должны на основании как раз-таки этой задачи заняться.",
  "40925": "другие языки ds1000 тоже очень похожий пример связанный с уже непосредственно работой там датсайентистов у нас тоже есть какое-то описание возможно дата-фреймов там в пандасе и задачи и мы И мы должны на основании как раз-таки этой задачи заняться генерацией кода на основании какого-то кодового контекста и привести какое-то решение.",
  "41500": "Ну и, да, современные, собственно, модели, которые там побивают всё это, естественно, GPT-чувствуют. И в дальнейшем его как-то засертить, что-то провести. Останавливаться особо сильно на нем не буду. В нем достаточно представимость данных по всем библиотекам, так или иначе, которые используют дата-сиентисты. Ну и, да, современные, собственно, модели, которые там побивают все это, естественно, GPT-Cho, Cloud, DeepSeq. бенч является достаточно уникальным интересным ввиду того что он решает задачу может ли какие-то В целом ничего удивительного тут нет.",
  "42775": "Также хорошим примером бенчмарка является SVE-бенч. Он тоже зачастую, очень часто используется при скорингах всех моделей, которые так или иначе связаны с кодом. интересным ввиду того что он решает задачу может ли какие-то ломки достаточно хорошо справляться иначе и шесть ним были все просто с закрытием и шли с на гитхабе так у нас есть какие-то примеры и шьюз пропоршн и с самими там разработчиками этого бенчмарка там порядка 90 тысяч пиаров было проанализировано так или 90 тысяч пиаров было проанализировано так или иначе, иши с ним были все просмотрены, потому что в этом бенчмарке они заранее все известны. иши обычно идут с какими-то кусками кода, и мы должны на основании как раз-таки всех данных, которые приведены в том или ином иши, сгенерировать либо код, который сможет помочь решить это ишью, и потом, собственно, сгенерировать тесты, точнее, провести тесты под него, под него потому что на в этом бенчмарке они заранее все известны ну вот да ты ими должны сгенерировать как там не знаю в ds1000 джипить решение это конец вот на этом бенчмарке тоже не особо большие какие-то результаты и достаточно затюнены как будто бы только подход модели но выбиваются вперед то есть там не встретишь просто то есть там не встретишь просто как там не знаю в дэс тысячи джипе течет клауд нет тут встреч хорошо с этим печем как-то справляются.",
  "45100": "",
  "45150": "конкретно какие-то вещи типа у дикта которые как-то запромтировали клад там и свои агент опять же на клауде что-то связанное с чепики я не знаю название этого приложения так или иначе вот то есть созданы там людьми специальный промпт какой-то какая-то инструкция которая позволяет промпт какой-то какая-то инструкция которая позволяет хорошо с этим печем как-то справляться можно профильтровать внутри самого бенчмарка найти там непосредственно просто какие-то модельки но наших там кодовых помощников именно такой пример использовали. хотел показать именно вот это плюс метрики не особо сильно там опять же большие что говорит о сложности такого бенчмарка но это бенчмарк крайне полезен потому что мы хотим видеть дальнейшем от помощников именно такой пример использования. написать несколько строчек кода, но мы хотим, чтобы такие помошники...",
  "46400": "Да, на бичмарках по коду все. Мы хотим в целом в будущем от подобных помощников добиться как раз-таки генерации сложного кода. Мультилайн это в целом уже достаточно сложная история. Это не просто закончить предложение, это еще и как-то продолжить его, как-то продолжить его, написать несколько строчек кода, от людей, которые не могут работать. но мы хотим, чтобы такие помощники думали куда дальше, возможно, написали как и целый скрипт, который поможет решить проблему внутри какого-то кода, либо как-то оптимизирует целую библиотеку, либо вообще сгенерирует целое репо под какую-то задачу, которую можно будет из коробки запускать. Пока таких решений, к сожалению, на рынке либо крайне мало, и они неэффективны, таких решений, к сожалению, на рынке либо крайне малы, и они неэффективны, либо вообще в целом нет. Мы хотим изобрести, естественно, специальную архитектуру, адаптированную под код, ровно такую же, как мы это видели, к примеру, в модальности по видео, когда мы делали какой-то проектор на токены, естественно, нового языка, тот же проектор, к примеру, для кода, в целом, возможно, был бы хорошей идеей, был хорошей идеей, я таких исследований еще не видел и не находил. я таких исследований еще не видел и не находил. Возможно, мы хотим более умный способ работы с данными, В этом году мы хотим изобрести крутые бенчмарки, связанные иметь связанных с кодом, как-то лучше использовать какой-то фильтринг, понимать, какой код у нас может нести действительно большую ценность для обучения модели. модели. представляют. Мы хотим изобрести крутые бенчмарки, связанные с код-геном, потому что все текущие бенчмарки, они так или иначе связаны на достаточно простые проблемы для текущих помощников. Помимо колд-бенча, там, SVE-бенча, мало что суперсложного можно найти. Мы хотим поддержку иметь не только там Python как языка, мы хотим вообще в целом все языки, в том числе и низкоуровневые, хотим вообще в целом все языки, в том числе низкоуровневые, которые представляются в текущем там ландшафте, которые могут быть для подобного иската. очень редко, очень мало, но так или иначе, как будто бы лампка умеет ходить в грамматике, поэтому why not, почему бы не обучить там какие-то сложные, тяжелые языки, почему бы не синтезировать там данные, которые могут быть для подобного языка релевантны. Мы хотим, естественно, иметь поддержку continuous было, чтобы она была в порядке. learning. Тут это супер сильно важно. Напоминаю, что такое это, когда мы продолжаем обучаться, даже после тренинга, какой-то нашей очередной модельке. Мы хотим понимать, какие кодовые фреймворки сейчас актуальны, что нам нужно сейчас обязательно, чтобы наша модель умела, что нам нужно сейчас обязательно чтобы наши модель умела чтобы она подстраивалась под текущие какие-то обстоятельства к примеру там не знаю выходит новая там статья про какую-то там оптимизацию и мы хотим естественно чтобы есть она революционная была то наш там универсальный помощник поддерживал подобный алгоритм которые были бы реализованы в этом новом алгоритм ну да естественно хотим решить проблему Ну да, естественно, хотим решить проблемы, связанные Это тоже будущий вызов, пока, естественно, не будет. Благодарю вас за внимание. с безопасностью. На каком-то уровне такие ломки должны понимать, где у нас находится там опасный код, где у нас находится неопасный код, причем как-то предупреждать об этом пользователя, предупреждать пользователь о возможных каких-то авторских правах, либо об утечке данных, и как-то нивелировать это. И как-то нивелировать это, это тоже будущий вызов, пока естественное решение особо нет. И такое же свойство есть на данном примере.",
  "52050": "Теперь поговорим про аудио. Олег, а можно вопрос про кодовую модальность? У моделей будет такое же свойство, например, если, ну вот, когда обучают LLM, большая часть данных на английском языке, добавляют немного языков, например, русский, китайский. И модель начинает с меньшим количеством данных понимать уже другие языки. с меньшим количеством данных понимает уже другие языки такое же свойство есть на там например весь все данные которые вот тут были рассмотрены код на питоне практически весь да и то есть мы к примеру какую-то питоновскую чисто модель засунем код связанный там си плюс плюс нам возможно выдастся код носит плюс плюс но возможно он будет нерабочий объясняю вообще почему такое явление вдруг может произойти на самом деле На самом деле, все данные, которые вот тут были рассмотрены на самом первом слайде, они и так или иначе используются в притринах, но при этом, почему у них такой большой размер?",
  "53200": "В комментариях у нас находится...",
  "53225": "",
  "53250": "Не всегда это код. Во время процесса дедупликации порой мы фильтрируем все комментарии, однако чаще всего, даже при разработке гигакода нашего российского, мы эти комментарии все оставляем. российского мы эти комментарии все оставляем комментариях у нас находится куча интересной большие представленности. информации которая связана во первых с другими какими-то языками программирования зачастую потому что вставляют примеру докстринговый сниппет там кода на си плюс плюс перепишет на питон что не подобное так и в целом комментарии несут очень много технической информации которые содержат как русский язык то китайский язык как такой английский какие-то такие большие китайский язык, как такой английский, отходно от всех данных для обучения. какие-то такие большие представленности. Поэтому представленность языков, она так или иначе есть, даже, блин, вот жалко ее не привел на стеки. Она достаточно большая, и все в целом про нее шарят, но в основном, конечно, питон, но несмотря на то, что представимость там, к примеру, там какого-нибудь Котлина, может быть, Kotlin, может быть, там 6% от всего, от всех данных для обучения, у нас только парочка связанных там 6% от всего, но все равно у нас ломки достаточно хорошо запоминают подобные данные и могут спокойно потом генерировать данные, там, с Kotlin связанные, даже несмотря на то, что там достаточно мало было примеров.",
  "54900": "",
  "54925": "Единственное, что проблема будет это хорошо как-то грамотно протестировать.",
  "55525": "",
  "55550": "У нас из всех бенчмарков, которые вообще в целом есть там для кода, бенчмарков, которые вообще в целом есть там для кода, у нас только парочка связанных с мультилингл для проведения общей работы. историей. Так у нас есть только мультилингл human-to-vile, но human-to-vile как бенчмарк, он не слишком сложный, поэтому нам тяжело будет сказать, насколько хорошо там наша моделька справляется с этим. Вот, но если мы там разработаем какие-то определенные наши тесты, которые там нам нужны для Но если мы там разработаем какие-то определенные наши тесты, они которые нам нужны для проверки эффективности работы на том или ином языке, нам, возможно, этого будет достаточно. Нужно ждать появления каких-то новых бенчмарков, которые нам помогут рассказать, типа, мультилингву, бигконд-бенч, к примеру, о качествах подобных моделей, которые будут ориентированы не только на Python, было бы вообще суперславно а так в целом все современные там помощники они так или иначе и было бы вообще суперславно, а так, в целом, все современные помощники, поддерживают практически все языки программирования как и любая в целом была ламка какую не спроси все в целом там на каких-то даже около мертвых языках умен говорить но понят делаем с не самым большим качеством того ч как-то развернуто ответил надеюсь ответил да спасибо да супер да давайте говорим про ответил. Да, спасибо.",
  "57500": "импорт, а зачастую мы хотим, чтобы мы могли использовать Да, супер. Да, давайте говорим про модальность аудио. По-прежнему, это не только для нас, но и для всех нас. Честно, это как одновременно достаточно простая тема, так и крольче нора, ввиду того, что зачастую мы от аудиомоделек не хотим добиваться того, что мы просто положили какое-то текстовое описание, наш какой-то аудиоинпут, и нам и нам на выходе получился просто какой-то текст и input зачастую мы хотим вот самую последнюю на выходе получился просто какой-то текст То есть мы добавляем, к примеру, какое-то аудио на вход, мы добавляем текст и получаем темноту. историю которые реализованы на картинке это аудио input и текст prompting и на выходе мы получаем зачастую нам даже текст аутпут не нужен мы хотим тоже получить аудио но про это рассказывать можно очень долго там много подходов по большей части сегодня сосредоточить просто на аудио как модальности модальности, то есть мы добавляем, к примеру, какое-то аудио на вход, мы добавляем текст и получаем там текст на выход. Добавление там вукодеров несет определенные и какие-то добавочные применения похожих модальностей. К примеру, мы можем генерировать музыку достаточно эффективно, причем неплохо делают это современные модели, но в основном мы хотим хотим просуммаризировать какое-то видео к примеру на ютюбе по аудио транскрипции здесь как раз нам просуммаризировать какое-то видео, сильно не отличается от вижен истории от слова совсем мы единственное что подменяем это помогают там спич плюс текст это текст истории вот что там нужно нам как-то поменять но на самом деле я не стал сильно растягивать историю связан с этой модельностью ровно почему потому что она от вижен истории от слова совсем мы единственное что подменяем это какой-то visual энкодер на у текстовых токенов.",
  "59725": "Это связано там со многими проблемами, которые аудио энкодер и в целом все готово у нас есть одна небольшая проблема она знакома тем кто занимался там как раз таки аудио моделями у нас у токенов аудио токенов сильно выше значение беден гав чем беден гав чем у текстовых токенов это связано там со многими причинами так или иначе и для обновить. того чтобы нам когда мы делаем нашу там прожектор который нам будет это все вот одно как раз это фаза ванная там пространство пихать нам необходимо и токи нормализовать и в целом но нормализация это там не то чтобы какой-то супер интересный процесс про это на и можно найти тысячи тысяч суперинтересный процесс, про это можно найти тысячи и тысячи, одну реализацию и статью в чтобы они были равно распределены. интернете, поэтому тут тоже говорить об этом сильно не буду. Мы достаточно эффективно умеем это делать, единственное, что об этом нужно знать, что там как раз-таки в нашем, когда мы подаем все в фьюзированный там embedding space, у нас могут быть разные значения там у аудиотокенов и текст-токенов, а желательно, значения там у аудио токенов и текст токенов желательно чтобы они были равно распределены Мы проверяем любую модель, которая так или иначе сотрудничает. потому что это все-таки должно слиться в какую-то одну интер лифт информацию когда мы должны учитывать и то и другое одновременно и правильно так чтобы у нас там не было переобучение какой-то из вот этих двух историй в плане бенчмаркирования люди сделали все очень просто на самом деле мы очень просто на самом деле мы проверяем любую модель который так или иначе затрагивает у нас Который мы подаем модель, мы промтим ее, чтобы она аудио input как обычно текстовую модель аудио input порой тестируем отдельно просто за инструктив модель на задачу автоматик спички к внешне то есть у нас есть какая-то голосовое там сообщение сообщение, которым мы подаем модель, мы промтим ее, чтобы она написала нам транскрипцию этого звука, модель выдает транскрипцию звука, и мы меряем по всем тем же как раз таки методам, которые современные является там борд р рейд есть модификации там чартер р рейд есть сентенсер рейд и так далее так люди там меряют, automatic speech recognition сервиса. Основной метрикой во всех этих сервисах модификации, там, character rate, есть sentence rate, и так далее, и так далее, но в основном вер, вер мерится Квик вообще убралось, осталось только выключить.",
  "62400": "",
  "62625": "",
  "62675": "просто как количество замен, количество вставок и количество удаления тех или иных символов на общее число символов в оригинальном как раз таки сообщении, то есть у нас есть, к примеру, вот оригинал, у нас слово, и дата, и транскрипция, у нас слово quick поменялось на brown, а точнее quick поменялось на браун, а точнее quick вообще убралось, осталось только браун, и lazy dog там Он. добавилось. Пример какое-то слово могло там неправильно как-то транскрибироваться, да, и мы посчитаем количество вот этих вставок там удалений и замен, разделим на общее число слов в оригинальном предложении, получим значение вверх. В современных моделях оно очень хорошее, порядка трех процентов порядка трех процентов то есть в целом очень редкие какие-то ошибки и зачастую мы никак не Для аудио данных в аудиоэнкодерах в любом случае можно использовать хотим учитывать аудио составляющего таких моделей потому что ошибка это маленькая это не картиночная история где ошибки могут быть достаточно большие и там вообще нет какой-то определенной истины вот Для аудио-данных в аудио-энкодерах в любом случае решили привести, в обменах, это голос от Сбера.",
  "65650": "и конкретно для российского рынка, для нероссийского рынка, можно найти где угодно и в большем количестве часов записи. Основным височником данных является OpenCT на русском. Там порядка 20 тысяч часов записи продиктованного текста в абсолютно различных доменах. доменах это голос от сбера тоже очень очень много часов порядка 18 тысяч транскрибации все это транскрибировать и потом на этом сидели обучимся вам public спичей youtube происходит обычно с каких-то радио эфиров потому что очень хороший источник данных для нас там постоянно люди говорят поэтому давайте запишем большое количество там эфиров заставим людей там и потом на этом все деле обучимся. знакомый Биг Бенч, только не был в состоянии. Там паблик спичей, ютуба, аудиокниг, звонков и прочие какие-то истории. И есть еще небольшой LibriSpeech на 98 часов записи, но его зачастую используют как какую-то тестовую выборку для проверки навыков. Как раз просто посчитать веру.",
  "67425": "Единственным исключением, наверное, из правил, больше больше бенча не найдете наш любимый и знакомый big bench только теперь с приставкой аудио тоже бенчей вы не найдете, наш любимый только вместе с голосовыми сообщениями. тысячи там аудио каких-то вопросов мы хотим посмотреть на как раз таки какой-то спичи лиза нинг связать аудио наше сообщение с сообщением на текстовом языке что-то померить но больших отличий там от любого big bench натуральном языке на самом деле нет да тот же big bench только деле нет на тот же big bench только вместе с голосовыми сообщениями поэтому он достаточно что углубляться не будем по прокат рассказали про vision тоже сильно там резаный но при этом учитывается несколько задач как тексту текст так спичку спич тексту спички спичку текст для моделей потому что там есть такая вот модификацию там у этого бенчмарка так или иначе до закончили на самом деле с модальностями проуди больше больше углубляться не будем.",
  "68675": "Про код рассказали, про Vision тоже. Да, теперь кратко давайте расскажем, как вообще в целом занимаются тем, что собирают сервис на базе LLM. Моделька у нас готовая есть. Теперь нам необходимо ее обернуть в какой-то сервис, который действительно на нашем железе достаточно бы эффективно работал. Причем мы предъявляем сразу несколько сразу несколько требований к нашему сервису. требований к нашему сервису. с своими запросами. Естественно, он должен быть быстрый. Естественно, он должен быть быстрый. Медленные сервисы нас не особо сильно интересуют. Дожидаясь ответа «год», любое физическое лицо, которое пользуется твоим сервисом, просто уйдет. У этого сервиса должна быть достаточно большая пропускная способность. То есть мы хотим, чтобы наша скорость не сильно страдала при огромном потоке пользователей огромном потоке пользователей с своими запросами в наш сервис. что это тут признается. Ну, естественно, сервис должен функционировать, функционировать неправильно, без нарушения функциональности, и он должен быть стабильный во времени, чтобы никогда не падал и так далее. Но наши все основные требования как раз таки связаны со скоростью и с рутпутом. Объясняю почему. Зачастую сервисы вообще делятся на два типа. Зачастую сервисы вообще делятся на два типа. Я тут презентацию не представил, поэтому голос там расскажу. Мы, на самом деле, не хотим добиться от нее... И у каждого есть какая-то своя метрика, которая является основной. Так, к примеру, если мы делаем сервис по типу Нейра, который Яндексовская, как мы не хотим чтобы потому что она очень часто занимается там к примеру кем-то задачами связаны На самом деле, нам здесь очень будет важно, там как раз-таки пропускная способность. просумеризируем не это видео в это самый популярный запрос там у нейро берутся какое-то видео там на ютюбе мы хотим чтобы она просумеризировалась нам не суть важно тут tokens per second на самом деле нам здесь очень будет важно там как раз таки пропускная способность то есть мы хотим чтобы ассистента, к примеру, если вы там зайдете там сейчас в при очень большой нагрузки у нас наш сервис все равно стабильно работал выдавая тот же токен сперсик он которым выдает в обычном режиме чтобы сильно не страдал когда в каких-то онлайн к примеру если вы там зайдете там сейчас в telegram да но она напишите там гига чату что-то нам тут до какое-то время в очереди на запрос потому что там пропускная способность можно позволить целом будет важно конечно так из персикон супер важно но нам будет важно и тайм ту ферст окин это самый первый шаг для любой ломки потому что нужно заниматься контекст декодингом нужно просто этом очереди на запрос потому что там пропускная способность можно позволить да вы сильно сервиса не учитывается порой первый токен а его отбрасывают потому что это как отдельный метод от таки не зация будет все это дело зависеть так из декодинг нам позволяет как раз таки ломки сначала обработать ваш запрос сгенерировать на него ответ а затем уже как раз таки итеративно идти это будет сильно быстрее поэтому зачастую у нас таким сперсик он как в метрике качества там работы не учитывается порой первый токен его отбрасывают потому что это как отдельная метрика у нас хотя идеальный квадрант у нас очень маленькое время на time to first token. можно не отбрасывать в целом тогда будут там чуть другие но очень схожи все равно значения современной модельки как-то распределены на этом графике по как раз таки двум этим метриком самый нас очень маленькое время на тайм ту ферз токен и очень большое количество токен спер секанс понятное происходит типа дирси дипсика у них очень большой обычно time to first окин но при этом можете дело такого достигают обычно какие-то маленькие модельки либо моделька стамп с приставку flash ну а какие-то очень мощные модели одних достаточно там мощный какой-то декоринг У них очень большой обычно time-to-first токен, но при этом может сильно отличаться в зависимости от задач, которые ставятся перед моделькой по tokens-per-sequence. В tokens-per-sequence очень важно в целом учитывать то, с чем мы работаем.",
  "75300": "",
  "75400": "",
  "75425": "Так у нас может быть история связана с тем, что нам важно максимизировать TPS по питону, по математике. с тем, что нам важно максимизировать TPS по питону, по математике, по русскому языку или по английскому. Ввиду того, что мы меряем TPS GPT-C и меряем... И мы можем очень быстро заметить, что токены на самом деле по разным доменам, они сильно разные. Поэтому нужно обращать на это внимание. Иногда производится замер по TPS сильно, как сказать, смещенный, мы там меряем там тпс джипе течет и меряем тпс там не знаю код помощника как раз разработчика и мы цена токена в GPT-4 она там четыре с чем-то на русском языке? знаем что там фертильность и кинезации то есть в среднем размер токина какой-то он на коде сильно выше чем на русском языке так мы можем наблюдать здесь что у нас там здесь видно средний там длина Средняя длина токена в GPT-4, она там 4 с чем-то, на русском языке это 2 и там 2.",
  "76825": "",
  "76850": "Вдруг мы очень быстро, как будто бы, сжали все снаряды.",
  "76975": "",
  "77000": "",
  "77050": "",
  "77075": "То есть сгенерировать такой токен, сгенерировать другой, на самом деле требует разных скоростей вдруг.",
  "77100": "",
  "77275": "И о чем очень хочется сильно поговорить, что вообще так или иначе повлияет на риски использования LLAM, это промерч методы оптимизации, которые в сервисах используют. которые в сервис используют вдруг мы очень быстро на самом деле начинает понимать что все наши современные видеокарточки и за модели которые мы разработали безумно долгие и требует очень много памяти излишней памяти мы это все можем сильно оптимизировать при этом никак не потеряв качестве зачастую оптимизации добавляя самый легкий способ это просто добавить новых железяк понят дело может купить там со самые понятное дело может купить там самые современные видеокарточки и забыть о нашей текущей проблеме внутримодными. но на самом деле мы можем куда лучше мы иногда можем заменить какие-то архитектурные способности модели там маешки в целом микшеров экспорта они быстрее работают чем обычные там dance до нейросети но зачастую объединяют методы связанные с какими-то архитектурными решениями объединяют методы, связанные с какими-то архитектурными решениями внутри моделей, Видеопамятью, так что у нас в целом все начнет считаться и железную историю, то есть оптимизация именно хардвера, и скрестили это все дело в мерч-методы, когда мы достаточно эффективно в тех вещах, которые мы уже давно знаем, можем использовать управление виртуальной памятью, управление видеопамятью, управление там видеопамятью так что у нас целом все начнет считаться в несколько раз быстрее при сильно объяснять как-то это не надо. Мы хотим... этом это никак не потеряет в качестве потому что мы сохраняем ровно ту же логику работы так примеру самый эффективный способ вообще там по оптимизации любого лм сервисом и подробно сегодня разберем таковы кэш continuous бачинг идея в нем супер просто она на картинке мне кажется даже на картинке и мне кажется даже сильно там объяснять как-то это не надо мы хотим когда у нас вот такой поместился батч S6, который состоял всего там из двух токенов условно. вот бач при этом он там заканчивается достаточно рано мы хотим в оставшуюся часть бача потому что она у нас просто западе на и вставить какой-то другой бач который был там сильно меньше всего нашего сэмпла и мы таких бачей скорее всего найдем так вот к примеру там в с1 вот эту всю историю до Вместился батч S6, который состоял всего из двух токенов условно. Это флешмобильный инструмент, который позволяет пользователям внести в себя вред. Это флешмобильный инструмент, который позволяет пользователям внести в себя вред. Это флешмобильный инструмент, который позволяет пользователям внести в себя вред. Это флешмобильный инструмент, который позволяет пользователям Здесь вместился S5, который тоже из двух токенов состоял, при этом из трех. внести в себя вред. Это флешмобильный инструмент, который позволяет пользователям внести вред. Это флешмобильный инструмент, который позволяет пользователям внести вред. Это флешмобильный инструмент, который позволяет пользователям Это из двух, наверное, а тут просто падинг. внести вред. Это флешмобильный инструмент, который позволяет пользователям внести вред. И мы таким образом сильно скомпонуем наше пространство батчей, Это флешмобильный инструмент, который позволяет пользователям внести вред. которые мы подаем нашу ломку, и сильно быстрее посчитаем. При этом мы будем очень хорошо знать, где у нас что заканчивается, потому что мы специальные токены будем использовать которые end of sentence являются это flash потому что мы специальные токены будем использовать, которые end-of-sentence являются. лампок на int8. attention их целое семейство тоже сегодня подробно не поговорим это page detention он ничего общего с словом attention вообще не имеет но подход очень интересный я отдельно статью тут оставил на эту всю историю но мы смотреть на него не будем там квантизованные лоры интересные методы квантизации методы квантизации лмок на int8. когда несколько слоев у нас просто помощи Это использование, там, k-bit precision, для того, чтобы, там, как-то заквантить наши параметры, при этом не сильно поменять в качестве. Я, кстати, наверное, отдельную скину, сегодня тоже на смотреть на это не будем. Фьюзирование слоев. Это больше архитектурная, конечно, история, нежели чем мерч. Но зачастую это используется, но зачастую это используется, когда несколько слоев у нас просто по мощности обвиняются условно в один, Поэтому мы тут просто определенным образом... и быстрее считаться начинает. И есть целые фреймворки для оптимизации, это там Petals и Swarm, но давайте начнем с Flash Attention.",
  "82875": "Здесь не сильно устали, но не так много осталось.",
  "83050": "Нет, это тот бач, который мы подаем на вход, а ла-лам. на вход и лалом поэтому мы тут просто определенным образом синие то паттинги насколько понимаю либо к примеру весь наш вся наш весь наш бач зак какие-то специальные токены красные это end of sentence там желтые это собственно те данные которые мы хотим подать просто это как пример компоновки бача когда мы вместо там весь у нас Весь наш батч заканчивается, к примеру, на S6, то есть шестое там предложение. Начнем с Flash Attention. Это такая супер база для всех LLM. Мы укомпоновали, на самом деле, всего в четыре, хотя их было изначально шесть, просто было очень много паддингов, пробелов, которые нам особо не нужны, и мы это все дело скомпоновали, continuous patching здесь хорошо работает.",
  "84300": "Это такая супер база для всех LLM-ок, сервисов и так далее, которые используются. С чем это вдруг связано? С тем, что у нас подсчет нашего социального сообщества Она по своим результатам в целом сильно меняет тренинг тайм, она меняет инференс тайм в том числе.",
  "84625": "",
  "84650": "",
  "84675": "Какие-то модельки, обученные Flash Attention, обычно показывают рост производительности в три с половиной раза.",
  "84975": "",
  "85000": "это вдруг связано с тем что у нас подсчет нашего салфеттен шина на самом деле он очень не Макса и вот этот. оптимизирован по памяти от слова совсем у него есть несколько операций операция там матричного перемножения операция взять и софт макса и операции еще одного матричного там перемножения но не суть важно какого потому что она нигде в целом особо не оптимизируется вот и на самом деле подсчет софт И на самом деле почет софтмакса и вот это матричное приложение, какой-то, как будто бы жесткий дебил. которое первое наших коверисов, очень затратное. И обычно, как оно делается во всех фреймворках, они ее пихают в называемую там «Hardband with Memory» внутри GPU. Нашу GPU можно представить как какую-то consistent память, какую-то consistent память наш какой-то как будто бы жесткий диск внутри джипу и есть какая-то там всего 20 мб обычно, но при этом очень много. оперативка внутри джипу несмотря на то что полтора терабайта в секунду кажется гопухи это все равно этого какие скорости но на самом деле на огромных там пайлов данных и так далее это сильно замедляет процесс там и обучение всего остального когда у нас при этом есть очень маленькая потому что там при этом есть очень маленькая потому что там всего 20 гигабайт обычно но при этом очень пропускная по своей способности виртуальная память внутри нашей гипухи и flash attention они очень эффективно они вся основная суть почему вдруг этот кусочек стал использоваться научились работать с этим небольшим кусочком как раз таки связанным с видео оперативной памятью Вся основная суть, почему вдруг этот кусочек стал использоваться, мы обычно там матрицу перемножаем ведь на него вроде не положишь целую матрицу, а это в рамках самой первой реализации Flash Attention, в рамках перемножения матриц использования тайлинга, когда мы вместо подсчетов, когда мы считаем Q на K, Когда мы считаем Q на K, мы обычно матрицу перемножаем, И именно благодаря сфере развития, мы можем сделать как мы знаем, как мы их перемножаем. Там используется просто более эффективный метод, который резко сокращает количество операций, но это окей. В рамках подсчета как раз-таки Softmax используется так называемый онлайн Softmax, который считается у нас рекуррентно. считается у нас рекуррентно, и именно благодаря свойству рекуррентности при подсчете онлайн софтмакса во-первых, не просто быстро считать, так еще и мы очень хорошо и эффективно умеем хранить информацию как раз-таки о текущем рекуррентном состоянии этого софтмакса в очень быстрой памяти, и это нам позволяет как раз-таки вот эти две операции тайлинга и онлайн софтмакса, онлайн софтмакса во-первых не просто быстро считать так еще и производить вычисления на одном ядре гпу этого мы заметили там сильный прирост flash оттеншин 2 flash оттеншин 3 и по там при всем при этом прошлое все наши вычисления они слова парализовались сейчас это проявляются просто прекрасно так что вот на одном там ядре гпу это все дело считается и виду как раз таки Flash Attention 2, Flash Attention 3 и прочие какие-то модернизации подобных флешей, рак. они так или иначе продолжают идею авторов изначального Flash Attention. Но Flash Attention сейчас в той или иной реализации нет ни одной, наверное, лампки, которая бы не использовала его. Не знаю, правда. Поэтому он сейчас везде. сейчас везде и при этом все мы не теряем качестве от слова совсем никак потому что мы получаем ровно И при этом все мы не теряем качество от слова совсем никак, для подсчета как раз-таки Attention Scarf. тот же результат то есть это действительно очень хорошая оптимизация работы алгоритма причем не по там какой-то компетенции на класт то есть мы в целом имеем ту же сложность алгоритма которые была у нас до этого мы очень сильно имеем ниже требования по памяти которые требуются нам для как раз таки attention скоров ну и теперь кого кэш стараюсь быстренько по нему пройти идея очень 2 равно 4 по виду того что ну ломка у нас обычно это деколь простая как у нас работает лампка у нас есть изначальный какой-то запрос это примеру два плюс два она отсылается лампки лампка генерирует будет теперь два плюс два будет равно два плюс равно 4 по виду того что новая лампу у нас обычно это декодер она всегда берет какое-то предыдущее даже не так мы вот эти все куски свое состояние и на основании это предыдущее состояние генерирует следующий токен и так итеративно недурно можно заметить что на самом деле у нас есть в достаточно повторяющиеся куски Точнее, даже не так.",
  "91350": "значения. Мы вот эти все куски, которые у нас были там в качестве запроса, в качестве ответа, мы можем очень эффективно где-то хранить. Уже не дурно как идея. А теперь развеем эту идею совсем до крайностей. А почему бы нам просто вот эти куски кода не хранить в каком-то кэше, который у нас будет постоянно обновляться, постоянно обновляться а именно этот кэш который у нас всегда заложен как значение мы будем подавать а именно этот кэш, который у нас всегда заложен как или весь предыдущий какой-то контекст, ведь у нас в целом есть... всегда в л.м. качестве входного какого-то контекста то есть просто базового хранить этот весь кэш будет куда удобнее и куда более быстрой какой-то реализации чем мы будем заново скармливать модели заново скармливать модели весь предыдущий какой-то контекст ведь у нас в целом есть по киваю уже текст декодинга у нас никуда не пойдет, у нас ковер-краш появится только после этого. какие-то данные от модели которые мы можем есть достаточно эффективно скармливать это особенно важно для time to first о кино но точнее наоборот не сильно важно потому что у нас первый контекст текст-декодинг у нас никуда не пойдет, у нас KV-кэш появится только после этого. Однако этот способ сильно увеличивает ТПС, Однако, что очень важно, что вдруг научились его очень хорошо квантизовывать и оптимизировать. но в целом особо сильно никак не влияет в своем первоначальном виде на использование памяти.",
  "94675": "У нас все равно резервируется больше 30% на любой видюхе под KV-кэш. что вдруг научились его очень хорошо квантизовывать и оптимизировать. И поэтому научились кавыкаш-квантификации. У нас была проблема, что изначально веса модели при маленьких контекстах, то есть при маленьком кавэкэше, они у нас занимали большую часть памяти, понятное дело, но этот кавэкэш при больших контекстах сильно растет, так что у нас не хватает памяти на эффективное хранение весов. памяти на эффективное хранение весов не поэтому научились кого кэш квантизовать причем с помощью О реализации квантизации КВ-кавша, КВ-квант просто квантизации кого кэша можно во-первых сохранить те же самые результаты по модели которые используются но при этом сильно сократив потребление памяти на хранение кого кэша и текущие как раз таки таки реализации там квантизации кого каша там кого квант просто и называется они позволяют В конец добавляем... он достичь как раз таки ломком на продакшене там контекстного окна в 10 миллионов токен ровно потому что вот это оптимизировали кого кэш он хорошо поместится на любую железяку и последнее что мы раз а мы просто типов каждым на каждом шаге мы просто конка чем в последнем каждом шаге мы просто конка чем в последнем в конец добавляем имбединг токина да да все просто вот этот KVC, он просто отвечает за это.",
  "96775": "очень просто все так то есть тут нет какого-то супер но шестая ли гениальной идеи просто обратили внимание что он в целом по какие-то куски текста они постоянно повторяются их решили отдельно в каком-то кэше хранить значение там ключ значения по ним все тут ничего супер особенного нету вот Тут ничего супер-особенного нету. Вот этот KVCash просто отвечает за какой-то контекст, который на ЛЛМ-Инт-8.",
  "97525": "каждом шаге модель сама себе дает. Вот она может к нему очень эффективно быстро обратиться. С помощью квантизации он еще и весить мало начинает. Да, и последняя история тоже, она просто более умная квантизация, нежели чем там просто заквантить весы и найти какой-то фактор квантизации. Это там и найти какой-то фактор квантизации это там ллм-инт8 там прям так называется вот саду.",
  "97950": "что делают смотрят на значение в любой на самом деле матрицы находят как и какие-то условные обычные значения внутри матрицы которые как-то равномерно распределены между самими собой но матрицы мы также еще и чаще всего находим какие-то аутлайеры чаще всего из-за того что у нас мы также еще и чаще всего находим какие-то аутлайеры. Чаще всего из-за того, что у нас никак трогать в рамках квантизации, потому что из-за, там, квантизации достаточно спортированные порой бывают эмбеддинги, у нас эти аутлайеры действительно хранят какую-то супер ключевую информацию для лмпи. Поэтому было предложено, давайте мы на самом деле наши аутлайеры, в виду того, что они несут достаточно большую смысловую нагрузку для наших моделей, не будем Не будем никак трогать в рамках квантизации, потому что из-за квантизации подобных параметров сильно потом может пострадать качество. со стандартным алгоритмом квантизации, и потом все это дело... Мы оставим их как есть. И действительно, можно эти вещи оставить как есть, но при этом заняться квантизацией не аутлайеров, а просто каких-то значений, которые так или иначе в нашем алгоритме распределены. По стандартному алгоритму квантизации и потом все это дело уметь эффективно объединять. они поддерживают LMint 8. Подробнее про алгоритм, то, как он выбирает аутлайеры, то, как он выбирает регулярные значения, тут решил не касаться. Но идея тоже достаточно простая, интуитивная. И на самом деле очень весомая, потому что зачастую все современные реализации, которые можно скачать с Hugging Face, Современные реализации, которые можно скачать с Hugging Face, они поддерживают Element 8 внутри себя, Нам осталось поговорить по поводу фреймворка. и это дает еще меньшую просадку по качеству, чем при обычной квантизации полноценной. И при этом все позволяет сильно меньше памяти потреблять модели на инференции, на обучении и так далее.",
  "101250": "Нам осталось поговорить по поводу фреймворков, на которых работают разработчики, которые выводят модели в прод. это тендер это л это больше такой продакшен на самом деле фреймворк эти фреймворки У нас есть несколько фреймворков, которые точно хотелось бы затронуть, не суперподробно, но просто хотя бы рассказать, которые все опенсорсники так или иначе используют. Это больше такой продакшн, на самом деле, фреймворк. Эти фреймворки все в целом зачем-то нужны. имеют хорошую поддержку на каком-то... Они объединяют все то, что мы там обсудили до этого, то есть какие-то методы оптимизации, какие-то ускорения инференса, поддержка стабильности и функциональности нашего сервиса, как раз-таки внутри себя, имеют хорошую поддержку на каком-то более низком уровне, В отличие от всех других фреймворков, которые мы то есть по общению с железяками. К примеру, TensorRT, это непосредственно разработано NVIDIA, оно поддерживает самые современные ядра. Если выкатывается драйвер на какой-нибудь H100 GPU, на который вы будете учить свою модель, то, скорее всего, TensorRT обновится сиюсекундно, И у вас будет там самая современная поддержка без багов и так далее. и у вас будет самая современная поддержка без багов и так далее. В отличие от всех других фреймворков, потому что они зачастую просто реализованы китайцами, Не каждая лаборатория может позволить себе вдруг взять специалистов. у которых хоть и есть какие-то свои GPU, они не так распространены, и все равно все метятся в как раз-таки использовании GPU от NVIDIA. При этом все, TensorFlow RT, он production фреймворк в первую очередь, и у него очень сложный порог входа. порог входа. Не каждая лаборатория может позволить себе вдруг взять специалиста по этому фреймву появляет. и как-то на нем работать, продолжать. Зачастую самым популярным это VLLM используется. У него очень простой сам по себе. У него хорошая скорость как раз-таки, и инференсы модели после определенных операций, которые этот фреймворк делает. там после определенных операций которые этот фреймворк делает авторы вы как раз реализовали честно вот очень популярный это можно по звездочкам увидеть его по патч тэншин патч тэншин нам очень эффективно позволяет работать с кого кишон в удивление вот блочно как-то реализуют там какую-то структуру там по хранению этого к вкша в не особо разбираюсь очень популярный это можно по звездочкам увидеть его зачастую используют все современные там ломки простой но гениальной идеи, тоже какой-то фреймворк. Вот они есть. которые не супер большие они там на в лами так иначе написаны есть еще и ломде плой первые ребята которые там запустили там ламу один ламу 2 то сделали там от авторы континент бачинга тоже гениальные идеи, тоже какой-то фреймворк. Вот они есть такие. И заключительно, что хотелось бы сказать, генеративный искусственный интеллект это круто есть определенные понятно дело грехи а именно сделать какой-то определенный рекап, зачем мы это вообще вдруг все прошли на протяжении всех этих пяти лекций, ведь что нас дальше ждет. Мы в целом поговорили на самой первой лекции, что там там генеративный искусственный интеллект так и не может.",
  "105825": "это круто, но есть определенные, понятное дело, грехи на текущий момент, есть какие-то нерешенные у него проблемы, но что немаловажно, то, что есть определенные риски, которые являются не просто рисками, что мы там денежку какую-то потеряем, но это там топ-2 рисков по версии там международного экономического форума, форума такой самый большой наверное организации которые там так или иначе занимается тем что такой самой большой, наверное, организации, которая там нам добросовестно и очень качественно мерить в таком случае алан. подсвечивают какие-то риски мировые именно к чему все прислушиваются там самой большой компании так далее то есть это очень авторитетный источник да и эти риски связаны с галлюцинациями считают сильно не просто галлюцинациями но и дезинформации считают очень опасными и очень важно нам и очень важно нам добросовестно и очень качественно мерить в таком случае LLM. Мы поговорили на попавшийся бенчмарк на уметь. второй лекции про то, что используют вообще в рамках обучения LLM и какие модификации делают над LLM-ками, которые так или иначе влияют на работу самой LLM и их нужно учитывать. Вообще, в целом, что нужно уметь все правильно измерять, нужно не просто вслепую бросаться на первый измерять нужно не просто вслепую бросаться на первый попавшийся бичмарк но уметь как-то его что модальность — это не просто какой-то блокбокс. оценить оценить особенности нашей лампки что она может что не может не используйте где она не может она там обучить ее тоже определенным образом и до рассмотрели модальности как следующий шаг развития общих целых всех алла лампок некоторые нюансы связанных с их обучениями о том что нюансы, связанных с их обучениями, беды. о том, что модальность — это не просто какой-то блокбокс, но это все-таки состоящий из каких-то различных энкодеров, проекторов, там, cross-attention в истории, и то, что их качество измерить — это достаточно большой челлендж. Ну и сегодня поговорили в целом о каких-то методах оптимизации, которые так или иначе используются, и которые могут нам так или иначе повлиять на картину что мы можем увидеть что там хайф модельки и которые могут нам так или иначе Следующие лекции проведут Ваня Подпружников и Степан Пономарев, мои коллеги. которые мы загружаем там с ген фейса они могут отличаться того что мы это можем увидеть на сервисе и поэтому от этого нам собственно правильно как надо строить наше тестирование все дальнейшие Пани Подпружников и Степан Пономарев, мои коллеги. В дальнейшем вас ждет достаточно увлекательное и долгое путешествие мир раков, агентов, и проговорим про какие-то реальные истории жизни. и не просто ЛЛМ, а так таковой, но и их применений, как они правильно используются, как их правильно использовать, как узнать, что они правильно используются. проговорим про какие-то реальные истории жизни. Степан, как раз-таки по большей части сконцентрированный на диффузионных моделях, расскажет ровно про них. Скорее всего, там самый современный доклад будет связан с текстом видео, той вещью, которая развивается меньше года. Посмотрим на методы как раз-таки оценки подобных моделей, раз такие оценки подобных моделей, какие риски там какие риски там тоже могут быть, что с этим делать. тоже могут быть, что с этим делать. И если все пойдет гладко, то, возможно, пройдем бонусную лекцию, о которой говорили. Сразу предвкушая вопрос про домашнее здание, появится ли оно сегодня. На самом деле нет, не появится, появится оно в субботу. Вот. На все, что хотел сказать. Так, всем спасибо. Если есть вопросы, буду рад слышать. А есть какое-то понимание, про что домашка-то будет? А есть какое-то понимание, про что домашнее здание будет? Да, есть. Да, есть. Обычно домашка будет состоять из... У вас будет, скорее всего, инференция ллмки, достаточно быстрой, очень надеюсь на это. И нужно будет эту ллмку уметь прогнать на бенчмарках, бенчмарках, которые мы обсуждали как раз-таки в рамках этих лекций, возможно, в различных режимах, которые мы обсуждали как раз-таки в рамках этих лекций. И тогда всем спасибо. и написать по этому какие-то выгоды. В целом, это будет ровно про это. Спасибо. Сверхъестественного, да. Здесь сейчас еще есть вопросы, буду рад ответить. Но если вдруг нет, то пишите в чат. вдруг нет, то пишите в чат. И тогда всем спасибо. Получается, Ваня начнет с четверга, а по поводу первого домашнего задания сброшу как раз-таки на неделю информацию. Всем хорошего вечера. Спасибо, пока."
}