{
    "processed_frame_014825.jpg": "PART_1: Так, давайте начнем. Я сразу предупрежу, такой небольшой дисклеймер. Я тут недавно только отошел от непродолжительной простуды, отошел от непродолжительной простуды, поэтому если начну кашлять как дед, то постараюсь так называемому model free сеттинги когда про среду нам будет известно все меньше и меньше замиютиться. Но если вас это смущает, то скажите об этом. Итак, у нас сегодня второе занятие, и оно будет посвящено новой не то что парадигме, но мы чуть ли усложним себе задачу. Мы будем работать Model-free сеттинги, когда про среду нам будет известно все меньше и меньше, главным объектом изучения у нас был... а соответственно, какие среды у нас на самом деле присутствуют в реальном мире повсеместно, поэтому они будут иметь для нас особое значение. И в целом, тетап соответствует тому, какие задачи мы будем решать в реальной жизни. Но перед этим давайте вспомним, что мы изучали в прошлый раз.",
    "processed_frame_016150.jpg": "PART_1: Быстренько пробежимся. Главным объектом изучения у нас был так называемый марковский процесс принятия решения или сокращенный MDP. Быстренько пробежимся. Состояние, в котором мы находимся, ну, по сути, не обеспечивает никакой безопасности. MDP состоял из нескольких компонентов. Из action space, то есть тех действий, которые мы можем принимать, те решения, которые мы можем принимать в тех или иных ситуациях и, соответственно, влиять на исход и на ту награду, которую мы получим. на ту награду, которую мы получим. Состояние, в котором мы находимся, ну, по сути, некоторое такое. Способ описания того, что из себя вообще представляет следа. Третье — это вот такая условная вероятность. То есть, по сути, вероятность того, что, находясь в состоянии S и выполнив действие A, вы перейдете в состояние S3. Тут важное свойство зашито, важное свойство зашито что это вероятность зависит только от текущей пары состоянии действия и не То есть здесь на самом деле зашита марковость и санкциональность. что эта вероятность зависит от того, что мы делаем. зависит от той истории то есть вам не совсем не вам не важно как именно вы пришли в состоянии штрих если вы знаете состояние предыдущие это действие которое в предыдущем состоянии совершили То есть здесь, на самом деле, зашита марковость и стационарность. А вот так можно было это представить, как Марио бегается, прыгается. И последнее, мы договорились, что награда у нас будет детерминированной функцией от состояния и действий. В целом, можно рассматривать более общий сетап, где награда стахастическая, но в целях облегчения задачи себе и выкладок на слайде, давайте договоримся, что награда будет детерминированной. А вот так можно было это представить. Марио бегает, прыгает, вертится, в конце концов собирает какую-то промежуточную награду, Политика, напомню тоже, это то правило, которое мы в конце концов попадает в трубу, и уровень закончен, переходим к следующему. С точки зрения некоторой формализации мы максимизируем вот такую ожидаемую, дисконтированную кумулятивную награду. По нашей политике. Политика, напомню тоже, это то правило, исходя из которого вы в текущем состоянии принимаете решения. GT — это вот эта кумулятивная награда в целом. Политика может быть детерминированная, когда это правило детерминировано, и политика может быть стахастической, когда это некоторое вероятное распределение, из которого вы сэмплируете.",
    "processed_frame_019925.jpg": "PART_1: Дальше у нас появляются новые персонажи. новые персонажи GT это вот эта кумулятивная награда в целом это некоторые случайная величина вот Ответ на этот вопрос дает как раз функция, которая потому что все ее компоненты это тоже случайные величины вот но можно взять от него от ожидания и задаться следующим вопросом какую награду я могу получить из состояния с вот на вопрос на этот Ответ на этот вопрос дает как раз функция v. v с пометкой p, это значит, что эта функция считается для определенной политики. Ну и по сути, она говорит то, сколько награды мы можем получить, если мы начнем играть из состояния s и дальше будем действовать исходя из политики p. Одной v функции нам будет недостаточно, мы ввели q функцию, ввели пью функцию вот тоже с пометкой пи это такое расширение а что будет если находясь состоянии с вот тоже с пометкой p. reward или reward to go, часто будем его так называть. Это, конечно, не все. я выполню действие а затем буду действовать исходя из моей политики сколько награды я получил да это просто рекуррентное соотношение вот этот жить и называется кумулятивный Коммунитивный reward или reward to go, часто будем его так называть, GT равняется RT.",
    "processed_frame_021700.jpg": "PART_1: просто считается с помощью дисконтирования к текущему моменту всех immediate reward, которые вы получите на своей траектории. И подчиняется, тут написано, давайте я это пропишу на всякий случай, подчиняется следующему рекуррентному соотношению. рекуррентному соотношению. GT равняется RT плюс гамма GT плюс один. Какие у нас сейчас есть объекты, благосостояния по сравнению с той наградой, которую мы получаем сейчас. То есть предельный случай... дискаунд фактор говорит то, насколько будущие награды имеют вклад в наше текущее благосостояние с той награды, которую мы получаем сейчас. То есть в предельный случай, гамма равна нулю, мы максимизируем нашу текущую награду. Ту награду, которую мы получим, если мы выполним действие A, ее мы и максимизируем. Гамма равна единице, все награды на протяжении нашей игры имеют одинаковый вес. Но гамма между нулем и единицей имеет, как принято, какое-то промежуточное значение, значение типа такое экспоненциальное убывание вес и у нас еще есть так называемый длина эпизода вот типа такое экспоненциальное убывание веса. Но если t меньше бесконечности, то следа называются эпизодичными. целом можно рассматривать среды где это равно бесконечности так даже теоретически более удобно Но если t меньше бесконечности, то следа называется эпизодической. Соответственно, через какое-то фиксированное количество шагов взаимодействие у вас заканчивается. На самом деле, я, по-моему, упоминал, что можно закладывать информацию в эпизодических средствах. Часто вы даже знаете, сколько шагов как максимум вам придется проиграть в этой игре. Можете закладывать эту информацию как-то в тейп. что можно закладывать информацию в эпизодических следов в State как по такой формуле. Вот, например, интересная штука, которая, по-моему, Типа буквально говорите, на каком промежутке взаимодействия от нуля до единицы вы сейчас находитесь. сейчас находитесь. Вот агент тоже это может эксплуатировать. Вот, например, интересная штука, как раз в... Агент тоже это может эксплуатировать. Там, если вот это сделать, в конце агент, в какой-то момент он, он же не дурак, он поймет, что можно... Сейчас скажу. В муравье, в continuous control задачи, мы рассматривали ее в прошлый раз, где такая туча с четырьмя лапами ходит. В конце агент, в какой-то момент он же не дурак, он поймет, что можно, что взаимодействие заканчивается через какое-то количество шагов, и в конце, чтобы… А ему дают награду за то, насколько далеко он отошел от своей первоначальной точки. Он в конце может прыгать, то есть он буквально перед концом эпизода прыгает вперед, потому что ему уже все равно, типа упадет, не упадет. Ему главное вот добить вот это расстояние, получить получить вот эту большую награду за такой отчаянный кружок в конце. вот эту большую награду за такой отчаянный прыжок на самом деле много вложенных в это ожиданий. в конце. Такие спецэффекты тоже бывают. Так, двигаемся дальше. Соответственно, да, максимизируем G0 по нашей политике. На самом деле, вот это мотожидание, просто напомню, у нас два источника стахастики в среде. Это политика и сама среда. Соответственно, будет вот это мотожидание по P, будет вот этом от ожидания попьет на самом деле много вложенных мы-то жида не чередующихся среда это та вероятность, с которой вы перейдете в то или иное состояние, и состояние S под действием S. политика 3d политика какие у нас были сам что чтобы все работало хорошо в алгоритмах полисе его вытер рейшн 1 1 сам шин довольно сильный мягко говоря это то что динамика среды то есть та вероятность, с которой вы перейдете в то или иное состояние, и состояние S, сделав действие A, она известна.",
    "processed_frame_027775.jpg": "PART_1: Говорим, что space конечен, в целом достаточно много. Не так уж и много средств из реального мира удовлетворяет этому условию, поэтому насколько это реалистичный сценарий, судите сами. А два других ассамбляжа вроде бы не очень-то и строгие. Говорим, что space конечен, в целом достаточно много сред удовлетворяет этому условию. И action space конечен, еще больше сред удовлетворяет этому условию. Вот если все выполнено, то можем сначала вывести уравнение Беллмана, который связывает как раз вейку функции в текущем состоянии.\nPART_2: \nPART_3: ",
    "processed_frame_029250.jpg": "PART_1: которые связывают как раз в A и Q функции в текущем состоянии делю. через в A и Q функции, через состояние, в которое потенциально из этого состояния можно попасть. Это верно для V функции. Для Q функции вы просто в качестве вашей минимальной единицы рассматриваете пару состояния действия. То есть вы находитесь в состоянии S, делаете действие A и дальше смотрите а в какое состояние во первых я могу попасть под этим делю под действием под и дальше смотрите, в какое состояние, во-первых, я могу попасть под этим, выписать эти уравнения в IP и купи. Еще у нас была такая политика, которая лучше всех, оптимальна. действием это в действие надо избегать таких татологий сделав определенное действие и какие действия могу сделать в этом состоянии с штрих соответственно для любой политики можно как раз уравнения в IP и QP. Еще у нас была такая политика, которая лучше всех, оптимальная политика. Вот для вейку функции, для такой политики мы будем называть вейку созвездой. Соответственно, здесь уравнения меняются. Обратите внимание, что уравнения практически остаются теми же с точностью до замены от ожидания по среде на максимум. На самом деле, если вспомните, у нас было такое утверждение, утверждения, что Максимум, сейчас, не посреди, а по полной. которое на самом деле теорема, что у конечной МДП существует детерминированная оптимальная политика. Ну и соответственно, здесь это некоторая такая контенсенция это некоторая кинтенсенция этого утверждения что в с звездой кусок для такой политики будут которые на самом деле интегралы, превращающиеся в криптовалюты. подчиняться вот такому уравнению хотел я что-то здесь сказать она кстати обратить внимание что почему здесь везде суммы стоят именно из-за вот наших а сам шина в 2 и 3 все от ожидания которые а сам что в 2 и 3 все мат ожидания которые на самом деле интегралы превращаются в конечные суммы о каком-то конечному носителю соответственно вот так таком в такой удобной форме можно эти парадигме reinforcement learning. Первый это value iteration, просто вспоминаем, что все четыре эти уравнения это уравнения записать а и у нас было два алгоритма первые наши алгоритмы для решения задачи в это в аллее террористом просто вспоминаем что все четыре эти уравнения это на самом деле если их рассматривать как операторы которые переводят функцию из одного банка из банкового пространства гамма соответственно что нам говорит теорема себя то есть действует на эту функцию то они на самом деле и отжатие коэффициентом сжатие Соответственно, что нам говорит теорема банных ограничивающих отображений, что существует единственная неподвижная точка, и если мы запустим алгоритм простой итерации, то какое бы начальное приближение мы ни взяли, мы сойдемся к вот этой неподвижной точке.\nPART_2: \nPART_3: \nPART_4: Соответственно, ну давайте это эксплуатировать, давайте запустим наш алгоритм первый — Value Trade. Соответственно, давайте это эксплуатировать, давайте запустим наш алгоритм первый, Value Iteration. По сути, Value Iteration вскрывает в себя два этапа, объединенных в один.",
    "processed_frame_032875.jpg": "PART_1: \nPART_2: Напомню, что у нас алгоритмы в итоге сводились к двум действиям – Policy Evaluation и Policy Evaluation. к двум действиям – Policy Evaluation и Policy Improvement. вот это полисе и влечем один шаг управления То есть на этапе Policy Evaluation мы оцениваем текущую политику, то есть сколько награды мы можем получить благодаря этой политике. На этапе Policy Improvement мы улучшаем политику, улучшаем с помощью какого-то детерминированного правила. Соответственно, здесь зашли-то два действия. зашито два действия вот это полисе революция один шаг про мнения один шаг по сути полисе Типа, где полис им... полисе в лыжи на для текущей политики а это полисе и полный в вала iteration это не не так очевидно но мы сейчас посмотрим на поле ситой шин и там все станет совсем очевидно И там все станет совсем очевидно, где policy evaluation, где policy improvement. Удовольствие от этого не имеет никакого значения. В этом случае, если вы хотите, чтобы я мог предоставить вам дополнительные данные, то вы можете предоставлять их в виде сайта. В этом случае, если вы хотите, чтобы я мог предоставить вам дополнительные данные, то вы можете предоставить их в виде сайта. В этом случае, если вы хотите, чтобы я мог предоставить Ну и просто картинка для того, чтобы визуализировать вот это уравнение. вам дополнительные данные, то вы можете предоставить их в виде сайта. В этом случае, если вы хотите, чтобы я мог предоставить вам дополнительные данные, то вы можете предоставить их в виде сайта. В этом случае, если вы хотите, чтобы я мог предоставить Что мы говорим? Мы говорим, на самом деле, что мы можем представить игру как некоторое дерево. вам дополнительные данные, то вы можете предоставить их в виде сайта. В этом случае, если вы хотите, чтобы я мог предоставить Мы сейчас находимся в состоянии ST. Мы можем... Что мы можем сделать? Мы можем посмотреть, а куда мы... посмотреть, а куда мы, какие действия мы можем в этом состоянии совершить, куда мы можем попасть. Какие действия мы можем в этом состоянии совершить? получаем награду здесь это плюс один на самом деле это рт Дальше мы совершаем какое-то действие, попадаем сюда, например, дальше мы смотрим, попадание, на самом деле, условное, потому что тут черное это действие, поэтому мы не переходим в состояние, это просто визуализация. получаем награду здесь это плюс один на самом деле это рт в разных источниках по-разному пишется то есть туда, штрих Т плюс один. А какую-то награду мы... У каждого этого действия есть оценка. индексного сути не меняя вот а дальше мы смотрим типа в какие состояния мы можем попасть под этим под под действием если мы это действие применим в среде ну вот можем попасть сюда можем попасть У каждого этого действия есть оценка этого действия, сколько награды мы сможем получить, если начнем играть из этого действия. Соответственно, достаточно посчитать от ожидания вот эти награды и дисконтировать в настоящий момент. После этого мы сделали так для каждого действия, по Мы так сделали для каждого действия, по сути, а дальше мы выбираем действие жадно. сути, а дальше мы выбираем действие жадно. Типа, какое действие приведет меня к максимальной награде из этого состояния. Второй алгоритм. Вот что говорит нам уравнение Bellman Optimality Equation, но здесь мы его используем еще как некоторую последовательность для того, чтобы обновлять нашу функцию. Второй алгоритм состоит, как я говорил, из двух этапов. Первый — это policy evaluation, то есть что вы хотите сделать. У вас есть политика P, вы хотите для этой политики получить ее оценку в APS, для любого S. Соответственно, чтобы это сделать, вот вы запускаете Соответственно, чтобы это сделать, вот вы запускаете такую итеративную процедуру, Здесь как бы логика такую итеративную процедуру, утверждается, что она сойдется Утверждается, что в России есть много интересных и интересных вопросов. к некоторой, ну, к настоящей VPATS, такой, что VPATS подчиняется уравнению Беллмана. Вот, то есть сойдется к некоторой неподвижной точке. некоторые неподвижны точно здесь как бы логика логика оценки в функции встань с такая же только Когда вы заканчиваете, вы получаете... вы не берете максимум вы от ожидаете по вашей политики типа берете ваши награды с тем весом которое ваша политика приписывает как раз этим действием этот процесс называется полностью Этот процесс называется policy evaluation. Когда вы заканчиваете, вы получаете полную оценку для вашей политики. И если вы возьмете по такому моменту, то вы можете Что дальше? Когда вы это сделали?",
    "processed_frame_041500.jpg": "PART_1: Можно улучшить политику. То есть, если у вас была как раз некоторая пи-штрих, вот эта пи-штрих предыдущей политики, ну и здесь, соответственно, пи-штрих, Если вы возьмете по такому правилу новую политику, зададите ее с помощью такого правила, то утверждается, что Пи будет не хуже, чем Пи со звездой. «А вот это значит, что...». Это значит, что любого состояния в IP от S будет не меньше, чем в IP от S. А. То есть буквально благодаря этой процедуре вы можете при имеющейся оценке на вашу политику ее детерминированно улучшать. на вашу политику ее детерминировано улучшать. И опять же, поскольку у нас четыре уравнения, что изменится изменится сложность вашего алгоритмом потому что а мы сфокусировались только на двух для ваших, то тут стоит отметить, что то же самое все можно сделать и для Q функции, то есть тоже запустить policy iteration, запустить value iteration. В value iteration что изменится? Изменится сложность вашего алгоритма, потому что после этого вы оперируете не только состояниями, но еще и парами состояния действия. Соответственно, у вас как бы до какого-то произведения получается тех элементов, которые вам нужно обновить. Вот это на самом деле важно, что в данном случае как бы V-функцию можно представить как вектор S1, TTT, S... как вектор, S1, TTT, S, сколько у нас будет действий, S большой. кина. А куфунцу можно представить как табличку. Табличку, давайте как-то ее более формально напишем. даже не табличку, а вот матричку типа количество состояний на количество действий. Даже не табличку, а матричку, типа количество состояний потому что по функции лежит информация о качестве вашей. Вот. Ну и соответственно, если вы используете Q-функцию, то обновлять вам нужно целую таблицу, а не просто вектор. Важно вот что отметить, что если вы учите V-функцию, то прежде чем делать policy improvement, вам нужно восстановить Q-функцию, функцию потому что по функции лежит информация о качестве ваших действий и уже после этого братья максим а если вы учите к функцию то этого этот шаг можно пропустить можно сразу брать от максим нам придется сойтись к одному из этих вариантов можно догадаться какому именно почему я на это заостряю внимание потому что через непродолжительное время у нас не останется выбор можно догадаться какому именно. Ну и последнее, что было на семинаре, это если вам все эти Q и V двигая ваше распределение в сторону элитных траекторий.",
    "processed_frame_046350.jpg": "PART_1: функции не понравились и теория не привлекла внимание, а задачи такие решать хочется, то можно вдохновиться успехами алгоритмов эволюционных стратегий и запустить похожую процедуру, процедуру двигая ваше распределение в сторону элитных траекторий элитных по отношению к какому-то эффективен с точки зрения того сколько сэмпов вы используете он абсолютно не критерию но в данном случае поскольку мы оптимизируем награду элитные будут те у кого награда выше соответственно этот алгоритм есть есть надежда что он сойдется к чему-то хорошему насколько он с точки зрения того сколько сэмплов вы используете он абсолютно не эффективен ритмос от ленин в целом люди пытались ограничить бюджет и типа придумать алгоритм в рамках определенного бюджета. славится тем насколько он не эффективен по количеству данных которые вам нужно чтобы обучиться а вот сейчас и как идет тренд на то чтобы в целом даже не сейчас уже какое-то несколько лет придумать алгоритм в рамках определенного бюджета по взаимодействию с вашей средой. Вот эти алгоритмы, поскольку они не предполагают ничего о том объекте, с которым вы работаете, то они очень неэффективны. Чем меньше Inductive Biases вы вкладываете, тем меньше информации ваш алгоритм использует из среды, информации ваш алгоритм использует из среды, тем в целом эффективность одного сэмпла меньше. тем в целом эффективность одного проекта. Меньше вклад в качество всего алгоритма. Если популяция очень большая, то вклад отдельного индивидуума уже не такой заметный. Это был краткий рекап нашей лекции. Есть ли у вас какие-то вопросы?",
    "processed_frame_049350.jpg": "PART_1: Вот, это был краткий рекорд нашей лекции. Есть ли у вас какие-то вопросы? Наша предыдущая лекция. Нет, это еще формально не началось. Сергей, у меня небольшой вопрос по домашке, ну, про policy iteration в целом. У нас там есть условия, что нам нужно остановиться по критерию остановы. критерию остановы. нему, а максимум, который будет внести в свою деятельность. Вот, я хотел бы спросить, нам нужно ли еще сверять то, Вот, я хотел бы спросить, вам нужно ли еще сверять то, что у нас политика не изменяется? что у нас политика не изменяется? Нет, потому что политика... На самом деле, если у вас есть два действия, которые одинаково хорошие, то есть, у вас есть две детерминированные оптимальные политики, то между ними можно прыгать и как бы там как повезет. Обычно есть на этих действиях у вас какой-то порядок, то типа индекс одного действия меньше другого, поэтому типа, индекс одного действия меньше другого, поэтому Так. аргмаксимум, который вы будете брать, будет предпочитать одно действие относительно другого. Но в целом, как бы, это излишне. Достаточно проверять только, что функция удовлетворяет уравнение Беллмана с какой-то точностью. Угу, спасибо. тогда начинаю вторую лекцию просто вот так выглядит у нас вот наш цикл наш вот этот Но в реальном мире, конечно же, не так. В реальном мире мы в худшем случае срезаем...\nPART_2: фидбэк-клуб агент общается со средой и раньше все было отлично мы про этот мир этот мир был нам совершенно понятен вы про него все знали вы могли мат ожидания считать все было шикарно но Но в реальном мире, конечно же, не так. В реальном мире мы в худшем случае среду даже видеть не можем. То есть нам просто какие-то эфемерные награды приходят, которые еще бывают довольно шумными. и поэтому понять как она реально устроена довольно сложно на каждое явление есть какая-то своя модель может быть несколько с довольно может быть не нетривиальными может быть не нетривиальными дефицитами уравнениями поэтому может быть сложно но вот как бы примеры есть вы вряд ли с вероятностью близко к нулю вы будете знать динамику средств которые с динамикой которых и завладать довольно сложно и таких средств большинство то близко к нулю вы будете знать динамику среды с которой вы будете работать. Поэтому то, что где используется по произойдет, первое наблюдение, чтобы проводить policy improvement, v-функция нам больше не понадобится. То есть учить ее смысла больше не будет, потому что не сможем из нее, к сожалению, восстановить политику. Это не значит, что она абсолютно бесполезна. Мы увидим, что она много Это не значит, что она абсолютно бесполезна.",
    "processed_frame_052725.jpg": "PART_1: ",
    "processed_frame_053350.jpg": "PART_1: взяв эмпирическое среднее. Мы увидим, что она много где используется по-прежнему, Даже есть теоретические... но конкретно в нашем сетапе она не пригодится.\nPART_2: ",
    "processed_frame_054075.jpg": "PART_1: Ну и, да, тут я уже забежал вперед. Поскольку Q-функция — это мат ожидания, а мы знаем, что мат ожидания можно оценивать по сэмплам из этого распределения, сэмплом из этого распределения взяв эмпирическое среднее даже есть теоретически гарантия так он чисел усиленный усиленный закон чисел больших чисел поэтому ну давайте делаем то же самое то есть у вас ваша траектория она возьму другой цвет то есть буквально мы вы находите сейчас состояние ст начинаете играть в но и совершайте действия она возьмут другой цвет она точно уже пойдет сюда вот но но а дальше как бы у вас есть развивка но Вы добегаете до конца.\nPART_2: вы начинаете спал вас есть какая-то текущая политика пи да тут важно что это все еще к юг и политику мы пока не только нас считается на фиктивы начинаете играть вот у вас одна траектория Начинайте играть, вот у вас одна траектория, вы добегаете до конца, посчитали эмульсивную награду до конца эпизода. Соответственно, чтобы получить вам одно наблюдение, вам нужно доиграть до конца. Это как бы один ваш темп. Здесь у нас, обратите внимание, у нас есть две размерности. Первая размерность, это как бы размерность, по которой мы усредняем, а вторая размерность, это время внутри вашей траектории. траектории соответственно чтобы получить вам одно наблюдение вам нужно доиграть до конца вашу игру же сам. что дальше дальше ну например политика вы саплируйте из политика политика не случайно вы или или среда стокастическая вы пришли сюда у вас здесь получается даже gta 1 вот это gta 2 ну и и по двум же то у 2 ну и и по двум же сэмплом оценивать вот забудет g-tow 3 вот у вас 33 числа вы как раз рук ваша политика. Иногда вам хочется... обновляйте ваше значение к функции причем но обратите внимание что на самом деле если вдруг Иногда вам хочется это делать в онлайне, то есть у вас есть, например, пока две траектории, эквивалентно это как бы ваши утре назовем так а вот это ваша третьих пока нет. Соответственно, вы можете посчитать текущую вашу аппроксимацию, потом, когда третья траектория придет, с помощью такой нехитрой формулы, которая на самом деле просто А вот это ваша q2, то есть предыдущая аппроксимация. Н минус один. Вот. Ну, неважно. Ладно, это на самом деле уже излишне означает, что Вы ее перевзвешиваете, только не только это, а вот сейчас. На самом деле предыдущая аппроксимация будет вот это делить на n-1. Но не важно, ладно, это на самом деле уже излишнее Ну, много ходить не могу. Я не знаю, что это за шутка. значение. Я не знаю, что это за шутка. Важно то, что вы на самом деле насчитывать можете Я не знаю, что это за шутка. Я не знаю, что это за шутка. аппроксимацию в онлайне, ну не совсем прям в онлайне, Я не знаю, что это за шутка. Я не знаю, что это за шутка. но как только у вас есть устоявшееся количество траекторий, вы Я не знаю, что это за шутка. Я не знаю, что это за шутка. уже можете посчитать вашу аппроксимацию, а потом, когда Я не знаю, что это за шутка. Я не знаю, что это за шутка. новая траектория придет, посчитать новую аппроксимацию. Я не знаю, что это за шутка. А в чем проблема, как вы думаете, в чем проблема такого алгоритма? Ну, много ходить надо, много сэмплов. Да, да, это правда, но вот обратите внимание, что вам, во-первых, нужно какое-то количество траекторий набрать, но сами траектории. Вот какие особенности трек твой? Обратите внимание, где траектория заканчивается. После этого, в результате, Что они вообще конечны. в результате, Да, да, абсолютно правильно. То есть у вас есть предположение, что ваше взаимодействие когда-то закончится, и в целом оно может закончиться через десять шагов, через десять минут, а может закончиться через десять минут, а может закончиться через десять лет. через десять лет. Разница большая. Поэтому ждать может быть довольно долго, во-первых. А вот это такое основное. Во-вторых, среда может быть не эпизодической. Она может вообще никогда не заканчиваться, тогда непонятно, а в какой момент останавливаться. Но это такой основной недостаток. А что вы думаете о том, что мы должны делать? Есть еще второй недостаток. На самом деле, что вы делаете? Вы оцениваете Q, это некоторое наджидание, вы его оцениваете с помощью эмпирического среднего, но на самом деле, если вы посмотрите внутрь этого наджидания, то там как бы тоже есть над ожиданием. то там как бы тоже есть наджидание. И что вы с этими над ожиданиями делаете? То есть вы внутренним от ожидания заменяете еще на их несмещенную. Вы их тоже заменяете на сэмплы. Причем сэмплы типа R1, потом у вас еще одна награда пришла, вы заменяете на сэмпл R2, получается, гамма R2 и так далее. То есть вы внутренним от ожидания заменяете еще на их несмещенные оценки, то даже с теоретическими гарантиями вы можете сходиться без проблем. но по очень маленькому количеству сэмплов, буквально по одному. Поэтому у этого алгоритма огромная дисперсия. Огромная дисперсия. И когда вы запустите... Среда сама в себе может быть довольно стахастической. Но если у вас и у алгоритма большая дисперсия, дисперсия то даже с теоретическими гарантиями вы можете сходиться бесконечно долго но неужели открытия. алгоритма есть только одни недостатки на самом деле у него есть достоинства он оценивается то что вам нужно причем не смещена и в так у него стереотипе грань а минусы как мы с вами отметили это мы должны оканчивать эпизоды доигрывать до конца что в не эпизодических следов не позволить оканчивать эпизоды, доигрывать до конца, что в неэпизодических средах непозволительно. И у него большая дисперсия. Есть ли альтернативы? А вспомню, что у нас на самом деле есть уравнение Балмана, который связывают Вэйку через один шаг.\nPART_3: ",
    "processed_frame_064000.jpg": "PART_1: Ну, кстати, давайте поиграем с вами в ассоциативный ряд. Здесь есть четыре уравнения. четыре уравнения какой из них лишний такое было бы выкинули Какое из них лишнее? Какое вы бы в виду какой именно. выкинули? какой меньше всего похоже на остальные право верхние так право верхние это еще один ответ а вот в чате написали 4 имеется ввиду в чате написали 4 имеется ввиду какой именно 4 давайте я вот так вот пронумерую один пью со звездой я понял так у нас есть два ответа давайте 3 дождемся и узнаем правильный ответ Похоже, третий будет мой. распределению, а здесь максимум. В общем, самое странное уравнение, не похожее на другое, — это вот это уравнение, потому что все остальные представляют из себя правой частью от ожидания по какому-то Правая часть — это мотожидание по какому-то распределению, а здесь максимум от мотожидания. Мы хотим оценить надожидание от... Почему для нас это важно?",
    "processed_frame_066600.jpg": "PART_1: Потому что есть такая теория статистической аппроксимации, которая в упрощенном виде, она на самом деле более сложная, но в упрощенном виде, в том сетапе, в котором мы сейчас работаем, сетапе в котором мы сейчас работаем мы хотим оценить над ожидания от неизвестного нам распределения из которого нам в лучшем случае доступны центу соответственно можно запустить вот такой градиентного спуска и это первая интерпретация вторая вот итерационный процесс обратить внимание что он похож на всем нам известный алгоритм градиентного И это первая его интерпретация, вторая его интерпретация Если αk равняется один делить на k, то это будет равен через вот такое вглаженное среднее.\nPART_2: То есть у нас есть уже текущая аппроксимация ttk, когда нам приходит новый сэмпл xk, мы с какими-то весами берем старую аппроксимацию, с какими-то весами берем новую аппроксимацию. Если альфа k равняется 1 делить на k, то мы просто получим формулу детерминированного пересчета среднего. А что вы думаете о том, что мы должны делать? Если у вас есть k-минус один элемент, вы посчитали среднее, у вас приходит k-элемент. Это такая формула онлайн-апдейта для вашего среднего, которую вы можете использовать. который вы можете использовать. Это значит, что мы от ожидания квадрата разности... Что? Зачем это нам?",
    "processed_frame_068925.jpg": "PART_1: На самом деле есть такая теорема, теорема Робинсона-Ван Ро, которая утверждает, что если альфа-каты удовлетворяют такому условию, и есть еще некоторые технические условия, то последовательность тет-такатых сходится к тет-созвездой в L2. В L2 это значит, что от ожидания квадрата разности сходится к нулю. Вот, то есть, вираж, который мы видим, это, конечно, вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим Довольно сильная сходимость. в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, Из этой сходимости, в частности, следует сходимость по распределению и сходимость по... это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом случае, это вираж, который мы видим в этом Исходимость по распределению сходится, когда следует сходимость по... Ой, извините. Из сходимости в L2 следует сходимость по вероятности, вероятности необходимость по вероятности исходимость по распределению то есть в из сходимости по вероятности следует сходимость по распределению. иерархии этот вид сходимости довольно высоко зачем это нам как вдохновляющий пример для Сервалейшн у нас был алгоритм, который на самом деле очень похож на вот эту процедуру.\nPART_2: \nPART_3: \nPART_4: \nPART_5: задач задача статическая у нас мы оцениваем одном от ожидания для одного распределения здесь у нас этих распределений по количеству пар состоянии действия вот поэтому как бы и в общем случае может зависеть от состояния действий. Еще один вариант, здесь мы использовали Bellman есть дополнительные ограничения, то действие «Аштрик» должно прийти из той позиции, которую мы уже говорили.",
    "processed_frame_070275.jpg": "PART_1: того как бы что что мы что мы сделали какой алгоритм у нас уже был вот монтекарло полисе похож на на вот эту процедуру то есть у нас есть какая-то здесь правда с точной замены минус на процедуры мы как раз плюс вот как бы тета-ката это наша текущая проектироваться на тюка то это наша тета-ката терминов джека от это наша x как и вот соответственно с помощью такой итерационной С помощью такой итерационной процедуры мы как раз сможем оценивать наше среднее, и у нас даже есть теоретические гарантии.\nPART_2: \nPART_3: \nPART_4: что можно да мы уже мы уже знаем что-то горит не идеальный его есть недостатки можно сделать лучше давайте вспомним что на самом деле у нас есть уравнение балмана и мы можем как бы из у нас есть уравнение Беллмана, и мы можем использовать это уравнение Беллмана, чтобы придумать наш следующий То есть QK, просто Q функция для политики Пи — это не только сэмпл, таргет, на который мы будем учиться.\nPART_5: \nPART_6: \nPART_7: ",
    "processed_frame_071600.jpg": "PART_1: \nPART_2: Для политики P это не только матожидание для G, для кумулятивной награды, но это еще и награда плюс гамма матожидание от Q, Q'H'. Вот, мы-то ждали его. штрих а штрих вот мы-то ждали по штриха штрих вот соответственно как бы можно использовать Почему это лучше? Потому что здесь мы как бы используем один сэмпл. можем использовать и плюс гамма на состоянии штрих а стрикер состоянии штрих действие а здесь мы как бы используем один сантом один информацию о среде при этом мы используем как С', сделав действие A, бы нашу текущую оценку для и кучу оценку то текущую аппликацию по функции и нам не нужно доигрывать эпизоды конца буквально нужно сделать получается один шаг среде то есть и состояние шаг среде то есть и состоянии с штрих сделав действие перейти состоянии здесь есть индекс посчитать и аку функции с точки зрения с точки зрения каких-то терминов sts то плюс один и состоянии ст перейти состоянии 100 плюс 1 а дальше посмотреть просамплировать состояние от и плюс один из этого состояния 100 плюс 1 и вот для него С точки зрения каких-то терминов, вот у нас R плюс гамма Q будет называться таргетом, ТД-1 это монтаж. а вот эта разность будет называться temporal difference. Отсюда и название алгоритма. На самом деле алгоритм TD0 называется. CD1 это монтагарда. Таргеты нам никто не дал? Потом чуть позже поймем почему от 0 до 1. В общем, вот такой алгоритм позволяет не дайвировать эпизоды до конца, позволяет использовать и информацию из среды и свою текущую аппроксимацию. Буквально, смотрите, раскрывается важная фича обучения с Обращение с подкреплением. Таргеты нам никто не дал. Настоящие таргеты довольно шумные. Вот этот таргет шумный. Давайте как бы тянуть за волосы сами себя. Придумывать себе таргет на ходу, выкладывать перед собой железную дорогу, чтобы ехать на нашем паровозе. подкреплением. Соответственно, вот, таргеты мы придумываем на ходу и на нашем паровозе. Соответственно, вот, таргеты мы придумываем на ходу, используя нашу текущую аппроксимацию. Этот процесс еще называется bootstrapping, но не тот bootstrapping, когда мы из распределения а именно в контексте reinforcement learning. Вопросы? Сергей, можно вопрос про альфа-каты? В данном контексте почему она зависит от состояния действия и вообще где ее взять? Альфа-каты можно воспринимать как learning rate, и в целом для каждой пары состояния действия он может быть свой. Ну, то есть это гиперпараметр, который мы изначально как-то задаем? Или последовательный гиперпараметр. Например, альфа-каты равна один делить на х. альфа-каты равна один делить на k. Это хороший вопрос, вы поймете почему важно чтобы он еще думаю. зависел состояние и действие. Ну вот здесь получается глобальный параметр который не зависит от состояния и действия и он кстати даже подчиняется условиям Робинсона Монро. Сумма один делить на k расходится, а сумма сумма один делить на карте сходится а сумма один делить на к квадрат и квадрат на шесть что мы как бы находимся в тесном порядке. неважно короче она сходится так что вот вот пример почему сейчас поймем почему важно что почему альфа к ты должна зависит и они действия вот представьте что что мы как бы находимся в мы пытаемся подтянуть себя под действие вот этой теоремы здесь у нас альфа-каты может зависеть от состояния действий.\nPART_3: expectation equation. Давайте использовать Bellman optimality equation. Вот используем такой таргет, кажется, что разница небольшая, на самом деле обратите внимание, что здесь у нас есть здесь у нас есть дополнительные ограничения, то действие h' должно прийти из той политики, откуда приходит состояние S&A, поэтому в целом как бы... которую мы оцениваем. Мы все еще мысленно здесь везде прописываем π. А здесь такого ограничения нет, потому что в состоянии штрих мы берем максимум его действия. При этом у нас нет ограничений, откуда приходит состояние с ей поэтому в целом как бы кажется есть такое ощущение что этот алгоритм для которой вы пытаетесь выучиться. чем-то лучше чем чем чем но мы поймем поймем какие свойства есть у того и другого просто сам факт того что здесь по-прежнему нужно обеспечить что и штрих у вас приходят именно из той среды для Кстати, вопрос.\nPART_4: При фиксированной политике к чему вот такой алгоритм сойдется? Кому сойдется такой алгоритм? Фиксируемую политику Пи? хочется сказать что там к среднему и сойдется среднем очень почему именно ну какого-то или там V для всех стритов. среди аналогию как бы с полисе трейшн сказать что вот как бы мы там мы там тоже находили что вот как бы мы там мы там тоже находили с там в для всех стоит of прификсированной политики ну Утверждается, что... извая можно получить кью вот так и сделаем к этому и сидя у нас здесь ваших нет у нас есть только кружка я так понимаю но смотрите у нас есть политика пи у вас есть некоторые настоящее значение функции утверждается что вот этот процесс сойдется как юг и вот настоящих кипи по l2 до поля 2 освободления по вероятности. если все все условия тире он будет выполнен ну как минимум давайте давайте более слабое Ну, как минимум, давайте более слабое распределение. Да, он подойдет как звездочка, причем, как бы, в том числе, По вероятности, точно сойдется. Соответственно, теперь вопрос, к чему сойдется вот этот алгоритм? — Ну, к Q звездочкам. — К Q звездочкам. да он подойдет а какие звездочку причем неважно обратить внимание что этот алгоритм агностика И у этого есть... Это довольно серьезное утверждение, потому что... по отношению к политике то есть несмотря на то что действие у вас могут приходить из какой-то политики может быть разных политик здесь нет ограничений то сойдетесь все равно к юсу звездой есть это довольно серьезное утверждение потому что если это так то кажется что вот этот алгоритм всем сейчас Монте-Кай. всем лучше и если заезд и создать давайте потом восстановим детерминированным образом исключить нашу политику и считаемые мы решили задачу целом вот мы с вами по сути рассмотрели три алгоритма Мы, по сути, рассмотрели три алгоритма сейчас.",
    "processed_frame_081925.jpg": "PART_1: \nPART_2: Монте-Кайло. Этот называется Cool Learning. Это называется Saksa. Надо понять, надо на чем-то остановиться или понять, в каких ситуациях тот или иной алгоритм лучше. каких ситуациях тот или иной алгоритм лучше. Давайте как бы препарировать свойства каждого из этих знаете такой перед придав между тем чтобы как можно сильнее удовлетворять как алгоритмов, но прежде чем это сделаем, давайте обратим внимание на вот это условие. У нас, сильнее удовлетворять каким-то формальным условиям можно было создать а штрих это что а штрих здесь СТ плюс один. Короче, А штрих это АТ плюс один вот на этой картинке.\nPART_3: каким-то теоретическим гарантиям, но при этом... В состоянии с штрих. С т плюс один. Ну, да, с штрих равняется с т плюс один. Вот в этом состоянии. Ага, спасибо большое. Так. Пытаюсь зарекарить, о чем я говорил.",
    "processed_frame_087125.jpg": "PART_1: или или вот здесь ну вот это а штрих это то действие которое мы сделаем состояние с с штрих Короче, а штрих это а т плюс один вот на этой картинке.\nPART_2: Да, предофф, мы пытаемся как можно ближе быть к каким-то теоретическим гарантиям, как можно ближе быть к каким-то теоретическим гарантиям но при этом хотим расширять скуп чтобы в ней было бесконечное число флагов. тех задач которые мы можем решать вот одно из теоретических требований да и одно из требований теоремы это чтобы сумма коэффициентов альфа-каты была бесконечность так называемый инфинит визит и почему инфинит визит ищут потому что чтобы эта сумма была бесконечная необходимо как минимум Чтобы эта сумма была бесконечной, необходимо как минимум, чтобы в ней было бесконечное число слагаемых. Чтобы в ней было бесконечное число слагаемых, вы должны пару состояния действия SA посетить бесконечное число раз. Что это значит? Это значит, что у вас политика, с помощью которой вы в среде действуете, должна постоянно исследовать. исследовать, то есть она не может пренебрегать какими-то парами состояния действия, потому что состояние действий. Скорее, нет, потому что в некоторых условиях, в которых То есть она не может принять в виду, что это не так. в противном случае вы для этой пары не сможете сойтись к оптимальному значению. Может быть, оно вам и не надо. Хороший вопрос. Нужно ли вам оценивать функцию для всех пар состоянии пар состоит скорее нет потому что в некоторое состояние вы попадете с вероятностью И, конечно, вам нужно перейти к следующему вопросу. Вам нужно перейти к следующему вопросу. Вы можете перейти к следующему вопросу. ноль просто потому что они но слишком плохие вы не хотите там быть поэтому вы не хотели там Вы можете перейти к следующему вопросу. Вы можете перейти к следующему вопросу. оценивать к функцию но с точки зрения теории чтобы все то везде сходилось нужно чтобы каждую пару Вы можете перейти к следующему вопросу. Вы можете перейти к следующему вопросу. посетили бесконечность ну и в целом как бы что это обеспечивает что достаточно чтобы это произошло обеспечивают что достаточно чтобы это произошло вам нужно когда статистическая политика которая стороны спектра. с ненулевой вероятностью для каждого состояния ходит вовсе по все действия возвращает и действия для фиксированных сцене с какой-то не верой вероятности давайте посмотрим на две стороны это нас приводит на самом деле к такой довольно релевантной дилемме которые в реальной жизни по появляется а именно exploration exploitation то есть какое когда можно свои знания о мире возможно они не полны использовать для того чтобы максимизировать свою награду какую-то а когда лучше по исследовать свою награду какую-то а когда лучше по исследовать по уходить по недавно открытым рестораном чтобы действия, но опять же, в конечном итоге, если вы...",
    "processed_frame_091875.jpg": "PART_1: найти какой-то получше вы за это платите у вас есть так называемый черный тип коз то есть пока вы исследуете вы действуете неоптимально ну или или тратите время на какие-то неоптимальные действия какие-то неоптимальные действия. пользоваться всем своим знанием. Но опять же, в конечном итоге, если вы найдете какой-то, в данном случае ресторан, который намного лучше, чем то, куда вы ходили до этого, то все ваши страдания опутаются. Но это может быть, и это может и не случиться. Поэтому тут, опять же, нужно какой-то баланс соблюдать. Соблюдать между тем, соблюдать между тем, чтобы пользоваться всем своим Вот у нас есть две стороны спектра. между тем, чтобы пользоваться знанием, опытом жадно, и тем, чтобы быть открытым к каким-то новым вещам, скажем так. Не лекция по философии, чисто пытаюсь на пальцах объяснить, что это значит.",
    "processed_frame_094325.jpg": "PART_1: Давайте вернемся в более формальное русло. более формальную более формальное русло вот у нас есть две стороны спектра exploration чисто Есть, например, кулу. exploration это вот прям самый самый что ни на есть экспрессия это просто случайно политика политика которая в каждом состоянии выбирает каждое действие с одинаковой вероятности по другую сторону спектра у нас жадная политика причем жадная данном случае оптимально политик Причем жадная в данном случае оптимальная политика, например, Q со звездой. Важно считать, что в этом случае мы должны поддержать и поддержать общую связь между людьми. В этом случае мы должны поддержать общую связь между людьми. Кстати, не обязательно. В этом случае мы должны поддержать общую связь между людьми. Если у вас есть какая-то текущая оптимация Q, вы можете действовать жадно по отношению к ней, В этом случае мы должны поддержать общую связь между людьми. но тут зависит от того, насколько ваша оптимация далека от реальности. Если она близка, то вы будете действовать около оптимальной или оптимальной. оптимально если она плохая то вы будете можете действовать сколь угодно не оптимально важно что Если она плоха, то вы можете действовать сколько угодно неоптимально. действуем случайно, с вероятностью 0, 5. эта политика детерминированная она никогда никогда не исследует то вот можете ли вы предложить политику которая лежит где-то посередине между этими но можно просто взять какую-то вероятность и сложить эти две политики с вероятностями мы кидаем монетку с вероятностью 05 например делаем То есть мы кидаем монетку с вероятностью 0, 5, например, действуем случайно с вероятностью 0, 5 жадно. плохо. Вопрос на засыпку. Какая вероятность... Да, да, абсолютно верно. Это называется Эпсилон-жадный поликлип.",
    "processed_frame_096500.jpg": "PART_1: То есть мы вводим новый параметр Эпсилон, который буквально говорит нам о том, что с вероятностью Эпсилона мы можем выбирать действие случайно, а с вероятностью 1, 5 Эпсилона выбирать действие жадно. В целом, довольно неплохо. вопрос на засыпку какая вероятность в этой политике выбрать жадное действие это Эпсилона, это то, что у вас есть здесь развилка, но еще... написано же один или вопрос не в этом вопрос в этом она смотрите один нас но смотрите один из акций на то что у вас есть здесь развилка но еще вы когда выбираете случайное То есть вот такая у вас вероятность выбрать действие. действие у вас есть вероятность выбрать это действие поэтому вероятность но один из их сон до плюс плюс epsilon делить на н н количество возможных действий за это позицию да абсолютно Да, абсолютно верно. То есть вот такая у вас вероятность выбрать жадное действие в отношении к вашей текущей аппроксимации ку. Опять же, можно несколько шагов делать аргументскую функцию. Еще вопрос. Можете ли вы придумать какую-то еще политику, которая стокастическая, но которая использует ку-функцию? опять же можно несколько шагов делать ароматской функции а потом на ком-то на каждом кодам шаги Вот, например, вот это, например, вот это, например, вот это, делать шаг случайно смотрите важно что политика определяет ваши действия в конкретном состоянии то есть давайте зафиксируем состояние из из этого действия из этого состояния у вас есть есть один А1, А2, А, Н. Вот можете ли вы придумать такую политику, которая зависит от состояния С? Что она, с одной стороны, она случайная, то есть она стахистическая, с другой стороны, она используется как функция. дам подсказку вот у вас есть нейронка вы решаете задачу многоклассной классификации у вас есть можно также софтмакс навесить и все да да абсолютно верно сейчас я сотру нейронка который возвращает вам логиты н штук по количеству классов вот что вы делаете после этого все да да абсолютно верно сейчас я сотру чтобы это не фиксировалось так называем большинстве к детерминированной политике, либо к политике стократической.",
    "processed_frame_100800.jpg": "PART_1: политики берем softmax в определенном действие по состояниям и на самом деле еще есть некоторая температура вот некоторые скейл фактор который в зависимости от своего значения приводит либо к политики стокастической то есть вот сюда чем чем выше альфа тем короче если альфа стремится к бесконечности, то вы сходитесь к юниферам распоряжения. к юниферам распоряжения. — А можно еще разок про вероятность в эпсилон-гриде политики? Почему там вероятность будет единица минус эпсилон плюс эпсилон на n? Откуда эпсилон-грид? — Смотрите, у вас есть n действий, и, допустим, arg-максимум — это первое действие. Вероятность попасть в него, можно попасть в его двумя способами. Вот на этом этапе подбросили монетку, Вероятность попасть в него можно попасть двумя способами. разыгрывается n случайных величин вы по сути выбираете одно число из с вероятностью 1 минус эпсилон выпал этот арт-максимум Q функции, то есть вероятность 1 минус эпсилон. Есть еще и второй вариант. Подбросили монетку, попали вот в эту ветку, дальше как бы там Н случайных величин. Вы, по сути, выбираете одно число из из N возможно. Понятно, спасибо большое. Вероятно, что это первое, как раз один на N, один на N, но она еще умножается на вероятность того, что вы вообще попали в эту ветку. Ну и получается плюс Эпсиона делить на N. Эпсилон жадной политики. Вот. Эпсион жадной политики. На самом деле пока это у нас будет first class citizen, мы Да, вот у нас есть два алгоритма пока. как бы будем жить в классе эпсилон жадных политик, и посмотрим, почему нас это приведет. Да, я уже выцветил.\nPART_2: ",
    "processed_frame_104325.jpg": "PART_1: у нас есть два два алгоритма пока разную про мунтака вообще не говорим говорим только про по сравнению друг с другом. кулер нин против сарса целом на самом деле я уже сказал что кажется что первый алгоритм более привлекательный но давайте разберем почему и какие есть преимущества и недостатки у них по преимущества и недостатки у них по сравнению друг с другом. Вот второй как раз, ему нужно, чтобы учиться, приходить из той среды, из динамик этой среды, которую мы оцениваем. нужно сэмплировать из своей же политики. У первого алгоритма такого нет ограничения, он может... На пары S и A у нас вообще никаких ограничений нет. На S штрих у нас есть ограничение, что он должен приходить из той среды, из динамик этой среды, которую мы оцениваем, но остальное, как бы, собран не ею. То есть буквально вы можете... для кулернинга берем максимум, для SARS берем темпл из нашей же политики. Вот. А на самом деле здесь есть фундаментальное различие. Обратите внимание, что первая стратегия может учиться с опыта, который То есть буквально вы можете стоять, у вас есть старший Более того, брат, который не дает вам играть в компьютер. Вы смотрите на то, как он играет, но можно сказать, что вы, если вы смотрите, вы какие-то вещи запоминаете, вы чему-то учитесь, но вы сами не играете, то есть вы учитесь с чужого опыта. Вот, здесь как бы во втором случае вам нужно играть как бы во втором случае вам нужно играть самостоятельно чтобы учиться более того разный полет, который, как правило, не пройдет. самостоятельно, чтобы учиться. более того а более того да можно учиться что можно учиться если у вас есть два старших брата которые оба не дает вам играть компьютера но оба игра можно смотреть на то как они играют оба и учиться как бы с двух разных политик вот две как бы с двух разных политик вот две разные политики которые собирают вам опыт но вы с него вот по такой формуле учить учитесь а давайте чуть формализуем а здесь возникает принципиальное вы учитесь учите политику мил ее различие между он полисе и о в полисе режимом он полисе режим он полисе лёдлинг это когда вы Политику Mew учите с ее же опыта.",
    "processed_frame_107825.jpg": "PART_1: с опыта, который собран в России. То есть вам нужно генерировать сэмплы с помощью этой же политики, которую вы пытаетесь выучить. А противовес офф-полисе. Офф-полисе, как я говорил, мы учим некоторую таргет-полисе с опыта который собран другой политикой Mew. P называется target policy, Mew называется какие-то логи, чего бы то ни было, проанализировать их и пытаться... behavior policy. Какие, опять же, примеры, помимо примера, который я уже привел, мы можем собрать чего бы то ни было, проанализировать их и пытаться обучиться с ним, сделать такой имитейшн-лёрнинг. можем... Сами мы при этом ничего не сделали, просто логи собрали, но это не наша логия, конечно, но вот можем делать такой имитейшн-лёрнинг. Можем учиться с несколькими полетами. Более конкретный пример, Более конкретный пример — можем учить жадную политику, действуя некоторой политикой, которая поощряет исследования. То есть можем учить как раз жадную политику с помощью сэмплов из эпсилон-жадной политики. помощью сэмплов из эпсиложадной политики. Ну и последнее свойство это то, что мы можем такие стейты которые постулируют некоторые не то что парадигмы но по сути да по сути две ветки использовать наш опыт, предыдущий опыт взаимодействия. Прошу прощения. Вот, все это супер, на самом деле, пока это Некоторые, не то что парадигмы, но по сути, да, по сути, две ветки алгоритмов. On-policy и off-policy. Но мы поймем, на самом деле, в чем преимущество. У нас дальше это будет проследоваться. Какие алгоритмы on-policy, как можно попытаться их делать off-policy, почему в policy он более sample-efficient, вот такие вещи. Пока мы просто постулируем эти вещи и говорим про преимущества и недостатки. недостатки. Пока кажется, что в он-полисе преимуществ нет, но это на самом деле не так. Кулернинг, да, я про него уже говорил, теперь давайте его просто формализуем. У нас есть два параметра, параметр для нашей обстановленной политики и наш ленинградский. Обратите внимание, что мы тут забиваем на наше второе условие для Пока кажется, что в on-policy преимуществ нет, но это на самом деле не так.",
    "processed_frame_112025.jpg": "PART_1: условия для теоремы робинсона-монро то есть теперь у нас как бы есть и интернет визит эйшн графы не меньше бесконечности. Ну и ладно, как оно обычно бывает, у нас есть теория, а есть практика. благодаря в оксано-жадной политики но но сумма альфа-квадратов конечно сумма квадратов альфы не меньше бесконечности ну ладно как она обычно бывает нас есть теория есть практика первый опыт пытаться что-то сделать. инициализируем кушки какие какими-то дальше у нас прошел прошел процесс обратите внимание что в reinforcement learning мы у нас нет фиксированно с это то есть когда мы начинаем мы начинаем буквально бед без ничего мы посылаем агента который еще плохо обучен собирать как бы свой агента который еще плохо обучен собирать свой первый опыт и пытаться что-то сделать среде также здесь мы собираем наш опыт действуем с помощью эпсилон жадный здесь как бы собираем опыт и с него же в онлайне учимся все алгоритмы пока онлайн это как бы особенность онлайн рейтинг в этот момент мы посмотрим что значит оффлайн рейтинг но пока как бы собираем учимся собираем учимся соответственно что здесь происходит вот происходит вот здесь мы собираем наш опыт действуем с помощью и всем жадной политики мил соля возвращает награду и следующий стоит мы вот по такому правилу обновляем нашу текущую оценку функции довольно довольно естественно да как мы уже выяснили кулер нинк учиться учат курса звездой Да, как мы уже выяснили, Q-learning учит курсом звездой, при этом используя сэмплы с другой политикой. То есть здесь политика в явном виде не участвует. Если он учит курсом звездой, то в какой-то момент, когда Q-K будет достаточно близко к курсу звездой, мы сможем найти P-звездой в нашу оптимальную политику. нашу как бы оптимальную политику. Здесь политика в явном виде не участвует, мы учим по функции, здесь вам среда возвращает это но из по функции мы сможем восстановить нашу политику. Сарса, да, пожалуйста. В чем суть второго пункта? Объекты. Просто типа здесь у вас агент что-то делает, вас агент что-то делает здесь вам среда возвращает это это короче с инженерной точки зрения здесь на смысле с того как вы это имплементирует и здесь вы вызовете что-то типа а полисе дот сэмпл а здесь вы вызовите n в дот степ — А если просто получим награду и следующее состояние?",
    "processed_frame_115925.jpg": "PART_1: — Да, абсолютно верно. — Спасибо. — Сарс, почти то же самое, те же параметры, инициализируем кушки, только здесь у нас два вызова.\nPART_2: Только здесь у нас два вызова policy.sample, dot sample на s, и sample s'. И все это было вне зоны, вне зоны, вне зоны. Появился в этом случае, как правило, неизвестный. В этом случае, если бы мы не были виноваты в том, что мы не могли бы выяснить, что это было, то мы бы не могли бы не ответить. Но мы не могли бы не ответить. Мы не могли бы не ответить. И только после этого, когда мы это сделали, мы оцениваем q-функцию. Мы не могли бы не ответить. это сделали мы оцениваем куфунсу вот соответственно а штрих мы в среду уже не посылаем хотя хотя хотя Хотя, хотя, хороший вопрос. Нет, наверное, все-таки не посылаем, потому что... не посылаем потому что это интересно вопрос а штрих мы на следующем на следующем этапе посылаем наверное э. посылаем наверное да я думаю что чтобы сэкономить как бы 11 одно сомплирование имеет смысл послать поиска. поскольку мы здесь обновляем только для с а короче поскольку мы обновляем только для с а для с 3h3 h3 ничего не изменит мы можем послать этот сэмпл в среду но это уже как бы детали оптимизации можно этот сэмпл в среду но это уже как бы детали оптимизации можно формуле как будто новое у вас же на самом деле что происходит вы были в с сделать действие и перешли в с3 новый реворд и новое состояние не нужно зачем тогда посылать среду но да ну смотрите это правда но у «Сделали действие A, перешли в S-штрих». Вы будете ли высупливать второй раз из политики здесь? Дальше, ну как бы вы в новом состоянии, вам нужно будет здесь сделать какой-то H-штрих. Вопрос в том, сколько вы посамплируете. Вы можете выселить второй раз из политики здесь. Я смог ответить на вопрос? Такое ощущение, что нет, что можно после этого... То есть буквально схлопывается после этого вот этот и вот этот пункт, они сливаются в один. Вы не будете после этого селпли. Я смог ответить на вопрос? Или запутал? А, ну, короче, но нам как будто не нужно, потому что мы Ну, я, если честно... Так по итогу, ходим второй раз? Смотрите, у нас онлайн-вызаимодействие со средой, поэтому мы в любом случае второй раз пойдем. Нет, вопрос в том... А, ну, короче, но нам как будто не нужны результаты эти. А у нас же Q уже другая будет, ну Q плюс один, то есть по идее там и... Ну, смотрите, на следующем этапе у нас уже получается как бы в с-штрих, аж-штрих. Чтобы второй раз не сэмплировать из политики, возможно, мы можем переиспользовать результаты предыдущего сэмплирования. Вот о чем мои размышления были. у нас же кожа другая будет куплюсь один то есть по идее там и сэмплировать в другом И когда ты задаешь себе вопрос и начинаешь рассуждать об этом вслух, то он как бы не может быть ответственным. изменится к в одном состоянии из предыдущего в последнем в паре состоянии действия в штрих а штрих она не изменится так что ладно мне кажется иногда бывает рассказываешь рассказываешь просто просто задаешь себе вопрос и начинаешь рассуждать об этом вслух. В целом, теоретические гарантии не сломаются, если мы просамплируем второй раз. во всех состояниях и действиях. Поэтому стамплировать Единственное, что оно будет в конечной ситуации, когда нам обязательно надо будет стамплировать второй раз. Поскольку если Q это у вас нейронка, например, то вот после этого обновления у вас потенциально изменится Q-функция, как правильно было замечено, функции как правильно было замечено во всех состояниях и действия поэтому Однако, в результате, в результате стамплировать нужно будет второй раз ссоре за суету который навел нужно будет можно еще вопрос если кейт они рамка мы армагдером или вот софт макс температуры это будет на следующем занятии краткий ответ мы берем в такой обосновке мы берем арт макс мы берем в такой постановке мы берем artmax есть другая постановка задачи reinforcement learning цеплировать и сапмакса так называемые энтропии энтропии регулировать reinforcement learning там можно будет брать из софтмаркса так то еще как бы вот если нас сакса вспомни что она была вдохновлена уравнением Но в целом это просто последовательность от ожидания по...\nPART_3: бэллмана на самом деле уравнение бэллмана здесь у нас здесь мотожидание по s3 h3 вот но что что в Что в целом это просто последовательность над ожиданием по h3 и по h3. брать оценку по... брать в отожидание, по-честному брать в отожидание от нашего... Ну давайте, чтобы дисперсию сбить, поскольку у нас конечное множество состояний, конечное множество действий, давайте вместо того, чтобы брать оценку по одному сэмплу, по выбрать в от ожидании по честно убрать на ожидании от наш политика можно поскольку это дискретное распределение можно просто с определенными весами просуммировать ваше значение по функции будет называться алгоритм экспертно вот такой вопрос у вас есть два алга Вот такой вопрос. У вас есть два алгоритма — кулернинг и SARS. Как вы думаете, какой алгоритм к какому... к какой политике сойдет?",
    "processed_frame_125825.jpg": "PART_1: \nPART_2: ",
    "processed_frame_127525.jpg": "PART_1: Окулежник тоже не подходит. Кулерник, наверное, сойдется к оптимальному пути, который ближе к лифу в данном вопросе. Старция сойдется вот к такой безопасной политике. Почему так? А SARS-CoV-2 более своего уровня. Потому что, на самом деле, ну, кулерник учит курсу звездой, соответственно, ну, здесь очевидно... А, я про среду-то... соответственно здесь очевидно я проследу ты сказал вот такой как бы лабиринтик без без слабее без состояние, если... И за каждое время, проведенное в среде... самих коридоров все клетки доступны вам нужно из вот этой точки попасть в эту но есть у вас обрыв если вы вдруг по бы попадаете то вы получаете награду минус то и возвращаетесь начальное И за каждое время, проведенное в среде, вы получаете еще минус единичку. соответственно. То есть на самом деле несложно тут посчитать, что оптимальная награда это минус 13. То есть вам минимум 13 шагов нужно пройти, чтобы добраться из точки С в терминальное состояние. в терминальном состоянии, чтобы выучить ее с помощью. вот, соответственно, типа, это оптимальные награды, которые вы можете достигнуть. Ну и, да, правильно, кулерник сойдется к кусу звездой, и с кусу звездой мы можем восстановить оптимальную политику, сойдемся к к вот этому кратчайшему пути. Почему Сарса сойдется к безопасному пути? Потому что, когда вы учите когда вы учите саксу вы поскольку вы учите ее с помощью эпсилон жадной политики вы по сути Сарсу, вы, поскольку политики есть только самые детерминированные. ограничиваете пространство политик ограничить пространство политик который вы можете выучить вот вы говорите что политика бы мы теперь живем в мире где запрещены как бы детерминированной Есть только самые детерминированные, политика с минимальной У него есть... энтропией, которую вы можете получить, это политика, которая просто эпсил-жадная. Соответственно, еще как бы у SARS, в SARS заложено вот Вот это отвращение к риску, что если он пройдет здесь, у него есть, из-за того, что политика стократическая, есть ненулевая вероятность прыгнуть в лаву, прыгнуть с обрыва. это отвращение к риску, что если он пройдет здесь, кулер нинг кулер нинг там таких рисков не зашито поэтому можно выучить оптимальную политику Соответственно, он в итоге выбирает самый безопасный путь, который только возможен, чтобы как раз минимизировать эти риски. там таких рисков не зашито, поэтому можно выучить оптимальную политику, хотя и важно, что на протяжении этого взаимодействия, Соответственно, у этого есть такой парадокс, что поскольку взаимодействуете вы с помощью оптимозаданной политики, у вас есть вероятность прыгнуть с обрыва, достаточно большая, если вы проходите особенно к нему очень близко. У этого есть такой парадокс, что... Нет. Сейчас, мальчишка. Я не могу. Так, я свое художество смотрю. Я не могу. Да, у этого есть довольно не совсем интуитивные последствия. Я не могу.",
    "processed_frame_134100.jpg": "PART_1: Эффективные последствия? Однако, если мы не будем виноваты в том, что мы не получили от этого отчаяния, то мы не сможем выяснить, что мы получили от этого отчаяния."
}