{
  "processed_frame_014825.jpg": "### Второе занятие: Model-Free Reinforcement Learning\n\n- **Предупреждение**: Лектор недавно переболел простудой, возможны кашель и прерывания.\n- **Тема занятия**: Model-Free Reinforcement Learning (RL).\n- **Цель**: Усложнение задачи, работа в условиях, когда информация о среде минимальна.\n- **Основной объект изучения**: Среды, которые присутствуют в реальном мире.\n- **Значимость**: Реальные задачи, которые будут решаться в реальной жизни.\n- **Вспоминание предыдущего занятия**: Необходимо вспомнить, что изучалось ранее.",
  "processed_frame_016150.jpg": "### Основные компоненты MDP (Markov Decision Process)\n\n- **Состояние (State, S)**:\n  - Описание текущего состояния системы.\n  - Не обеспечивает безопасности.\n  - Описание текущего состояния системы.\n\n- **Action Space**:\n  - Множество действий, которые можно предпринять в данном состоянии.\n  - Влияет на исход и награду.\n\n- **Переходная вероятность (Transition Probability)**:\n  - Вероятность перехода из состояния S в состояние S' при выполнении действия A.\n  - Зависит только от текущего состояния и действия (марковость и стационарность).\n  - Не зависит от предыдущей истории.\n\n- **Награда (Reward)**:\n  - Детерминированная функция от состояния и действия.\n  - Можно рассматривать более общий случай с стохастической наградой, но для упрощения задачи награда детерминированная.\n\n- **Политика (Policy)**:\n  - Правило принятия решений в текущем состоянии.\n  - Может быть детерминированной или стохастической.\n  - Цель: максимизация ожидаемой дисконтированной кумулятивной награды.\n\n- **Пример**:\n  - Марио бегает, прыгает, собирает награды, переходит на следующий уровень.\n  - Политика определяет, как Марио принимает решения в каждом состоянии.",
  "processed_frame_019925.jpg": "### Новые понятия и функции в MDP\n\n- **Кумулятивная награда (Cumulative Reward, GT)**:\n  - Случайная величина, представляющая общую награду.\n\n- **Функция стоимости (Value Function, V)**:\n  - Ожидаемая кумулятивная награда, начиная с состояния S и следуя политике P.\n  - V^p(s): ожидаемая награда, начиная с состояния S и следуя политике P.\n\n- **Q-функция (Q-Function)**:\n  - Расширение функции стоимости.\n  - Q^p(s, a): ожидаемая награда, начиная с состояния S, выполняя действие A, и затем следуя политике P.\n  - Рекуррентное соотношение: Q^p(s, a) = R(s, a) + γ * V^p(s').\n\n- **Кумулятивный reward (Cumulative Reward, GT)**:\n  - Общая награда, полученная за весь процесс.\n  - GT = RT.",
  "processed_frame_021700.jpg": "### Кумулятивная награда и дисконтирование\n\n- **Кумулятивная награда (Cumulative Reward, GT)**:\n  - Считается с помощью дисконтирования всех immediate reward к текущему моменту.\n  - Рекуррентное соотношение: GT = RT + γ * GT+1.\n\n- **Дисконт-фактор (Discount Factor, γ)**:\n  - Определяет, насколько будущие награды влияют на текущее благосостояние.\n  - γ = 0: максимизация текущей награды.\n  - γ = 1: все награды имеют одинаковый вес.\n  - 0 < γ < 1: экспоненциальное убывание веса будущих наград.\n\n### Эпизодические и непрерывные среды\n\n- **Эпизодические среды**:\n  - Взаимодействие заканчивается через фиксированное количество шагов.\n  - Информация о длине эпизода может быть заложена в state.\n\n- **Непрерывные среды**:\n  - Взаимодействие может продолжаться бесконечно.\n  - Теоретически более удобно для анализа.\n\n### Пример с агентом в continuous control задаче\n\n- **Агент может использовать информацию о длине эпизода**:\n  - В конце эпизода агент может предпринимать экстремальные действия для получения максимальной награды.\n  - Пример: агент прыгает в конце эпизода, чтобы получить награду за расстояние.\n\n### Максимизация кумулятивной награды\n\n- **Максимизация G0 по политике**:\n  - G0: кумулятивная награда, начиная с первого шага.\n  - Политика определяет действия агента.\n\n### Источники стахастики\n\n- **Политика (Policy, P)**:\n  - Определяет вероятность выбора действия в данном состоянии.\n\n- **Среда (Environment)**:\n  - Определяет вероятность перехода в новое состояние при выполнении действия.\n\n### Динамика среды\n\n- **Динамика среды**:\n  - Вероятность перехода в новое состояние при выполнении действия известна.\n  - Важна для корректной работы алгоритмов обучения.",
  "processed_frame_027775.jpg": "### Условия для пространства состояний и действий\n\n- **Пространство состояний (State Space)**:\n  - Конечное, но достаточно большое.\n  - Многие реальные среды удовлетворяют этому условию.\n\n- **Пространство действий (Action Space)**:\n  - Конечное.\n  - Большинство сред удовлетворяют этому условию.\n\n### Уравнение Беллмана\n\n- **Уравнение Беллмана**:\n  - Связывает значение функции в текущем состоянии.\n  - Выводится при выполнении условий конечности пространства состояний и действий.",
  "processed_frame_029250.jpg": "### Уравнения Беллмана для V и Q функций\n\n- **V функция**:\n  - Связывает значение функции в текущем состоянии.\n  - Учитывает возможные переходы в другие состояния.\n\n- **Q функция**:\n  - Рассматривает пару состояния-действия.\n  - Учитывает переходы в другие состояния после выполнения действия.\n\n### Оптимальная политика\n\n- **Оптимальная политика**:\n  - Политика, которая лучше всех.\n  - Для V функции оптимальной политики используется значение функции V*.\n  - Уравнения для оптимальной политики изменяются: ожидание заменяется на максимум.\n\n### Теорема о существовании оптимальной политики\n\n- **Теорема**:\n  - В конечной МДП существует детерминированная оптимальная политика.\n  - Уравнения для V* и Q* подчиняются интегралам, которые превращаются в конечные суммы.\n\n### Алгоритмы Reinforcement Learning\n\n- **Value Iteration**:\n  - Алгоритм для решения задачи.\n  - Включает два этапа, объединенных в один.\n  - Уравнения рассматриваются как операторы, переводящие функцию из одного пространства в другое.\n  - Теорема о банных ограничивающих отображениях: существует единственная неподвижная точка, к которой сходится алгоритм.",
  "processed_frame_032875.jpg": "### Алгоритмы Policy Iteration\n\n- **Policy Iteration**:\n  - Состоит из двух этапов: Policy Evaluation и Policy Improvement.\n  - **Policy Evaluation**:\n    - Оценка текущей политики.\n    - Определение награды, которую можно получить благодаря этой политике.\n  - **Policy Improvement**:\n    - Улучшение политики с помощью детерминированного правила.\n\n### Визуализация и пример\n\n- **Игра как дерево**:\n  - Находимся в состоянии S.\n  - Определяем возможные действия и переходы в другие состояния.\n  - Получаем награду за каждое действие.\n  - Оцениваем ожидаемую награду и дисконтируем её.\n\n### Policy Evaluation\n\n- **Процесс**:\n  - Итеративная процедура для оценки политики.\n  - Сходится к неподвижной точке, подчиняющейся уравнению Беллмана.\n  - Ожидание по политике для оценки награды.\n\n### Policy Improvement\n\n- **Процесс**:\n  - Выбор действия, которое приведет к максимальной награде.\n  - Жадный выбор действия на основе оценки.\n\n### Второй алгоритм\n\n- **Policy Evaluation**:\n  - Оценка политики P для любого состояния S.\n  - Итеративная процедура для получения оценки политики.\n  - Сходится к неподвижной точке, подчиняющейся уравнению Беллмана.\n  - Ожидание по политике для оценки награды.\n\n- **Policy Improvement**:\n  - Выбор действия, которое приведет к максимальной награде.\n  - Жадный выбор действия на основе оценки.",
  "processed_frame_041500.jpg": "### Улучшение политики\n\n- **Policy Improvement**:\n  - Новая политика π' не хуже предыдущей π*.\n  - В любом состоянии S ожидаемая награда от новой политики не меньше, чем от предыдущей.\n\n### Сложность алгоритмов\n\n- **Policy Iteration**:\n  - Фокус на двух этапах: Policy Evaluation и Policy Improvement.\n  - Можно применять и к Q-функции.\n\n- **Value Iteration**:\n  - Оперирует парами состояния-действие.\n  - Сложность увеличивается из-за необходимости обновления большего количества элементов.\n\n### Q-функция и V-функция\n\n- **V-функция**:\n  - Представляется как вектор состояний.\n  - Для Policy Improvement нужно восстановить Q-функцию.\n\n- **Q-функция**:\n  - Представляется как матрица состояний и действий.\n  - Обновление требует больше ресурсов.\n  - Можно сразу брать максимум, пропуская шаг восстановления Q-функции.\n\n### Заключение\n\n- **Выбор между V и Q**:\n  - В будущем останется только один вариант.\n  - Q-функция предпочтительнее для обновления и использования в алгоритмах.",
  "processed_frame_046350.jpg": "### Вдохновение от эволюционных стратегий\n\n- **Идея**:\n  - Вдохновение от успехов эволюционных стратегий.\n  - Движение распределения в сторону элитных траекторий.\n\n- **Элитные траектории**:\n  - Определяются по критерию награды.\n  - Элитные траектории имеют высокую награду.\n\n- **Эффективность**:\n  - Алгоритм может сойтись к хорошему решению.\n  - Неэффективен по количеству сэмплов.\n\n### Ограничения и бюджет\n\n- **Ограничения**:\n  - Люди пытались ограничить бюджет.\n  - Алгоритмы в рамках определенного бюджета.\n\n- **Эффективность**:\n  - Алгоритмы неэффективны по количеству данных.\n  - Тренд на создание алгоритмов в рамках определенного бюджета.\n\n### Индуктивные предпочтения (Inductive Biases)\n\n- **Информация из среды**:\n  - Чем меньше Inductive Biases, тем меньше информации используется.\n  - Эффективность одного сэмпла меньше.\n\n- **Вклад в качество алгоритма**:\n  - Меньше вклад в качество всего алгоритма.\n  - Если популяция большая, вклад отдельного индивидуума менее заметен.\n\n### Заключение\n\n- **Краткий рекап**:\n  - Вдохновение от эволюционных стратегий.\n  - Элитные траектории и их определение.\n  - Ограничения и бюджет.\n  - Индуктивные предпочтения и их влияние на эффективность.",
  "processed_frame_049350.jpg": "### Краткий рекорд лекции\n\n- **Вопрос по домашнему заданию**:\n  - **Policy Iteration**:\n    - Условия остановки.\n    - Необходимость сверки политики.\n    - Две детерминированные оптимальные политики.\n    - Проверка функции удовлетворяет уравнению Беллмана.\n\n### Вторая лекция\n\n- **Цикл в реальном мире**:\n  - В реальном мире среда может быть невидимой.\n  - Эфемерные и шумные награды.\n  - Сложность понимания динамики среды.\n  - Модели могут быть сложными и нетривиальными.\n\n- **Policy Improvement**:\n  - V-функция больше не нужна для восстановления политики.\n  - V-функция не бесполезна, но её использование ограничено.",
  "processed_frame_053350.jpg": "### Вторая лекция (продолжение)\n\n- **Использование V-функции**:\n  - Эмпирическое среднее.\n  - Теоретические применения.\n  - В данном сетапе V-функция не пригодится.",
  "processed_frame_054075.jpg": "### Вторая лекция (продолжение)\n\n- **Q-функция и математическое ожидание**:\n  - Q-функция — это математическое ожидание.\n  - Математическое ожидание можно оценивать по сэмплам.\n  - Использование закона больших чисел для оценки.\n\n- **Траектория и эмпирическое среднее**:\n  - Траектория начинается с состояния \\( s \\).\n  - Действия в траектории.\n  - Эмпирическое среднее награды до конца эпизода.\n\n- **Двумерная структура**:\n  - Первая размерность: усреднение.\n  - Вторая размерность: время внутри траектории.\n\n- **Оценка Q-функции**:\n  - Политика и сэмплы.\n  - Обновление значения Q-функции.\n  - Онлайн-обновление аппроксимации.\n\n- **Недостатки алгоритма**:\n  - Требуется много траекторий и сэмплов.\n  - Траектории конечны, но могут быть очень длинными.\n  - Среда может быть не эпизодической.\n  - Большая дисперсия.\n\n- **Достоинства алгоритма**:\n  - Оценка Q-функции не смещена.\n  - Теоретические гарантии сходимости.\n\n- **Альтернативы**:\n  - Уравнение Беллмана.\n  - Связь между Q-функцией и V-функцией через один шаг.",
  "processed_frame_064000.jpg": "### Третья лекция\n\n- **Ассоциативный ряд и уравнения**:\n  - Четыре уравнения.\n  - Определение лишнего уравнения.\n  - Ответы: право верхние, 4.\n\n- **Анализ уравнений**:\n  - Уравнение 3 выделяется.\n  - Правая часть уравнений: математическое ожидание по распределению.\n  - Уравнение 3: максимум от математического ожидания.\n\n- **Важность**:\n  - Оценка математического ожидания.\n  - Важность для алгоритма.",
  "processed_frame_066600.jpg": "### Четвертая лекция\n\n- **Теория статистической аппроксимации**:\n  - Упрощенная версия.\n  - Оценка математического ожидания от неизвестного распределения.\n  - Доступны только центы.\n\n- **Градиентный спуск**:\n  - Первая интерпретация: градиентный спуск.\n  - Вторая интерпретация: итерационный процесс.\n\n- **Аппроксимация**:\n  - Текущая аппроксимация \\( t_{tk} \\).\n  - Новый сэмпл \\( x_k \\).\n  - Веса для старой и новой аппроксимации.\n\n- **Формула детерминированного пересчета среднего**:\n  - Если \\( \\alpha_k = \\frac{1}{k} \\), то получаем формулу детерминированного пересчета среднего.\n\n- **Онлайн-апдейт среднего**:\n  - Формула онлайн-апдейта для среднего.\n  - От ожидания квадрата разности.",
  "processed_frame_068925.jpg": "### Четвертая лекция\n\n- **Теорема Робинсона-Ван Ро**:\n  - Условия для \\( \\alpha_k \\).\n  - Технические условия.\n  - Последовательность \\( \\theta_k \\) сходится к \\( \\theta^* \\) в \\( L^2 \\).\n  - От ожидания квадрата разности сходится к нулю.\n  - Сильная сходимость.\n\n- **Сходимость**:\n  - Сходимость по распределению.\n  - Сходимость по вероятности.\n  - Из сходимости в \\( L^2 \\) следует сходимость по вероятности и по распределению.\n\n- **Пример для Reinforcement Learning**:\n  - Алгоритм похож на описанную процедуру.\n\n- **Задача статическая**:\n  - Оценка математического ожидания для одного распределения.\n  - В общем случае зависит от состояния и действия.\n\n- **Bellman**:\n  - Дополнительные ограничения.\n  - Действие \\( A^* \\) должно приходить из определенной позиции.",
  "processed_frame_070275.jpg": "### Четвертая лекция\n\n- **Алгоритм Монте-Карло**:\n  - Подобен описанной процедуре.\n  - Использует \\( \\theta_k \\) и \\( \\theta^* \\).\n  - Итерационная процедура для оценки среднего.\n  - Теоретические гарантии.\n\n- **Уравнение Беллмана**:\n  - Используется для улучшения алгоритма.\n  - Q-функция для политики \\( \\pi \\).\n  - Q-функция не только сэмпл, но и таргет.",
  "processed_frame_071600.jpg": "### Вторая лекция\n\n- **Политика \\( \\pi \\)**:\n  - Не только матожидание для \\( G \\), но и награда плюс \\( \\gamma \\) матожидание от \\( Q \\).\n  - Использует один сэмпл.\n  - Информация из среды и текущая оценка.\n  - Не нужно доигрывать эпизоды до конца.\n  - Один шаг в среде.\n  - Таргет: \\( R + \\gamma Q \\).\n  - TD-1: разность между таргетом и текущей оценкой.\n  - Алгоритм TD0.\n  - Bootstrapping: придумывание таргетов на ходу.\n  - Альфа-каты: learning rate, зависит от состояния и действия.\n  - Гиперпараметр, может быть глобальным или зависеть от состояния и действия.\n  - Сходимость: сумма \\( \\frac{1}{k} \\) сходится, сумма \\( \\frac{1}{k^2} \\) сходится.\n\n### Третья лекция\n\n- **Уравнение Беллмана**:\n  - Используется для оптимизации.\n  - Дополнительные ограничения на действие \\( h' \\).\n  - Политика \\( \\pi \\) везде прописывается.\n  - В состоянии \\( s' \\) берется максимум по действиям.\n  - Нет ограничений на состояние \\( s' \\).\n\n### Четвертая лекция\n\n- **Алгоритм при фиксированной политике**:\n  - Сходится к среднему \\( V \\) для всех состояний.\n  - Сходимость к \\( Q^* \\) по \\( L^2 \\) норме.\n  - Алгоритм агностичен к политике.\n  - Сходимость к \\( Q^* \\) независимо от политики.\n  - Серьезное утверждение: алгоритм лучше Монте-Карло.\n  - Рассмотрены три алгоритма.",
  "processed_frame_081925.jpg": "### Вторая лекция\n\n- **Алгоритмы**:\n  - Монте-Карло.\n  - Cool Learning.\n  - Saksa.\n  - Сравнение алгоритмов.\n  - Определение лучшего алгоритма для конкретных ситуаций.\n  - Условие: \\( a' = a_{t+1} \\).\n\n### Третья лекция\n\n- **Теоретические гарантии**:\n  - В состоянии \\( s' \\) (или \\( s_{t+1} \\)).\n  - \\( s' = s_{t+1} \\).",
  "processed_frame_087125.jpg": "### Вторая лекция\n\n- **Алгоритмы**:\n  - \\( a' \\) — действие, которое мы совершим.\n  - \\( s' \\) — новое состояние.\n  - \\( a' = a_{t+1} \\).\n\n### Третья лекция\n\n- **Теоретические гарантии**:\n  - Сумма коэффициентов \\( \\alpha \\) должна быть бесконечной (инфинит визит).\n  - Необходимо посетить каждую пару состояния-действия бесконечное число раз.\n  - Политика должна постоянно исследовать, не пренебрегая парами состояния-действия.\n  - Необходимо оценивать функцию для всех пар состояния-действия для теоретической сходимости.\n  - **Дилемма exploration vs. exploitation**:\n    - Использование знаний для максимизации награды.\n    - Исследование новых действий для получения новой информации.\n    - Политика должна с ненулевой вероятностью посещать все действия для каждого состояния.",
  "processed_frame_091875.jpg": "### Дилемма exploration vs. exploitation\n\n- **Исследование (exploration)**:\n  - Действия могут быть неоптимальными.\n  - Требует времени и ресурсов.\n  - Возможность найти лучшие решения (например, лучший ресторан).\n\n- **Эксплуатация (exploitation)**:\n  - Использование текущих знаний для максимизации награды.\n  - Оптимальные действия на основе имеющейся информации.\n\n- **Баланс между exploration и exploitation**:\n  - Необходимо соблюдать баланс между использованием знаний и исследованием новых возможностей.\n  - Важно быть открытым к новым вещам, но не забывать о текущих знаниях.",
  "processed_frame_094325.jpg": "### Политики в Reinforcement Learning\n\n- **Exploration (исследование)**:\n  - **Пример**: Политика, которая выбирает каждое действие с одинаковой вероятностью.\n  - **Цель**: Исследовать все возможные действия и состояния.\n\n- **Exploitation (эксплуатация)**:\n  - **Пример**: Оптимальная политика, такая как Q-значение (Q-star).\n  - **Цель**: Использовать текущие знания для максимизации награды.\n\n- **Эпсилон-жадная политика (Epsilon-greedy policy)**:\n  - **Описание**: Политика, которая сочетает exploration и exploitation.\n  - **Механизм**:\n    - С вероятностью ε (эпсилон) действует случайно (exploration).\n    - С вероятностью 1 - ε действует жадно (exploitation).\n  - **Пример**: Кидание монетки с вероятностью 0.5 для выбора между случайным и жадным действием.",
  "processed_frame_096500.jpg": "### Политики в Reinforcement Learning\n\n- **Эпсилон-жадная политика (Epsilon-greedy policy)**:\n  - **Параметр**: Эпсилон (ε).\n  - **Механизм**:\n    - С вероятностью ε выбирает действие случайно (exploration).\n    - С вероятностью 1 - ε выбирает действие жадно (exploitation).\n  - **Вероятность жадного действия**:\n    - Вероятность выбрать жадное действие: 1 - ε.\n    - Вероятность выбрать случайное действие: ε / N, где N — количество возможных действий.\n\n- **Стохастическая политика с использованием Q-функции**:\n  - **Описание**: Политика, которая использует Q-функцию для определения вероятностей действий.\n  - **Механизм**:\n    - Использует нейронную сеть для вычисления логитов для каждого действия.\n    - Применяет softmax к логитам для получения вероятностей действий.\n  - **Пример**:\n    - Нейронная сеть возвращает логиты для каждого действия.\n    - Softmax применяется к логитам для получения вероятностей.\n    - Политика становится стохастической, так как действия выбираются с вероятностями, а не детерминированно.",
  "processed_frame_100800.jpg": "### Политики в Reinforcement Learning\n\n- **Эпсилон-жадная политика (Epsilon-greedy policy)**:\n  - **Параметр**: Эпсилон (ε).\n  - **Механизм**:\n    - С вероятностью ε выбирает действие случайно (exploration).\n    - С вероятностью 1 - ε выбирает действие жадно (exploitation).\n  - **Вероятность жадного действия**:\n    - Вероятность выбрать жадное действие: 1 - ε + ε / N, где N — количество возможных действий.\n    - Объяснение:\n      - С вероятностью 1 - ε выбирается жадное действие.\n      - С вероятностью ε выбирается случайное действие, и вероятность выбрать конкретное жадное действие в этом случае равна 1 / N.\n  - **Общая вероятность жадного действия**:\n    - (1 - ε) + (ε / N).\n\n- **Softmax политика**:\n  - **Описание**: Политика, которая использует softmax для вычисления вероятностей действий.\n  - **Механизм**:\n    - Использует нейронную сеть для вычисления логитов для каждого действия.\n    - Применяет softmax к логитам для получения вероятностей действий.\n  - **Температурный параметр (Temperature)**:\n    - Параметр, который влияет на степень стохастичности политики.\n    - Чем выше температура, тем более равномерное распределение вероятностей.\n    - Если температура стремится к бесконечности, политика становится униформенной (uniform distribution).\n\n- **Эпсилон-жадная политика как основной алгоритм**:\n  - **Статус**: Основной алгоритм (first class citizen).\n  - **Цель**: Исследование и эксплуатация в зависимости от значения ε.",
  "processed_frame_104325.jpg": "### Сравнение алгоритмов Q-learning и SARSA\n\n- **Q-learning**:\n  - **Преимущества**:\n    - Может учиться с опыта, собранного из внешних источников (например, наблюдение за другими агентами).\n    - Не требует сэмплирования из текущей политики.\n    - Может использовать максимальное значение Q-функции для обновления.\n  - **Недостатки**:\n    - Может быть менее стабильным из-за использования максимального значения Q-функции.\n    - Не учитывает текущую политику при обновлении Q-значений.\n\n- **SARSA (State-Action-Reward-State-Action)**:\n  - **Преимущества**:\n    - Учитывает текущую политику при обновлении Q-значений.\n    - Более стабильный алгоритм, так как использует текущее действие и следующее состояние.\n  - **Недостатки**:\n    - Требует сэмплирования из текущей политики.\n    - Не может учиться с опыта, собранного из внешних источников.\n\n- **Фундаментальное различие**:\n  - **Q-learning**:\n    - Может учиться с опыта, собранного из внешних источников (например, наблюдение за другими агентами).\n    - Не требует сэмплирования из текущей политики.\n  - **SARSA**:\n    - Требует сэмплирования из текущей политики.\n    - Не может учиться с опыта, собранного из внешних источников.\n\n- **Примеры использования**:\n  - **Q-learning**:\n    - Можно учиться, наблюдая за действиями других агентов (например, старших братьев).\n  - **SARSA**:\n    - Требует самостоятельного взаимодействия с окружающей средой для обучения.\n\n- **Формализация**:\n  - **Q-learning**:\n    - Обучение политики Mew с ее же опыта.\n  - **SARSA**:\n    - Обучение политики On-policy, где обновления Q-значений зависят от текущей политики.",
  "processed_frame_107825.jpg": "### Сравнение On-policy и Off-policy алгоритмов\n\n- **On-policy**:\n  - **Определение**:\n    - Обучение политики с опыта, собранного этой же политикой.\n  - **Примеры**:\n    - SARSA.\n  - **Преимущества**:\n    - Стабильность и консистентность в обучении.\n  - **Недостатки**:\n    - Менее sample-efficient (требует больше сэмплов для обучения).\n\n- **Off-policy**:\n  - **Определение**:\n    - Обучение политики с опыта, собранного другой политикой (behavior policy).\n  - **Примеры**:\n    - Q-learning.\n  - **Преимущества**:\n    - Более sample-efficient (может использовать опыт, собранный другими политиками).\n  - **Недостатки**:\n    - Менее стабильный из-за использования опыта, собранного другой политикой.\n\n- **Примеры использования Off-policy**:\n  - **Имитейшн-лёрнинг**:\n    - Обучение с логов, собранных другими агентами.\n  - **Обучение жадной политики**:\n    - Обучение жадной политики с помощью сэмплов из эпсилон-жадной политики.\n\n- **Формализация**:\n  - **On-policy**:\n    - Обучение политики Mew с ее же опыта.\n  - **Off-policy**:\n    - Обучение target policy (P) с опыта, собранного behavior policy (Mew).\n\n- **Преимущества и недостатки**:\n  - **On-policy**:\n    - Преимущества: стабильность.\n    - Недостатки: менее sample-efficient.\n  - **Off-policy**:\n    - Преимущества: более sample-efficient.\n    - Недостатки: менее стабильный.\n\n- **Дополнительные свойства**:\n  - **Использование предыдущего опыта**:\n    - Возможность использования предыдущего опыта взаимодействия для обучения.",
  "processed_frame_112025.jpg": "### Условия для теоремы Робинсона-Монро и практические аспекты обучения в Reinforcement Learning\n\n- **Теорема Робинсона-Монро**:\n  - **Условия**:\n    - Сумма квадратов альфы (α) должна быть бесконечной.\n    - Сумма альфы должна быть конечной.\n\n- **Практика обучения в Reinforcement Learning**:\n  - **Инициализация**:\n    - Начало обучения без предварительного опыта.\n    - Агент начинает с плохой обученности и собирает первый опыт.\n\n- **Онлайн обучение**:\n  - **Процесс**:\n    - Агент действует в среде с помощью эпсилон-жадной политики.\n    - Собирает опыт и учится онлайн.\n  - **Обновление оценки**:\n    - Агент получает награду и следующее состояние.\n    - Обновляет текущую оценку функции на основе полученного опыта.\n\n- **Q-learning**:\n  - **Обучение**:\n    - Обучение функции Q с использованием сэмплов из другой политики.\n  - **Политика**:\n    - Политика в явном виде не участвует.\n    - Оптимальная политика P* восстанавливается из функции Q.\n\n- **SARSA**:\n  - **Процесс**:\n    - Агент действует в среде.\n    - Получает награду и следующее состояние.\n  - **Обновление**:\n    - Обновляет оценку функции на основе текущего состояния, действия, награды и следующего состояния.\n\n- **Имплементация**:\n  - **Агент**:\n    - Вызывает метод `sample` для получения действия.\n  - **Среда**:\n    - Возвращает награду и следующее состояние.\n  - **Обновление**:\n    - Обновляет оценку функции на основе полученного опыта.",
  "processed_frame_115925.jpg": "### SARSA и Q-learning: Сравнение и оптимизация\n\n- **SARSA**:\n  - **Процесс**:\n    - Два вызова `policy.sample`:\n      - `policy.sample(s)` для текущего состояния `s`.\n      - `policy.sample(s')` для следующего состояния `s'`.\n  - **Обновление Q-функции**:\n    - Оценка Q-функции происходит после получения награды и следующего состояния.\n  - **Оптимизация**:\n    - Возможность переиспользовать результаты предыдущего сэмплирования для экономии вычислений.\n    - В онлайн-режиме всегда будет второй вызов, но результаты могут быть переиспользованы.\n\n- **Q-learning**:\n  - **Процесс**:\n    - Обучение функции Q с использованием сэмплов из другой политики.\n  - **Политика**:\n    - Политика в явном виде не участвует.\n    - Оптимальная политика P* восстанавливается из функции Q.\n\n- **Теоретические гарантии**:\n  - Теоретические гарантии не нарушаются, если просамплировать второй раз.\n  - В конечной ситуации, если Q-функция изменяется (например, нейронная сеть), может потребоваться второй сэмпл.\n\n- **Уравнение Беллмана**:\n  - Ожидание по `s'` и `a'`.\n  - В дискретном пространстве состояний и действий можно использовать весовые суммы для оценки.\n\n- **Алгоритмы и политики**:\n  - **Q-learning**:\n    - Сходится к оптимальной политике.\n  - **SARSA**:\n    - Сходится к политике, которая соответствует текущей политике обучения.\n\n- **Дополнительные вопросы**:\n  - **Армагдер и Softmax**:\n    - Армагдер и Softmax будут рассмотрены на следующем занятии.\n  - **Энтропия**:\n    - Регулирование энтропии в Reinforcement Learning.\n  - **Экспертный алгоритм**:\n    - Алгоритм, который использует весовые суммы для оценки функции.",
  "processed_frame_127525.jpg": "### Сравнение Q-learning и SARSA\n\n- **Q-learning**:\n  - **Сходимость**:\n    - Сходится к оптимальной политике.\n  - **Оптимальная награда**:\n    - Минимальное количество шагов для достижения терминального состояния.\n  - **Политика**:\n    - Оптимальная политика восстанавливается из функции Q.\n  - **Риски**:\n    - Нет встроенного отвращения к риску.\n  - **Пример**:\n    - В лабиринте с обрывами Q-learning может выбрать оптимальный путь, несмотря на риски.\n\n- **SARSA**:\n  - **Сходимость**:\n    - Сходится к политике, которая соответствует текущей политике обучения.\n  - **Политика**:\n    - Использует эпсилон-жадную политику.\n  - **Риски**:\n    - Встроенное отвращение к риску из-за стократической политики.\n  - **Пример**:\n    - В лабиринте с обрывами SARSA выбирает самый безопасный путь, минимизируя риски.\n  - **Парадокс**:\n    - Возможность прыжка с обрыва при взаимодействии с оптимизированной политикой.\n\n- **Сравнение**:\n  - **Q-learning**:\n    - Выбирает оптимальный путь, несмотря на риски.\n  - **SARSA**:\n    - Выбирает безопасный путь, минимизируя риски.\n  - **Политика**:\n    - Q-learning учит оптимальную политику.\n    - SARSA учит политику с минимальной энтропией.",
  "processed_frame_134100.jpg": "### Эффективные последствия\n\n- **Необходимость анализа**:\n  - Если не будем виноваты в том, что не получили от этого отчаяния, то не сможем выяснить, что получили от этого отчаяния."
}