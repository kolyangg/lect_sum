[550.00-569.00] Всё, что я могу сказать, я могу сказать только на этом. [569.00-550.00]  [557.58-559.64] Всем привет. Прошу прощения за задержку. Давайте наступать. Скажите, пожалуйста, меня слышно, видно? [560.62-NA] Да, слышно, видно.
[575.00-576.60] Скажите, пожалуйста, меня слышно, видно? [578.68-579.66] Да, слышно, видно. [593.30-595.28] Так, давайте начнем. [595.70-598.68] Я сразу предупрежу, такой небольшой дисклеймер. [599.14-602.52] Я тут недавно только отошел от непродолжительной простуды, [602.52-575.00]  [577.28-NA] Поэтому, если начну какашить как дед,
[600.00-606.28] отошел от непродолжительной простуды, поэтому если начну кашлять как дед, то постараюсь [606.28-616.28] замиютиться. Но если вас это смущает, то скажите об этом. Итак, у нас сегодня второе занятие, и оно [616.28-624.06] будет посвящено новой не то что парадигме, но мы чуть ли усложним себе задачу. Мы будем работать [624.06-600.00]  [605.18-NA] так называемому model free сеттинги когда про среду нам будет известно все меньше и меньше
[625.00-629.54] Model-free сеттинги, когда про среду нам будет известно все меньше и меньше, [630.32-636.26] а соответственно, какие среды у нас на самом деле присутствуют в реальном мире повсеместно, [636.74-639.48] поэтому они будут иметь для нас особое значение. [640.06-644.90] И в целом, тетап соответствует тому, какие задачи мы будем решать в реальной жизни. [645.50-649.44] Но перед этим давайте вспомним, что мы изучали в прошлый раз. [650.60-651.50] Быстренько пробежимся. [652.34-625.00]  [628.50-NA] главным объектом изучения у нас был...
[650.00-658.46] Быстренько пробежимся. Главным объектом изучения у нас был так называемый марковский процесс принятия решения или сокращенный MDP. [659.00-668.42] MDP состоял из нескольких компонентов. Из action space, то есть тех действий, которые мы можем принимать, [668.58-676.30] те решения, которые мы можем принимать в тех или иных ситуациях и, соответственно, влиять на исход и на ту награду, которую мы получим. [676.82-650.00]  [653.70-NA] Состояние, в котором мы находимся, ну, по сути, не обеспечивает никакой безопасности.
[675.00-676.30] на ту награду, которую мы получим. [676.92-678.80] Состояние, в котором мы находимся, ну, по сути, [679.80-680.70] некоторое такое. [682.24-686.28] Способ описания того, что из себя вообще представляет следа. [687.52-689.44] Третье — это вот такая условная вероятность. [689.68-694.48] То есть, по сути, вероятность того, что, находясь в состоянии S [694.48-698.46] и выполнив действие A, вы перейдете в состояние S3. [699.22-701.18] Тут важное свойство зашито, [702.18-675.00]  [704.98-NA] что эта вероятность зависит от того, что мы делаем.
[700.00-707.72] важное свойство зашито что это вероятность зависит только от текущей пары состоянии действия и не [707.72-717.82] зависит от той истории то есть вам не совсем не вам не важно как именно вы пришли в состоянии [717.82-724.72] штрих если вы знаете состояние предыдущие это действие которое в предыдущем состоянии совершили [724.72-700.00]  [704.78-NA] То есть здесь на самом деле зашита марковость и санкциональность.
[725.00-729.50] То есть здесь, на самом деле, зашита марковость и стационарность. [729.88-735.18] И последнее, мы договорились, что награда у нас будет детерминированной функцией от состояния и действий. [735.50-740.66] В целом, можно рассматривать более общий сетап, где награда стахастическая, [740.82-750.26] но в целях облегчения задачи себе и выкладок на слайде, давайте договоримся, что награда будет детерминированной. [750.26-725.00]  [729.74-NA] А вот так можно было это представить, как Марио бегается, прыгается.
[750.00-753.20] А вот так можно было это представить. [753.40-759.32] Марио бегает, прыгает, вертится, в конце концов собирает какую-то промежуточную награду, [760.38-764.08] в конце концов попадает в трубу, и уровень закончен, переходим к следующему. [764.80-770.00] С точки зрения некоторой формализации мы максимизируем вот такую ожидаемую, [770.72-772.72] дисконтированную кумулятивную награду. [773.70-775.16] По нашей политике. [775.54-750.00]  [754.84-NA] Политика, напомню тоже, это то правило, которое мы
[775.00-784.30] Политика, напомню тоже, это то правило, исходя из которого вы в текущем состоянии принимаете решения. [784.58-788.20] Политика может быть детерминированная, когда это правило детерминировано, [788.44-794.18] и политика может быть стахастической, когда это некоторое вероятное распределение, из которого вы сэмплируете. [797.06-801.00] Дальше у нас появляются новые персонажи. [801.48-775.00]  [778.94-NA] GT — это вот эта кумулятивная награда в целом.
[800.00-810.86] новые персонажи GT это вот эта кумулятивная награда в целом это некоторые случайная величина вот [810.86-816.26] потому что все ее компоненты это тоже случайные величины вот но можно взять от него от ожидания [816.26-824.72] и задаться следующим вопросом какую награду я могу получить из состояния с вот на вопрос на этот [824.72-800.00]  [805.26-NA] Ответ на этот вопрос дает как раз функция, которая
[825.00-830.96] Ответ на этот вопрос дает как раз функция v. [831.32-836.68] v с пометкой p, это значит, что эта функция считается для определенной политики. [836.96-840.72] Ну и по сути, она говорит то, сколько награды мы можем получить, [840.86-846.32] если мы начнем играть из состояния s и дальше будем действовать исходя из политики p. [847.46-852.32] Одной v функции нам будет недостаточно, мы ввели q функцию, [852.32-854.00] вот тоже с пометкой p. [854.58-825.00]  [854.98-NA] Это, конечно, не все.
[850.00-858.94] ввели пью функцию вот тоже с пометкой пи это такое расширение а что будет если находясь состоянии с [858.94-865.44] я выполню действие а затем буду действовать исходя из моей политики сколько награды я получил [865.44-875.32] да это просто рекуррентное соотношение вот этот жить и называется кумулятивный [875.32-850.00]  [854.24-NA] reward или reward to go, часто будем его так называть.
[875.00-879.56] Коммунитивный reward или reward to go, часто будем его так называть, [880.82-887.30] просто считается с помощью дисконтирования к текущему моменту всех immediate reward, [887.40-889.82] которые вы получите на своей траектории. [889.82-898.22] И подчиняется, тут написано, давайте я это пропишу на всякий случай, [899.28-902.10] подчиняется следующему рекуррентному соотношению. [903.10-875.00]  [877.88-NA] GT равняется RT.
[900.00-915.58] рекуррентному соотношению. GT равняется RT плюс гамма GT плюс один. Какие у нас сейчас есть объекты, [915.58-922.50] дискаунд фактор говорит то, насколько будущие награды имеют вклад в наше текущее благосостояние [922.50-900.00]  [904.00-904.76] благосостояния по сравнению с той наградой, которую мы получаем сейчас. [905.34-907.58] То есть предельный случай...
[925.00-927.24] с той награды, которую мы получаем сейчас. [927.86-931.14] То есть в предельный случай, гамма равна нулю, [931.68-933.90] мы максимизируем нашу текущую награду. [934.44-937.58] Ту награду, которую мы получим, если мы выполним действие A, [938.12-939.16] ее мы и максимизируем. [939.60-942.66] Гамма равна единице, все награды на протяжении нашей игры [942.66-943.80] имеют одинаковый вес. [944.56-949.44] Но гамма между нулем и единицей имеет, как принято, [949.44-950.94] какое-то промежуточное значение, [951.08-954.44] типа такое экспоненциальное убывание веса.
[950.00-963.56] значение типа такое экспоненциальное убывание вес и у нас еще есть так называемый длина эпизода вот [963.56-975.12] целом можно рассматривать среды где это равно бесконечности так даже теоретически более удобно [975.12-950.00]  [954.94-NA] Но если t меньше бесконечности, то следа называются эпизодичными.
[975.00-980.22] Но если t меньше бесконечности, то следа называется эпизодической. [980.94-985.06] Соответственно, через какое-то фиксированное количество шагов взаимодействие у вас заканчивается. [985.40-993.42] Часто вы даже знаете, сколько шагов как максимум вам придется проиграть в этой игре. [994.06-998.02] Можете закладывать эту информацию как-то в тейп. [998.02-975.00]  [982.14-NA] На самом деле, я, по-моему, упоминал, что можно закладывать информацию в эпизодических средствах.
[1000.00-1011.68] что можно закладывать информацию в эпизодических следов в State как по такой формуле. [1011.68-1025.82] Типа буквально говорите, на каком промежутке взаимодействия от нуля до единицы вы сейчас находитесь. [1026.38-1028.14] Агент тоже это может эксплуатировать. [1028.66-1000.00]  [1002.00-NA] Вот, например, интересная штука, которая, по-моему,
[1025.00-1032.00] сейчас находитесь. Вот агент тоже это может эксплуатировать. Вот, например, интересная штука, как раз в... [1034.00-1047.98] Сейчас скажу. В муравье, в continuous control задачи, мы рассматривали ее в прошлый раз, где такая туча с четырьмя лапами ходит. [1047.98-1025.00]  [1028.86-1032.02] Там, если вот это сделать, в конце агент, в какой-то момент он, он же не дурак, он поймет, что можно...
[1050.00-1053.56] В конце агент, в какой-то момент он же не дурак, он [1053.56-1059.20] поймет, что можно, что взаимодействие заканчивается через какое-то [1059.20-1061.50] количество шагов, и в конце, чтобы… [1061.50-1064.12] А ему дают награду за то, насколько далеко он отошел [1064.12-1065.82] от своей первоначальной точки. [1065.82-1067.96] Он в конце может прыгать, то есть он буквально перед [1067.96-1071.38] концом эпизода прыгает вперед, потому что ему уже все равно, [1071.38-1072.54] типа упадет, не упадет. [1072.54-1075.84] Ему главное вот добить вот это расстояние, получить [1075.84-1078.98] вот эту большую награду за такой отчаянный прыжок [1078.98-1079.98] в конце.
[1075.00-1079.28] получить вот эту большую награду за такой отчаянный кружок в конце. [1080.10-1081.64] Такие спецэффекты тоже бывают. [1085.00-1086.52] Так, двигаемся дальше. [1087.36-1090.66] Соответственно, да, максимизируем G0 по нашей политике. [1092.08-1094.12] На самом деле, вот это мотожидание, просто напомню, [1094.34-1096.28] у нас два источника стахастики в среде. [1096.54-1098.72] Это политика и сама среда. [1099.58-1102.38] Соответственно, будет вот это мотожидание по P, [1102.38-1075.00]  [1077.60-NA] на самом деле много вложенных в это ожиданий.
[1100.00-1106.30] будет вот этом от ожидания попьет на самом деле много вложенных мы-то жида не чередующихся среда [1106.30-1116.12] политика 3d политика какие у нас были сам что чтобы все работало хорошо в алгоритмах полисе [1116.12-1124.78] его вытер рейшн 1 1 сам шин довольно сильный мягко говоря это то что динамика среды то есть та [1124.78-1100.00]  [1102.48-1105.22] это та вероятность, с которой вы перейдете в то или иное состояние, и состояние S под действием S.
[1125.00-1127.90] вероятность, с которой вы перейдете в то или иное состояние, [1128.24-1132.92] и состояние S, сделав действие A, она известна. [1135.06-1139.92] Не так уж и много средств из реального мира удовлетворяет этому условию, [1140.88-1144.40] поэтому насколько это реалистичный сценарий, судите сами. [1144.92-1149.76] А два других ассамбляжа вроде бы не очень-то и строгие. [1150.26-1125.00]  [1129.84-NA] Говорим, что space конечен, в целом достаточно много.
[1150.00-1160.42] Говорим, что space конечен, в целом достаточно много сред удовлетворяет этому условию. [1160.72-1164.34] И action space конечен, еще больше сред удовлетворяет этому условию. [1166.30-1174.20] Вот если все выполнено, то можем сначала вывести уравнение Беллмана, [1174.20-1179.20] который связывает как раз вейку функции в текущем состоянии.
[1175.00-1180.08] которые связывают как раз в A и Q функции в текущем состоянии [1180.08-1183.74] через в A и Q функции, через состояние, [1183.78-1186.70] в которое потенциально из этого состояния можно попасть. [1188.46-1191.28] Это верно для V функции. [1191.62-1195.58] Для Q функции вы просто в качестве вашей минимальной единицы [1195.58-1197.52] рассматриваете пару состояния действия. [1197.64-1200.36] То есть вы находитесь в состоянии S, делаете действие A [1200.36-1204.62] и дальше смотрите, в какое состояние, во-первых, я могу попасть под этим, [1204.88-1175.00]  [1175.36-NA] делю.
[1200.00-1206.78] и дальше смотрите а в какое состояние во первых я могу попасть под этим делю под действием под [1206.78-1215.12] действием это в действие надо избегать таких татологий сделав определенное действие и какие [1215.12-1223.82] действия могу сделать в этом состоянии с штрих соответственно для любой политики можно как раз [1223.82-1200.00]  [1206.18-NA] выписать эти уравнения в IP и купи. Еще у нас была такая политика, которая лучше всех, оптимальна.
[1225.00-1226.14] уравнения в IP и QP. [1226.60-1229.38] Еще у нас была такая политика, которая лучше всех, [1229.62-1230.52] оптимальная политика. [1231.10-1233.90] Вот для вейку функции, для такой политики [1233.90-1235.94] мы будем называть вейку созвездой. [1236.80-1239.58] Соответственно, здесь уравнения меняются. [1239.76-1243.32] Обратите внимание, что уравнения практически [1243.32-1246.30] остаются теми же с точностью до замены [1246.30-1251.90] от ожидания по среде на максимум. [1252.68-1225.00]  [1254.98-NA] Максимум, сейчас, не посреди, а по полной.
[1250.00-1265.00] На самом деле, если вспомните, у нас было такое утверждение, [1265.00-1270.66] которое на самом деле теорема, что у конечной МДП существует [1270.66-1272.40] детерминированная оптимальная политика. [1272.40-1278.84] Ну и соответственно, здесь это некоторая такая контенсенция [1278.84-1250.00]  [1251.12-NA] утверждения, что
[1275.00-1284.84] это некоторая кинтенсенция этого утверждения что в с звездой кусок для такой политики будут [1284.84-1293.22] подчиняться вот такому уравнению хотел я что-то здесь сказать она кстати обратить внимание что [1293.22-1303.74] почему здесь везде суммы стоят именно из-за вот наших а сам шина в 2 и 3 все от ожидания которые [1303.74-1275.00]  [1276.26-NA] которые на самом деле интегралы, превращающиеся в криптовалюты.
[1300.00-1306.60] а сам что в 2 и 3 все мат ожидания которые на самом деле интегралы превращаются в конечные [1306.60-1312.90] суммы о каком-то конечному носителю соответственно вот так таком в такой удобной форме можно эти [1312.90-1321.50] уравнения записать а и у нас было два алгоритма первые наши алгоритмы для решения задачи в [1321.50-1300.00]  [1308.40-NA] парадигме reinforcement learning. Первый это value iteration, просто вспоминаем, что все четыре эти уравнения это
[1325.00-1330.82] это в аллее террористом просто вспоминаем что все четыре эти уравнения это на самом деле если [1330.82-1338.80] их рассматривать как операторы которые переводят функцию из одного банка из банкового пространства [1338.80-1347.38] себя то есть действует на эту функцию то они на самом деле и отжатие коэффициентом сжатие [1347.38-1325.00]  [1333.42-NA] гамма соответственно что нам говорит теорема
[1350.00-1375.14] Соответственно, что нам говорит теорема банных ограничивающих отображений, что существует единственная неподвижная точка, и если мы запустим алгоритм простой итерации, то какое бы начальное приближение мы ни взяли, мы сойдемся к вот этой неподвижной точке. [1375.38-1350.00]  [1354.84-NA] Соответственно, ну давайте это эксплуатировать, давайте запустим наш алгоритм первый — Value Trade.
[1375.00-1392.78] Соответственно, давайте это эксплуатировать, давайте запустим наш алгоритм первый, Value Iteration. По сути, Value Iteration вскрывает в себя два этапа, объединенных в один. [1392.78-1375.00]  [1387.50-NA] Напомню, что у нас алгоритмы в итоге сводились к двум действиям – Policy Evaluation и Policy Evaluation.
[1400.00-1406.18] к двум действиям – Policy Evaluation и Policy Improvement. [1406.82-1412.16] То есть на этапе Policy Evaluation мы оцениваем текущую политику, [1412.34-1415.48] то есть сколько награды мы можем получить благодаря этой политике. [1415.48-1419.50] На этапе Policy Improvement мы улучшаем политику, [1419.80-1422.48] улучшаем с помощью какого-то детерминированного правила. [1423.64-1425.88] Соответственно, здесь зашли-то два действия. [1425.96-1400.00]  [1405.06-NA] вот это полисе и влечем один шаг управления
[1425.00-1433.70] зашито два действия вот это полисе революция один шаг про мнения один шаг по сути полисе [1433.70-1446.72] полисе в лыжи на для текущей политики а это полисе и полный в вала iteration это не не [1446.72-1452.66] так очевидно но мы сейчас посмотрим на поле ситой шин и там все станет совсем очевидно [1452.66-1425.00]  [1427.34-NA] Типа, где полис им...
[1450.00-1457.06] И там все станет совсем очевидно, где policy evaluation, где policy improvement. [1457.76-1463.10] Ну и просто картинка для того, чтобы визуализировать вот это уравнение. [1463.10-1469.16] Что мы говорим? Мы говорим, на самом деле, что мы можем представить игру как некоторое дерево. [1469.26-1470.58] Мы сейчас находимся в состоянии ST. [1471.86-1474.28] Мы можем... Что мы можем сделать? [1474.36-1477.60] Мы можем посмотреть, а куда мы... [1477.60-1479.48] Какие действия мы можем в этом состоянии совершить? [1479.92-1450.00]  [1451.00-1452.00] Удовольствие от этого не имеет никакого значения. В этом случае, если вы хотите, чтобы я мог предоставить [1452.00-1453.00] вам дополнительные данные, то вы можете предоставлять [1453.00-1454.00] их в виде сайта. [1454.00-1455.00] В этом случае, если вы хотите, чтобы я мог предоставить [1455.00-1456.00] вам дополнительные данные, то вы можете предоставить [1456.00-1457.00] их в виде сайта. [1457.00-1458.00] В этом случае, если вы хотите, чтобы я мог предоставить [1458.00-1459.00] вам дополнительные данные, то вы можете предоставить [1459.00-1460.00] их в виде сайта. [1460.00-1461.00] В этом случае, если вы хотите, чтобы я мог предоставить [1461.00-1462.00] вам дополнительные данные, то вы можете предоставить [1462.00-1463.00] их в виде сайта. [1463.00-1464.00] В этом случае, если вы хотите, чтобы я мог предоставить [1464.00-1465.00] вам дополнительные данные, то вы можете предоставить [1465.00-1466.00] их в виде сайта. [1466.00-1467.00] В этом случае, если вы хотите, чтобы я мог предоставить
[1475.00-1480.76] посмотреть, а куда мы, какие действия мы можем в этом состоянии совершить, куда мы можем попасть. [1481.30-1486.56] Дальше мы совершаем какое-то действие, попадаем сюда, например, [1487.40-1495.10] дальше мы смотрим, попадание, на самом деле, условное, потому что тут черное это действие, [1495.40-1498.66] поэтому мы не переходим в состояние, это просто визуализация. [1500.28-1475.00]  [1480.64-NA] получаем награду здесь это плюс один на самом деле это рт
[1500.00-1507.68] получаем награду здесь это плюс один на самом деле это рт в разных источниках по-разному пишется [1507.68-1512.96] индексного сути не меняя вот а дальше мы смотрим типа в какие состояния мы можем попасть под этим [1512.96-1520.52] под под действием если мы это действие применим в среде ну вот можем попасть сюда можем попасть [1520.52-1500.00]  [1503.24-1504.22] то есть туда, штрих Т плюс один. [1506.72-1509.50] А какую-то награду мы... У каждого этого действия есть оценка.
[1525.00-1531.80] У каждого этого действия есть оценка этого действия, [1531.80-1534.88] сколько награды мы сможем получить, если начнем играть [1534.88-1535.88] из этого действия. [1535.88-1540.02] Соответственно, достаточно посчитать от ожидания вот [1540.02-1543.06] эти награды и дисконтировать в настоящий момент. [1543.06-1552.02] После этого мы сделали так для каждого действия, по [1552.02-1555.00] сути, а дальше мы выбираем действие жадно.
[1550.00-1554.64] Мы так сделали для каждого действия, по сути, а дальше мы выбираем действие жадно. [1554.88-1559.16] Типа, какое действие приведет меня к максимальной награде из этого состояния. [1559.16-1567.32] Вот что говорит нам уравнение Bellman Optimality Equation, [1567.44-1573.30] но здесь мы его используем еще как некоторую последовательность для того, чтобы обновлять нашу функцию. [1577.10-1550.00]  [1556.70-NA] Второй алгоритм.
[1575.00-1585.14] Второй алгоритм состоит, как я говорил, из двух этапов. [1585.40-1589.14] Первый — это policy evaluation, то есть что вы хотите сделать. [1589.22-1596.70] У вас есть политика P, вы хотите для этой политики получить ее оценку в APS, для любого S. [1600.30-1603.90] Соответственно, чтобы это сделать, вот вы запускаете такую итеративную процедуру, [1604.22-1575.00]  [1604.98-NA] Утверждается, что в России есть много интересных и интересных вопросов.
[1600.00-1602.62] Соответственно, чтобы это сделать, вот вы запускаете [1602.62-1606.26] такую итеративную процедуру, утверждается, что она сойдется [1606.26-1618.40] к некоторой, ну, к настоящей VPATS, такой, что VPATS подчиняется [1618.40-1619.40] уравнению Беллмана. [1619.40-1627.50] Вот, то есть сойдется к некоторой неподвижной точке. [1627.50-1600.00]  [1602.50-NA] Здесь как бы логика
[1625.00-1635.38] некоторые неподвижны точно здесь как бы логика логика оценки в функции встань с такая же только [1635.38-1645.22] вы не берете максимум вы от ожидаете по вашей политики типа берете ваши награды с тем весом [1645.22-1651.72] которое ваша политика приписывает как раз этим действием этот процесс называется полностью [1651.72-1625.00]  [1628.28-NA] Когда вы заканчиваете, вы получаете...
[1650.00-1652.46] Этот процесс называется policy evaluation. [1652.92-1656.62] Когда вы заканчиваете, вы получаете полную оценку для вашей политики. [1658.42-1659.78] Что дальше? Когда вы это сделали? [1660.40-1661.84] Можно улучшить политику. [1661.84-1670.34] То есть, если у вас была как раз некоторая пи-штрих, [1671.22-1674.98] вот эта пи-штрих предыдущей политики, ну и здесь, соответственно, пи-штрих, [1675.60-1650.00]  [1655.02-NA] И если вы возьмете по такому моменту, то вы можете
[1675.00-1698.68] Если вы возьмете по такому правилу новую политику, зададите ее с помощью такого правила, то утверждается, что Пи будет не хуже, чем Пи со звездой. [1702.68-1675.00]  [1681.32-NA] «А вот это значит, что...».
[1700.00-1717.88] Это значит, что любого состояния в IP от S будет не меньше, чем в IP от S. [1718.78-1728.46] То есть буквально благодаря этой процедуре вы можете при имеющейся оценке на вашу политику ее детерминированно улучшать. [1729.92-1700.00]  [1701.00-NA] А.
[1725.00-1733.24] на вашу политику ее детерминировано улучшать. И опять же, поскольку у нас четыре уравнения, [1733.24-1741.28] а мы сфокусировались только на двух для ваших, то тут стоит отметить, что то же самое все можно [1741.28-1750.68] сделать и для Q функции, то есть тоже запустить policy iteration, запустить value iteration. [1750.68-1725.00]  [1729.32-NA] что изменится изменится сложность вашего алгоритмом потому что
[1750.00-1752.78] В value iteration что изменится? [1753.02-1758.22] Изменится сложность вашего алгоритма, потому что после этого вы оперируете не только состояниями, [1758.32-1761.14] но еще и парами состояния действия. [1761.14-1767.92] Соответственно, у вас как бы до какого-то произведения получается тех элементов, которые вам нужно обновить. [1768.20-1750.00]  [1762.10-NA] Вот это на самом деле важно, что в данном случае как бы V-функцию можно представить как вектор S1, TTT, S...
[1775.00-1783.58] как вектор, S1, TTT, S, сколько у нас будет действий, S большой. [1785.02-1787.30] А куфунцу можно представить как табличку. [1787.30-1799.32] Табличку, давайте как-то ее более формально напишем. [1800.18-1804.30] Даже не табличку, а матричку, типа количество состояний [1804.30-1775.00]  [1775.40-NA] кина.
[1800.00-1807.68] даже не табличку, а вот матричку типа количество состояний на количество действий. [1808.94-1809.20] Вот. [1810.08-1812.40] Ну и соответственно, если вы используете Q-функцию, [1812.52-1815.00] то обновлять вам нужно целую таблицу, а не просто вектор. [1818.44-1821.14] Важно вот что отметить, что если вы учите V-функцию, [1821.46-1825.44] то прежде чем делать policy improvement, вам нужно восстановить Q-функцию, [1825.44-1800.00]  [1804.56-NA] потому что по функции лежит информация о качестве вашей.
[1825.00-1831.66] функцию потому что по функции лежит информация о качестве ваших действий и уже после этого братья [1831.66-1838.56] максим а если вы учите к функцию то этого этот шаг можно пропустить можно сразу брать от максим [1838.56-1846.06] почему я на это заостряю внимание потому что через непродолжительное время у нас не останется выбор [1846.06-1825.00]  [1832.14-NA] нам придется сойтись к одному из этих вариантов можно догадаться какому именно
[1850.00-1860.32] можно догадаться какому именно. Ну и последнее, что было на семинаре, это если вам все эти Q и V [1860.32-1867.64] функции не понравились и теория не привлекла внимание, а задачи такие решать хочется, то [1867.64-1875.72] можно вдохновиться успехами алгоритмов эволюционных стратегий и запустить похожую процедуру, [1875.72-1850.00]  [1854.14-NA] двигая ваше распределение в сторону элитных траекторий.
[1875.00-1882.68] процедуру двигая ваше распределение в сторону элитных траекторий элитных по отношению к какому-то [1882.68-1887.38] критерию но в данном случае поскольку мы оптимизируем награду элитные будут те у кого награда выше [1887.38-1899.44] соответственно этот алгоритм есть есть надежда что он сойдется к чему-то хорошему насколько он [1899.44-1875.00]  [1880.50-NA] эффективен с точки зрения того сколько сэмпов вы используете он абсолютно не
[1900.00-1907.88] с точки зрения того сколько сэмплов вы используете он абсолютно не эффективен ритмос от ленин в целом [1907.88-1913.18] славится тем насколько он не эффективен по количеству данных которые вам нужно чтобы [1913.18-1921.76] обучиться а вот сейчас и как идет тренд на то чтобы в целом даже не сейчас уже какое-то несколько лет [1921.76-1900.00]  [1905.64-NA] люди пытались ограничить бюджет и типа придумать алгоритм в рамках определенного бюджета.
[1925.00-1931.64] придумать алгоритм в рамках определенного бюджета по взаимодействию с вашей средой. [1931.90-1937.84] Вот эти алгоритмы, поскольку они не предполагают ничего о том объекте, с которым вы работаете, [1937.84-1940.28] то они очень неэффективны. [1940.96-1952.10] Чем меньше Inductive Biases вы вкладываете, тем меньше информации ваш алгоритм использует из среды, [1952.42-1925.00]  [1954.98-NA] тем в целом эффективность одного проекта.
[1950.00-1958.52] информации ваш алгоритм использует из среды, тем в целом эффективность одного сэмпла меньше. [1958.52-1967.76] Меньше вклад в качество всего алгоритма. Если популяция очень большая, то вклад отдельного [1967.76-1978.62] индивидуума уже не такой заметный. Это был краткий рекап нашей лекции. Есть ли у вас какие-то вопросы?
[1975.00-1977.76] Вот, это был краткий рекорд нашей лекции. [1978.00-1978.98] Есть ли у вас какие-то вопросы? [1980.46-1981.28] Наша предыдущая лекция. [1981.56-1983.82] Нет, это еще формально не началось. [1991.68-1993.58] Сергей, у меня небольшой вопрос по домашке, [1993.68-1996.52] ну, про policy iteration в целом. [1997.66-1999.68] У нас там есть условия, что нам нужно остановиться [1999.68-2000.88] по критерию остановы. [2001.62-2003.58] Вот, я хотел бы спросить, нам нужно ли еще сверять то, [2003.60-2004.80] что у нас политика не изменяется?
[2000.00-2000.90] критерию остановы. [2001.68-2003.56] Вот, я хотел бы спросить, вам нужно ли еще сверять то, [2003.60-2004.78] что у нас политика не изменяется? [2006.58-2008.66] Нет, потому что политика... [2008.66-2011.86] На самом деле, если у вас есть два действия, которые [2011.86-2016.08] одинаково хорошие, то есть, у вас есть две детерминированные [2016.08-2019.30] оптимальные политики, то между ними можно прыгать [2019.30-2021.44] и как бы там как повезет. [2022.12-2024.84] Обычно есть на этих действиях у вас какой-то порядок, [2024.84-2028.66] то типа индекс одного действия меньше другого, поэтому [2028.66-2000.00]  [2001.34-NA] нему, а максимум, который будет внести в свою деятельность.
[2025.00-2028.66] типа, индекс одного действия меньше другого, поэтому [2028.66-2031.70] аргмаксимум, который вы будете брать, будет предпочитать [2031.70-2035.36] одно действие относительно другого. [2035.64-2037.26] Но в целом, как бы, это излишне. [2037.44-2042.56] Достаточно проверять только, что функция удовлетворяет [2042.56-2044.02] уравнение Беллмана с какой-то точностью. [2046.04-2046.86] Угу, спасибо. [2053.70-2025.00]  [2027.00-NA] Так.
[2050.00-2063.20] тогда начинаю вторую лекцию просто вот так выглядит у нас вот наш цикл наш вот этот [2063.20-2070.68] фидбэк-клуб агент общается со средой и раньше все было отлично мы про этот мир этот мир был [2070.68-2075.40] нам совершенно понятен вы про него все знали вы могли мат ожидания считать все было шикарно но [2075.40-2050.00]  [2051.70-2052.20] Но в реальном мире, конечно же, не так. [2054.60-NA] В реальном мире мы в худшем случае срезаем...
[2075.00-2087.74] Но в реальном мире, конечно же, не так. В реальном мире мы в худшем случае среду даже видеть не можем. То есть нам просто какие-то эфемерные награды приходят, которые еще бывают довольно шумными. [2087.74-2075.00]  [2083.50-2091.44] и поэтому понять как она реально устроена довольно сложно на каждое явление есть какая-то своя модель может быть несколько с довольно может быть не нетривиальными
[2100.00-2111.10] может быть не нетривиальными дефицитами уравнениями поэтому может быть сложно но вот как бы примеры [2111.10-2120.82] средств которые с динамикой которых и завладать довольно сложно и таких средств большинство то [2120.82-2100.00]  [2108.40-NA] есть вы вряд ли с вероятностью близко к нулю вы будете знать динамику
[2125.00-2135.80] близко к нулю вы будете знать динамику среды с которой вы будете работать. Поэтому то, что [2135.80-2143.12] произойдет, первое наблюдение, чтобы проводить policy improvement, v-функция нам больше не [2143.12-2149.06] понадобится. То есть учить ее смысла больше не будет, потому что не сможем из нее, к сожалению, [2149.06-2153.88] восстановить политику. Это не значит, что она абсолютно бесполезна. Мы увидим, что она много [2153.88-2125.00]  [2128.06-NA] где используется по
[2150.00-2151.98] Это не значит, что она абсолютно бесполезна. [2152.48-2155.40] Мы увидим, что она много где используется по-прежнему, [2155.94-2161.36] но конкретно в нашем сетапе она не пригодится. [2163.72-2166.82] Ну и, да, тут я уже забежал вперед. [2167.88-2169.92] Поскольку Q-функция — это мат ожидания, [2170.62-2177.02] а мы знаем, что мат ожидания можно оценивать по сэмплам из этого распределения, [2177.62-2150.00]  [2152.14-2152.32] взяв эмпирическое среднее. [2153.06-NA] Даже есть теоретические...
[2175.00-2181.42] сэмплом из этого распределения взяв эмпирическое среднее даже есть теоретически гарантия так он [2181.42-2187.76] чисел усиленный усиленный закон чисел больших чисел поэтому ну давайте делаем то же самое [2187.76-2195.82] то есть буквально мы вы находите сейчас состояние ст начинаете играть в но и совершайте действия [2195.82-2175.00]  [2182.46-NA] то есть у вас ваша траектория она возьму другой цвет
[2200.00-2214.58] она возьмут другой цвет она точно уже пойдет сюда вот но но а дальше как бы у вас есть развивка но [2214.58-2221.16] вы начинаете спал вас есть какая-то текущая политика пи да тут важно что это все еще к юг и политику [2221.16-2227.42] мы пока не только нас считается на фиктивы начинаете играть вот у вас одна траектория [2227.42-2200.00]  [2202.58-NA] Вы добегаете до конца.
[2225.00-2234.32] Начинайте играть, вот у вас одна траектория, вы добегаете до конца, посчитали эмульсивную награду до конца эпизода. [2235.42-2236.84] Это как бы один ваш темп. [2237.32-2240.94] Здесь у нас, обратите внимание, у нас есть две размерности. [2241.06-2250.62] Первая размерность, это как бы размерность, по которой мы усредняем, а вторая размерность, это время внутри вашей траектории. [2250.74-2225.00]  [2227.54-2229.38] Соответственно, чтобы получить вам одно наблюдение, вам нужно доиграть до конца.
[2250.00-2256.66] траектории соответственно чтобы получить вам одно наблюдение вам нужно доиграть до конца вашу игру [2256.66-2265.72] что дальше дальше ну например политика вы саплируйте из политика политика не случайно вы или или среда [2265.72-2279.70] стокастическая вы пришли сюда у вас здесь получается даже gta 1 вот это gta 2 ну и и по двум же [2279.70-2250.00]  [2250.28-NA] же сам.
[2275.00-2288.38] то у 2 ну и и по двум же сэмплом оценивать вот забудет g-tow 3 вот у вас 33 числа вы как раз [2288.38-2297.24] обновляйте ваше значение к функции причем но обратите внимание что на самом деле если вдруг [2297.24-2275.00]  [2280.46-2281.46] рук ваша политика. Иногда вам хочется...
[2300.00-2309.92] Иногда вам хочется это делать в онлайне, то есть у вас есть, например, пока две траектории, [2309.92-2317.56] третьих пока нет. Соответственно, вы можете посчитать текущую вашу аппроксимацию, потом, [2317.56-2322.02] когда третья траектория придет, с помощью такой нехитрой формулы, которая на самом деле просто [2322.02-2300.00]  [2306.84-NA] эквивалентно это как бы ваши утре назовем так а вот это ваша
[2325.00-2334.12] А вот это ваша q2, то есть предыдущая аппроксимация. [2334.12-2342.72] Вы ее перевзвешиваете, только не только это, а вот сейчас. [2342.72-2344.48] На самом деле предыдущая аппроксимация будет вот [2344.48-2347.24] это делить на n-1. [2347.24-2325.00]  [2325.78-2328.08] Н минус один. [2328.42-2328.56] Вот. [2329.16-2329.34] Ну, неважно. [2332.76-NA] Ладно, это на самом деле уже излишне означает, что
[2350.00-2354.74] Но не важно, ладно, это на самом деле уже излишнее [2354.74-2355.74] значение. [2355.74-2357.24] Важно то, что вы на самом деле насчитывать можете [2357.24-2360.20] аппроксимацию в онлайне, ну не совсем прям в онлайне, [2360.20-2364.04] но как только у вас есть устоявшееся количество траекторий, вы [2364.04-2367.24] уже можете посчитать вашу аппроксимацию, а потом, когда [2367.24-2370.76] новая траектория придет, посчитать новую аппроксимацию. [2370.76-2379.16] А в чем проблема, как вы думаете, в чем проблема такого алгоритма? [2379.16-2350.00]  [2351.00-2352.00] Ну, много ходить не могу. Я не знаю, что это за шутка. [2352.00-2353.00] Я не знаю, что это за шутка. [2353.00-2354.00] Я не знаю, что это за шутка. [2354.00-2355.00] Я не знаю, что это за шутка. [2355.00-2356.00] Я не знаю, что это за шутка. [2356.00-2357.00] Я не знаю, что это за шутка. [2357.00-2358.00] Я не знаю, что это за шутка. [2358.00-2359.00] Я не знаю, что это за шутка. [2359.00-2360.00] Я не знаю, что это за шутка. [2360.00-2361.00] Я не знаю, что это за шутка. [2361.00-2362.00] Я не знаю, что это за шутка. [2362.00-2363.00] Я не знаю, что это за шутка. [2363.00-2364.00] Я не знаю, что это за шутка. [2364.00-2365.00] Я не знаю, что это за шутка. [2365.00-2366.00] Я не знаю, что это за шутка. [2366.00-2367.00] Я не знаю, что это за шутка. [2367.00-2368.00] Я не знаю, что это за шутка. [2368.00-2369.00] Я не знаю, что это за шутка.
[2375.00-2382.60] Ну, много ходить надо, много сэмплов. [2382.60-2394.24] Да, да, это правда, но вот обратите внимание, что вам, во-первых, нужно какое-то количество траекторий набрать, но сами траектории. [2394.24-2375.00]  [2385.76-NA] Вот какие особенности трек твой?
[2400.00-2408.68] Обратите внимание, где траектория заканчивается. [2408.68-2413.34] Что они вообще конечны. [2413.34-2416.68] Да, да, абсолютно правильно. [2416.68-2420.32] То есть у вас есть предположение, что ваше взаимодействие [2420.32-2423.56] когда-то закончится, и в целом оно может закончиться [2423.56-2426.54] через десять шагов, через десять минут, а может закончиться [2426.54-2429.54] через десять лет. [2429.54-2400.00]  [2401.00-2402.00] После этого, в результате, в результате, в результате, в результате, в результате, в результате, в результате, [2402.00-2403.00] в результате, в результате, в результате, в результате, [2403.00-2404.00] в результате, в результате, в результате, в результате, [2404.00-2405.00] в результате, в результате, в результате, в результате, [2405.00-2406.00] в результате, в результате, в результате, в результате, [2406.00-2407.00] в результате, в результате, в результате, в результате, [2407.00-2408.00] в результате, в результате, в результате, в результате, [2408.00-2409.00] в результате, в результате, в результате, в результате, [2409.00-2410.00] в результате, в результате, в результате, в результате, [2410.00-2411.00] в результате, в результате, в результате, в результате, [2411.00-2412.00] в результате, в результате, в результате, в результате, [2412.00-2413.00] в результате, в результате, в результате, в результате,
[2425.00-2429.00] через десять минут, а может закончиться через десять лет. [2429.00-2431.12] Разница большая. [2431.12-2438.24] Поэтому ждать может быть довольно долго, во-первых. [2438.24-2441.88] Во-вторых, среда может быть не эпизодической. [2441.88-2443.40] Она может вообще никогда не заканчиваться, тогда [2443.40-2445.62] непонятно, а в какой момент останавливаться. [2445.62-2425.00]  [2434.38-NA] А вот это такое основное.
[2450.00-2458.36] Но это такой основной недостаток. [2458.46-2459.86] Есть еще второй недостаток. [2460.34-2461.92] На самом деле, что вы делаете? [2462.04-2467.16] Вы оцениваете Q, это некоторое наджидание, [2467.76-2471.50] вы его оцениваете с помощью эмпирического среднего, [2472.16-2475.16] но на самом деле, если вы посмотрите внутрь этого наджидания, [2475.16-2477.66] то там как бы тоже есть наджидание. [2478.78-2450.00]  [2452.34-NA] А что вы думаете о том, что мы должны делать?
[2475.00-2477.68] то там как бы тоже есть над ожиданием. [2478.78-2481.50] И что вы с этими над ожиданиями делаете? [2481.58-2484.44] Вы их тоже заменяете на сэмплы. [2485.42-2493.88] Причем сэмплы типа R1, потом у вас еще одна награда пришла, [2494.24-2499.60] вы заменяете на сэмпл R2, получается, гамма R2 и так далее. [2499.60-2475.00]  [2480.44-NA] То есть вы внутренним от ожидания заменяете еще на их несмещенную.
[2500.00-2505.56] То есть вы внутренним от ожидания заменяете еще на их несмещенные оценки, [2505.62-2509.80] но по очень маленькому количеству сэмплов, буквально по одному. [2510.30-2512.62] Поэтому у этого алгоритма огромная дисперсия. [2513.48-2514.50] Огромная дисперсия. [2515.36-2518.90] И когда вы запустите... [2518.90-2522.50] Среда сама в себе может быть довольно стахастической. [2522.92-2525.76] Но если у вас и у алгоритма большая дисперсия, [2525.76-2500.00]  [2504.40-NA] то даже с теоретическими гарантиями вы можете сходиться без проблем.
[2525.00-2534.66] дисперсия то даже с теоретическими гарантиями вы можете сходиться бесконечно долго но неужели [2534.66-2540.26] алгоритма есть только одни недостатки на самом деле у него есть достоинства он оценивается то [2540.26-2549.18] что вам нужно причем не смещена и в так у него стереотипе грань а минусы как мы с вами отметили [2549.18-2554.84] это мы должны оканчивать эпизоды доигрывать до конца что в не эпизодических следов не позволить [2554.84-2525.00]  [2525.16-NA] открытия.
[2550.00-2552.52] оканчивать эпизоды, доигрывать до конца, [2552.90-2554.38] что в неэпизодических средах [2554.38-2556.40] непозволительно. И у него большая [2556.40-2558.30] дисперсия. Есть ли [2558.30-2560.40] альтернативы? А вспомню, что у нас на самом деле [2560.40-2562.06] есть [2562.06-2564.02] уравнение Балмана, [2564.20-2565.76] который связывают [2565.76-2568.76] Вэйку [2568.76-2570.02] через один шаг. [2570.58-2572.34] Ну, кстати, давайте поиграем с вами [2572.34-2573.68] в ассоциативный ряд. [2574.18-2575.84] Здесь есть четыре уравнения. [2576.28-2578.46] Какое из них лишнее? Какое вы бы [2578.46-2578.94] выкинули?
[2575.00-2578.84] четыре уравнения какой из них лишний такое было бы выкинули [2578.84-2588.20] какой меньше всего похоже на остальные [2588.20-2603.48] право верхние так право верхние это еще один ответ а вот в чате написали 4 имеется ввиду [2603.48-2575.00]  [2576.30-NA] в виду какой именно.
[2600.00-2609.34] в чате написали 4 имеется ввиду какой именно 4 давайте я вот так вот пронумерую один [2609.34-2621.32] пью со звездой я понял так у нас есть два ответа давайте 3 дождемся и узнаем правильный ответ
[2625.00-2635.18] Похоже, третий будет мой. [2635.18-2640.32] В общем, самое странное уравнение, не похожее на [2640.32-2647.20] другое, — это вот это уравнение, потому что все остальные [2647.20-2651.76] представляют из себя правой частью от ожидания по какому-то [2651.76-2625.00]  [2627.70-NA] распределению, а здесь максимум.
[2650.00-2652.30] Правая часть — это мотожидание по какому-то распределению, [2652.80-2656.54] а здесь максимум от мотожидания. [2657.82-2659.12] Почему для нас это важно? [2662.72-2663.92] Почему для нас это важно? [2664.04-2667.24] Потому что есть такая теория статистической аппроксимации, [2668.56-2672.26] которая в упрощенном виде, она на самом деле более сложная, [2672.86-2676.42] но в упрощенном виде, в том сетапе, в котором мы сейчас работаем, [2676.42-2650.00]  [2653.00-NA] Мы хотим оценить надожидание от...
[2675.00-2682.44] сетапе в котором мы сейчас работаем мы хотим оценить над ожидания от неизвестного нам распределения из [2682.44-2688.32] которого нам в лучшем случае доступны центу соответственно можно запустить вот такой [2688.32-2696.66] итерационный процесс обратить внимание что он похож на всем нам известный алгоритм градиентного [2696.66-2675.00]  [2683.24-NA] градиентного спуска и это первая интерпретация вторая вот
[2700.00-2705.52] И это первая его интерпретация, вторая его интерпретация [2705.52-2710.62] через вот такое вглаженное среднее. [2710.62-2715.30] То есть у нас есть уже текущая аппроксимация ttk, когда нам [2715.30-2721.20] приходит новый сэмпл xk, мы с какими-то весами берем [2721.20-2724.04] старую аппроксимацию, с какими-то весами берем новую [2724.04-2725.04] аппроксимацию. [2725.04-2700.00]  [2705.00-NA] Если αk равняется один делить на k, то это будет равен
[2725.00-2737.12] Если альфа k равняется 1 делить на k, то мы просто получим формулу детерминированного пересчета среднего. [2737.76-2745.02] Если у вас есть k-минус один элемент, вы посчитали среднее, у вас приходит k-элемент. [2745.02-2752.24] Это такая формула онлайн-апдейта для вашего среднего, которую вы можете использовать. [2754.18-2725.00]  [2727.76-NA] А что вы думаете о том, что мы должны делать?
[2750.00-2752.24] который вы можете использовать. [2754.24-2757.00] Что? Зачем это нам? [2757.36-2760.26] На самом деле есть такая теорема, теорема Робинсона-Ван Ро, [2760.68-2765.76] которая утверждает, что если альфа-каты удовлетворяют такому условию, [2766.24-2768.72] и есть еще некоторые технические условия, [2768.72-2775.98] то последовательность тет-такатых сходится к тет-созвездой в L2. [2776.32-2750.00]  [2754.02-NA] Это значит, что мы от ожидания квадрата разности...
[2775.00-2780.66] В L2 это значит, что от ожидания квадрата разности сходится к нулю. [2782.08-2784.10] Довольно сильная сходимость. [2784.72-2790.00] Из этой сходимости, в частности, следует сходимость по распределению и сходимость по... [2791.04-2794.00] Исходимость по распределению сходится, когда следует сходимость по... [2794.00-2794.94] Ой, извините. [2795.68-2800.60] Из сходимости в L2 следует сходимость по вероятности, [2800.74-2803.50] из сходимости по вероятности следует сходимость по распределению. [2804.30-2775.00]  [2776.00-2777.00] Вот, то есть, вираж, который мы видим, это, конечно, вираж, который мы видим в этом случае, это вираж, который [2777.00-2778.00] мы видим в этом случае, это вираж, который мы видим [2778.00-2779.00] в этом случае, это вираж, который мы видим в этом [2779.00-2780.00] случае, это вираж, который мы видим в этом случае, [2780.00-2781.00] это вираж, который мы видим в этом случае, это вираж, [2781.00-2782.00] который мы видим в этом случае, это вираж, который [2782.00-2783.00] мы видим в этом случае, это вираж, который мы видим [2783.00-2784.00] в этом случае, это вираж, который мы видим в этом [2784.00-2785.00] случае, это вираж, который мы видим в этом случае, [2785.00-2786.00] это вираж, который мы видим в этом случае, это вираж, который [2786.00-2787.00] мы видим в этом случае, это вираж, который мы видим в этом [2787.00-2788.00] случае, это вираж, который мы видим в этом случае, это вираж, который [2788.00-2789.00] мы видим в этом случае, это вираж, который мы видим в этом
[2800.00-2804.68] вероятности необходимость по вероятности исходимость по распределению то есть в [2804.68-2812.60] иерархии этот вид сходимости довольно высоко зачем это нам как вдохновляющий пример для [2812.60-2821.24] того как бы что что мы что мы сделали какой алгоритм у нас уже был вот монтекарло полисе [2821.24-2800.00]  [2807.92-NA] Сервалейшн у нас был алгоритм, который на самом деле очень похож на вот эту процедуру.
[2825.00-2837.42] похож на на вот эту процедуру то есть у нас есть какая-то здесь правда с точной замены минус на [2837.42-2843.84] плюс вот как бы тета-ката это наша текущая проектироваться на тюка то это наша тета-ката [2843.84-2853.86] терминов джека от это наша x как и вот соответственно с помощью такой итерационной [2853.86-2825.00]  [2826.00-NA] процедуры мы как раз
[2850.00-2861.56] С помощью такой итерационной процедуры мы как раз сможем оценивать наше среднее, и у нас даже есть теоретические гарантии. [2864.14-2850.00]  [2860.12-2868.28] что можно да мы уже мы уже знаем что-то горит не идеальный его есть недостатки можно сделать лучше давайте вспомним что на самом деле у нас есть уравнение балмана и мы можем как бы из
[2875.00-2880.38] у нас есть уравнение Беллмана, и мы можем использовать [2880.38-2889.22] это уравнение Беллмана, чтобы придумать наш следующий [2889.22-2891.94] сэмпл, таргет, на который мы будем учиться. [2891.94-2875.00]  [2888.06-NA] То есть QK, просто Q функция для политики Пи — это не только
[2900.00-2928.08] Для политики P это не только матожидание для G, для кумулятивной награды, но это еще и награда плюс гамма матожидание от Q, Q'H'. [2928.08-2900.00]  [2901.92-NA] Вот, мы-то ждали его.
[2925.00-2934.70] штрих а штрих вот мы-то ждали по штриха штрих вот соответственно как бы можно использовать [2934.70-2948.12] можем использовать и плюс гамма на состоянии штрих а стрикер состоянии штрих действие а [2948.12-2925.00]  [2931.06-NA] Почему это лучше? Потому что здесь мы как бы используем один сэмпл.
[2950.00-2959.16] здесь мы как бы используем один сантом один информацию о среде при этом мы используем как [2959.16-2969.50] бы нашу текущую оценку для и кучу оценку то текущую аппликацию по функции и нам не нужно [2969.50-2977.30] доигрывать эпизоды конца буквально нужно сделать получается один шаг среде то есть и состояние [2977.30-2950.00]  [2952.58-NA] С', сделав действие A,
[2975.00-2983.34] шаг среде то есть и состоянии с штрих сделав действие перейти состоянии здесь есть индекс [2983.34-2991.50] sts то плюс один и состоянии ст перейти состоянии 100 плюс 1 а дальше посмотреть [2991.50-2997.98] просамплировать состояние от и плюс один из этого состояния 100 плюс 1 и вот для него [2997.98-2975.00]  [2981.88-NA] посчитать и аку функции с точки зрения с точки зрения каких-то терминов
[3000.00-3009.88] С точки зрения каких-то терминов, вот у нас R плюс гамма Q будет называться таргетом, [3010.28-3012.80] а вот эта разность будет называться temporal difference. [3012.80-3015.80] Отсюда и название алгоритма. [3016.46-3022.52] На самом деле алгоритм TD0 называется. [3023.32-3000.00]  [3007.10-NA] ТД-1 это монтаж.
[3025.00-3030.16] CD1 это монтагарда. [3030.16-3034.44] Потом чуть позже поймем почему от 0 до 1. [3034.44-3040.58] В общем, вот такой алгоритм позволяет не дайвировать [3040.58-3043.44] эпизоды до конца, позволяет использовать и информацию [3043.44-3046.50] из среды и свою текущую аппроксимацию. [3046.50-3050.96] Буквально, смотрите, раскрывается важная фича обучения с [3050.96-3051.96] подкреплением. [3051.96-3025.00]  [3026.56-NA] Таргеты нам никто не дал?
[3050.00-3076.24] Обращение с подкреплением. Таргеты нам никто не дал. Настоящие таргеты довольно шумные. Вот этот таргет шумный. Давайте как бы тянуть за волосы сами себя. Придумывать себе таргет на ходу, выкладывать перед собой железную дорогу, чтобы ехать на нашем паровозе. [3076.88-3050.00]  [3053.76-NA] Соответственно, вот, таргеты мы придумываем на ходу и
[3075.00-3080.70] на нашем паровозе. Соответственно, вот, таргеты мы придумываем на ходу, используя нашу текущую [3080.70-3089.06] аппроксимацию. Этот процесс еще называется bootstrapping, но не тот bootstrapping, когда мы из распределения [3089.06-3075.00]  [3089.28-NA] а именно в контексте reinforcement learning. Вопросы? Сергей, можно вопрос про альфа-каты?
[3100.00-3103.76] Сергей, можно вопрос про альфа-каты? [3103.76-3106.72] В данном контексте почему она зависит от состояния [3106.72-3109.56] действия и вообще где ее взять? [3109.56-3114.20] Альфа-каты можно воспринимать как learning rate, и в целом для [3114.20-3116.76] каждой пары состояния действия он может быть свой. [3116.76-3121.18] Ну, то есть это гиперпараметр, который мы изначально как-то [3121.18-3122.18] задаем? [3122.18-3124.18] Или последовательный гиперпараметр. [3124.18-3129.98] Например, альфа-каты равна один делить на х.
[3125.00-3134.16] альфа-каты равна один делить на k. Это хороший вопрос, вы поймете почему важно чтобы он еще [3134.16-3142.50] зависел состояние и действие. Ну вот здесь получается глобальный параметр который не зависит от состояния и [3142.50-3154.00] действия и он кстати даже подчиняется условиям Робинсона Монро. Сумма один делить на k расходится, а сумма [3154.00-3125.00]  [3126.00-NA] думаю.
[3150.00-3160.74] сумма один делить на карте сходится а сумма один делить на к квадрат и квадрат на шесть [3160.74-3170.20] неважно короче она сходится так что вот вот пример почему сейчас поймем почему важно что [3170.20-3175.38] почему альфа к ты должна зависит и они действия вот представьте что [3175.38-3150.00]  [3154.60-NA] что мы как бы находимся в тесном порядке.
[3175.00-3185.86] что мы как бы находимся в мы пытаемся подтянуть себя под действие вот этой теоремы здесь у нас [3185.86-3192.62] задач задача статическая у нас мы оцениваем одном от ожидания для одного распределения здесь у нас [3192.62-3199.56] этих распределений по количеству пар состоянии действия вот поэтому как бы и в общем случае [3199.56-3175.00]  [3177.70-NA] альфа-каты может зависеть от состояния действий.
[3200.00-3214.22] может зависеть от состояния действий. Еще один вариант, здесь мы использовали Bellman [3214.22-3221.04] expectation equation. Давайте использовать Bellman optimality equation. Вот используем такой таргет, [3221.04-3225.44] кажется, что разница небольшая, на самом деле обратите внимание, что здесь у нас есть [3225.44-3200.00]  [3203.32-3204.54] есть дополнительные ограничения, то действие «Аштрик» должно прийти из той позиции, которую мы уже говорили.
[3225.00-3230.38] здесь у нас есть дополнительные ограничения, то действие h' должно прийти из той политики, [3230.38-3243.84] которую мы оцениваем. Мы все еще мысленно здесь везде прописываем π. А здесь такого ограничения [3243.84-3250.08] нет, потому что в состоянии штрих мы берем максимум его действия. При этом у нас нет ограничений, [3250.08-3225.00]  [3229.92-NA] откуда приходит состояние S&A, поэтому в целом как бы...
[3250.00-3259.48] откуда приходит состояние с ей поэтому в целом как бы кажется есть такое ощущение что этот алгоритм [3259.48-3267.18] чем-то лучше чем чем чем но мы поймем поймем какие свойства есть у того и другого просто сам факт [3267.18-3273.76] того что здесь по-прежнему нужно обеспечить что и штрих у вас приходят именно из той среды для [3273.76-3250.00]  [3252.32-NA] для которой вы пытаетесь выучиться.
[3275.00-3281.96] Кстати, вопрос. [3281.96-3285.86] При фиксированной политике к чему вот такой алгоритм [3285.86-3286.86] сойдется? [3286.86-3292.76] Фиксируемую политику Пи? [3292.76-3275.00]  [3286.62-NA] Кому сойдется такой алгоритм?
[3300.00-3320.94] хочется сказать что там к среднему и сойдется среднем очень почему именно ну какого-то [3320.94-3327.24] среди аналогию как бы с полисе трейшн сказать что вот как бы мы там мы там тоже находили [3327.24-3300.00]  [3302.44-NA] или там V для всех стритов.
[3325.00-3331.96] что вот как бы мы там мы там тоже находили с там в для всех стоит of прификсированной политики ну [3331.96-3340.90] извая можно получить кью вот так и сделаем к этому и сидя у нас здесь ваших нет у нас есть только [3340.90-3350.44] кружка я так понимаю но смотрите у нас есть политика пи у вас есть некоторые настоящее значение функции [3350.44-3325.00]  [3329.22-NA] Утверждается, что...
[3350.00-3364.52] утверждается что вот этот процесс сойдется как юг и вот настоящих кипи по l2 до поля 2 [3364.52-3378.18] если все все условия тире он будет выполнен ну как минимум давайте давайте более слабое [3378.18-3350.00]  [3351.78-NA] освободления по вероятности.
[3375.00-3379.50] Ну, как минимум, давайте более слабое распределение. [3379.50-3380.76] По вероятности, точно сойдется. [3380.76-3389.50] Соответственно, теперь вопрос, к чему сойдется вот этот [3389.50-3390.50] алгоритм? [3390.50-3397.50] — Ну, к Q звездочкам. [3397.50-3401.50] — К Q звездочкам. [3401.50-3375.00]  [3378.50-NA] Да, он подойдет как звездочка, причем, как бы, в том числе,
[3400.00-3409.02] да он подойдет а какие звездочку причем неважно обратить внимание что этот алгоритм агностика [3409.02-3415.96] по отношению к политике то есть несмотря на то что действие у вас могут приходить из какой-то [3415.96-3421.18] политики может быть разных политик здесь нет ограничений то сойдетесь все равно к юсу звездой [3421.18-3400.00]  [3408.82-NA] И у этого есть... Это довольно серьезное утверждение, потому что...
[3425.00-3433.28] есть это довольно серьезное утверждение потому что если это так то кажется что вот этот алгоритм всем [3433.28-3440.88] всем лучше и если заезд и создать давайте потом восстановим детерминированным образом исключить [3440.88-3453.72] нашу политику и считаемые мы решили задачу целом вот мы с вами по сути рассмотрели три алгоритма [3453.72-3425.00]  [3426.24-NA] сейчас Монте-Кай.
[3450.00-3454.28] Мы, по сути, рассмотрели три алгоритма сейчас. [3454.54-3455.14] Монте-Кайло. [3459.10-3460.42] Этот называется Cool Learning. [3464.14-3465.96] Это называется Saksa. [3470.58-3473.12] Надо понять, надо на чем-то остановиться [3473.12-3478.66] или понять, в каких ситуациях тот или иной алгоритм лучше.
[3475.00-3484.54] каких ситуациях тот или иной алгоритм лучше. Давайте как бы препарировать свойства каждого из этих [3484.54-3495.50] алгоритмов, но прежде чем это сделаем, давайте обратим внимание на вот это условие. У нас, [3495.50-3475.00]  [3484.44-NA] знаете такой перед придав между тем чтобы как можно сильнее удовлетворять как
[3500.00-3515.24] сильнее удовлетворять каким-то формальным условиям можно было создать а штрих это что а штрих здесь [3515.24-3524.38] или или вот здесь ну вот это а штрих это то действие которое мы сделаем состояние с с штрих [3524.38-3500.00]  [3505.38-NA] СТ плюс один. Короче, А штрих это АТ плюс один вот на этой картинке.
[3525.00-3530.00] Короче, а штрих это а т плюс один вот на этой картинке. [3530.00-3532.00] В состоянии с штрих. [3532.00-3534.00] С т плюс один. [3534.00-3537.00] Ну, да, с штрих равняется с т плюс один. [3537.00-3539.00] Вот в этом состоянии. [3539.00-3541.00] Ага, спасибо большое. [3541.00-3544.00] Так. [3544.00-3547.00] Пытаюсь зарекарить, о чем я говорил. [3547.00-3553.00] Да, предофф, мы пытаемся как можно ближе быть к каким-то теоретическим гарантиям, [3553.00-3525.00]  [3527.00-NA] каким-то теоретическим гарантиям, но при этом...
[3550.00-3556.00] как можно ближе быть к каким-то теоретическим гарантиям но при этом хотим расширять скуп [3556.00-3563.68] тех задач которые мы можем решать вот одно из теоретических требований да и одно из требований [3563.68-3572.08] теоремы это чтобы сумма коэффициентов альфа-каты была бесконечность так называемый инфинит визит [3572.08-3578.14] и почему инфинит визит ищут потому что чтобы эта сумма была бесконечная необходимо как минимум [3578.14-3550.00]  [3551.84-NA] чтобы в ней было бесконечное число флагов.
[3575.00-3580.32] Чтобы эта сумма была бесконечной, необходимо как минимум, чтобы в ней было бесконечное число слагаемых. [3580.98-3589.00] Чтобы в ней было бесконечное число слагаемых, вы должны пару состояния действия SA посетить бесконечное число раз. [3591.06-3592.06] Что это значит? [3592.18-3600.94] Это значит, что у вас политика, с помощью которой вы в среде действуете, должна постоянно исследовать. [3600.94-3575.00]  [3604.98-NA] То есть она не может принять в виду, что это не так.
[3600.00-3608.70] исследовать, то есть она не может пренебрегать какими-то парами состояния действия, потому что [3608.70-3619.24] в противном случае вы для этой пары не сможете сойтись к оптимальному значению. [3619.70-3626.36] Может быть, оно вам и не надо. Хороший вопрос. Нужно ли вам оценивать функцию для всех пар [3626.36-3600.00]  [3601.00-3603.64] состояние действий. Скорее, нет, потому что в некоторых условиях, в которых
[3625.00-3631.96] состоянии пар состоит скорее нет потому что в некоторое состояние вы попадете с вероятностью [3631.96-3636.82] ноль просто потому что они но слишком плохие вы не хотите там быть поэтому вы не хотели там [3636.82-3646.10] оценивать к функцию но с точки зрения теории чтобы все то везде сходилось нужно чтобы каждую пару [3646.10-3654.28] посетили бесконечность ну и в целом как бы что это обеспечивает что достаточно чтобы это произошло [3654.28-3625.00]  [3626.00-3627.00] И, конечно, вам нужно перейти к следующему вопросу. Вам нужно перейти к следующему вопросу. [3627.00-3628.00] Вы можете перейти к следующему вопросу. [3628.00-3629.00] Вы можете перейти к следующему вопросу. [3629.00-3630.00] Вы можете перейти к следующему вопросу. [3630.00-3631.00] Вы можете перейти к следующему вопросу. [3631.00-3632.00] Вы можете перейти к следующему вопросу. [3632.00-3633.00] Вы можете перейти к следующему вопросу. [3633.00-3634.00] Вы можете перейти к следующему вопросу. [3634.00-3635.00] Вы можете перейти к следующему вопросу. [3635.00-3636.00] Вы можете перейти к следующему вопросу. [3636.00-3637.00] Вы можете перейти к следующему вопросу. [3637.00-3638.00] Вы можете перейти к следующему вопросу. [3638.00-3639.00] Вы можете перейти к следующему вопросу. [3639.00-3640.00] Вы можете перейти к следующему вопросу. [3640.00-3641.00] Вы можете перейти к следующему вопросу. [3641.00-3642.00] Вы можете перейти к следующему вопросу. [3642.00-3643.00] Вы можете перейти к следующему вопросу. [3643.00-3644.00] Вы можете перейти к следующему вопросу.
[3650.00-3656.68] обеспечивают что достаточно чтобы это произошло вам нужно когда статистическая политика которая [3656.68-3666.92] с ненулевой вероятностью для каждого состояния ходит вовсе по все действия возвращает и действия [3666.92-3673.62] для фиксированных сцене с какой-то не верой вероятности давайте посмотрим на две стороны [3673.62-3650.00]  [3656.40-NA] стороны спектра.
[3675.00-3685.26] это нас приводит на самом деле к такой довольно релевантной дилемме которые в реальной жизни [3685.26-3695.20] появляется а именно exploration exploitation то есть какое когда можно свои знания о мире возможно они [3695.20-3704.56] не полны использовать для того чтобы максимизировать свою награду какую-то а когда лучше по исследовать [3704.56-3675.00]  [3677.46-NA] по
[3700.00-3709.88] свою награду какую-то а когда лучше по исследовать по уходить по недавно открытым рестораном чтобы [3709.88-3718.84] найти какой-то получше вы за это платите у вас есть так называемый черный тип коз то есть пока [3718.84-3727.06] вы исследуете вы действуете неоптимально ну или или тратите время на какие-то неоптимальные действия [3727.06-3700.00]  [3702.94-NA] действия, но опять же, в конечном итоге, если вы...
[3725.00-3727.44] какие-то неоптимальные действия. [3727.74-3729.30] Но опять же, в конечном итоге, [3729.78-3731.90] если вы найдете какой-то, [3732.06-3733.32] в данном случае ресторан, [3733.40-3735.22] который намного лучше, [3735.90-3738.40] чем то, куда вы ходили до этого, [3738.60-3741.96] то все ваши страдания опутаются. [3742.42-3743.32] Но это может быть, [3743.74-3744.88] и это может и не случиться. [3745.44-3747.44] Поэтому тут, опять же, [3747.44-3749.34] нужно какой-то баланс соблюдать. [3749.86-3751.22] Соблюдать между тем, [3752.46-3753.44] между тем, чтобы пользоваться [3754.04-3725.00]  [3726.56-NA] пользоваться всем своим знанием.
[3750.00-3754.76] соблюдать между тем, чтобы пользоваться всем своим [3754.76-3759.72] знанием, опытом жадно, и тем, чтобы быть открытым [3759.72-3766.96] к каким-то новым вещам, скажем так. [3766.96-3771.72] Не лекция по философии, чисто пытаюсь на пальцах объяснить, [3771.72-3773.62] что это значит. [3773.62-3778.18] Давайте вернемся в более формальное русло. [3778.18-3750.00]  [3751.82-NA] Вот у нас есть две стороны спектра.
[3775.00-3783.10] более формальную более формальное русло вот у нас есть две стороны спектра exploration чисто [3783.10-3791.26] exploration это вот прям самый самый что ни на есть экспрессия это просто случайно политика [3791.26-3797.08] политика которая в каждом состоянии выбирает каждое действие с одинаковой вероятности по [3797.08-3802.90] другую сторону спектра у нас жадная политика причем жадная данном случае оптимально политик [3802.90-3775.00]  [3777.08-NA] Есть, например, кулу.
[3800.00-3805.88] Причем жадная в данном случае оптимальная политика, например, Q со звездой. [3809.88-3811.80] Кстати, не обязательно. [3812.10-3817.24] Если у вас есть какая-то текущая оптимация Q, вы можете действовать жадно по отношению к ней, [3817.42-3821.54] но тут зависит от того, насколько ваша оптимация далека от реальности. [3821.68-3825.48] Если она близка, то вы будете действовать около оптимальной или оптимальной. [3825.48-3829.58] Если она плоха, то вы можете действовать сколько угодно неоптимально. [3829.64-3800.00]  [3801.00-3802.00] Важно считать, что в этом случае мы должны поддержать и поддержать общую связь между людьми. [3802.00-3803.00] В этом случае мы должны поддержать общую связь между людьми. [3803.00-3804.00] В этом случае мы должны поддержать общую связь между людьми. [3804.00-3805.00] В этом случае мы должны поддержать общую связь между людьми. [3805.00-3806.00] В этом случае мы должны поддержать общую связь между людьми. [3806.00-3807.00] В этом случае мы должны поддержать общую связь между людьми. [3807.00-3808.00] В этом случае мы должны поддержать общую связь между людьми. [3808.00-3809.00] В этом случае мы должны поддержать общую связь между людьми. [3809.00-3810.00] В этом случае мы должны поддержать общую связь между людьми. [3810.00-3811.00] В этом случае мы должны поддержать общую связь между людьми. [3811.00-3812.00] В этом случае мы должны поддержать общую связь между людьми. [3812.00-3813.00] В этом случае мы должны поддержать общую связь между людьми. [3813.00-3814.00] В этом случае мы должны поддержать общую связь между людьми.
[3825.00-3830.00] оптимально если она плохая то вы будете можете действовать сколь угодно не оптимально важно что [3830.00-3837.42] эта политика детерминированная она никогда никогда не исследует то вот можете ли вы предложить [3837.42-3846.30] политику которая лежит где-то посередине между этими но можно просто взять какую-то вероятность [3846.30-3852.80] и сложить эти две политики с вероятностями мы кидаем монетку с вероятностью 05 например делаем [3852.80-3825.00]  [3827.14-NA] действуем случайно, с вероятностью 0,5.
[3850.00-3855.26] То есть мы кидаем монетку с вероятностью 0,5, например, действуем случайно с вероятностью 0,5 жадно. [3856.30-3858.02] Да, да, абсолютно верно. [3859.66-3861.84] Это называется Эпсилон-жадный поликлип. [3861.84-3867.30] То есть мы вводим новый параметр Эпсилон, который буквально говорит нам о том, [3867.50-3870.16] что с вероятностью Эпсилона мы можем выбирать действие случайно, [3870.24-3872.90] а с вероятностью 1,5 Эпсилона выбирать действие жадно. [3873.76-3874.84] В целом, довольно неплохо. [3876.82-3850.00]  [3851.00-3853.46] плохо. Вопрос на засыпку. [3853.46-3854.62] Какая вероятность...
[3875.00-3884.44] вопрос на засыпку какая вероятность в этой политике выбрать жадное действие это [3884.44-3901.32] написано же один или вопрос не в этом вопрос в этом она смотрите один нас [3901.32-3875.00]  [3878.68-NA] Эпсилона, это то, что у вас есть здесь развилка, но еще...
[3900.00-3906.22] но смотрите один из акций на то что у вас есть здесь развилка но еще вы когда выбираете случайное [3906.22-3917.16] действие у вас есть вероятность выбрать это действие поэтому вероятность но один из их [3917.16-3925.92] сон до плюс плюс epsilon делить на н н количество возможных действий за это позицию да абсолютно [3925.92-3900.00]  [3903.68-NA] То есть вот такая у вас вероятность выбрать действие.
[3925.00-3936.62] Да, абсолютно верно. То есть вот такая у вас вероятность выбрать жадное действие в отношении к вашей текущей аппроксимации ку. [3937.08-3946.12] Еще вопрос. Можете ли вы придумать какую-то еще политику, которая стокастическая, но которая использует ку-функцию? [3949.78-3925.00]  [3933.88-3938.88] Опять же, можно несколько шагов делать аргументскую функцию.
[3950.00-3958.76] опять же можно несколько шагов делать ароматской функции а потом на ком-то на каждом кодам шаги [3958.76-3967.72] делать шаг случайно смотрите важно что политика определяет ваши действия в конкретном состоянии [3967.72-3975.36] то есть давайте зафиксируем состояние из из этого действия из этого состояния у вас есть есть один [3975.36-3950.00]  [3954.64-NA] Вот, например, вот это, например, вот это, например, вот это,
[3975.00-3978.14] А1, А2, А, А, Н. [3979.22-3983.26] Вот можете ли вы придумать такую политику, [3984.00-3985.48] которая зависит от состояния С? [3988.68-3991.10] Что она, с одной стороны, она случайная, [3991.42-3993.42] то есть она стахистическая, [3993.42-3997.76] с другой стороны, она используется как функция.
[4000.00-4013.26] дам подсказку вот у вас есть нейронка вы решаете задачу многоклассной классификации у вас есть [4013.26-4021.60] нейронка который возвращает вам логиты н штук по количеству классов вот что вы делаете после этого [4021.60-4000.00]  [4008.34-NA] можно также софтмакс навесить и все да да абсолютно верно сейчас я сотру
[4025.00-4033.06] все да да абсолютно верно сейчас я сотру чтобы это не фиксировалось так называем большинстве [4033.06-4039.94] политики берем softmax в определенном действие по состояниям и на самом деле еще есть некоторая [4039.94-4047.38] температура вот некоторые скейл фактор который в зависимости от своего значения приводит либо к [4047.38-4025.00]  [4032.62-NA] к детерминированной политике, либо к политике стократической.
[4050.00-4067.88] политики стокастической то есть вот сюда чем чем выше альфа тем короче если альфа стремится [4067.88-4050.00]  [4059.76-NA] к бесконечности, то вы сходитесь к юниферам распоряжения.
[4075.00-4077.58] к юниферам распоряжения. [4077.58-4085.72] — А можно еще разок про вероятность в эпсилон-гриде политики? [4085.72-4088.22] Почему там вероятность будет единица минус эпсилон плюс [4088.22-4089.82] эпсилон на n? [4089.82-4090.82] Откуда эпсилон-грид? [4090.82-4099.32] — Смотрите, у вас есть n действий, и, допустим, arg-максимум — это [4099.32-4100.32] первое действие. [4100.32-4104.98] Вероятность попасть в него можно попасть двумя способами.
[4100.00-4110.08] Вероятность попасть в него, можно попасть в его двумя способами. Вот на этом этапе подбросили монетку, [4110.08-4117.12] с вероятностью 1 минус эпсилон выпал этот арт-максимум Q функции, то есть вероятность 1 минус эпсилон. [4117.12-4123.82] Есть еще и второй вариант. Подбросили монетку, попали вот в эту ветку, дальше как бы там [4123.82-4100.00]  [4105.66-NA] разыгрывается n случайных величин вы по сути выбираете одно число из
[4125.00-4126.66] Н случайных величин. [4127.06-4129.22] Вы, по сути, выбираете одно число [4129.22-4129.62] из [4129.62-4132.48] из N [4132.48-4134.08] возможно. [4134.56-4135.82] Понятно, спасибо большое. [4136.14-4138.20] Вероятно, что это первое, как раз [4138.20-4140.18] один на N, [4141.18-4142.74] один на N, но она еще умножается [4142.74-4144.80] на вероятность того, что вы вообще попали в эту ветку. [4145.72-4146.46] Ну и получается [4146.46-4148.00] плюс Эпсиона делить на N. [4151.12-4151.68] Вот. [4153.46-4154.70] Эпсион жадной политики.
[4150.00-4155.40] Эпсилон жадной политики. [4155.40-4159.72] На самом деле пока это у нас будет first class citizen, мы [4159.72-4164.84] как бы будем жить в классе эпсилон жадных политик, [4164.84-4166.24] и посмотрим, почему нас это приведет. [4166.24-4173.08] Да, я уже выцветил. [4173.08-4150.00]  [4156.92-NA] Да, вот у нас есть два алгоритма пока.
[4175.00-4183.52] у нас есть два два алгоритма пока разную про мунтака вообще не говорим говорим только про [4183.52-4193.06] кулер нин против сарса целом на самом деле я уже сказал что кажется что первый алгоритм [4193.06-4203.02] более привлекательный но давайте разберем почему и какие есть преимущества и недостатки у них по [4203.02-4175.00]  [4176.98-NA] по сравнению друг с другом.
[4200.00-4209.02] преимущества и недостатки у них по сравнению друг с другом. Вот второй как раз, ему нужно, чтобы учиться, [4209.02-4216.88] нужно сэмплировать из своей же политики. У первого алгоритма такого нет ограничения, он может... [4216.88-4225.18] На пары S и A у нас вообще никаких ограничений нет. На S штрих у нас есть ограничение, что он должен [4225.18-4200.00]  [4204.06-NA] приходить из той среды, из динамик этой среды, которую мы оцениваем.
[4225.00-4231.30] приходить из той среды, из динамик этой среды, которую мы оцениваем, но остальное, как бы, [4231.30-4240.84] для кулернинга берем максимум, для SARS берем темпл из нашей же политики. Вот. А на самом деле здесь [4240.84-4248.28] есть фундаментальное различие. Обратите внимание, что первая стратегия может учиться с опыта, который [4248.28-4225.00]  [4226.36-4231.70] собран не ею. То есть буквально вы можете...
[4250.00-4256.28] То есть буквально вы можете стоять, у вас есть старший [4256.28-4259.78] брат, который не дает вам играть в компьютер. [4259.78-4263.48] Вы смотрите на то, как он играет, но можно сказать, [4263.48-4267.40] что вы, если вы смотрите, вы какие-то вещи запоминаете, [4267.40-4269.70] вы чему-то учитесь, но вы сами не играете, то есть [4269.70-4271.20] вы учитесь с чужого опыта. [4271.20-4276.42] Вот, здесь как бы во втором случае вам нужно играть [4276.42-4279.00] самостоятельно, чтобы учиться. [4279.00-4250.00]  [4250.98-NA] Более того,
[4275.00-4279.88] как бы во втором случае вам нужно играть самостоятельно чтобы учиться более того [4279.88-4287.62] более того а более того да можно учиться что [4287.62-4296.06] можно учиться если у вас есть два старших брата которые оба не дает вам играть компьютера но оба [4296.06-4304.50] игра можно смотреть на то как они играют оба и учиться как бы с двух разных политик вот две [4304.50-4275.00]  [4275.50-NA] разный полет, который, как правило, не пройдет.
[4300.00-4308.16] как бы с двух разных политик вот две разные политики которые собирают вам опыт но вы с [4308.16-4315.78] него вот по такой формуле учить учитесь а давайте чуть формализуем а здесь возникает принципиальное [4315.78-4323.54] различие между он полисе и о в полисе режимом он полисе режим он полисе лёдлинг это когда вы [4323.54-4300.00]  [4308.18-NA] вы учитесь учите политику мил ее
[4325.00-4330.50] Политику Mew учите с ее же опыта. [4330.50-4339.94] То есть вам нужно генерировать сэмплы с помощью этой же [4339.94-4343.40] политики, которую вы пытаетесь выучить. [4343.40-4345.18] А противовес офф-полисе. [4345.18-4350.12] Офф-полисе, как я говорил, мы учим некоторую таргет-полисе [4350.12-4325.00]  [4329.90-NA] с опыта, который собран в России.
[4350.00-4361.60] с опыта который собран другой политикой Mew. P называется target policy, Mew называется [4361.60-4373.88] behavior policy. Какие, опять же, примеры, помимо примера, который я уже привел, мы можем собрать [4373.88-4350.00]  [4356.12-NA] какие-то логи, чего бы то ни было, проанализировать их и пытаться...
[4375.00-4384.30] чего бы то ни было, проанализировать их и пытаться обучиться с ним, сделать такой имитейшн-лёрнинг. [4384.30-4393.16] Сами мы при этом ничего не сделали, просто логи собрали, но это не наша логия, конечно, но вот можем [4393.16-4403.24] делать такой имитейшн-лёрнинг. Можем учиться с несколькими полетами. Более конкретный пример, [4403.24-4375.00]  [4376.76-NA] можем...
[4400.00-4415.96] Более конкретный пример — можем учить жадную политику, действуя некоторой политикой, которая поощряет исследования. [4415.96-4427.76] То есть можем учить как раз жадную политику с помощью сэмплов из эпсилон-жадной политики.
[4425.00-4434.72] помощью сэмплов из эпсиложадной политики. Ну и последнее свойство это то, что мы можем [4434.72-4445.00] использовать наш опыт, предыдущий опыт взаимодействия. Прошу прощения. Вот, все это супер, на самом деле, пока это [4445.00-4425.00]  [4434.38-NA] такие стейты которые постулируют некоторые не то что парадигмы но по сути да по сути две ветки
[4450.00-4455.64] Некоторые, не то что парадигмы, но по сути, да, по сути, две ветки алгоритмов. [4456.40-4457.42] On-policy и off-policy. [4457.74-4460.54] Но мы поймем, на самом деле, в чем преимущество. [4461.32-4462.66] У нас дальше это будет проследоваться. [4462.78-4466.36] Какие алгоритмы on-policy, как можно попытаться их делать off-policy, [4466.90-4470.86] почему в policy он более sample-efficient, вот такие вещи. [4470.96-4475.62] Пока мы просто постулируем эти вещи и говорим про преимущества и недостатки. [4475.62-4479.96] Пока кажется, что в on-policy преимуществ нет, но это на самом деле не так.
[4475.00-4502.00] недостатки. Пока кажется, что в он-полисе преимуществ нет, но это на самом деле не так. Кулернинг, да, я про него уже говорил, теперь давайте его просто формализуем. У нас есть два параметра, параметр для нашей обстановленной политики и наш ленинградский. Обратите внимание, что мы тут забиваем на наше второе условие для
[4500.00-4512.42] условия для теоремы робинсона-монро то есть теперь у нас как бы есть и интернет визит эйшн [4512.42-4522.88] благодаря в оксано-жадной политики но но сумма альфа-квадратов конечно сумма квадратов альфы [4522.88-4500.00]  [4504.12-4506.70] графы не меньше бесконечности. Ну и ладно, как оно обычно бывает, у нас есть теория, [4506.70-4507.20] а есть практика.
[4525.00-4529.82] не меньше бесконечности ну ладно как она обычно бывает нас есть теория есть практика [4529.82-4540.28] инициализируем кушки какие какими-то дальше у нас прошел прошел процесс обратите внимание что [4540.28-4546.94] в reinforcement learning мы у нас нет фиксированно с это то есть когда мы начинаем мы начинаем [4546.94-4553.18] буквально бед без ничего мы посылаем агента который еще плохо обучен собирать как бы свой [4553.18-4525.00]  [4526.80-NA] первый опыт пытаться что-то сделать.
[4550.00-4556.02] агента который еще плохо обучен собирать свой первый опыт и пытаться что-то сделать среде также [4556.02-4563.38] здесь как бы собираем опыт и с него же в онлайне учимся все алгоритмы пока онлайн это как бы [4563.38-4569.04] особенность онлайн рейтинг в этот момент мы посмотрим что значит оффлайн рейтинг но пока [4569.04-4575.56] как бы собираем учимся собираем учимся соответственно что здесь происходит вот [4575.56-4550.00]  [4554.42-NA] здесь мы собираем наш опыт действуем с помощью эпсилон жадный
[4575.00-4583.20] происходит вот здесь мы собираем наш опыт действуем с помощью и всем жадной политики мил соля возвращает [4583.20-4590.84] награду и следующий стоит мы вот по такому правилу обновляем нашу текущую оценку функции [4590.84-4575.00]  [4589.10-NA] довольно довольно естественно да как мы уже выяснили кулер нинк учиться учат курса звездой
[4600.00-4610.28] Да, как мы уже выяснили, Q-learning учит курсом звездой, при этом используя сэмплы с другой политикой. [4610.28-4626.72] Если он учит курсом звездой, то в какой-то момент, когда Q-K будет достаточно близко к курсу звездой, мы сможем найти P-звездой в нашу оптимальную политику. [4626.72-4600.00]  [4602.96-NA] То есть здесь политика в явном виде не участвует.
[4625.00-4631.90] нашу как бы оптимальную политику. Здесь политика в явном виде не участвует, мы учим по функции, [4631.90-4640.84] но из по функции мы сможем восстановить нашу политику. Сарса, да, пожалуйста. В чем суть [4640.84-4651.34] второго пункта? В чем суть второго пункта? Объекты. Просто типа здесь у вас агент что-то делает, [4651.34-4625.00]  [4629.62-NA] здесь вам среда возвращает это
[4650.00-4657.92] вас агент что-то делает здесь вам среда возвращает это это короче с инженерной [4657.92-4664.10] точки зрения здесь на смысле с того как вы это имплементирует и здесь вы вызовете что-то типа [4664.10-4650.00]  [4658.70-4663.44] а полисе дот сэмпл а здесь вы вызовите n в дот степ
[4675.00-4685.00] — А если просто получим награду и следующее состояние? [4685.00-4690.00] — Да, абсолютно верно. [4690.00-4691.00] — Спасибо. [4691.00-4698.20] — Сарс, почти то же самое, те же параметры, инициализируем [4698.20-4704.96] кушки, только здесь у нас два вызова.
[4700.00-4720.50] Только здесь у нас два вызова policy.sample, dot sample на s, и sample s'. [4720.50-4728.80] И только после этого, когда мы это сделали, мы оцениваем q-функцию. [4728.80-4700.00]  [4701.00-4700.00] И все это было вне зоны, вне зоны, вне зоны. [4701.00-4702.00] Появился в этом случае, как правило, неизвестный. В этом случае, если бы мы не были виноваты в том, что [4702.00-4703.00] мы не могли бы выяснить, что это было, то мы бы не могли [4703.00-4704.00] бы не ответить. [4704.00-4705.00] Но мы не могли бы не ответить. [4705.00-4706.00] Мы не могли бы не ответить. [4706.00-4707.00] Мы не могли бы не ответить. [4707.00-4708.00] Мы не могли бы не ответить. [4708.00-4709.00] Мы не могли бы не ответить. [4709.00-4710.00] Мы не могли бы не ответить. [4710.00-4711.00] Мы не могли бы не ответить. [4711.00-4712.00] Мы не могли бы не ответить. [4712.00-4713.00] Мы не могли бы не ответить. [4713.00-4714.00] Мы не могли бы не ответить. [4714.00-4715.00] Мы не могли бы не ответить. [4715.00-4716.00] Мы не могли бы не ответить. [4716.00-4717.00] Мы не могли бы не ответить. [4717.00-4718.00] Мы не могли бы не ответить. [4718.00-4719.00] Мы не могли бы не ответить. [4719.00-4720.00] Мы не могли бы не ответить. [4720.00-4721.00] Мы не могли бы не ответить. [4721.00-4722.00] Мы не могли бы не ответить. [4722.00-4723.00] Мы не могли бы не ответить.
[4725.00-4747.74] это сделали мы оцениваем куфунсу вот соответственно а штрих мы в среду уже не посылаем хотя хотя хотя [4747.74-4725.00]  [4726.16-4726.52] Хотя, хотя, хороший вопрос. [4730.16-NA] Нет, наверное, все-таки не посылаем, потому что...
[4750.00-4777.54] не посылаем потому что это интересно вопрос а штрих мы на следующем на следующем этапе посылаем наверное [4777.54-4750.00]  [4751.00-NA] э.
[4775.00-4786.94] посылаем наверное да я думаю что чтобы сэкономить как бы 11 одно сомплирование имеет смысл послать [4786.94-4795.94] поскольку мы здесь обновляем только для с а короче поскольку мы обновляем только для с а для с 3h3 [4795.94-4804.88] h3 ничего не изменит мы можем послать этот сэмпл в среду но это уже как бы детали оптимизации можно [4804.88-4775.00]  [4776.00-NA] поиска.
[4800.00-4810.88] этот сэмпл в среду но это уже как бы детали оптимизации можно формуле как будто новое [4810.88-4819.14] новый реворд и новое состояние не нужно зачем тогда посылать среду но да ну смотрите это правда но у [4819.14-4800.00]  [4809.84-NA] у вас же на самом деле что происходит вы были в с сделать действие и перешли в с3
[4825.00-4834.16] «Сделали действие A, перешли в S-штрих». [4834.16-4836.52] Дальше, ну как бы вы в новом состоянии, вам нужно будет [4836.52-4839.62] здесь сделать какой-то H-штрих. [4839.62-4847.24] Вопрос в том, сколько вы посамплируете. [4847.24-4825.00]  [4830.76-NA] Вы будете ли высупливать второй раз из политики здесь?
[4850.00-4860.58] Вы можете выселить второй раз из политики здесь. [4860.58-4863.18] Такое ощущение, что нет, что можно после этого... [4863.18-4867.20] То есть буквально схлопывается после этого вот этот и [4867.20-4870.54] вот этот пункт, они сливаются в один. [4870.54-4872.18] Вы не будете после этого селпли. [4872.18-4850.00]  [4857.80-NA] Я смог ответить на вопрос?
[4875.00-4881.32] Я смог ответить на вопрос? Или запутал? [4883.32-4887.04] Ну, я, если честно... Так по итогу, ходим второй раз? [4890.00-4895.16] Смотрите, у нас онлайн-вызаимодействие со средой, поэтому мы в любом случае второй раз пойдем. [4895.74-4899.00] Нет, вопрос в том... [4901.70-4875.00]  [4881.00-NA] А, ну, короче, но нам как будто не нужно, потому что мы
[4900.00-4905.96] А, ну, короче, но нам как будто не нужны результаты эти. [4907.96-4912.44] Ну, смотрите, на следующем этапе у нас уже получается как бы в с-штрих, аж-штрих. [4912.88-4918.24] Чтобы второй раз не сэмплировать из политики, возможно, мы можем переиспользовать результаты предыдущего сэмплирования. [4919.18-4924.04] Вот о чем мои размышления были. [4924.04-4900.00]  [4904.40-4905.96] А у нас же Q уже другая будет, ну Q плюс один, то есть по идее там и...
[4925.00-4931.82] у нас же кожа другая будет куплюсь один то есть по идее там и сэмплировать в другом [4931.82-4940.30] изменится к в одном состоянии из предыдущего в последнем в паре состоянии действия в штрих [4940.30-4950.48] а штрих она не изменится так что ладно мне кажется иногда бывает рассказываешь рассказываешь просто [4950.48-4925.00]  [4927.24-4929.54] И когда ты задаешь себе вопрос и начинаешь рассуждать об этом вслух, то он как бы не может быть ответственным.
[4950.00-4953.30] просто задаешь себе вопрос и начинаешь рассуждать об этом вслух. [4953.30-4962.10] В целом, теоретические гарантии не сломаются, если мы просамплируем второй раз. [4962.10-4966.68] Единственное, что оно будет в конечной ситуации, когда нам обязательно надо будет [4966.68-4971.62] стамплировать второй раз. Поскольку если Q это у вас нейронка, например, то вот после этого [4971.62-4976.44] обновления у вас потенциально изменится Q-функция, как правильно было замечено, [4976.44-4950.00]  [4953.54-NA] во всех состояниях и действиях. Поэтому стамплировать
[4975.00-4979.00] функции как правильно было замечено во всех состояниях и действия поэтому [4979.00-4988.96] стамплировать нужно будет второй раз ссоре за суету который навел нужно будет можно [4988.96-4993.84] еще вопрос если кейт они рамка мы армагдером или вот софт макс температуры [4993.84-5002.54] это будет на следующем занятии краткий ответ мы берем в такой обосновке мы берем арт макс [5002.54-4975.00]  [4975.00-NA] Однако, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате, в результате
[5000.00-5011.28] мы берем в такой постановке мы берем artmax есть другая постановка задачи reinforcement learning [5011.28-5023.98] так называемые энтропии энтропии регулировать reinforcement learning там можно будет брать [5023.98-5000.00]  [5003.98-NA] цеплировать и сапмакса
[5025.00-5039.64] из софтмаркса так то еще как бы вот если нас сакса вспомни что она была вдохновлена уравнением [5039.64-5051.12] бэллмана на самом деле уравнение бэллмана здесь у нас здесь мотожидание по s3 h3 вот но что что в [5051.12-5025.00]  [5028.88-NA] Но в целом это просто последовательность от ожидания по...
[5050.00-5058.86] Что в целом это просто последовательность над ожиданием по h3 и по h3. [5059.44-5065.42] Ну давайте, чтобы дисперсию сбить, поскольку у нас конечное множество состояний, [5066.94-5073.68] конечное множество действий, давайте вместо того, чтобы брать оценку по одному сэмплу, [5073.68-5050.00]  [5056.32-NA] брать оценку по... брать в отожидание, по-честному брать в отожидание от нашего...
[5075.00-5083.32] по выбрать в от ожидании по честно убрать на ожидании от наш политика можно поскольку это [5083.32-5091.28] дискретное распределение можно просто с определенными весами просуммировать ваше значение по функции [5091.28-5075.00]  [5088.62-NA] будет называться алгоритм экспертно вот такой вопрос у вас есть два алга
[5100.00-5119.52] Вот такой вопрос. У вас есть два алгоритма — кулернинг и SARS. Как вы думаете, какой алгоритм к какому... к какой политике сойдет? [5119.52-5100.00]  [5110.48-NA] Окулежник тоже не подходит.
[5125.00-5136.00] Кулерник, наверное, сойдется к оптимальному пути, который ближе к лифу в данном вопросе. [5136.00-5140.00] А SARS-CoV-2 более своего уровня. [5140.00-5125.00]  [5127.50-5127.58] Старция сойдется вот к такой безопасной политике. [5128.00-5128.08] Почему так? [5137.28-5140.00] Потому что, на самом деле, ну, кулерник учит курсу звездой, соответственно, ну, здесь очевидно... А, я про среду-то...
[5150.00-5159.36] соответственно здесь очевидно я проследу ты сказал вот такой как бы лабиринтик без без слабее без [5159.36-5167.94] самих коридоров все клетки доступны вам нужно из вот этой точки попасть в эту но есть у вас обрыв [5167.94-5174.04] если вы вдруг по бы попадаете то вы получаете награду минус то и возвращаетесь начальное [5174.04-5150.00]  [5153.02-5156.02] состояние, если... И за каждое время, проведенное в среде...
[5175.00-5181.72] И за каждое время, проведенное в среде, вы получаете еще минус единичку. [5183.72-5189.28] То есть на самом деле несложно тут посчитать, что оптимальная награда это минус 13. [5189.28-5202.28] То есть вам минимум 13 шагов нужно пройти, чтобы добраться из точки С в терминальное состояние. [5203.68-5175.00]  [5177.00-NA] соответственно.
[5200.00-5202.50] в терминальном состоянии, [5203.18-5204.22] вот, соответственно, [5206.00-5206.50] типа, это [5206.50-5208.60] оптимальные награды, которые вы можете [5208.60-5210.58] достигнуть. Ну и, да, правильно, [5210.82-5212.78] кулерник сойдется к кусу звездой, [5212.86-5214.98] и с кусу звездой мы можем восстановить оптимальную политику, [5215.42-5216.20] сойдемся к [5216.20-5217.50] к [5217.50-5220.10] вот этому кратчайшему пути. [5220.38-5222.40] Почему Сарса сойдется [5222.40-5224.10] к безопасному пути? [5224.10-5226.30] Потому что, когда вы учите [5226.30-5228.38] Сарсу, вы, поскольку [5228.38-5200.00]  [5201.60-NA] чтобы выучить ее с помощью.
[5225.00-5232.92] когда вы учите саксу вы поскольку вы учите ее с помощью эпсилон жадной политики вы по сути [5232.92-5240.30] ограничиваете пространство политик ограничить пространство политик который вы можете выучить [5240.30-5249.36] вот вы говорите что политика бы мы теперь живем в мире где запрещены как бы детерминированной [5249.36-5225.00]  [5230.20-NA] политики есть только самые детерминированные.
[5250.00-5256.30] Есть только самые детерминированные, политика с минимальной [5256.30-5261.76] энтропией, которую вы можете получить, это политика, [5261.76-5265.44] которая просто эпсил-жадная. [5265.44-5276.32] Соответственно, еще как бы у SARS, в SARS заложено вот [5276.32-5279.16] это отвращение к риску, что если он пройдет здесь, [5279.16-5250.00]  [5250.84-NA] У него есть...
[5275.00-5287.42] Вот это отвращение к риску, что если он пройдет здесь, у него есть, из-за того, что политика стократическая, есть ненулевая вероятность прыгнуть в лаву, прыгнуть с обрыва. [5287.94-5295.80] Соответственно, он в итоге выбирает самый безопасный путь, который только возможен, чтобы как раз минимизировать эти риски. [5295.80-5275.00]  [5284.18-NA] кулер нинг кулер нинг там таких рисков не зашито поэтому можно выучить оптимальную политику
[5300.00-5305.06] там таких рисков не зашито, поэтому можно выучить оптимальную [5305.06-5310.82] политику, хотя и важно, что на протяжении этого взаимодействия, [5310.82-5313.50] поскольку взаимодействуете вы с помощью оптимозаданной [5313.50-5317.54] политики, у вас есть вероятность прыгнуть с обрыва, достаточно [5317.54-5320.54] большая, если вы проходите особенно к нему очень близко. [5320.54-5300.00]  [5309.46-NA] Соответственно, у этого есть такой парадокс, что
[5325.00-5332.00] У этого есть такой парадокс, что... [5332.00-5339.48] Так, я свое художество смотрю. [5339.48-5325.00]  [5326.00-5327.00] Нет. Сейчас, мальчишка. [5327.00-5328.00] Я не могу. [5328.00-5329.00] Я не могу. [5329.00-5330.00] Я не могу. [5330.00-5331.00] Я не могу. [5331.00-5332.00] Я не могу. [5332.00-5333.00] Я не могу. [5333.00-5334.00] Я не могу. [5334.00-5335.00] Я не могу. [5335.00-5336.00] Я не могу. [5336.00-5337.00] Я не могу. [5337.00-5338.00] Я не могу. [5338.00-5339.00] Я не могу. [5339.00-5340.00] Я не могу. [5340.00-5341.00] Я не могу. [5341.00-5342.00] Я не могу. [5342.00-5343.00] Я не могу. [5343.00-5344.00] Я не могу. [5344.00-5345.00] Я не могу. [5345.00-5346.00] Я не могу. [5346.00-5347.00] Я не могу. [5347.00-5348.00] Я не могу. [5348.00-5349.00] Я не могу. [5349.00-5350.00] Я не могу. [5350.00-5351.00] Я не могу. [5351.00-5352.00] Я не могу. [5352.00-5353.00] Я не могу.
[5350.00-5376.00] Да, у этого есть довольно не совсем интуитивные последствия. [5376.00-5350.00]  [5379.98-NA] Однако, если мы не будем виноваты в том, что мы не получили от этого отчаяния, то мы не сможем выяснить, что мы получили от этого отчаяния.
[5375.00-5404.98] Эффективные последствия?
[5400.00-5427.06] Так, ладно, какая-то странная тема. [5427.06-5400.00]  [5402.94-NA] Извините, у меня что-то случилось с двумя двумя двумя двумя двумями.
[5425.00-5430.20] Так, извините, у меня что-то случилось с Zoom. [5435.20-5438.42] Я не могу найти свой суток. [5444.86-5447.66] Поэтому не могу продолжить презентацию сейчас с минуту.
[5450.00-5467.00] А самое время сделать небольшой перерывчик? [5467.00-5469.00] Да, давайте сделаем перерыв. [5469.00-5472.00] В целом, как раз у нас полтора часа прошло. [5472.00-5473.00] Хорошая затея. [5473.00-5450.00]  [5457.00-NA] Вот и вернемся, давайте через десять минут всех в душе.