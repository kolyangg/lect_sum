{
  "processed_frame_014825.jpg": "- **Вступительные замечания лектора**  \n  - Лектор предупреждает о возможных помехах (кашель) из-за недавней простуды.  \n  - Упоминание термина **model-free settings** (исправлено с возможной опечатки в оригинале) — подход, при котором информация о среде становится менее доступной.  \n\n- **Тема занятия**  \n  - Второе занятие посвящено переходу к **model-free** методам в reinforcement learning.  \n  - Основной фокус: усложнение задач за счёт работы в условиях, где известно меньше информации о среде.  \n\n- **Ключевые особенности model-free подходов**  \n  - Отражают реальные ситуации, где полные данные о среде недоступны (например, реальный мир).  \n  - Имеют практическую значимость, так как большинство реальных сред относятся к категории **model-free**.  \n\n- **Связь с предыдущим материалом**  \n  - На прошлом занятии изучались основы reinforcement learning, сейчас происходит переход к более сложной парадигме.  \n  - Акцент на задачах, которые требуют адаптации к неизвестным или частично известным условиям среды.  \n\n- **Важность темы**  \n  - Model-free методы — основа для решения задач в реальных условиях, где точная модель среды отсутствует.  \n  - Подготовка к работе с практическими кейсами (робототехника, игры, автономные системы).",
  "processed_frame_016150.jpg": "**Повторение материала: Марковский процесс принятия решений (MDP)**  \n\n### **Ключевые компоненты MDP**  \n1. **Action Space (Пространство действий)**  \n   - Множество доступных действий, влияющих на переход между состояниями и получение награды.  \n\n2. **Состояния (States)**  \n   - Описание текущей ситуации в среде. Не содержит информации о предыдущих шагах (марковское свойство).  \n\n3. **Функция перехода**  \n   - Вероятность перехода в состояние \\( S' \\) из текущего состояния \\( S \\) при выполнении действия \\( A \\):  \n     \\[ P(S' | S, A) \\]  \n   - **Марковское свойство**: Вероятность зависит **только** от текущей пары (состояние, действие), а не от истории.  \n   - **Стационарность**: Вероятности переходов не меняются со временем.  \n\n4. **Награда (Reward)**  \n   - Детерминированная функция \\( R(S, A) \\). В тексте упоминается возможность стохастической награды, но для упрощения используется детерминированный вариант.  \n\n---\n\n### **Свойства и пояснения**  \n- **Пример-иллюстрация (Марио)**:  \n  - Действия (прыжки, бег) влияют на переходы между состояниями и накопление награды (например, сбор монет).  \n  - Завершение уровня (состояние \"труба\") — финальная цель.  \n\n- **Политика (Policy)**  \n  - Определяет выбор действий в конкретном состоянии.  \n  - Может быть:  \n    - **Детерминированной**: Чёткое правило \\( \\pi(S) = A \\).  \n    - **Стохастической**: Вероятностное распределение \\( \\pi(A | S) \\).  \n\n- **Целевая функция**  \n  - Максимизация **дисконтированной кумулятивной награды**:  \n    \\[ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\]  \n    где \\( \\gamma \\) — коэффициент дисконтирования.  \n\n---\n\n### **Исправления/уточнения**  \n- Исправлены термины:  \n  - \"стахастическая\" → **стохастическая**.  \n  - Убраны повторы (\"Быстренько пробежимся\").  \n- **Важно**: Марковское свойство исключает зависимость от истории, что упрощает моделирование и вычисления.  \n- Акцент на практическое применение MDP в реальных задачах (например, игры, управление роботами).  \n\n_Следующий шаг: переход к model-free методам, где модель среды (P, R) неизвестна._",
  "processed_frame_019925.jpg": "### **Функции ценности в Reinforcement Learning**  \n\n#### **Кумулятивная награда (GT)**  \n- **GT (Reward-to-go)** — случайная величина, представляющая сумму дисконтированных наград от текущего момента до завершения эпизода:  \n  \\[  \n  G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\dots  \n  \\]  \n  - Зависит от последовательности состояний, действий и стохастичности среды.  \n\n---\n\n#### **Функция ценности состояния (V-функция)**  \n- **Vπ(s)** — ожидаемая кумулятивная награда при старте из состояния \\( s \\) и следовании политике \\( \\pi \\):  \n  \\[  \n  V^\\pi(s) = \\mathbb{E}\\left[ G_t \\mid S_t = s, \\pi \\right]  \n  \\]  \n  - **Ключевой аспект**: Оценивает \"качество\" состояния \\( s \\) при выбранной стратегии \\( \\pi \\).  \n\n---\n\n#### **Функция ценности действия (Q-функция)**  \n- **Qπ(s, a)** — ожидаемая кумулятивная награда за выполнение действия \\( a \\) в состоянии \\( s \\) с последующим следованием политике \\( \\pi \\):  \n  \\[  \n  Q^\\pi(s, a) = \\mathbb{E}\\left[ G_t \\mid S_t = s, A_t = a, \\pi \\right]  \n  \\]  \n  - **Отличие от V-функции**: Учитывает конкретное действие \\( a \\), что полезно для оптимизации политики.  \n\n---\n\n#### **Рекуррентное соотношение для кумулятивной награды**  \n- **Связь между V и Q**:  \n  \\[  \n  Q^\\pi(s, a) = R(s, a) + \\gamma \\cdot \\mathbb{E}\\left[ V^\\pi(S_{t+1}) \\mid S_t = s, A_t = a \\right]  \n  \\]  \n  - **Интерпретация**: Награда за действие \\( a \\) плюс дисконтированная ценность следующего состояния.  \n\n- **Уравнение Беллмана для V-функции** (связь с политикой \\( \\pi \\)):  \n  \\[  \n  V^\\pi(s) = \\sum_{a} \\pi(a | s) \\cdot Q^\\pi(s, a)  \n  \\]  \n\n---\n\n### **Комментарии и уточнения**  \n1. **Стохастическая vs. детерминированная награда**  \n   - В тексте рассматривается детерминированная награда \\( R(s, a) \\), но Q-функция может обобщаться на стохастические награды через взятие матожидания.  \n\n2. **Политика \\( \\pi \\)**:  \n   - Детерминированная: \\( \\pi(s) = a \\) (однозначный выбор действия).  \n   - Стохастическая: \\( \\pi(a | s) \\) (вероятностное распределение).  \n\n3. **Термины**  \n   - \"Reward-to-go\" — синоним кумулятивной награды \\( G_t \\).  \n   - Корректировка опечаток: \"пью функция\" → **Q-функция**.  \n\n---\n\n### **Итог**  \n- **V-функция** помогает оценить ценность состояний при заданной политике.  \n- **Q-функция** расширяет анализ до уровня пар (состояние, действие), что критично для улучшения стратегии.  \n- Рекуррентные формулы (уравнения Беллмана) связывают текущие и будущие оценки, формируя основу для алгоритмов RL (например, Q-Learning).  \n\n_Следующий шаг: Алгоритмы оценки и оптимизации политик на основе V и Q._",
  "processed_frame_021700.jpg": "### **Дисконтированная кумулятивная награда и её особенности**  \n\n#### **Рекуррентное соотношение для GT**  \n- Кумулятивная награда \\( G_t \\) вычисляется через сумму **дисконтированных мгновенных наград**:  \n  \\[\n  G_t = R_t + \\gamma G_{t+1}\n  \\]  \n  где:  \n  - \\( R_t \\) — награда на шаге \\( t \\),  \n  - \\( \\gamma \\) (дисконт-фактор) — коэффициент, определяющий важность будущих наград.  \n\n---\n\n#### **Роль дисконт-фактора \\( \\gamma \\)**  \n- **\\(\\gamma = 0\\)**:  \n  Агент максимизирует только текущую награду \\( R_t \\), игнорируя будущее.  \n- **\\(\\gamma = 1\\)**:  \n  Все награды на всём пути имеют одинаковый вес (недисконтированный случай).  \n- **\\(0 < \\gamma < 1\\)**:  \n  Будущие награды экспоненциально затухают. Чем дальше награда, тем меньше её вклад.  \n\n---\n\n### **Эпизодические и бесконечные задачи**  \n- **Эпизодические среды**:  \n  - Взаимодействие завершается через фиксированное количество шагов.  \n  - Например: уровни в играх, задачи управления с ограниченным временем.  \n- **Бесконечные среды**:  \n  - Взаимодействие не имеет финишного состояния (теоретически удобнее для анализа).  \n\n---\n\n#### **Эксплуатация в эпизодических средах**  \n- **Пример с муравьём (Continuous Control)**:  \n  - Агент получает награду за расстояние от стартовой точки.  \n  - Перед завершением эпизода агент может совершить «отчаянный прыжок», чтобы максимизировать финальную награду, игнорируя стабильность (*reward hacking*).  \n  - **Причина**: Агент осознает, что после прыжка последствия (например, падение) не учитываются, так как эпизод завершается.  \n\n---\n\n### **Источники стохастичности**  \n1. **Политика \\( \\pi \\)**  \n   - Стохастический выбор действий (\\( \\pi(a|s) \\)).  \n2. **Динамика среды**  \n   - Вероятность перехода \\( P(S'|S, A) \\).  \n\n  Матожидание \\( \\mathbb{E}[G_t] \\) учитывает оба источника случайности.  \n\n---\n\n### **Проблемы в алгоритмах**  \n- **Функциональная аппроксимация**:  \n  - Модель среды \\( P(S'|S, A) \\) часто неизвестна. Алгоритмы вроде Policy Gradient и Q-Learning работают без её знания (**model-free**).  \n  - Ошибки аппроксимации могут приводить к неоптимальному поведению (например, \"галлюцинациям\" — неверным предсказаниям).  \n\n---\n\n### **Итоговые замечания**  \n- **Внедрение информации о времени**:  \n  - В эпизодических средах полезно добавлять в состояние \\( S \\) метку времени (например, нормализованный шаг \\( t/T \\)).  \n  - Агент может использовать это для планирования действий (например, подготовиться к финальному рывку).  \n\n- **Баланс оптимизации**:  \n  - Выбор \\( \\gamma \\) влияет на стратегию: краткосрочная выгода vs. долгосрочное планирование.  \n\n_Следующий шаг: Алгоритмы для работы в model-free условиях (например, Q-Learning) и методы борьбы с reward hacking._",
  "processed_frame_027775.jpg": "### **Условия конечности пространств состояний и действий**  \n1. **Конечность State Space и Action Space**:  \n   - Предполагается, что множества состояний \\( \\mathcal{S} \\) и действий \\( \\mathcal{A} \\) **конечны**.  \n   - Это условие упрощает математический анализ и алгоритмическую реализацию задач RL.  \n\n2. **Реалистичность сценария**:  \n   - Многие моделируемые среды (игры, тестовые задачи) удовлетворяют этому условию.  \n   - В реальном мире часто встречаются **бесконечные** или **очень большие** пространства (например, непрерывные сенсорные данные).  \n\n---\n\n### **Вывод уравнения Беллмана**  \nПри выполнении условий конечности \\( \\mathcal{S} \\), \\( \\mathcal{A} \\) и стационарности среды:  \n\n- **Уравнение Беллмана для \\( V^\\pi(s) \\)** связывает ценность текущего состояния с ценностью следующего состояния:  \n  \\[\n  V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right]\n  \\]  \n  - Здесь:  \n    - \\( R(s, a) \\) — награда за действие \\( a \\) в состоянии \\( s \\),  \n    - \\( P(s'|s, a) \\) — вероятность перехода в состояние \\( s' \\),  \n    - \\( \\gamma \\) — дисконт-фактор.  \n\n- **Уравнение Беллмана для \\( Q^\\pi(s, a) \\)** расширяет это для пар (состояние, действие):  \n  \\[\n  Q^\\pi(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a')\n  \\]  \n\n---\n\n### **Практическая значимость**  \n- **Решение уравнения Беллмана**:  \n  - Позволяет находить оптимальные политики через итеративные методы (Value Iteration, Policy Iteration).  \n  - В **model-free** условиях (где \\( P(s'|s, a) \\) неизвестна) применяются алгоритмы вроде Q-Learning, основанные на оценке \\( Q \\)-функции.  \n\n- **Ограничения**:  \n  - Конечность пространств критична для точного решения. В непрерывных или крупномасштабных средах требуются аппроксимации (DQN, Policy Gradient).  \n\n---\n\n### **Дополнение к обсуждению**  \n- **Ассамбляжи (сборки)**:  \n  - Упоминаемые «два других ассамбляжа» могут относиться к:  \n    1. **Модельным** (model-based) подходам, где \\( P(s'|s, a) \\) известна.  \n    2. **Эпизодическим** задачам с фиксированным горизонтом.  \n  - Эти условия менее строги, так как не требуют полной информации о среде.  \n\n_Следующий шаг: Исследование алгоритмов для бесконечных/непрерывных пространств или model-free методов._",
  "processed_frame_029250.jpg": "### **Алгоритм Value Iteration и уравнения Беллмана**  \n\n#### **Связь между V- и Q-функциями**  \n- **Для V-функции**:  \n  Значение \\( V^\\pi(s) \\) зависит от наград и ценностей **следующих состояний**, достижимых из \\( s \\).  \n- **Для Q-функции**:  \n  Значение \\( Q^\\pi(s, a) \\) оценивает ожидаемую награду при выполнении действия \\( a \\) в состоянии \\( s \\), а затем следовании политике \\( \\pi \\), включая переходы в состояния \\( s' \\).  \n\n---\n\n#### **Оптимальная политика**  \n- Определяется через **максимизацию V- или Q-функции**:  \n  \\[\n  V^*(s) = \\max_a Q^*(s, a), \\quad Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s')\n  \\]  \n- **Свойство**:  \n  Для конечных MDP существует **детерминированная оптимальная политика** \\( \\pi^* \\), выбирающая действие, дающее максимальную Q-ценность.  \n\n---\n\n#### **Уравнения Беллмана**  \n- **Для оптимальных функций**:  \n  \\[\n  V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right]\n  \\]  \n  \\[\n  Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) \\max_{a'} Q^*(s', a')\n  \\]  \n- **Изменение оператора**:  \n  По сравнению с неоптимальным случаем, **ожидание** (суммирование по политике) заменяется на **максимум**.  \n\n---\n\n#### **Теорема Банаха о сжатии**  \n- **Применимость**:  \n  Оператор Беллмана является **сжимающим** в пространстве функций с нормой \\( \\|\\cdot\\|_\\infty \\).  \n- **Гарантии сходимости**:  \n  - Существует единственная **неподвижная точка** (оптимальные \\( V^* \\) или \\( Q^* \\)).  \n  - При любом начальном приближении алгоритм Value Iteration сходится к этой точке.  \n\n---\n\n### **Алгоритм Value Iteration**  \n1. **Инициализация**:  \n   Задаём начальные значения \\( V_0(s) \\) для всех состояний.  \n2. **Итерационное обновление**:  \n   Для каждого состояния \\( s \\):  \n   \\[\n   V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right]\n   \\]  \n3. **Критерий остановки**:  \n   Прекратить при \\( \\|V_{k+1} - V_k\\| < \\epsilon \\) (достигнута достаточная точность).  \n\n---\n\n#### **Особенности алгоритма**  \n- **Два этапа в одном шаге**:  \n  Объединяет **оценку политики** (расчёт \\( V \\)) и **улучшение политики** (выбор максимума).  \n- **Требования к среде**:  \n  Необходимо знание **модели среды** (вероятности переходов \\( P \\), награды \\( R \\)).  \n\n---\n\n### **Практические замечания**  \n- **Конечность пространств**:  \n  Уравнения записаны в виде сумм благодаря **конечности** \\( \\mathcal{S} \\) и \\( \\mathcal{A} \\). В непрерывных случаях заменяются интегралами.  \n- **Эпизодические среды**:  \n  Если траектории конечны, Value Iteration применяется для каждого шага от конца к началу.  \n\n---\n\n**Следующий шаг**: Реализация алгоритма и сравнение с Policy Iteration.",
  "processed_frame_032875.jpg": "### **Алгоритмы Policy Iteration и Value Iteration**  \n\n---\n\n#### **Основные этапы алгоритмов RL**  \n1. **Policy Evaluation**:  \n   - **Цель**: Оценка текущей политики \\( \\pi \\) через расчёт **функции ценности** \\( V^\\pi(s) \\).  \n   - **Метод**: Решение уравнения Беллмана для \\( V^\\pi(s) \\):  \n     \\[\n     V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right]\n     \\]  \n   - **Процесс**: Итеративное обновление \\( V(s) \\) до сходимости (например, методом Якоби или Гаусса-Зейделя).  \n\n2. **Policy Improvement**:  \n   - **Цель**: Улучшение политики путём **жадного выбора** действий относительно текущей \\( V^\\pi \\).  \n   - **Формула обновления политики**:  \n     \\[\n     \\pi_{\\text{new}}(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right]\n     \\]  \n\n---\n\n### **Сравнение Policy Iteration и Value Iteration**  \n| **Аспект**               | **Policy Iteration**                  | **Value Iteration**                     |  \n|--------------------------|---------------------------------------|-----------------------------------------|  \n| **Этапы**                | Два чётких этапа: оценка → улучшение  | Объединяет оценку и улучшение в один шаг |  \n| **Сходимость**            | Быстрее при точных вычислениях        | Медленнее, но проще в реализации        |  \n| **Использование**         | Требует полной оценки политики        | Не требует полной оценки на каждом шаге |  \n| **Применение**           | Подходит для небольших пространств    | Работает в более общих сценариях        |  \n\n---\n\n### **Визуализация через дерево решений**  \n- **Пример**: Состояние \\( S_t \\) → возможные действия \\( A_1, A_2, \\dots \\).  \n  - Каждое действие ведёт к состояниям \\( S_{t+1}' \\) с наградой \\( R(S_t, A_i) \\).  \n  - **Q-значение** действия \\( A_i \\):  \n    \\[\n    Q(S_t, A_i) = R(S_t, A_i) + \\gamma \\sum_{s'} P(s'|S_t, A_i) V^{\\pi}(s')\n    \\]  \n  - **Выбор действия**: Жадно выбираем действие **с максимальным \\( Q(S_t, A_i) \\)**.  \n\n---\n\n### **Теоретическая основа**  \n- **Bellman Optimality Equation**:  \n  \\[\n  V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right]\n  \\]  \n  - Оптимальная политика \\( \\pi^* \\) достигается через **жадные действия** относительно \\( V^* \\).  \n- **Теорема сходимости**:  \n  Для конечных MDP гарантируется существование **единственной оптимальной политики** и её достижимость алгоритмами.  \n\n---\n\n### **Практические сложности**  \n1. **Вычислительная сложность**:  \n   - **Policy Evaluation** требует \\( O(|\\mathcal{S}|^3) \\) операций (решение системы уравнений).  \n   - В больших пространствах используют аппроксимации (DQN, Policy Gradients).  \n2. **Модель среды**:  \n   - Требует знания \\( P(s'|s, a) \\) и \\( R(s, a) \\). В **model-free** подходах (Q-Learning) эти данные заменяются выборками.  \n3. **Границы эпизодов**:  \n   - Агент может \"эксплуатировать\" фиксированные эпизоды (например, совершать рискованные действия перед завершением).  \n\n---\n\n### **Примеры применения**  \n- **Игры**: Оптимизация стратегии в шахматах, Go, Atari.  \n- **Робототехника**: Планирование движений робота с учётом дисконтированных наград.  \n- **Управление ресурсами**: Динамическое распределение ресурсов в реальном времени.  \n\n---\n\n### **Итоги**  \n- **Policy Iteration** подходит для точного решения задач с небольшими пространствами.  \n- **Value Iteration** эффективен при работе с неточными моделями или крупными пространствами.  \n- В основе обоих алгоритмов — уравнения Беллмана, связывающие текущие и будущие ценности через дисконтирование.  \n\n_Следующий шаг: Переход к model-free методам (Q-Learning, SARSA) и методам аппроксимации (нейросетевые подходы)._",
  "processed_frame_041500.jpg": "### **Алгоритмы улучшения политики и анализ V/Q-функций**  \n\n---\n\n#### **Процесс улучшения политики**  \n- **Policy Improvement (Улучшение политики)**:  \n  - Дана текущая политика \\( \\pi_{\\text{old}} \\), строится новая политика \\( \\pi_{\\text{new}} \\) **жадным выбором** действий относительно \\( V^{\\pi_{\\text{old}}} \\).  \n  - Формально:  \n    \\[\n    \\pi_{\\text{new}}(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^{\\pi_{\\text{old}}}(s') \\right]\n    \\]  \n  - **Гарантия улучшения**:  \n    Для любого состояния \\( s \\) выполняется \\( V^{\\pi_{\\text{new}}}(s) \\geq V^{\\pi_{\\text{old}}}(s) \\).  \n\n---\n\n#### **Сравнение V- и Q-функций**  \n\n1. **V-функция (ценность состояний)**:  \n   - Представление: Вектор размерности \\( |\\mathcal{S}| \\), где \\( |\\mathcal{S}| \\) — количество состояний.  \n   - **Недостаток**:  \n     Для улучшения политики необходимо **восстанавливать Q-функцию** через:  \n     \\[\n     Q(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V(s')\n     \\]  \n\n2. **Q-функция (ценность действий)**:  \n   - Представление: Матрица размерности \\( |\\mathcal{S}| \\times |\\mathcal{A}| \\), где \\( |\\mathcal{A}| \\) — количество действий.  \n   - Преимущество: Позволяет **напрямую выбирать оптимальные действия** без дополнительных вычислений:  \n     \\[\n     \\pi_{\\text{new}}(s) = \\arg\\max_a Q(s, a)\n     \\]  \n\n---\n\n### **Сложность алгоритмов**  \n- **Value Iteration**:  \n  - Работает с V-функцией. Сложность обновлений: \\( O(|\\mathcal{S}|^2 \\cdot |\\mathcal{A}|) \\).  \n  - Для больших пространств требует много ресурсов.  \n- **Policy Iteration**:  \n  - Использует Q-функцию. Сложность: \\( O(|\\mathcal{S}| \\cdot |\\mathcal{A}|) \\) на шаг улучшения.  \n  - Зависит от числа итераций для сходимости оценки политики.  \n\n---\n\n#### **Практические замечания**  \n- **Эффективность Q-функции**:  \n  При использовании Q-функции шаг **Policy Improvement упрощается**, так как максимизация происходит напрямую по матрице Q.  \n- **Пример алгоритма на Q-функции**:  \n  - **Q-Learning** (model-free метод), где обновления производятся по правилу:  \n    \\[\n    Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n    \\]  \n\n---\n\n### **Оптимизация распределения траекторий**  \n- **Сдвиг в сторону элитных траекторий**:  \n  - Методы наподобие **Policy Gradients** или **Evolution Strategies** обновляют политику, увеличивая вероятность действий, ведущих к высоким наградам.  \n  - Пример: Отбор траекторий с максимальным \\( G_t \\) и корректировка политики в их пользу.  \n\n---\n\n#### **Итог**  \n- **Policy Improvement Theorem** гарантирует, что улучшение политики строго увеличивает ожидаемую награду.  \n- **Выбор между V и Q** зависит от задачи:  \n  - V-функция проще для анализа (вектор),  \n  - Q-функция практичнее для оптимизации (прямая максимизация).  \n- В model-free средах Q-функции становятся основным инструментом (DQN, SARSA).  \n\n_Следующий шаг: Обсуждение model-free алгоритмов и аппроксимационных методов (нейросетевые Q-функции)._",
  "processed_frame_046350.jpg": "### **Ключевые темы лекции**  \n- **Эволюционные стратегии как альтернатива V/Q-функциям**:  \n  - Если традиционные методы (Value/Policy Iteration) кажутся сложными или неприменимыми, можно использовать **эволюционные стратегии** для оптимизации политики.  \n  - Основная идея:  \n    - Сдвигать распределение политики в сторону **элитных траекторий** — тех, которые дают высокую награду.  \n    - Пример: Отбор лучших сэмплов (траекторий) и корректировка политики для их повторения.  \n\n---\n\n### **Особенности эволюционных стратегий**  \n1. **Простота и универсальность**:  \n   - Не требуют знания модели среды, градиентов или сложной математики.  \n   - Подходят для задач с **высоким уровнем шума** или **недифференцируемыми** функциями.  \n\n2. **Недостатки**:  \n   - **Низкая эффективность**:  \n     - Требуют **огромного количества сэмплов** для обучения (пример: десятки тысяч эпизодов).  \n     - Отдельный сэмпл вносит малый вклад в общее качество, особенно при большой популяции.  \n   - **Проблема бюджета**:  \n     - На практике сложно уложиться в ограниченное число взаимодействий со средой.  \n\n3. **Современные тренды**:  \n   - Разработка алгоритмов, работающих с **фиксированным бюджетом взаимодействий**.  \n   - Повышение эффективности через **индуктивные предпосылки** (Inductive Biases):  \n     - Использование априорных знаний о задаче (например, структура нейросетей) для ускорения обучения.  \n\n---\n\n### **Критика эволюционных методов**  \n- **Сравнение с градиентными методами**:  \n  - Алгоритмы вроде **Policy Gradients** или **Q-Learning** используют информацию из среды (например, градиенты) и обучаются быстрее.  \n  - Эволюционные стратегии игнорируют структуру среды, что снижает их эффективность.  \n- **Применение**:  \n  - Актуальны в задачах, где невозможно получить градиенты (например, чёрный ящик), но требуют значительных вычислительных ресурсов.  \n\n---\n\n### **Итоговые выводы**  \n- Эволюционные стратегии — инструмент **последнего выбора** для задач с экстремальной неопределённостью.  \n- Для большинства практических задач предпочтительны методы, использующие **структуру задачи** (V/Q-функции, нейросети).  \n- Главный вызов: баланс между **универсальностью** и **эффективностью** в условиях ограниченных ресурсов.  \n\n---\n\n**Следующие шаги**:  \n- Изучение гибридных методов (например, CMA-ES + нейросети).  \n- Анализ мета-обучения для уменьшения зависимости от числа сэмплов.",
  "processed_frame_049350.jpg": "**Структурированный конспект лекции**\n\n---\n\n### **Ответ на вопрос по алгоритму Policy Iteration**  \n- **Критерий остановки**:  \n  - Основной критерий — достижение заданной точности уравнения Беллмана.  \n  - Проверка неизменности политики **не обязательна**, так как:  \n    - При наличии **нескольких оптимальных действий** алгоритм может переключаться между ними (например, из-за порядка действий в реализации `argmax`).  \n    - Решение считается успешным, если функция ценности удовлетворяет уравнению Беллмана в пределах погрешности.\n\n---\n\n### **Переход к реальным задачам**  \n- **Проблемы реального мира**:  \n  1. **Неизвестная динамика среды**:  \n     - Модель перехода \\( P(s'|s, a) \\) и награды \\( R(s, a) \\) часто непознаваемы.  \n     - Пример: Системы с нелинейными уравнениями или шумными сенсорами.  \n  2. **Шумные награды**:  \n     - Награды могут быть зашумлёнными или запаздывающими.  \n  3. **Ограниченная наблюдаемость**:  \n     - Агент получает лишь частичную информацию о состоянии среды.\n\n---\n\n### **Роль V-функции в model-free условиях**  \n- **Ограничения**:  \n  - В реальных задачах V-функция **недоступна** для прямого использования, так как её расчёт требует знания динамики среды.  \n  - Даже при наличии оценки \\( V(s) \\), восстановление политики осложнено из-за отсутствия данных о \\( P(s'|s, a) \\).  \n\n- **Оставшаяся польза**:  \n  - V-функция может служить **индикатором качества состояний** в гибридных подходах.  \n  - Используется в некоторых оффлайн-алгоритмах для анализа накопительной награды.\n\n---\n\n### **Key Takeaways**  \n- **Model-free методы** (Q-Learning, Policy Gradients) становятся основными инструментами в условиях неопределённости.  \n- **Эволюционные стратегии** — альтернатива, но требуют большого числа взаимодействий со средой.  \n- **Индуктивные предпосылки** (например, структура нейросетей) повышают эффективность обучения в реальных задачах.\n\n---\n\n_Следующий шаг:_ Углубление в алгоритмы, работающие без знания модели среды (DQN, Actor-Critic), и методы борьбы с шумом в наградах.",
  "processed_frame_053350.jpg": "### **Использование V-функции и эмпирического среднего**  \n#### **Роль V-функции в условиях неопределённости**  \n- **Ограничения в model-free средах**:  \n  - В реальных задачах прямая оценка \\( V(s) \\) через уравнение Беллмана **невозможна** из-за отсутствия данных о динамике среды (\\( P(s'|s, a) \\)).  \n  - Даже при наличии приближённой \\( V(s) \\), восстановление политики требует знания переходов между состояниями, что делает её **практически бесполезной** на данном этапе.  \n\n- **Альтернативные подходы**:  \n  - **Эмпирическое среднее**: Для оценки \\( V(s) \\) или \\( Q(s, a) \\) используются **сэмплы взаимодействий** (например, метод Монте-Карло).  \n  - Пример для оценки \\( V(s) \\):  \n    \\[\n    V(s) \\approx \\frac{1}{N} \\sum_{i=1}^{N} G_t^{(i)}, \\quad \\text{где } G_t^{(i)} \\text{ — награда из } i\\text{-го эпизода, стартовавшего в } s.\n    \\]  \n\n#### **Теоретическая значимость V-функции**  \n- **Базис для алгоритмов**:  \n  - V-функция остаётся ключевым объектом в **теоретическом анализе** сходимости и оптимальности политик.  \n  - Используется в гибридных методах (например, Actor-Critic), где \\( V(s) \\) выступает как **базовая линия** для уменьшения дисперсии оценок.  \n\n- **Практические исключения**:  \n  - В задачах с **частично наблюдаемыми состояниями** (POMDP) или при использовании аппроксиматоров (нейросети) V-функция может быть полезна для агрегации информации.  \n\n---\n\n### **Ключевой вывод**  \n- **Текущий сетап**:  \n  - В рамках рассматриваемой задачи (model-free, неизвестная динамика) **V-функция неприменима** для непосредственного улучшения политики.  \n- **Будущее применение**:  \n  - В продвинутых алгоритмах (например, **TRPO**, **PPO**) оценки \\( V(s) \\) используются для оптимизации градиентов политики, демонстрируя её **косвенную значимость**.  \n\n---\n\n#### **Сравнение с идеализированными условиями**  \n| **Аспект**               | **Идеализированный случай**                 | **Реальный мир**                          |  \n|--------------------------|---------------------------------------------|-------------------------------------------|  \n| **Данные о среде**        | Известны \\( P(s'|s, a) \\), \\( R(s, a) \\)    | Динамика неизвестна, награды шумные       |  \n| **Роль V-функции**        | Основа для вычисления политики              | Ограничена оценкой через сэмплы           |  \n| **Методы оптимизации**    | Точные (Value/Policy Iteration)             | Приближённые (Q-Learning, Policy Gradients) |  \n\n---\n\n**Следующие шаги**:  \n1. Изучение **метода Монте-Карло** для оценки V/Q-функций через эмпирическое среднее.  \n2. Анализ **Temporal Difference Learning** (TD) как компромисса между точностью и вычислительной сложностью.  \n3. Применение V-функции в **Advanced Policy Gradients** для стабилизации обучения.",
  "processed_frame_054075.jpg": "",
  "processed_frame_064000.jpg": "",
  "processed_frame_066600.jpg": "",
  "processed_frame_068925.jpg": "### **Структурированный конспект лекции**\n\n---\n\n#### **1. Переход к Model-Free методам**  \n- **Проблема Model-Based подходов**:  \n  В реальных задачах динамика среды (\\( P(s'|s, a) \\)) и награды (\\( R(s, a) \\)) часто неизвестны.  \n  - Методы, основанные на V-функции, становятся **неприменимыми**, так как требуют знания \\( P \\).\n\n- **Эволюционные стратегии** (как альтернатива):  \n  - **Идея**: Сдвигать распределение политики в сторону \"элитных\" траекторий.  \n  - **Недостатки**:  \n    - Огромная **вычислительная сложность** (требует тысячи сэмплов).  \n    - Низкая эффективность в задачах с ограниченным бюджетом взаимодействий.  \n\n---\n\n#### **2. Оценка Q-функции через эмпирические средние**  \n- **Метод Монте-Карло**:  \n  - Оценка \\( Q(s, a) \\) через среднее наград по траекториям:  \n    \\[\n    Q(s, a) \\approx \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)},\n    \\]  \n    где \\( G_t^{(i)} \\) — кумулятивная награда \\( i \\)-й траектории, стартовавшей в \\( (s, a) \\).  \n  - **Проблемы**:  \n    - Высокая **дисперсия** оценок (зависит от стохастичности среды).  \n    - Необходимость **дожидаться конца эпизода** (неприменимо в неэпизодических средах).  \n\n- **Дилемма сходимости**:  \n  Закон больших чисел гарантирует сходимость, но на практике требуется **слишком много данных**.  \n\n---\n\n#### **3. Уравнение Беллмана и оптимизация алгоритмов**  \n- **Bellman Optimality Equation**:  \n  Уникальность уравнения с **оператором максимума**:  \n  \\[\n  Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) \\max_{a'} Q^*(s', a').\n  \\]  \n  - Отличается от остальных уравнений отсутствием явного усреднения по политике.  \n\n- **Теорема Роббинса-Монро**:  \n  Условия сходимости для **онлайн-обновления Q-функции**:  \n  - Шаг обучения \\( \\alpha_k \\) должен удовлетворять:  \n    \\sum_{k=1}^\\infty \\alpha_k = \\infty \\quad \\text{и} \\quad \\sum_{k=1}^\\infty \\alpha_k^2 < \\infty.  \n  - Пример: \\( \\alpha_k = \\frac{1}{k} \\).  \n  - Гарантирует **сходимость в \\( L^2 \\)** (и по вероятности).  \n\n---\n\n#### **4. Онлайн-алгоритмы для Q-функции**  \n- **Формула обновления**:  \n  \\[\n  Q_{\\text{new}}(s, a) = (1 - \\alpha) Q_{\\text{old}}(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q_{\\text{old}}(s', a') \\right).\n  \\]  \n  - **Плюсы**:  \n    - Не требует полных траекторий (работает с **одним шагом**).  \n    - Подходит для **неэпизодических** сред.  \n  - **Минусы**:  \n    - Введение **смещения** в обмен на снижение дисперсии.  \n\n---\n\n#### **5. Проблемы реального мира**  \n- **Частичная наблюдаемость** (POMDP):  \n  Агент получает неполную информацию о состоянии среды.  \n- **Шумные награды**:  \n  Награды могут быть зашумлёнными или запаздывающими.  \n- **Нестационарность среды**:  \n  Динамика \\( P(s'|s, a) \\) может меняться со временем.  \n\n---\n\n### **Итоги**  \n- **Эффективные методы**:  \n  - **Temporal Difference (TD) Learning**: Компромисс между смещением и дисперсией.  \n  - **Q-Learning**: Обновление Q-значений через Bellman equation с гарантией сходимости.  \n- **Перспективы**:  \n  - Гибридные методы (например, **Actor-Critic**).  \n  - Использование нейросетей для аппроксимации Q-функции (**DQN**).  \n\n---\n\n**Финальный вывод**:  \nПереход от теоретических моделей (MDP, V/Q-функции) к алгоритмам, адаптированным для реальных условий (Q-Learning, Policy Gradients), требует учёта **ограничений данных** и баланса между точностью и вычислительной сложностью.",
  "processed_frame_070275.jpg": "### **Структурированный конспект лекции**\n\n---\n\n#### **1. Связь алгоритмов с уравнением Беллмана**  \n- **Метод Монте-Карло** оценивает Q-функцию через **эмпирические средние** наград по траекториям:  \n  \\[\n  Q(s, a) \\approx \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)},\n  \\]  \n  где \\( G_t^{(i)} \\) — кумулятивная награда \\( i \\)-го эпизода.  \n\n- **Недостатки метода**:  \n  - Требует завершения эпизодов.  \n  - Высокая **дисперсия** из-за зависимости от полных траекторий.  \n\n---\n\n#### **2. Уравнение Беллмана как основа для улучшения**  \n- **Оптимальная Q-функция** может быть выражена через рекуррентное соотношение:  \n  \\[\n  Q^*(s, a) = R(s, a) + \\gamma \\mathbb{E}_{s'} \\left[ \\max_{a'} Q^*(s', a') \\right].\n  \\]  \n  - Это позволяет обновлять Q-значения **локально**, без необходимости ждать конца эпизода.  \n\n---\n\n#### **3. Алгоритмы на основе уравнения Беллмана**  \n- **Q-Learning**:  \n  Обновление Q-функции с использованием **максимального Q-значения следующего состояния**:  \n  \\[\n  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right].\n  \\]  \n  - **Преимущества**:  \n    - Работает в **онлайн-режиме** (не требует полных траекторий).  \n    - **Снижает дисперсию** за счёт локальных обновлений.  \n  - **Недостатки**:  \n    - Вносит **смещение** из-за максимизации по оценкам Q-значений.  \n\n- **Теоретические гарантии**:  \n  При выполнении условий Роббинса-Монро (\\( \\sum \\alpha_k = \\infty \\), \\( \\sum \\alpha_k^2 < \\infty \\)) алгоритм **сходится к оптимальной Q-функции**.  \n\n---\n\n#### **4. Сравнение подходов**  \n| **Аспект**              | **Метод Монте-Карло**             | **Q-Learning**                   |  \n|-------------------------|------------------------------------|-----------------------------------|  \n| **Требуемые данные**     | Полные траектории                  | Один шаг (s, a, r, s')           |  \n| **Дисперсия оценок**    | Высокая                           | Умеренная                         |  \n| **Смещение**            | Нет                               | Есть (из-за максимизации)         |  \n| **Применимость**        | Только эпизодические среды        | Любые среды (включая непрерывные) |  \n\n---\n\n#### **5. Способы улучшения алгоритмов**  \n- **Комбинация методов**:  \n  Гибридные подходы (например, **TD(λ)**) объединяют преимущества Монте-Карло и Temporal Difference.  \n- **Аппроксимация Q-функции**:  \n  Использование нейросетей (DQN) для работы с **большими или непрерывными пространствами**.  \n- **Снижение дисперсии**:  \n  Внедрение **базовых линий** (например, в Actor-Critic) для стабилизации обучения.  \n\n---\n\n### **Итоги**  \n- **Q-Learning** преодолевает ключевые ограничения метода Монте-Карло через использование уравнения Беллмана.  \n- **Теоретические гарантии** сходимости делают его надёжным инструментом в model-free обучении.  \n- Для сложных сред **гибридные методы** и аппроксимация функций остаются ключевыми направлениями развития.  \n\n**Следующие шаги**:  \n- Изучение **Deep Q-Networks (DQN)** для масштабирования на высокоразмерные пространства.  \n- Внедрение механизмов **воспроизведения опыта** (Experience Replay) для стабилизации обучения.  \n- Анализ **двойного Q-Learning** для борьбы с переоценкой Q-значений.",
  "processed_frame_071600.jpg": "### **Структурированный конспект лекции**\n\n---\n\n#### **1. Temporal Difference (TD) Learning: Основные идеи**\n- **Проблема методов Монте-Карло**:  \n  Требуют завершения эпизодов, что ограничивает применение в неэпизодических средах.\n\n- **Решение через уравнение Беллмана**:  \n  Обновление Q-функции через **локальные переходы** (s, a, r, s'):\n  \\[\n  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\cdot \\underbrace{\\max_{a'} Q(s', a')}_{\\text{Таргет}} - Q(s, a) \\right].\n  \\]\n  - **Таргет**: Использует текущую оценку Q(s', a'), а не полную траекторию.  \n  - **Преимущество**: Не требует дожидаться конца эпизода.  \n\n---\n\n#### **2. Алгоритм Q-Learning**\n- **Ключевые особенности**:  \n  - **Bootstrapping**: Оценка будущих наград через текущие значения Q (самообучение).  \n  - **Off-policy**: Максимизация по **всем возможным действиям**, а не текущей политике (агностичен к способу выбора действий).  \n\n- **Сходимость**:  \n  При выполнении условий Роббинса-Монро (\\( \\sum \\alpha_k = \\infty \\), \\( \\sum \\alpha_k^2 < \\infty \\)) сходится к **оптимальной Q-функции**.  \n\n---\n\n#### **3. Сравнение алгоритмов**\n| **Аспект**          | **Q-Learning**                                | **TD(0)** (SARSA)                          |  \n|----------------------|----------------------------------------------|--------------------------------------------|  \n| **Таргет**           | \\( r + \\gamma \\max_{a'} Q(s', a') \\)         | \\( r + \\gamma Q_{\\text{current}}(s', a') \\) |  \n| **Тип политики**     | Off-policy (максимизация)                    | On-policy (следует текущей политике)       |  \n| **Смещение/Дисперсия** | Смещение из-за максимизации               | Меньше смещения, но выше дисперсия         |  \n\n---\n\n#### **4. Роль коэффициента обучения (α)**\n- **Настройка α**:  \n  - Может быть **глобальной** (для всех пар (s, a)) или **адаптивной** (отдельно для каждого состояния-действия).  \n  - Пример: \\( \\alpha_k = \\frac{1}{\\text{число посещений (s, a)} } \\).  \n\n- **Теоретическое обоснование**:  \n  Для гарантии сходимости α должна удовлетворять:  \n  - \\( \\sum_{k=1}^\\infty \\alpha_k(s, a) = \\infty \\),  \n  - \\( \\sum_{k=1}^\\infty \\alpha_k^2(s, a) < \\infty \\).  \n\n---\n\n#### **5. Проблемы и ограничения**\n- **Переоценка Q-значений**:  \n  Максимизация может приводить к завышению оценок. Решение — **Double Q-Learning** (раздельные оценки).  \n- **Нестационарность среды**:  \n  Динамика \\( P(s'|s, a) \\) меняется → требуется постоянная адаптация Q-функции.  \n\n---\n\n### **Итоги**\n- **Q-Learning**:  \n  - Мощный инструмент для задач с **неизвестной динамикой** среды.  \n  - **Способность к off-policy обучению** позволяет исследовать среду, не следуя текущей политике.  \n- **Перспективы улучшения**:  \n  - Внедрение **нейросетей** (DQN) для масштабирования.  \n  - Механизмы **Experience Replay** для снижения корреляции данных.  \n\n**Следующие шаги:**  \n- Реализация Q-Learning в симуляторах (OpenAI Gym).  \n- Анализ **Epsilon-Greedy** стратегии для баланса исследования/эксплуатации.  \n\n_Обновление 13:37 (дополнено сравнение Q-Learning и SARSA)._",
  "processed_frame_081925.jpg": "### **Сравнение алгоритмов RL: Monte Carlo, Q-Learning, SARSA**  \n\n---\n\n#### **1. Основные свойства алгоритмов**  \n| **Свойство**            | **Monte Carlo**                            | **Q-Learning**                          | **SARSA**                              |  \n|-------------------------|-------------------------------------------|-----------------------------------------|----------------------------------------|  \n| **Тип обучения**         | Off-policy/On-policy (зависит от сэмплов) | Off-policy                              | On-policy                              |  \n| **Использование данных** | Полные траектории эпизодов                | Один шаг (s, a, r, s')                 | Один шаг (s, a, r, s', a')            |  \n| **Таргет**              | Кумулятивная награда \\( G_t \\)             | \\( r + \\gamma \\max_{a'} Q(s', a') \\)    | \\( r + \\gamma Q(s', a') \\)             |  \n| **Смещение/Дисперсия**  | Нет смещения, высокая дисперсия           | Смещение из-за максимума, умеренная дисперсия | Меньше смещения, выше дисперсия       |  \n| **Сходимость**           | Сходится к \\( V^\\pi \\)                    | Сходится к \\( Q^* \\)                    | Сходится к \\( Q^\\pi \\)                 |  \n\n---\n\n#### **2. Когда какой алгоритм лучше?**  \n- **Monte Carlo**:  \n  - **Плюсы**:  \n    - Точная оценка политики (несмещённые оценки).  \n    - Подходит для **эпизодических сред** с четкими границами шагов.  \n  - **Минусы**:  \n    - Требует завершения эпизодов → неприменим в бесконечных средах.  \n    - Высокая дисперсия из-за зависимости от полных траекторий.  \n  - **Когда использовать**:  \n    Для оценки политики в симуляциях, где можно генерировать много эпизодов.  \n\n- **Q-Learning**:  \n  - **Плюсы**:  \n    - **Off-policy**: Может обучаться на данных, собранных любой политикой.  \n    - Эффективен для поиска **оптимальной политики** \\( \\pi^* \\).  \n  - **Минусы**:  \n    - **Переоценка Q-значений** из-за максимизации.  \n  - **Когда использовать**:  \n    Для задач с **неизвестной средой**, где нужно находить оптимальные стратегии.  \n\n- **SARSA**:  \n  - **Плюсы**:  \n    - **On-policy**: Учитывает текущую политику, лучше для рискованных сред (например, cliffs).  \n    - Меньше переоценки по сравнению с Q-Learning.  \n  - **Минусы**:  \n    - Сходится к политике, зависящей от текущего исследования.  \n  - **Когда использовать**:  \n    Для **безопасного обучения** (робототехника, управление), где важно избегать опасных состояний.  \n\n---\n\n#### **3. Теоретические гарантии**  \n- **Monte Carlo**:  \n  Сходится к \\( V^\\pi \\) при бесконечных эпизодах (закон больших чисел).  \n\n- **Q-Learning**:  \n  Сходится к \\( Q^* \\), если:  \n  - Все пары (s, a) посещаются бесконечно часто.  \n  - \\( \\alpha \\) (learning rate) удовлетворяет условиям Роббинса-Монро.  \n\n- **SARSA**:  \n  Сходится к \\( Q^\\pi \\), если политика **становится жадной** (например, с затухающим \\( \\epsilon \\)-greedy).  \n\n---\n\n### **Практические выводы**  \n- **Для задач с известной моделью среды**:  \n  Используйте **Value/Policy Iteration** (высокая точность, но требует ресурсов).  \n- **Для model-free задач**:  \n  - Если нужна **оптимальная стратегия** → **Q-Learning**.  \n  - Если важна **безопасность/стабильность** → **SARSA**.  \n  - Для анализа уже собранных траекторий → **Monte Carlo**.  \n\n_Следующий шаг:_ Эксперименты с балансом **exploration-exploitation** (ε-greedy, softmax) для улучшения сходимости.",
  "processed_frame_087125.jpg": "**Исследование (Exploration) vs. Эксплуатация (Exploitation): Основы**\n\n---\n\n### **Проблема баланса**\nВ reinforcement learning (RL) агент должен решать, когда:\n1. **Эксплуатировать** текущие знания (*exploit*): Выбирать действия, которые, по мнению агента, максимизируют награду.\n2. **Исследовать** (*explore*): Пробовать новые действия, чтобы уточнить оценки Q-функции и найти потенциально более эффективные стратегии.\n\n### **Почему это важно?**\n- **Слишком много исследования**: Агент тратит время на случайные действия, не используя найденные хорошие стратегии.\n- **Слишком много эксплуатации**: Агент «застревает» в субоптимальных действиях, не обнаруживая более выгодных вариантов.\n\n---\n\n### **Методы управления exploration-exploitation**\n\n#### 1. **ε-Greedy**  \n- **Идея**:  \n  - С вероятностью **ε** выбрать случайное действие (exploration).  \n  - С вероятностью **1–ε** выбрать действие с максимальным Q-значением (exploitation).  \n- **Настройка**:  \n  - Начинать с высокого ε (например, 1.0) и постепенно уменьшать (например, ε_k = 1/k).  \n- **Плюсы**: Простота реализации.  \n- **Минусы**: Даже при малых ε есть риск случайных \"глупых\" действий.\n\n#### 2. **Оптимистичная инициализация**  \n- **Идея**:  \n  - Инициализировать Q-значения завышенными значениями (например, 100 вместо 0).  \n  - Агент будет активнее исследовать действия, пока не обучится их истинной ценности.  \n- **Плюсы**: Не требует явного параметра ε.  \n- **Минусы**: Работает только в начале обучения.\n\n#### 3. **Softmax (Boltzmann Exploration)**  \n- **Идея**:  \n  - Выбирать действия с вероятностями, пропорциональными их Q-значениям:  \n    \\[\n    P(a|s) = \\frac{e^{Q(s,a)/τ}}{\\sum_{a'} e^{Q(s,a')/τ}},\n    \\]  \n    где **τ** (\"температура\") регулирует разброс вероятностей.  \n- **Плюсы**: Учитывает относительные преимущества действий.  \n- **Минусы**: Требует настройки τ.\n\n#### 4. **Upper Confidence Bound (UCB)**  \n- **Идея**:  \n  - Выбирать действие, которое максимизирует сумму:  \n    \\[\n    Q(s,a) + c \\cdot \\sqrt{\\frac{\\ln t}{N(s,a)}},\n    \\]  \n    где \\( N(s,a) \\) — число посещений (s,a), \\( t \\) — общее число шагов.  \n- **Плюсы**: Балансирует exploration/exploitation через статистическую уверенность.  \n- **Минусы**: Вычислительная сложность.\n\n---\n\n### **Теоретическая гарантия: GLIE (Greedy in the Limit with Infinite Exploration)**  \nДля сходимости к оптимальной политике требуется:  \n1. **Бесконечное исследование**: Каждое действие выбирается бесконечно часто (\\( \\lim_{t \\to ∞} N(s,a) = ∞ \\)).  \n2. **Постепенный переход к эксплуатации**: ε → 0 (например, ε_k = 1/k).  \n\nПример: **ε-Greedy** с затуханием ε удовлетворяет GLIE.\n\n---\n\n### **Практические советы**  \n- **Для простых сред**: ε-Greedy с затуханием.  \n- **Для сложных/стохастических сред**: Комбинировать методы (например, UCB + Softmax).  \n- **В глубоком RL**: Добавлять энтропийный штраф (в Policy Gradients) или использовать Noisy Nets.  \n\n---\n\n### **Пример алгоритма (Q-Learning + ε-Greedy)**  \n```python\nepsilon = 1.0  \nalpha = 0.1  \ngamma = 0.99  \n\nfor episode in range(num_episodes):  \n    state = env.reset()  \n    done = False  \n    while not done:  \n        if random.uniform(0, 1) < epsilon:  \n            action = env.action_space.sample()  # Exploration  \n        else:  \n            action = np.argmax(Q[state, :])     # Exploitation  \n        next_state, reward, done, _ = env.step(action)  \n        # Обновление Q-функции  \n        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])  \n        state = next_state  \n    epsilon = max(epsilon * 0.995, 0.01)        # Затухание ε  \n```\n\n---\n\n**Key Takeaway**:  \nИнтеллектуальный баланс между исследованием и эксплуатацией — ключ к эффективному обучению в RL. Выбор метода зависит от задачи, но начинать лучше с **ε-Greedy**.",
  "processed_frame_091875.jpg": "### **Баланс между исследованием (Exploration) и эксплуатацией (Exploitation) в Reinforcement Learning**\n\n---\n\n#### **1. Проблема в двух словах**\nВ Reinforcement Learning (RL) агент постоянно сталкивается с дилеммой:\n- **Эксплуатация** — использовать текущие знания для максимизации награды (например, выбирать действие с наивысшим Q-значением).\n- **Исследование** — пробовать новые действия, чтобы улучшить знания о среде (даже если это временно снижает награду).\n\nЕсли агент слишком часто эксплуатирует, он может пропустить более выгодные стратегии. Если слишком много исследует — «теряет время» на случайные действия.  \n**Пример из жизни**:  \nВы ходите в любимый ресторан (эксплуатация), но иногда пробуете новые заведения (исследование). Если новое место окажется лучше, вы выигрываете в долгосрочной перспективе.\n\n---\n\n#### **2. Почему это критично для RL?**\n- **Сходимость к оптимальной стратегии**: Без исследования агент никогда не узнает, что некоторые действия могут давать бóльшую награду.\n- **Trade-off (компромисс)**: Исследование имеет «стоимость» — агент временно действует неоптимально. Но это плата за потенциальное улучшение стратегии в будущем.\n\n---\n\n#### **3. Методы балансировки**\n| **Метод**               | **Как работает**                                                                 | **Пример**                                   |\n|--------------------------|---------------------------------------------------------------------------------|---------------------------------------------|\n| **ε-Greedy**             | С вероятностью ε выбирает случайное действие, иначе — жадное.                   | Q-Learning с затухающим ε (ε = 1 → 0).      |\n| **Softmax**              | Вероятность выбора действия зависит от его Q-значения (через температуру τ).    | \\( P(a) \\propto e^{Q(a)/τ} \\), τ ↓ со временем. |\n| **UCB**                  | Выбирает действия с учётом не только их ценности, но и неопределённости.        | \\( Q(a) + c \\sqrt{\\ln t / N(a)} \\).         |\n| **Optimistic Initialization** | Начинает с завышенных Q-значений, чтобы стимулировать исследование.          | Q(s,a) = 100 вместо 0.                      |\n\n---\n\n#### **4. Пример: ε-Greedy в Q-Learning**\n```python\nepsilon = 0.1  # 10% шанс на исследование\nalpha = 0.5    # Скорость обучения\ngamma = 0.9    # Коэффициент дисконтирования\n\nfor episode in episodes:\n    state = env.reset()\n    done = False\n    while not done:\n        if np.random.rand() < epsilon:\n            action = env.action_space.sample()  # Случайное действие (exploration)\n        else:\n            action = np.argmax(Q[state])        # Жадное действие (exploitation)\n        \n        next_state, reward, done, _ = env.step(action)\n        # Обновление Q-функции\n        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n        state = next_state\n```\n\n**Пояснение**:  \nАгент 90% времени использует лучшие известные действия, но 10% пробует новые варианты. Это гарантирует, что он не «застрянет» в локальном оптимуме.\n\n---\n\n#### **5. Теоретическая основа: GLIE (Greedy in the Limit with Infinite Exploration)**\nДля сходимости к оптимальной политике алгоритм должен удовлетворять:\n1. **Бесконечное исследование**: Каждое действие выбирается бесконечно много раз (\\( N(s,a) \\to ∞ \\)).\n2. **Постепенная эксплуатация**: Агент перестаёт исследовать в пределе (напр., ε → 0).  \n\nПример: ε-greedy с \\( \\epsilon_k = \\frac{1}{k} \\). Здесь ε уменьшается со временем, но каждое действие всё равно посещается бесконечно часто.\n\n---\n\n#### **6. Философия компромисса**\n- **Кратковременные потери vs долгосрочная выгода**: Исследование временно снижает награду, но открывает путь к улучшениям. Это аналогично инвестициям в обучение или R&D.\n- **Адаптивность**: Баланс должен меняться со временем. На старте обучения нужно больше исследования, позже — эксплуатации.\n- **Риски**: В некоторых средах (например, управление роботом) ошибки при исследовании могут быть критичными. Здесь методы вроде **Safe Exploration** ограничивают рискованные действия.\n\n---\n\n### **Заключение**\nБаланс между исследованием и эксплуатацией — это не просто техническая задача, а философский принцип RL. Как и в жизни, успех зависит от умения сочетать проверенные стратегии с готовностью пробовать новое. Алгоритмы вроде Q-Learning с ε-greedy формализуют этот баланс, обеспечивая сходимость к оптимальной стратегии ценой временных «потерь» на исследование.",
  "processed_frame_094325.jpg": "**Баланс между исследованием (exploration) и эксплуатацией (exploitation): формальное представление**\n\n---\n\n### **1. Определение ключевых понятий**\n1. **Жадная политика (Greedy Policy)**:  \n   Выбор действия с максимальной оценкой \\( Q(s, a) \\):  \n   \\[\n   \\pi_{\\text{г}}(a|s) = \n   \\begin{cases}\n   1, & \\text{если } a = \\arg\\max_{a'} Q(s, a'), \\\\\n   0, & \\text{иначе}.\n   \\end{cases}\n   \\]\n   - **Преимущество**: Максимизирует награду *согласно текущим знаниям*.  \n   - **Риск**: Может пропустить лучшие стратегии, если оценки \\( Q \\) неточны.\n\n2. **Случайная политика (Random Policy)**:  \n   Равномерное распределение вероятностей по всем действиям:  \n   \\[\n   \\pi_{\\text{с}}(a|s) = \\frac{1}{|\\mathcal{A}|}, \\quad \\forall a \\in \\mathcal{A}.\n   \\]\n   - **Преимущество**: Максимизирует исследование.  \n   - **Недостаток**: Действует неоптимально.\n\n---\n\n### **2. Компромисс: смешанные стратегии**\nДля баланса между исследованием и эксплуатацией используются **смешанные политики**, такие как ε-greedy.\n\n#### **ε-Greedy Политика**  \nКомбинация жадной и случайной политик:  \n\\[\n\\pi_{\\epsilon}(a|s) = \n\\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{если } a = \\arg\\max_{a'} Q(s, a'), \\\\\n\\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{иначе}.\n\\end{cases}\n\\]  \n- **Параметр ε**:  \n  - Контролирует баланс:  \n    - ε = 0 → чистая эксплуатация.  \n    - ε = 1 → чистое исследование.  \n\n#### **Пример**:  \nПри \\( \\epsilon=0.1 \\) и 4 действиях:  \n- Вероятность выбрать жадное действие: \\( 1 - 0.1 + \\frac{0.1}{4} = 0.925 \\).  \n- Вероятность других действий: \\( \\frac{0.1}{4} = 0.025 \\).\n\n---\n\n### **3. Теоретическая гарантия GLIE**\nДля сходимости к оптимальной политике \\( \\pi^* \\) алгоритм должен удовлетворять:  \n1. **Бесконечное исследование**:  \n   \\[\n   \\sum_{k=1}^\\infty N_k(s, a) = \\infty \\quad \\forall (s, a),  \n   \\]  \n   где \\( N_k(s, a) \\) — число посещений пары \\( (s, a) \\) на шаге \\( k \\).  \n\n2. **Постепенный переход к эксплуатации**:  \n   \\[\n   \\lim_{k \\to \\infty} \\epsilon_k = 0.  \n   \\]  \n   Пример: ε-жадный с \\( \\epsilon_k = \\frac{1}{k} \\).\n\n---\n\n### **4. Альтернативные методы баланса**\n#### **UCB (Upper Confidence Bound)**  \nВыбор действия через баланс ценности и неопределённости:  \n\\[\na^* = \\arg\\max_{a} \\left[ Q(s, a) + c \\cdot \\sqrt{\\frac{\\ln t}{N(s, a)}} \\right],  \n\\]  \nгде \\( N(s, a) \\) — число посещений действия \\( a \\), \\( t \\) — общее число шагов.  \n\n#### **Softmax (Boltzmann Exploration)**  \nВероятности действий экспоненциально зависят от их Q-значений:  \n\\[\n\\pi(a|s) = \\frac{e^{Q(s, a)/\\tau}}{\\sum_{a'} e^{Q(s, a')/\\tau}},  \n\\]  \nгде \\( \\tau \\) (\"температура\") регулирует случайность выбора.\n\n---\n\n#### **Примеры алгоритмов с балансов**\n| **Алгоритм**       | **Метод баланса**         | **Свойства**                                   |  \n|---------------------|---------------------------|-----------------------------------------------|  \n| **Q-Learning**      | ε-greedy                  | Off-policy, сходится к \\( Q^* \\).             |  \n| **SARSA**           | On-policy ε-greedy        | Учитывает текущую политику, безопаснее.       |  \n| **Thompson Sampling**| Байесовские оценки      | Использует распределение неопределённости.    |  \n\n---\n\n**Итог**:  \nБаланс между exploration и exploitation формализуется через параметры (ε, τ, c), которые контролируют компромисс между использованием текущих знаний и поиском новых стратегий. Теоретически обоснованные методы (GLIE) гарантируют сходимость к оптимальной политике.",
  "processed_frame_096500.jpg": "### **Стохастические политики на основе Q-функции**\n\nДля управления балансом между исследованием и эксплуатацией в RL используются **стохастические политики**, которые зависят от Q-функции. Вот примеры:\n\n---\n\n#### **1. Softmax (Boltzmann) Policy**  \nПреобразует Q-значения в вероятности с помощью **экспоненциального распределения**:  \n\\[\n\\pi(a|s) = \\frac{e^{Q(s,a) / \\tau}}{\\sum_{a'} e^{Q(s,a') / \\tau}},\n\\]  \nгде:  \n- \\( \\tau \\) (*температура*) — параметр, управляющий уровнем исследования:  \n  - \\( \\tau \\to 0 \\): Жадная политика (выбор действия с максимальным \\( Q \\)).  \n  - \\( \\tau \\to \\infty \\): Случайная политика (равные вероятности для всех действий).  \n\n**Пример**:  \nПри \\( Q(s, a_1)=2 \\), \\( Q(s, a_2)=1 \\), \\( \\tau=1 \\):  \n\\[\n\\pi(a_1|s) = \\frac{e^{2}}{e^{2} + e^{1}} \\approx 0.73, \\quad \\pi(a_2|s) = \\frac{e^{1}}{e^{2} + e^{1}} \\approx 0.27.\n\\]\n\n---\n\n#### **2. Политика с добавлением шума**  \nQ-функция модифицируется случайным шумом перед выбором действия:  \n\\[\n\\tilde{Q}(s, a) = Q(s, a) + \\mathcal{N}(0, \\sigma^2),  \n\\]  \n\\[\n\\pi(s) = \\arg\\max_a \\tilde{Q}(s, a),\n\\]  \nгде \\( \\sigma \\) регулирует силу шума.  \n\n**Пример (σ=0.5)**:  \nДля \\( Q(s, a_1)=3 \\), \\( Q(s, a_2)=2.8 \\):  \nДобавленный шум может сделать \\( a_2 \\) предпочтительнее.\n\n---\n\n#### **3. Thresholded Softmax**  \nСочетание детерминированного выбора и случайности:  \n- С вероятностью \\( \\delta \\) выбирается действие с максимальным \\( Q \\).  \n- С вероятностью \\( 1-\\delta \\) — случайное действие из остальных.  \n\n**Формула**:  \n\\[\n\\pi(a|s) = \n\\begin{cases}\n1 - \\delta + \\frac{\\delta}{|\\mathcal{A}|}, & \\text{если } a = \\arg\\max_{a'} Q(s, a'), \\\\\n\\frac{\\delta}{|\\mathcal{A}|}, & \\text{иначе}.\n\\end{cases}\n\\]  \n\n---\n\n#### **4. Персистентное исследование**  \nИспользование **истории действий** для смещения выбора:  \n- Увеличивать вероятность действий, которые выбирались реже.  \nПример:  \n\\[\n\\pi(a|s) \\propto \\frac{Q(s, a)}{N(s, a) + 1},\n\\]  \nгде \\( N(s, a) \\) — количество посещений пары \\( (s, a) \\).  \n\n---\n\n#### **5. Gradient-based Policy (в нейросетях)**  \nДля параметризованных политик (например, Actor-Critic):  \n- Вход: Состояние \\( s \\).  \n- Выход: Логиты действий \\( z_a \\), преобразованные через softmax.  \n- Обучение: Оптимизация через градиенты для максимизации \\( Q(s, a) \\).  \n\n**Пример архитектуры**:  \n\\[\n\\begin{cases}\nz_a = W \\cdot s + b, \\\\\n\\pi(a|s) = \\text{softmax}(z_a), \\\\\n\\end{cases}\n\\]  \nгде \\( W, b \\) — обучаемые веса.\n\n---\n\n### **Сравнение методов**\n| **Политика**       | **Плюсы**                                  | **Минусы**                             |  \n|---------------------|--------------------------------------------|----------------------------------------|  \n| **ε-greedy**        | Простота, интерпретируемость              | Повышенный риск неоптимальных действий |  \n| **Softmax**         | Учитывает относительные Q-значения          | Требует настройки \\( \\tau \\)            |  \n| **Шумовая модификация** | Устойчивость к локальным оптимумам       | Непредсказуемость                     |  \n| **Thresholded**     | Контроль уровня исследования              | Жёсткие границы между режимами         |  \n\n---\n\n**Вывод**:  \nSoftmax и шумовая модификация — наиболее популярные методы для стохастических политик.  \nДля задач с **стратегической неопределённостью** предпочтительны **Softmax** или **Gradient-based** политики.  \nДля **детерминированных алгоритмов** (DDPG, TD3) используется **шум** в пространстве действий (Ornstein-Uhlenbeck).",
  "processed_frame_100800.jpg": "",
  "processed_frame_104325.jpg": "**Сравнение Q-Learning (off-policy) и SARSA (on-policy)**\n\n---\n\n### **1. Off-policy (Q-Learning)**\n- **Цель**: Найти **оптимальную политику** вне зависимости от того, как собирались данные.  \n- **Обновление Q-функции**:  \n  \\[\n  Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\right].\n  \\]\n- **Преимущества**:  \n  - Учится на любых данных (даже случайных, неоптимальных).  \n  - Сходится к \\( Q^* \\), если все действия посещаются достаточно часто.  \n- **Недостатки**:  \n  - Переоценивает Q-значения из-за максимизации.  \n  - Может быть опасен в реальных задачах (например, робот падает со стола).  \n\n---\n\n### **2. On-policy (SARSA)**  \n- **Цель**: Оценить и улучшить **текущую политику**.  \n- **Обновление Q-функции**:  \n  \\[\n  Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s,a) \\right],\n  \\]  \n  где \\( a' \\) выбирается из текущей политики \\( \\pi(s') \\) (например, ε-greedy).  \n- **Преимущества**:  \n  - Учитывает **стратегию исследования** (например, риск избегания).  \n  - Более стабилен в рискованных средах.  \n- **Недостатки**:  \n  - Требует обновлять данные только от текущей политики.  \n  - Медленнее сходится, если политика слишком \"осторожная\".  \n\n---\n\n### **Как выбрать?**\n| **Критерий**       | **Q-Learning**                               | **SARSA**                                  |  \n|----------------------|----------------------------------------------|--------------------------------------------|  \n| **Источник данных**  | Любые данные (off-policy).                   | Только текущая политика (on-policy).       |  \n| **Оптимальность**    | Строит \\( Q^* \\).                           | Строит \\( Q^\\pi \\).                        |  \n| **Безопасность**     | Рискован (выбирает максимум).                | Осторожен (следует политике).              |  \n| **Пример применения**| Игры, задачи с чёткими правилами.            | Робототехника, управление с риском.        |  \n\n---\n\n### **Иллюстрация**\n- **SARSA**: В лабиринте с пропастями агент учится обходить опасные зоны.  \n- **Q-Learning**: Сокращает путь через пропасть, но может упасть при неточности Q-функции.  \n\n**Вывод**:  \nВыбор между Q-Learning и SARSA зависит от задачи.  \n- **Q-Learning** — для задач с допустимым риском и поиска оптимальности.  \n- **SARSA** — для безопасных сред или когда важна стабильность.  \nГибриды (Double Q-Learning) и методы вроде **PPO** объединяют плюсы обоих подходов.",
  "processed_frame_107825.jpg": "",
  "processed_frame_112025.jpg": "**Разъяснение SARSA и процесса обновления Q-функции**\n\n---\n\n### **1. Суть SARSA**  \nSARSA — **on-policy** алгоритм, который обновляет Q-значения на основе действий, выбранных **текущей политикой** (включая исследование через ε или softmax).  \n- Формула обновления:  \n  \\[\n  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s, a) \\right],\n  \\]  \n  где \\( a' \\) — действие, выбранное в состоянии \\( s' \\) **текущей политикой**.\n\n---\n\n### **2. Как работает SARSA на практике**  \n- **Шаг 1**: Агент в состоянии \\( s \\) выбирает действие \\( a \\) (например, через ε-greedy).  \n- **Шаг 2**: Переходит в состояние \\( s' \\), получает награду \\( r \\).  \n- **Шаг 3**: **Выбирает следующее действие \\( a' \\) в состоянии \\( s' \\)** (например, снова через ε-greedy).  \n- **Шаг 4**: Обновляет \\( Q(s, a) \\), используя \\( (s, a, r, s', a') \\).  \n\n**Пример**:  \nЕсли агент рядом с «пропастью» и выбирает осторожное действие \\( a' \\) (из-за ε-greedy), SARSA учится избегать рискованных шагов.\n\n---\n\n### **3. Особенности реализации**  \n- **Для обновления критично знать \\( a' \\)**:  \n  Невозможно обновить Q-функцию без выбора следующего действия \\( a' \\) **после перехода в \\( s' \\)**.  \n- **Причина**: SARSA оценивает политику, которая учитывает её **собственную случайность** (например, ε-greedy).  \n\n#### **Пример ошибки**:  \nЕсли вместо \\( a' \\) (выбранного политикой) использовать \\( \\max_{a'} Q(s', a') \\), алгоритм превратится в Q-Learning.  \n\n---\n\n### **4. Ответ на ваш вопрос**  \n**Если вы получили только \\( (s, a, r, s') \\)** (без \\( a' \\)), SARSA **нельзя корректно применить**:  \n1. **Данные неполные**: Нужен пятый элемент \\( a' \\), выбранный в \\( s' \\).  \n2. **Решение**:  \n   - Либо использовать Q-Learning (не требует \\( a' \\)).  \n   - Либо собирать полные данные \\( (s, a, r, s', a') \\).  \n\n---\n\n### **5. Сравнение с Q-Learning**  \n| **Аспект**      | **SARSA**                                    | **Q-Learning**                       |  \n|------------------|----------------------------------------------|---------------------------------------|  \n| **Требуемые данные** | Полный кортеж \\( (s, a, r, s', a') \\)       | Достаточно \\( (s, a, r, s') \\)       |  \n| **Где брать \\( a' \\)** | Выбирается текущей политикой в \\( s' \\).    | Не требуется — используется \\( \\max \\). |  \n| **Риск**         | Менее рискован (следит за политикой).       | Может переоценить награды.           |  \n\n---\n\n### **Практическая рекомендация**  \n- Если вы используете **опыт из буфера** (experience replay), SARSA не подходит — в буфере чаще хранят \\( (s, a, r, s') \\), а \\( a' \\) зависит от **актуальной политики**, которая меняется со временем.  \n- Если нужно обучаться оффлайн — выбирайте **Q-Learning** или **DQN**.  \n\n**Итог**:  \nSARSA требует явного выбора следующего действия \\( a' \\) в рамках текущей стратегии, что делает его стабильным для реальных задач, но ограничивает гибкость. Используйте его, когда важнее безопасность, а не скорость сходимости.",
  "processed_frame_115925.jpg": "",
  "processed_frame_127525.jpg": "**Сравнение Q-Learning и SARSA в контексте Exploration-Exploitation**\n\n---\n\n### **1. Q-Learning (Off-Policy)**  \n- **Цель**: Нахождение **оптимальной политики** (\\( \\pi^* \\)), игнорируя текущее поведение агента.  \n- **Свойства**:  \n  - Максимизирует Q-значения без учёта стратегии исследования (например, жадный выбор через \\( \\max_{a'} Q(s', a') \\)).  \n  - **Преимущество**: Может находить кратчайший путь (минимальные шаги в лабиринте) при точной оценке Q-функции.  \n  - **Риск**: Из-за максимизации может выбрать опасные действия (например, переход через «обрыв»).  \n\n---\n\n### **2. SARSA (On-Policy)**  \n- **Цель**: Оценка и улучшение **текущей политики** (\\( \\pi \\)) с учётом её стохастичности (например, ε-greedy).  \n- **Свойства**:  \n  - Учитывает риск случайных действий при обновлении Q-значений (например, вероятность «прыжка в лаву»).  \n  - **Преимущество**: Сходится к **безопасной политике**, избегая опасных переходов.  \n  - **Недостаток**: Длинные пути (например, обход обрыва), если Q-функция недооценивает риски.  \n\n---\n\n### **Почему SARSA выбирает безопасный путь?**  \nSARSA обновляет Q-значения на основе **следующего действия** \\( a' \\), выбранного текущей политикой (например, ε-greedy):  \n\\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s', \\underbrace{a'}_{\\text{выбрано через } \\pi}) - Q(s,a) \\right].\n\\]  \n- Если политика допускает случайные действия (\\( \\epsilon > 0 \\)), агент «осознаёт» риск перехода в опасные состояния (например, \\( s' \\) рядом с обрывом).  \n- Высокая вероятность случайного «падения» снижает Q-значения для рискованных действий → агент учится их избегать.  \n\n---\n\n### **Почему Q-Learning рискованнее?**  \nQ-Learning игнорирует способ выбора \\( a' \\):  \n\\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\underbrace{\\max_{a'} Q(s', a')}_{\\text{идеальный максимум}} - Q(s,a) \\right].\n\\]  \n- Предполагает, что все переходы оптимальны.  \n- Если Q-значения неточны, агент может выбрать опасный путь, не учитывая реальных рисков.  \n\n---\n\n### **Пример в лабиринте**  \n- **Q-Learning**:  \n  Сойдётся к кратчайшему пути, даже если он ведёт через обрыв. Если Q-функция переоценена, агент может упасть.  \n- **SARSA**:  \n  Избегает переходов вблизи обрыва из-за учёта случайных действий → выбирает обходной путь.  \n\n---\n\n### **Итог**  \n- **Q-Learning** лучше для задач, где важна **оптимальность**, а среда предсказуема.  \n- **SARSA** предпочтителен в **рискованных средах**, где случайные действия могут быть опасны.  \n\n**Парадокс SARSA**:  \nАгент учится избегать опасностей, даже если они возникают из-за его **собственной** исследовательской стратегии (например, ε-greedy). Это обеспечивает безопасность, но замедляет обучение.",
  "processed_frame_134100.jpg": ""
}