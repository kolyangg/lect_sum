{
  "processed_frame_014825.jpg": "* Второе занятие посвящено model-free сеттингам, когда информация о среде минимальна.\n* Model-free сеттинги важны для решения реальных задач.\n* Напоминание о прошлом занятии.",
  "processed_frame_016150.jpg": "* MDP (Марковский процесс принятия решений) состоит из:\n  * Action space - набор возможных действий в различных ситуациях, влияющих на исход и получаемую награду.\n  * Состояние - способ описания текущей ситуации.\n  * Условная вероятность перехода из состояния S в состояние S' при выполнении действия A, зависящая только от текущего состояния и действия (марковость и стационарность).\n  * Награда - детерминированная функция от состояния и действия.\n  * Политика - правило принятия решений, определяющее, какое действие выбрать в каждом состоянии.\n  * Вероятность перехода - вероятность перехода из состояния S в состояние S' при выполнении действия A, зависящая только от текущего состояния и действия (марковость и стационарность).\n  * Награда - детерминированная функция от состояния и действия.\n  * Политика - правило принятия решений, определяющее действие в каждом состоянии.\n  * Цель - максимизация ожидаемой дисконтированной суммы наград.\n  * Награда - детерминированная функция от состояния и",
  "processed_frame_019925.jpg": "* Новые понятия:\n  * GT - кумулятивная награда, случайная величина.\n  * Функция v(s, p) - ожидаемая награда, получаемая при начале игры из состояния s и дальнейшем следовании политике p.\n  * Функция q(s, a, p) - ожидаемая награда, получаемая при выполнении действия a в состоянии s и дальнейшем следовании политике p.\n  * Reward to go - кумулятивная награда, часто обозначаемая как GT или RT.",
  "processed_frame_021700.jpg": "* Кумулятивная награда (GT) считается с помощью дисконтирования всех immediate reward на траектории.\n* GT подчиняется рекуррентному соотношению: GT = RT + γGT+1.\n* Дискаунт фактор γ определяет вклад будущих наград в текущее благосостояние:\n  * γ = 0: максимизируется текущая награда.\n  * γ = 1: все награды имеют одинаковый вес.\n  * 0 < γ < 1: экспоненциальное убывание веса наград.\n* Эпизодические задачи: взаимодействие заканчивается через фиксированное количество шагов.\n* В эпизодических задачах можно закладывать информацию о текущем шаге в состояние.\n* Пример: в задаче continuous control агент может использовать информацию о конце эпизода для выполнения отчаянных действий.\n* Максимизация G0 по политике включает ожидание по политике и динамике среды.\n* Для корректной работы алгоритмов RL предполагается, что динамика среды известна.",
  "processed_frame_027775.jpg": "* Space конечен: не так много реальных сред удовлетворяют этому условию, что делает сценарий менее реалистичным.\n* Action space конечен: больше сред удовлетворяет этому условию, что делает его менее строгим.\n* Если выполнены оба условия, можно вывести уравнение Беллмана, связывающее функцию в текущем состоянии.",
  "processed_frame_029250.jpg": "* Уравнение Беллмана связывает A и Q функции в текущем состоянии через состояние, в которое можно попасть.\n* Для V функции рассматривается состояние, для Q функции - пара состояния и действия.\n* Оптимальная политика лучше всех, для неё V функция называется V*. Уравнения для V* остаются теми же, но с заменой ожидания на максимум.\n* Теорема о существовании детерминированной оптимальной политики в конечной МДП подтверждает это.\n* В уравнениях стоят суммы из-за конечности пространства состояний и действий.\n* Value Iteration - алгоритм решения задачи, использующий уравнения Беллмана как операторы сжатия.\n* Теорема Банаха о сжатых отображениях гарантирует сходимость Value Iteration к неподвижной точке, независимо от начального приближения.",
  "processed_frame_032875.jpg": "* Алгоритмы сводятся к двум действиям: Policy Evaluation и Policy Improvement.\n* Policy Evaluation оценивает текущую политику, определяя, сколько наград можно получить, следуя этой политике.\n* Policy Improvement улучшает политику, выбирая действие, которое максимизирует ожидаемую награду.\n* Для оптимальной политики V* функция подчиняется уравнению Беллмана с заменой ожидания на максимум.\n* Теорема о существовании детерминированной оптимальной политики в конечной МДП подтверждает корректность подхода.\n* В уравнении для V* функции заменяется ожидание на максимум.\n* Value Iteration и Policy Iteration используют эти уравнения.\n* Policy Evaluation оценивает текущую политику, Policy Improvement улучшает её, выбирая лучшее действие в каждом состоянии.\n* В конечной МДП существует детерминированная оптимальная политика.\n* Value Iteration и Policy Iteration - два основных алгоритма, использующие уравнения Беллмана.\n* В конечной МДП существует детерминированная оптимальная политика.\n* Value Iteration и Policy Iteration - алгоритмы, которые итеративно обновляют функцию ценности и политику соответственно.\n* В Value Iteration функция ценности обновляется, используя уравнение Беллмана.\n* Policy Evaluation и Policy Improvement - ключевые шаги в Value Iteration и Policy Iteration.\n* В Value Iteration функция ценности обновляется, используя уравнение Беллмана для обновления функции ценности и политики.\n* В Value Iteration функция ценности обновляется, используя уравнение",
  "processed_frame_041500.jpg": "* Можно улучшить политику, используя Policy Improvement, что гарантирует, что новая политика не хуже предыдущей.\n* Для улучшения политики необходимо оценить текущую политику (Policy Evaluation) и затем выбрать лучшие действия (Policy Improvement).\n* В Policy Iteration и Value Iteration можно использовать Q-функцию вместо V-функции, что изменяет сложность алгоритма, так как требуется обновлять пары состояния-действие.\n* V-функция представляет собой вектор состояний, а Q-функция - матрицу состояний на действия.\n* Если обучается V-функция, перед Policy Improvement нужно восстановить Q-функцию для оценки качества действий.\n* При использовании Q-функции шаг восстановления Q-функции можно пропустить, так как она уже содержит информацию о качестве действий.\n* Обучение Q-функции позволяет напрямую брать максимум, минуя шаг восстановления Q-функции из V-функции.\n* Важно понимать, что при обучении V-функции требуется дополнительный шаг восстановления Q-функции, в то время как при обучении Q-функции этот шаг не нужен.\n* При обучении V-функции перед Policy Improvement нужно восстановить Q-функцию, а при обучении Q-функции этот шаг можно пропустить.\n* В будущем выбор между V и Q-функциями может быть ограничен, и предпочтение будет отдаваться Q-функции для обновления таблицы состояний-действий, что упрощает процесс.\n* Q и V-функции помогают двигать распределение в сторону элитных траекторий, что упрощает процесс обучения и улучшает сходимость к оптимальной политике.",
  "processed_frame_046350.jpg": "* Если функции и теория не привлекают, но есть желание решать задачи, можно вдохновиться эволюционными стратегиями и запустить процедуру, двигающую распределение в сторону элитных траекторий.\n* Элитные траектории определяются по критерию высокой награды.\n* Эволюционные стратегии неэффективны с точки зрения количества сэмплов, но имеют потенциал сойтись к хорошему решению.\n* Тренд на ограничение бюджета взаимодействия с средой привел к попыткам создания алгоритмов с ограниченным бюджетом.\n* Алгоритмы без предположений об объекте неэффективны, так как не используют информацию из среды.\n* Чем меньше индуктивных предположений (Inductive Biases), тем меньше эффективность одного сэмпла и вклад в качество алгоритма.\n* В большой популяции вклад отдельного индивидуума менее заметен.\n* Рекап лекции: обсуждение эволюционных стратегий, их неэффективности в плане сэмплов, тренд на ограниченный бюджет взаимодействия и влияние индуктивных предположений на эффективность алгоритмов.",
  "processed_frame_049350.jpg": "* Краткий рекап лекции: обсуждение эволюционных стратегий, их неэффективности в плане сэмплов, тренд на ограниченный бюджет взаимодействия и влияние индуктивных предположений на эффективность алгоритмов.\n* Вопрос по домашке про Policy Iteration: достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью, без необходимости проверки неизменности политики.\n* В реальном мире среда может быть неизвестна и шумной, что усложняет понимание ее динамики.\n* В таких условиях v-функция может не быть полезной для Policy Improvement, так как из нее нельзя восстановить политику, но она все еще может иметь значение в других аспектах.",
  "processed_frame_053350.jpg": "* Вопрос по домашнему заданию о Policy Iteration: нужно ли проверять, что политика не меняется?\n    * Ответ: нет, достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью.\n    * Если есть два одинаково хороших действия, политика может меняться между ними, что не влияет на результат.\n    * В реальном мире среда может быть неизвестной и шумной, что усложняет понимание ее динамики.\n    * В реальном мире среда может быть неизвестной и шумной, что усложняет понимание ее динамики.\n    * В реальном мире среда может быть неизвестной и шумной, что усложняет понимание ее динамики.\n    * В Policy Iteration достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью.\n    * В Policy Iteration достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью.\n    * В Policy Iteration достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью, вместо проверки неизменности политики.\n    * В Policy Iteration достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью, вместо проверки неизменности политики.\n    * Если два действия одинаково хороши, политика может меняться между ними без ущерба для результата.\n    * В Policy Iteration достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью, вместо проверки неизменности политики.\n    * В Policy Iteration достаточно проверять, что функция удовлетворяет уравнению Беллмана с определенной точностью, вмест",
  "processed_frame_054075.jpg": "*",
  "processed_frame_064000.jpg": "*",
  "processed_frame_066600.jpg": "*",
  "processed_frame_068925.jpg": "*",
  "processed_frame_070275.jpg": "* Теорема Робинсона-Ван Ро утверждает сходимость последовательности тета-катов к тета-созвездой в L2 при удовлетворении определенных условий.\n* Сходимость в L2 влечет за собой сходимость по вероятности и по распределению.\n* Алгоритм монте-карло политики похож на итерационную процедуру, где тета-ката проектируется на текущую оценку, что позволяет оценивать среднее значение с теоретическими гарантиями.\n* Уравнение Беллмана может быть использовано для улучшения алгоритма, позволяя использовать Q-функцию для политики Пи не только как сэмпл, но и как таргет для обучения.\n* Использование уравнения Беллмана позволяет улучшить оценку, предоставляя более точные таргеты для обучения.",
  "processed_frame_071600.jpg": "* Для политики P, Q-функция не только матожидание для кумулятивной награды G, но и награда плюс гамма матожидание от Q, Q'H'.\n* Использование уравнения Беллмана позволяет использовать один сэмпл и информацию о среде, не доигрывая эпизоды до конца.\n* TD-1 алгоритм использует R + γQ как таргет, а разность между текущей оценкой и таргетом называется temporal difference.\n* TD-0 алгоритм позволяет использовать информацию из среды и текущую аппроксимацию, не доигрывая эпизоды до конца.\n* Альфа-каты можно воспринимать как learning rate, который может зависеть от состояния и действия.\n* Использование Bellman optimality equation позволяет улучшить алгоритм, так как здесь нет ограничений на действие h', которое должно прийти из той политики, откуда приходит состояние S&A.\n* При фиксированной политике, алгоритм сойдется к Q-функции, соответствующей этой политике.\n* Если алгоритм сойдется к Q-функции, соответствующей оптимальной политике, то это серьезное утверждение, так как алгоритм агностичен к политике и не имеет ограничений на действие.",
  "processed_frame_081925.jpg": "* Монте-Карло, Cool Learning и Saksa - это различные алгоритмы, которые нужно изучить и понять, в каких ситуациях каждый из них лучше.\n* Необходимо препарировать свойства каждого алгоритма, чтобы лучше понять их и удовлетворять формальным условиям.\n* Условие, на которое нужно обратить внимание: A штрих это AТ плюс один, а S штрих это SТ плюс один.\n* В состоянии S штрих (SТ плюс один) алгоритмы могут работать по-разному, и это нужно учитывать при выборе алгоритма.",
  "processed_frame_087125.jpg": "* A штрих - это действие, которое мы сделаем в состоянии S штрих. A штрих это AТ плюс один.\n* Стремимся быть ближе к теоретическим гарантиям, но хотим расширять скуп, чтобы в ней было бесконечное число флагов.\n* Одно из теоретических требований - сумма коэффициентов альфа-каты должна быть бесконечной (инфинит визит).\n* Для бесконечной суммы необходимо бесконечное число слагаемых, то есть пар состояния-действия SA нужно посетить бесконечное число раз.\n* Политика должна постоянно исследовать и не пренебрегать парами состояния-действия, чтобы сойтись к оптимальному значению.\n* Не обязательно оценивать функцию для всех пар состояния-действия, если вероятность попадания в некоторые состояния равна нулю.\n* Теоретически, для сходимости нужно посетить каждую пару состояния-действия бесконечное число раз.\n* Это приводит к дилемме exploration-exploitation: когда использовать свои знания для максимизации награды, а когда исследовать новые действия.",
  "processed_frame_091875.jpg": "* В процессе исследования вы действуете неоптимально, тратя время на неоптимальные действия.\n* Если найдете лучший вариант (например, ресторан), то все предыдущие \"страдания\" окупятся, но это не гарантировано.\n* Необходим баланс между использованием текущих знаний и открытием новых возможностей.\n* Существует дилемма exploration-exploitation: использовать текущие знания для максимизации награды или исследовать новые варианты.\n* Черный тип коз - это плата за исследование, так как вы временно действуете неоптимально.\n* Важно найти баланс между использованием текущих знаний и открытием новых возможностей.\n* Две стороны спектра: жадное использование опыта и открытость к новому.",
  "processed_frame_094325.jpg": "* Exploration и exploitation - две стороны спектра в Reinforcement Learning.\n* Exploration - случайная политика, выбирающая каждое действие с одинаковой вероятностью.\n* Exploitation - жадная политика, выбирающая оптимальное действие (например, Q*).\n* Жадная политика может быть оптимальной, если текущая оценка Q близка к реальности.\n* Если оценка Q далека от реальности, жадная политика может быть неоптимальной.\n* Эпсилон-жадная политика лежит между exploration и exploitation.\n* Эта политика выбирает случайное действие с вероятностью ε и жадное действие с вероятностью 1-ε.\n* Пример: кидаем монетку с вероятностью 0,5, если выпадает \"орел\" - действуем случайно, если \"решка\" - жадно.",
  "processed_frame_096500.jpg": "* Эпсилон-жадная политика:\n    * С вероятностью ε выбираем случайное действие.\n    * С вероятностью 1-ε выбираем жадное действие (оптимальное по текущей оценке Q).\n* Эта политика балансирует между exploration и exploitation.\n* Если ε близко к 1, то политика близка к чистому exploration.\n* Если ε близко к 0, то политика близка к чистому exploitation.\n* Эпсилон-жадная политика:\n    * С вероятностью ε выбираем случайное действие.\n    * С вероятностью 1-ε выбираем действие, максимизирующее Q-функцию.\n* Эпсилон-жадная политика:\n    * С вероятностью ε выбираем случайное действие.\n    * С вероятностью 1-ε выбираем действие, максимизирующее Q-функцию.\n* Если ε=0,5, то политика становится детерминированной и не исследует.\n* Можно использовать другие вероятности для баланса между exploration и exploitation.\n* Можно использовать функцию, зависящую от состояния, для динамического баланса exploration и exploitation.\n* Можно использовать нейронную сеть для оценки Q-функции и затем применять эпсилон-жадную политику.\n* Можно использовать softmax-функцию для выбора действия, что позволяет учитывать все возможные действия, но с разной вероятностью.\n* Можно использовать нейронную сеть для оценки Q-функции и затем применять эпсилон-жадную политику.\n* Можно использовать softmax-функцию для выбора действия, что позволяет учитывать все возможные действия с разной вероятностью.\n* Можно использовать нейронную сеть для оценки Q-функции и затем применять эпсилон-жадная",
  "processed_frame_100800.jpg": "* Эпсилон-жадная политика:\n    * С вероятностью ε выбираем случайное действие.\n    * С вероятностью 1-ε выбираем жадное действие (оптимальное по текущей оценке Q).\n* Вероятность выбрать жадное действие в эпсилон-жадной политике:\n    * С вероятностью 1-ε выбираем жадное действие.\n    * С вероятностью ε выбираем случайное действие из n возможных.\n    * Вероятность выбрать жадное действие = (1-ε) + (ε/n).\n* Эпсилон-грид политика:\n    * И",
  "processed_frame_104325.jpg": "* Сравнение алгоритмов Q-learning и SARSA:\n    * Q-learning:\n        * Может учиться с чужого опыта.\n        * Не имеет ограничений на пары состояний и действий (S, A).\n        * Для обновления Q-значений берет максимум по возможным действиям.\n        * Пример: учимся, наблюдая за игрой старшего брата.\n    * SARSA:\n        * Учится только на своем опыте.\n        * Должен сэмплировать из своей же политики.\n        * Для обновления Q-значений берет значение из текущей политики.\n        * Пример: учимся, играя самостоятельно.\n    * Фундаментальное различие:\n        * Q-learning - off-policy алгоритм: учит политику μ с чужого опыта.\n        * SARSA - on-policy алгоритм: учит политику μ с своего опыта.",
  "processed_frame_107825.jpg": "* On-policy vs Off-policy алгоритмы:\n    * On-policy:\n        * Учит политику μ с опыта, собранного этой же политикой.\n        * Пример: SARSA.\n        * Может быть более sample-efficient.\n    * Off-policy:\n        * Учит target policy P с опыта, собранного другой политикой μ (behavior policy).\n        * Пример: Q-learning.\n        * Может использовать имитейшн-лёрнинг и учиться с нескольких политик.\n        * Пример: учить жадную политику с помощью сэмплов из эпсилон-жадной политики.\n    * Преимущества и недостатки:\n        * На первый взгляд кажется, что on-policy алгоритмы не имеют преимуществ, но это не так.\n        * Q-learning формализуется с двумя параметрами: для обновляемой политики и для ленинградского.",
  "processed_frame_112025.jpg": "* Теорема Робинсона-Монро:\n    * Условие: сумма квадратов альфы не меньше бесконечности.\n* Практика в reinforcement learning:\n    * Инициализация Q-значений.\n    * Агент, плохо обученный, собирает первый опыт.\n    * Онлайн обучение: собираем опыт и учимся на нем.\n    * Эпсилон-жадная политика для сбора опыта.\n    * Обновление текущей оценки функции Q по правилу Q-learning.\n* Q-learning:\n    * Учит курсом звездой, используя сэмплы с другой политикой.\n    * Восстанавливает оптимальную политику P* из функции Q.\n* SARSA:\n    * Второй пункт: агент делает действие, среда возвращает награду и следующее состояние.\n    * Имплементация: вызовы типа policy.sample() и env.step().",
  "processed_frame_115925.jpg": "* SARSA:\n    * Инициализация Q-значений.\n    * Два вызова policy.sample: на s и s'.\n    * Неизвестный результат после первого вызова.\n    * Оценка Q-функции после выполнения действия.\n    * Необходимость повторного сэмплирования из политики на следующем этапе.\n    * Возможность переиспользования результатов предыдущего сэмплирования.\n    * Теоретические гарантии не сломаются при повторном сэмплировании.\n    * Необходимость повторного сэмплирования при изменении Q-функции.\n* Q-learning и SARSA:\n    * Q-learning не зависит от текущей политики.\n    * SARSA зависит от текущей политики.\n    * Выбор алгоритма зависит от конкретной задачи и политики.",
  "processed_frame_127525.jpg": "* Оптимальный путь и алгоритмы:\n    * Q-learning сходится к оптимальному пути, ближе к лифу.\n    * SARSA сходится к безопасной политике.\n* Окружение и награды:\n    * Лабиринт без слабых состояний.\n    * Минус единица за каждое проведенное время в среде.\n    * Минус награда за попадание в обрыв.\n    * Оптимальная награда - минус 13, требует минимум 13 шагов.\n* Q-learning:\n    * Учит курсу звездой.\n    * Может восстановить оптимальную политику.\n    * Не имеет заложенных рисков, может выучить оптимальную политику.\n* SARSA:\n    * Учится с помощью эпсилон-жадной политики.\n    * Ограничивает пространство политик.\n    * Заложено отвращение к риску.\n    * Выбирает самый безопасный путь, минимизируя риски.\n    * Парадокс: вероятность прыгнуть с обрыва при взаимодействии с оптимизированной политикой.",
  "processed_frame_134100.jpg": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
}