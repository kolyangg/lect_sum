<img src="temp/temp_slides2/processed_frame_014825.jpg" width="795" height="446" alt="Frame 14825" />

- **Дисклеймер**:  
  - Лектор предупреждает о возможных паузах/кашле из-за недавней простуды.  

- **Тема занятия**:  
  - Переход к **model-free settings** (средам, где информация о среде ограничена).  
  - Актуальность: такие среды широко распространены в реальном мире и представляют практический интерес.  

- **Связь с предыдущим материалом**:  
  - Напоминание о пройденных темах (предположительно, основы RL и model-based подходы).  
  - Указание на эскалацию сложности задач в рамках курса.

---

<img src="temp/temp_slides2/processed_frame_016150.jpg" width="795" height="445" alt="Frame 16150" />

- **Основы MDP (Markov Decision Process)**:  
  - **Компоненты MDP**:  
    1. **Action space (пространство действий)** — доступные действия, влияющие на переходы между состояниями и получаемую награду.  
    2. **Состояние (State)** — описание текущей ситуации в среде.  
    3. **Переходные вероятности** — вероятность перехода в состояние $ S' $ из состояния $ S $ при действии $ A $:  
       - Обладают **марковским свойством**: зависят только от текущей пары (состояние, действие), а не от истории.  
       - **Стационарность**: вероятности не меняются со временем.  
    4. **Награда (Reward)** — детерминированная функция от состояния и действия (для упрощения; в общем случае может быть стохастической).  

- **Политика (Policy)**:  
  - Правило принятия решений:  
    - **Детерминированная** — однозначное действие для каждого состояния.  
    - **Стохастическая** — вероятностное распределение действий для состояния.  

- **Цель RL**:  
  - Максимизация **ожидаемой дисконтированной кумулятивной награды** $ G_t $.  

- **Пример (аналогия)**:  
  - Игра в Марио: действия (прыжки, бег), состояния (позиция в игре), награды (сбор монет, завершение уровня).

---

<img src="temp/temp_slides2/processed_frame_019925.jpg" width="795" height="445" alt="Frame 19925" />

- **Кумулятивная награда (GT)**:  
  - Случайная величина, так как зависит от стохастических переходов между состояниями и действий.  
  - **Reward-to-go** — термин для обозначения GT (накопленная награда с текущего момента).  

- **Функции ценности**:  
  - **V-функция (Vπ)**:  
    - Ожидаемая кумулятивная награда при старте из состояния **s** и следовании политике **π**.  
    - Формально: $ V^π(s) = \mathbb{E}[G_t | S_t = s, \pi] $.  
  - **Q-функция (Qπ)**:  
    - Расширение V-функции: ожидаемая награда после выполнения действия **a** в состоянии **s** с последующим следованием политике **π**.  
    - Формально: $ Q^π(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a, \pi] $.  

- **Рекуррентное соотношение**:  
  - Кумулятивная награда выражается как:  
    $ G_t = R_t + \gamma G_{t+1} $  
    где $ R_t $ — мгновенная награда, $ \gamma $ — коэффициент дисконтирования.  

- **Связь V и Q**:  
  - $ V^π(s) $ зависит от $ Q^π(s, a) $, усредненного по действиям политики **π**.  
  - $ Q^π(s, a) $ включает мгновенную награду и дисконтированное значение $ V^π(s') $ для следующего состояния.  

- **Ключевой момент**:  
  - Обе функции ($ V^π $, $ Q^π $) специфичны для выбранной политики **π** и позволяют оценивать её эффективность.

---

<img src="temp/temp_slides2/processed_frame_021700.jpg" width="795" height="445" alt="Frame 21700" />

**Кумулятивная награда и дисконтирование**:  
- Рассчитывается как сумма мгновенных наград $ R_t $, дисконтированных к текущему моменту:  
  $ G_t = R_t + \gamma G_{t+1} $,  
  где $ \gamma $ (дисконт-фактор) определяет вклад будущих наград:  
  - **γ = 0**: максимизация только текущей награды.  
  - **γ = 1**: все награды в эпизоде имеют равный вес.  
  - **0 < γ < 1**: экспоненциальное убывание веса будущих наград.  

**Эпизодические задачи**:  
- Если эпизод конечен (T < ∞), среда называется **эпизодической**.  
- Агент может использовать информацию о времени до конца эпизода (например, добавляя в состояние параметр времени).  
- **Пример проблемы**: в задачах управления (например, движение муравья) агент может совершать рискованные действия в конце эпизода (например, прыжок), чтобы максимизировать финальную награду, игнорируя последствия.  

**Источники стохастики**:  
1. **Политика (π)**: вероятностный выбор действий.  
2. **Динамика среды**: вероятность перехода в состояние $ s' $ после действия $ a $ в состоянии $ s $.  

**Оптимизация**:  
- Цель: максимизация матожидания кумулятивной награды $ G_0 $.  
- Учёт неопределённости: матожидание учитывает стохастику политики и среды.  

**Технические детали**:  
- В эпизодических задачах длина эпизода может быть заложена в состояние (например, нормализованное время от 0 до 1).  
- Для алгоритмов RL важно знание динамики среды (вероятностей переходов), но на практике она часто неизвестна и оценивается через взаимодействие.

---

<img src="temp/temp_slides2/processed_frame_027775.jpg" width="795" height="445" alt="Frame 27775" />

**Конечные пространства состояний и действий**:  
- **State space** (пространство состояний) и **action space** (пространство действий) предполагаются **конечными**.  
  - Многие среды удовлетворяют этому условию, но в реальном мире это не всегда выполнимо.  
  - Action space чаще конечен, чем state space (больше сред подходят под это условие).  

**Уравнение Беллмана**:  
- При выполнении условий конечности пространств можно вывести **уравнение Беллмана**, связывающее value-функции (функции ценности) для текущего и последующих состояний.  
- **Цель**: формализовать оптимальную стратегию через баланс между мгновенной наградой и дисконтированной будущей ценностью.  

**Примечания**:  
- Условия (state и action space конечны) не являются строгими, но упрощают анализ.  
- Реалистичность таких предположений зависит от конкретной задачи (например, дискретные среды vs. непрерывные реальные системы).  

---

**Формат уравнения Беллмана** (общая идея):  
$ V(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right] $,  
где:  
- $ V(s) $ — ценность состояния $ s $,  
- $ R(s, a) — мгновенная награда за действие $ a $ в состоянии $ s $,  
- $ P(s' | s, a) $ — вероятность перехода в состояние $ s' $,  
- $ \gamma $ — дисконт-фактор.

---

<img src="temp/temp_slides2/processed_frame_029250.jpg" width="795" height="446" alt="Frame 29250" />

**Value-функции (V) и Q-функции**:  
- **V-функция** оценивает ценность *состояния*:  
  $ V(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right] $.  
- **Q-функция** оценивает ценность *пары (состояние, действие)*:  
  $ Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) \max_{a'} Q(s', a') $.  
  - Учитывает переходы в состояния $ s' $ после действия $ a $.  

**Оптимальная политика**:  
- Существует **детерминированная оптимальная политика** для конечных MDP (теорема).  
- Оптимальная политика выбирает действие, максимизирующее Q-функцию:  
  $ \pi^*(s) = \arg\max_a Q(s, a) $.  

**Ключевые особенности уравнений**:  
- **Суммы вместо интегралов**:  
  Из-за конечности пространств (state/action) мат. ожидания сводятся к суммам, а не интегралам.  
- **Связь V и Q**:  
  $ V(s) = \max_a Q(s, a) $, $ Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') $.  

**Value Iteration**:  
- Алгоритм для поиска оптимальной политики через **итеративное обновление value-функций**:  
  1. **Обновление V-функции**: $ V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V_k(s') \right] $.  
  2. **Сходимость**: По теореме Банаха о неподвижной точке, процесс сходится к единственному решению при любом начальном приближении.  
- **Преимущество**: Объединяет этапы оценки политики и её улучшения в одну итерацию.  

**Примечания**:  
- Уравнения для оптимальной политики аналогичны стандартным уравнениям Беллмана, но с заменой *ожидания* на *максимум*.  
- Конечность пространств упрощает вычисления, делая алгоритмы (например, Value Iteration) практически применимыми.

---

<img src="temp/temp_slides2/processed_frame_032875.jpg" width="795" height="445" alt="Frame 32875" />

**Алгоритмы Policy Evaluation и Policy Improvement**:  
- **Два этапа алгоритмов RL**:  
  1. **Policy Evaluation**: Оценка текущей политики (расчёт ожидаемой награды).  
  2. **Policy Improvement**: Улучшение политики через **жадный выбор действий**, максимизирующих Q-функцию.  

**Policy Evaluation**:  
- **Цель**: Найти value-функцию $ V^\pi(s) $ для текущей политики $ \pi $.  
- **Метод**: Итеративное обновление по уравнению Беллмана:  
  $ V_{k+1}(s) = \sum_a \pi(a|s) \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V_k(s') \right] $.  
  - **Ключевое отличие от Value Iteration**: Используется *ожидание* по действиям политики, а не максимум.  
- **Сходимость**: Приводит к $ V^\pi(s) $ (неподвижной точке) для заданной $ \pi $.  

**Policy Improvement**:  
- **Шаг улучшения**: Для каждого состояния выбирается действие, максимизирующее Q-значение:  
  $ \pi'(s) = \arg\max_a Q^\pi(s, a) $.  
- **Q-функция для политики**:  
  $ Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s') $.  

**Policy Iteration (алгоритм)**:  
1. **Оценка политики**: Итеративно вычисляется $ V^\pi $.  
2. **Улучшение политики**: Обновление $ \pi $ до $ \pi' $ на основе $ V^\pi $.  
3. **Повторение**: Процесс повторяется до сходимости к оптимальной политике.  

**Визуализация через дерево решений**:  
- Состояние $ s_t $ рассматривается как узел дерева.  
- Действия ведут к ветвям с наградами $ r_t $ и переходам в состояния $ s_{t+1} $.  
- Оценка действий: вычисление ожидаемой дисконтированной награды для каждого действия.  

**Ключевые моменты**:  
- Policy Iteration гарантирует улучшение политики на каждом шаге (теоретически).  
- Value Iteration объединяет оценку и улучшение в один шаг, Policy Iteration разделяет их.  
- В Policy Evaluation **не используется максимизация** — значения усредняются по распределению действий текущей политики.

---

<img src="temp/temp_slides2/processed_frame_041500.jpg" width="795" height="445" alt="Frame 41500" />

**Улучшение политики и особенности Q/V-функций**:  

### **Policy Improvement**:  
- **Детерминированное улучшение**:  
  - Новая политика $ \pi' $, заданная через правило $ \pi'(s) = \arg\max_a Q^\pi(s, a) $, гарантированно **не хуже** предыдущей ($ V^{\pi'}(s) \geq V^\pi(s) \, \forall s $).  
  - Сравнение с оптимальной политикой ($ \pi^* $): $ V^{\pi'}(s) \geq V^\pi(s) $, но не обязательно равна $ V^{\pi^*}(s) $.  

---

### **Q-функция vs V-функция**:  
1. **Структура данных**:  
   - **V-функция**: Вектор размерности $ |S| $ (по состояниям).  
   - **Q-функция**: Матрица размерности $ |S| \times |A| $ (состояния × действия).  

2. **Вычислительная сложность**:  
   - При использовании **Q-функции** требуется обновлять $ |S| \cdot |A| $ элементов (пары состояние-действие).  
   - Для **V-функции** необходимо предварительно восстанавливать Q-значения через $ Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s') $, чтобы выполнить Policy Improvement.  

3. **Оптимизация шагов**:  
   - С **Q-функцией** можно сразу выбирать действие через $ \arg\max_a Q(s, a) $.  
   - С **V-функцией** требуется дополнительный шаг вычисления Q-значений.  

---

### **Value Iteration vs Policy Iteration**:  
- **Value Iteration**:  
  - Объединяет Policy Evaluation и Improvement в один шаг: $ V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V_k(s') \right] $.  
  - Работает только с V-функцией.  

- **Policy Iteration**:  
  - Разделяет этапы:  
    1. Полная оценка $ V^\pi $ (Policy Evaluation).  
    2. Жадное улучшение политики (Policy Improvement).  
  - Может использовать как V-, так и Q-функции.  

---

### **Ключевые замечания**:  
- **Элитные траектории**: Процесс оптимизации политики направлен на смещение распределения действий в сторону наиболее эффективных траекторий (с максимальной наградой).  
- **Сходимость алгоритмов**:  
  - Policy Iteration гарантирует монотонное улучшение политики.  
  - Value Iteration сходится к оптимальной политике через прямое обновление V-функции.  

---

**Итог**: Выбор между Q/V-функциями влияет на вычислительные затраты и шаги алгоритма. Q-функция упрощает Policy Improvement, но требует больше памяти; V-функция компактнее, но требует дополнительных вычислений.

---

<img src="temp/temp_slides2/processed_frame_046350.jpg" width="795" height="445" alt="Frame 46350" />

**Эволюционные стратегии vs. RL: ключевые аспекты**  

---

### **Сравнение с RL**:  
- **Недостатки эволюционных стратегий**:  
  - Крайне **неэффективны по количеству сэмплов** — требуют огромного числа взаимодействий со средой.  
  - **Нет гарантии сходимости** к оптимальному решению, но есть надежда на улучшение при фокусе на "элитные траектории" (с высокой наградой).  
  - **Отсутствие индуктивных предубеждений** (Inductive Biases):  
    - Не используют структуру среды или задачи, что снижает эффективность каждого отдельного сэмпла.  

---

### **Бюджет и оптимизация**:  
- **Тренды в разработке алгоритмов**:  
  - Акцент на **ограничение бюджета взаимодействий** со средой (фиксированное число шагов/сэмплов).  
  - Попытки повысить "ценность" каждого сэмпла через включение априорных знаний о среде.  
- **Проблема больших популяций**:  
  - При увеличении размера популяции вклад отдельной особи (траектории) становится менее значимым.  

---

### **Ключевой вывод**:  
- Эволюционные стратегии **вдохновляют**, но уступают методам RL (Q/V-функции) в эффективности.  
- Современные подходы фокусируются на **балансе между бюджетом и качеством**, используя индуктивные предубеждения для снижения затрат.  

--- 

**Итог**: Эволюционные методы — инструмент "последнего выбора" при невозможности применения RL, но их развитие связано с оптимизацией данных и интеграцией знаний о среде.

---

<img src="temp/temp_slides2/processed_frame_049350.jpg" width="795" height="445" alt="Frame 49350" />

### **Policy Iteration: уточнения**  
---

- **Критерии остановки**:  
  - Основной критерий — проверка **удовлетворения уравнения Беллмана** с заданной точностью.  
  - Проверка изменения политики **не обязательна**, даже если есть несколько оптимальных действий:  
    - При наличии **одинаково хороших действий** алгоритм может "переключаться" между ними.  
    - **Argmax** разрешает неоднозначность через порядок действий (например, выбор действия с меньшим индексом).  

---

### **Реальный мир vs. Идеализированная среда**  
---

- **Проблемы реальных сред**:  
  - Динамика среды **неизвестна** или сложна для моделирования (нелинейные уравнения, шумные награды).  
  - Наблюдения за средой **ограничены** — получаем только эпизодические/зашумленные данные.  

- **Сложности использования V-функции**:  
  - В реальных условиях **V-функция теряет практическую ценность**:  
    - Невозможно точно восстановить политику из-за неполноты данных.  
    - Однако сохраняет теоретическую полезность для анализа (не полностью бесполезна).  

---

### **Ключевой вывод**:  
- В идеализированных задачах Policy Iteration работает строго, но в реальности **алгоритмы должны учитывать неопределённость среды** и ограничения данных.  
- Акцент смещается на методы, устойчивые к шуму и не требующие полного знания динамики.

---

<img src="temp/temp_slides2/processed_frame_053350.jpg" width="795" height="446" alt="Frame 53350" />

### **Дополнение по V-функции и эмпирическим оценкам**  
---  
- **Эмпирическое среднее**:  
  - Используется для **практической оценки** параметров в условиях неопределённости (шумные/ограниченные данные).  

- **Роль V-функции в реальных задачах**:  
  - Сохраняет **теоретическую значимость** в анализе и других методах RL.  
  - **Не применяется в текущем сетапе** (на данном этапе) из-за невозможности точного восстановления политики.  

- **Ключевой нюанс**:  
  - Даже при ограниченной практической пользе в конкретной задаче, V-функция остаётся **важным инструментом** в общем контексте reinforcement learning.

---

<img src="temp/temp_slides2/processed_frame_054075.jpg" width="795" height="445" alt="Frame 54075" />

### **Оценка Q-функции через эмпирическое среднее**  
---  
- **Основной подход**:  
  - Q-функция оценивается через **эмпирическое среднее** накопленных наград (сэмплов) по траекториям.  
  - Основано на **усиленном законе больших чисел** (сходимость к истинному матожиданию).  

- **Процесс оценки**:  
  1. Генерация **полных траекторий** до конца эпизода.  
  2. Расчёт **суммарной дисконтированной награды** (G) для каждой траектории.  
  3. Усреднение значений G по траекториям для оценки Q(s, a).  

- **Особенности**:  
  - **Онлайн-обновление**:  
    - При поступлении новых траекторий Q-функция пересчитывается через взвешенное среднее:  
      *Q_new = (Q_prev * (n-1) + G_new) / n*  
  - **Две размерности усреднения**:  
    - По количеству траекторий (n) и времени внутри траектории (t).  

- **Проблемы метода**:  
  1. **Высокая дисперсия**:  
     - Из-за замены **вложенных матожиданий** на единичные сэмплы (R1, R2, ...).  
  2. **Требование завершённых эпизодов**:  
     - Невозможность применения в **неэпизодических средах** (траектории бесконечны).  
  3. **Вычислительная затратность**:  
     - Долгое ожидание завершения эпизодов (особенно для длинных/стохастических сред).  

- **Преимущества**:  
  - **Несмещённость оценки** (гарантируется законом больших чисел).  

- **Альтернатива**:  
  - Использование **уравнения Беллмана** для оценки Q-функции через одношаговые обновления (устраняет необходимость полных траекторий).  

*Исправленные термины*:  
- "эмульсивную награду" → **эмпирическую награду**  
- "наджидание" → **матожидание**  
- "g-tow" → **G_t** (суммарная награда с дисконтированием).

---

<img src="temp/temp_slides2/processed_frame_064000.jpg" width="795" height="445" alt="Frame 64000" />

### **Анализ уравнений в контексте RL**  
---  
**Ключевое отличие**:  
- Три уравнения используют **матожидание** (оператор 𝔼) в правой части.  
- Одно уравнение содержит **максимум** (оператор max) вместо матожидания.  

**Почему уравнение с максимумом — лишнее**:  
1. **Контекст оценки vs оптимизации**:  
   - Уравнения с матожиданием типичны для **оценки** Q-функции или value-функции (например, Bellman equation).  
   - Уравнение с максимумом связано с **оптимизацией политики** (выбор наилучшего действия, как в Q-learning: *maxₐ Q(s, a)*).  

2. **Семантическое несоответствие**:  
   - Матожидание отражает усреднение по стохастичности среды/политики.  
   - Максимум вводит детерминированный выбор, нарушая логику "усреднения", характерную для остальных уравнений.  

**Примеры типичных уравнений**:  
- Ожидаемые:  
  - *V(s) = 𝔼[Gₜ | Sₜ = s]*  
  - *Q(s, a) = 𝔼[R + γV(s') | s, a]*  
- Лишнее:  
  - *Q(s, a) = maxₐ (R + γQ(s', a'))*  

**Итог**:  
Уравнение с **максимумом** исключается из группы, так как оно решает задачу **оптимизации действий**, тогда как остальные фокусируются на **стохастической оценке**.

---

<img src="temp/temp_slides2/processed_frame_066600.jpg" width="795" height="445" alt="Frame 66600" />

### **Основы статистической аппроксимации в RL**  
---  
**1. Суть подхода**:  
- **Стохастическая аппроксимация** — метод оценки матожидания 𝔼 по неизвестному распределению через **онлайн-обновления** с использованием ограниченных данных (например, центурированных выборок).  

**2. Две интерпретации процесса**:  
- **Градиентный спуск**:  
  - Итерационный процесс обновления параметров, напоминающий алгоритм градиентного спуска.  
  - Пример: *θₖ = θₖ₋₁ + αₖ(новый_сэмпл − θₖ₋₁)*.  
- **Взвешенное среднее**:  
  - При **αₖ = 1/k** формула превращается в **детерминированный пересчёт среднего**:  
    *θₖ = (1 − 1/k)θₖ₋₁ + (1/k)xₖ* → аналог скользящего среднего.  

**3. Онлайн-апдейт в RL**:  
- **Механизм**:  
  - Текущая оценка (θₖ₋₁) комбинируется с новым сэмплом (xₖ) через веса αₖ и (1 − αₖ).  
  - Позволяет **адаптироваться** к новым данным без хранения всей истории.  
- **Зачем нужно матожидание квадрата разности**:  
  - Оценка **дисперсии** или ошибки (например, в Temporal Difference Learning).  
  - Критично для оптимизации политик и стабильности обучения.  

**4. Важность выбора αₖ**:  
- **αₖ = 1/k**: Гарантирует сходимость к истинному среднему, но медленная адаптация.  
- **Постоянное αₖ (напр., 0.1)**: Быстрее реагирует на изменения, но вносит шум.  

**Итог**:  
Метод позволяет **эффективно оценивать параметры распределений** в RL, балансируя между точностью (малая αₖ) и скоростью адаптации (большая αₖ).

---

<img src="temp/temp_slides2/processed_frame_068925.jpg" width="795" height="445" alt="Frame 68925" />

### **Теорема Робинсона-Ван Ро (Robbins-Monro) и её применение в RL**  
---  
**1. Условия сходимости**:  
- **Требования к шагам αₖ**:  
  - ∑αₖ = ∞ (достаточная коррекция в долгосрочной перспективе),  
  - ∑αₖ² < ∞ (устранение шума).  
- **Технические условия**: Ограниченность дисперсии и градиентов.  

**2. Типы сходимости**:  
- **L₂-сходимость**: 𝔼[(θₖ − θ*)^2] → 0 (сильная сходимость в среднем квадратичном).  
- **Иерархия следствий**:  
  - L₂ → сходимость по вероятности → сходимость по распределению.  

**3. Связь с RL**:  
- **Аналогия с алгоритмами**:  
  - Процедура напоминает **TD-обучение** и **Q-learning**, где оценки обновляются онлайн.  
- **Динамика распределений**:  
  - В RL оцениваются **множественные распределения** (по состояниям-действиям), а не одно статическое.  

**4. Ограничения и особенности**:  
- **Условие на действия**:  
  - Действия должны выбираться согласно **текущей политике** (например, ε-жадной) для гарантии сходимости.  
- **Роль уравнения Беллмана**:  
  - Обеспечивает связь между оценками на соседних шагах, вводя **дополнительные ограничения** на обновления.  

**Итог**:  
Теорема обосновывает **стабильность** стохастической аппроксимации в RL, обеспечивая сходимость оценок даже при динамически меняющихся данных.

---

<img src="temp/temp_slides2/processed_frame_070275.jpg" width="795" height="446" alt="Frame 70275" />

### **Связь теоремы Робинсона-Ван Ро (Robbins-Monro) с алгоритмами RL**  
---  
**1. Алгоритм, аналогичный процедуре Робинсона-Ван Ро**:  
- **Метод Монте-Карло для оценки политики**:  
  - Использует **итеративное обновление** параметров θₖ (текущая оценка) через проекцию на пространство состояний-действий.  
  - Оценивает **матожидание награды** с теоретическими гарантиями сходимости.  

**2. Роль уравнения Беллмана**:  
- **Улучшение базового подхода**:  
  - Вместо прямого сэмплирования (как в Монте-Карло) используется **Q-функция** для политики π.  
  - Целевое значение (таргет) вычисляется через уравнение Беллмана, что снижает дисперсию оценок.  
- **Примеры алгоритмов**:  
  - **TD-обучение** и **Q-learning** — используют идею итеративного обновления с предсказанием будущих наград.  

**3. Теоретические гарантии**:  
- **Условия Робинсона-Ван Ро**:  
  - Шаги обучения αₖ должны удовлетворять:  
    - ∑αₖ = ∞ (полнота коррекции),  
    - ∑αₖ² < ∞ (устранение шума).  
  - Обеспечивают **L₂-сходимость** оценок: 𝔼[(θₖ − θ*)^2] → 0.  

**4. Преимущества и ограничения**:  
- **Недостатки Монте-Карло**:  
  - Высокая дисперсия из-за зависимости от полных эпизодов.  
- **Решение через Bellman equation**:  
  - Введение **бутстрэппинга** (обновление на основе текущих оценок) ускоряет сходимость и стабилизирует обучение.  

**Итог**:  
Теорема Робинсона-Ван Ро формализует сходимость **стохастических алгоритмов** в RL, а использование уравнения Беллмана позволяет преодолеть ограничения методов вроде Монте-Карло, делая обучение более эффективным.

---

<img src="temp/temp_slides2/processed_frame_071600.jpg" width="795" height="445" alt="Frame 71600" />

### **Алгоритмы на основе уравнения Беллмана и их свойства**  
---  
**1. TD-обучение vs. Монте-Карло**:  
- **TD-таргет**:  
  - Определяется как $ R + \gamma Q(s', a') $, где $ s' $ и $ a' $ — следующее состояние и действие.  
  - **Преимущество**:  
    - Не требует доигрывания эпизода до конца (достаточно одного шага в среде).  
    - Использует **текущую аппроксимацию** $ Q $-функции для предсказания будущих наград (бутстрэппинг).  
  - **Temporal Difference (TD)**:  
    - Разница между предсказанием и таргетом: $ \text{TD-ошибка} = (R + \gamma Q(s', a')) - Q(s, a) $.  

**2. Особенности алгоритмов**:  
- **Q-learning**:  
  - Использует **Bellman optimality equation**: $ Q(s, a) \leftarrow Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right) $.  
  - **Агностик к политике**:  
    - Сходится к $ Q^* $ (оптимальной $ Q $-функции) даже при неоптимальных действиях $ a' $.  
- **Сравнение с Монте-Карло**:  
  - Меньшая дисперсия за счет бутстрэппинга, но возможное смещение из-за неточных оценок $ Q $.  

**3. Learning rate (α)**:  
- **Условия сходимости**:  
  - Должны выполняться **условия Робинсона-Монро**:  
    - $ \sum \alpha_k = \infty $ (полнота обновлений),  
    - $ \sum \alpha_k^2 < \infty $ (снижение шума).  
  - **Пример**: $ \alpha_k = \frac{1}{k} $.  
- **Зависимость от состояния-действия**:  
  - В общем случае $ \alpha $ может быть индивидуальным для каждой пары $ (s, a) $, но на практике часто используется глобальный параметр.  

**4. Сходимость алгоритмов**:  
- **При фиксированной политике**:  
  - TD-алгоритмы сходятся к $ Q^\pi $ (функции ценности для политики $ \pi $) в $ L_2 $-норме или по вероятности.  
- **Q-learning**:  
  - Сходится к $ Q^* $ (оптимальной функции) **независимо от политики**, генерирующей действия.  

**5. Ключевые выводы**:  
- **Bootstrapping**:  
  - Позволяет обновлять оценки на основе текущих предсказаний, ускоряя обучение.  
- **Практическая ценность Q-learning**:  
  - Не требует следования конкретной политике, что делает его универсальным для задач поиска оптимальной стратегии.  
- **Ограничения**:  
  - Шумные таргеты и необходимость баланса между exploration и exploitation.  

**Итог**:  
Использование уравнения Беллмана в TD-методах и Q-learning устраняет ключевые недостатки Монте-Карло, обеспечивая более эффективное обучение с теоретическими гарантиями сходимости.

---

<img src="temp/temp_slides2/processed_frame_081925.jpg" width="795" height="445" alt="Frame 81925" />

### **Сравнение алгоритмов обучения с подкреплением**  
---

**1. Метод Монте-Карло vs. TD/Q-learning**:  
- **Монте-Карло**:  
  - Требует **полного эпизода** до завершения для оценки награды.  
  - Высокая дисперсия из-за зависимости от траекторий.  
- **TD-методы (Q-learning, SARSA)**:  
  - Используют **один шаг** взаимодействия со средой ($s_{t+1}, a_{t+1}$).  
  - Меньшая дисперсия за счет бутстрэппинга (обновление на основе текущих оценок $Q(s', a')$).  

**2. Особенности Q-learning**:  
  - **Bellman Optimality Equation**:  
    - $ Q(s, a) \leftarrow Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s, a) \right) $.  
  - **Агностик к политике**:  
    - Сходится к $Q^*$ (оптимальной функции) даже при неоптимальных действиях $a'$.  

**3. Ключевые термины и обозначения**:  
  - $s'$ (или $s_{t+1}$): следующее состояние после выполнения действия $a$.  
  - $a'$ (или $a_{t+1}$): действие, выбираемое в состоянии $s'$.  
  - **Таргет**: $R + \gamma Q(s', a')$ (для TD) или $R + \gamma \max_{a'} Q(s', a')$ (для Q-learning).  

**4. Условия сходимости**:  
  - **Learning rate ($\alpha$)**:  
    - Должен удовлетворять условиям Робинсона-Монро:  
      - $\sum \alpha_k = \infty$ (достаточное количество обновлений),  
      - $\sum \alpha_k^2 < \infty$ (снижение влияния шума).  
  - **Для Q-learning**:  
    - Сходится к $Q^*$ **независимо от политики** генерации действий (при выполнении условий исследования).  

**5. Практические рекомендации**:  
  - **Монте-Карло**:  
    - Подходит для эпизодических задач с четкими границами эпизодов.  
  - **TD/Q-learning**:  
    - Эффективны для задач с длинными или бесконечными горизонтами.  
    - Быстрее адаптируются за счет частичных обновлений.  

**Итог**:  
Выбор алгоритма зависит от задачи:  
- Монте-Карло — для точных оценок в завершенных эпизодах,  
- TD/Q-learning — для баланса между скоростью, стабильностью и оптимальностью.

---

<img src="temp/temp_slides2/processed_frame_087125.jpg" width="795" height="445" alt="Frame 87125" />

### **Теоретические гарантии и условия сходимости в RL**  
---

**1. Обозначения**:  
- **а' (а штрих)** = $a_{t+1}$ — действие, выбираемое на следующем шаге.  
- **s' (с штрих)** = $s_{t+1}$ — следующее состояние после выполнения действия $a_t$.  

**2. Условия сходимости (Роббинса-Монро)**:  
- **Learning rate ($\alpha$)**:  
  - $\sum \alpha_k = \infty$ — бесконечное количество обновлений (обеспечивает "инфинит визит" к состояниям).  
  - $\sum \alpha_k^2 < \infty$ — снижение шума в оценках.  
- **Требование к посещению состояний**:  
  - Каждая пара **(состояние, действие)** должна посещаться **бесконечное число раз** для гарантии сходимости к оптимальной политике.  

**3. Проблема исследования (exploration)**:  
- **Политика должна балансировать**:  
  - **Exploitation** — использование текущих знаний для максимизации награды.  
  - **Exploration** — исследование новых действий/состояний для сбора информации.  
- **Критическое условие**:  
  - Даже "плохие" состояния/действия должны исследоваться с **ненулевой вероятностью**, чтобы избежать застревания в субоптимальных решениях.  

**4. Практические ограничения**:  
- **Не все состояния важны**:  
  - Некоторые состояния могут быть недостижимы или нерелевантны (например, с нулевой вероятностью перехода).  
  - Теоретически требуется оценка **всех** пар $(s, a)$, но на практике фокус — на достижимых и значимых состояниях.  

**5. Итог**:  
- **Exploration vs. Exploitation** — ключевая дилемма RL:  
  - Без достаточного исследования алгоритм может не найти оптимальную политику.  
  - Теоретические гарантии сходимости требуют **бесконечного исследования**, но в реальных задачах применяют компромиссы (ε-greedy, soft policies и т.д.).

---

<img src="temp/temp_slides2/processed_frame_091875.jpg" width="795" height="445" alt="Frame 91875" />

### **Дилемма исследования-использования (Exploration-Exploitation Trade-off)**  

1. **Стоимость исследования**:  
   - **Плата за исследование**: Во время исследования агент совершает **неоптимальные действия** (например, тратит время/ресурсы).  
   - Пример: Посещение новых ресторанов вместо проверенных (риск получить плохой опыт).  

2. **Потенциальная выгода**:  
   - Обнаружение **существенно лучших вариантов** (например, идеальный ресторан) может компенсировать временные потери.  
   - **Нет гарантии успеха**: Возможность найти улучшение существует, но не гарантирована.  

3. **Балансировка стратегий**:  
   - **Exploitation**: Максимизация краткосрочной награды через известные оптимальные действия.  
   - **Exploration**: Сбор информации для долгосрочного улучшения политики.  
   - **Ключевой принцип**: Оптимальная стратегия требует **динамического баланса** между этими подходами.  

4. **Практические методы**:  
   - **ε-greedy**: С вероятностью ε выбирать случайное действие (исследование), иначе — оптимальное (использование).  
   - **Softmax**: Вероятность выбора действия зависит от текущих оценок (более "умное" исследование).  
   - **Оптимистичная инициализация**: Начальные высокие оценки для стимула исследования всех действий.  

5. **Философский аспект**:  
   - Аналогия с жизненными решениями: баланс между стабильностью (использование известного) и риском (поиск новых возможностей).  
   - **Итог**: Нет универсального решения — выбор зависит от горизонта планирования, уровня неопределенности и цены ошибки.

---

<img src="temp/temp_slides2/processed_frame_094325.jpg" width="795" height="445" alt="Frame 94325" />

### **Формальный анализ дилеммы Exploration-Exploitation**  

#### **1. Два полюса стратегий**  
- **Чистое исследование (Pure Exploration)**:  
  - **Случайная политика**: В каждом состоянии все действия выбираются с **равной вероятностью** (например, равномерное распределение).  
  - Пример: `policy(a|s) = 1/|A|` для всех действий `a` в состоянии `s`.  

- **Чистое использование (Pure Exploitation)**:  
  - **Жадная политика**: Выбор действия с **максимальной оценкой** (например, `Q*(s,a)`).  
  - Пример: `policy(a|s) = 1`, если `a = argmax_a Q*(s,a)`, иначе `0`.  

---

#### **2. Промежуточные стратегии**  
- **ε-жадная политика (ε-greedy)**:  
  - **Механизм**:  
    - С вероятностью **ε** выбирается **случайное действие** (exploration).  
    - С вероятностью **1-ε** выбирается **жадное действие** (exploitation).  
  - Формула:  
    ```  
    policy(a|s) = ε/|A| + (1-ε) * I(a = argmax_a Q(s,a))  
    ```  
  - **Пример**: При `ε=0.1` агент исследует в 10% случаев, эксплуатирует — в 90%.  

- **Проблема примера с ε=0.5**:  
  - Высокий ε (например, 0.5) приводит к **чрезмерному исследованию**, снижая эффективность.  
  - **Рекомендация**: Использовать **малое ε** (0.1-0.2) или **затухающий ε** (ε уменьшается со временем).  

---

#### **3. Ключевые аспекты**  
- **Адаптивность**:  
  - При неточных оценках Q(s,a) жадная политика может быть **субоптимальной**.  
  - Чем хуже оценка Q, тем выше должен быть ε для коррекции ошибок.  

- **Динамические стратегии**:  
  - **Decaying ε**: Начинать с высокого ε, постепенно уменьшая его (например, `ε_t = 1/t`).  
  - **Оптимистичная инициализация**: Присвоить высокие начальные значения Q(s,a), стимулируя исследование.  

---

#### **4. Ограничения ε-greedy**  
- **Слепое исследование**: Не учитывает **потенциальную ценность** действий (все случайные выборы равнозначны).  
- **Альтернативы**:  
  - **Softmax**: Вероятность выбора действия зависит от его текущей оценки (например, через Boltzmann распределение).  
  - **Upper Confidence Bound (UCB)**: Баланс между оценкой и неопределенностью действия.  

---

**Итог**: ε-greedy — простой, но эффективный метод балансировки, требующий тонкой настройки ε. Для сложных сред предпочтительны адаптивные или основанные на неопределенности подходы.

---

<img src="temp/temp_slides2/processed_frame_096500.jpg" width="795" height="446" alt="Frame 96500" />

### **Уточнение по ε-жадной политике**  
- **Вероятность выбора жадного действия**:  
  - Для **жадного действия**: `(1 - ε) + ε / |A|`, где `|A|` — количество возможных действий.  
  - Для **остальных действий**: `ε / |A|`.  
  - *Пример*: При `ε=0.1` и 4 действиях:  
    - Жадное действие: `0.9 + 0.1/4 = 0.925`,  
    - Остальные: `0.1/4 = 0.025`.  

---

### **Альтернативные стохастические политики на основе Q-функции**  
1. **Softmax (Boltzmann распределение)**:  
   - **Механизм**: Вероятность выбора действия зависит от его **относительной оценки Q(s,a)**.  
   - Формула:  
     ```  
     policy(a|s) = exp(Q(s,a)/τ) / Σ_{a'} exp(Q(s,a')/τ)  
     ```  
     - `τ` (температура) регулирует **уровень случайности**:  
       - Высокий τ → равномерное распределение (больше exploration),  
       - Низкий τ → жадный выбор (exploitation).  

2. **Политика на основе доверительных интервалов (UCB)**:  
   - Выбор действия с учетом **оценки Q(s,a) + неопределённость**.  
   - Формула: `a = argmax_a [Q(s,a) + c * sqrt(ln(t)/N(a))]`,  
     где `N(a)` — число выборов действия, `t` — общее число шагов.  

3. **Гибридные подходы**:  
   - Комбинация ε-жадной и softmax политик (например, ε-softmax).  
   - Динамическая настройка ε или τ в зависимости от прогресса обучения.  

---

### **Пример с нейросетью**  
- **Логиты → Softmax**:  
  - Нейросеть предсказывает логиты (сырые оценки) для действий.  
  - Softmax преобразует логиты в вероятности:  
    ```  
    probs = softmax(logits)  
    ```  
  - Это позволяет получить **стохастическую политику**, зависящую от Q-значений (логитов).  

---

**Итог**:  
- ε-жадная — простой метод, но softmax и UCB эффективнее учитывают **относительные оценки** и **неопределённость**.  
- Нейросети + softmax — стандартный подход для сложных сред (например, DQN с exploration на основе температурного параметра).

---

<img src="temp/temp_slides2/processed_frame_100800.jpg" width="795" height="445" alt="Frame 100800" />

### **Детализация ε-жадной политики**  
- **Формула вероятности для жадного действия**:  
  - **Два сценария выбора**:  
    1. **Жадный выбор** (без случайности): `1 - ε`  
    2. **Случайный выбор** (с учетом жадного действия): `ε / n`, где `n` — число действий.  
  - **Итоговая вероятность**:  
    ```  
    P(жадное действие) = (1 - ε) + ε / n  
    ```  
  - *Пример*: При `ε=0.2`, `n=5`:  
    `0.8 + 0.2/5 = 0.84` (84% выбрать оптимальное действие).  

---

### **Softmax-политика с температурой**  
- **Формула**:  
  ```  
  P(a|s) = exp(Q(s,a) / τ) / Σ_{a'} exp(Q(s,a') / τ)  
  ```  
  - **Роль температуры (τ)**:  
    - **τ → ∞**: Распределение становится равномерным (`1/n` для всех действий).  
    - **τ → 0**: Политика становится жадной (выбор действия с максимальным Q-значением).  
  - *Пример*:  
    - При высоком τ (напр., 10) все действия имеют почти одинаковые вероятности.  
    - При низком τ (напр., 0.1) доминирует действие с наибольшим Q-значением.  

---

### **Почему в ε-жадной политике "+ ε/n"?**  
1. **Случайная ветка**:  
   - С вероятностью `ε` агент выбирает действие **равномерно** из `n` вариантов.  
   - Вероятность выбрать жадное действие в этой ветке: `1/n`.  
   - Вклад случайной ветки в итоговую вероятность: `ε * (1/n)`.  

2. **Суммирование вероятностей**:  
   - Жадная ветка: `1 - ε`  
   - Случайная ветка: `ε * (1/n)`  
   - **Итого**: `(1 - ε) + ε/n`.  

---

### **Ключевые выводы**  
- ε-жадная политика **гарантирует exploration** даже при малом `ε` (через `ε/n`).  
- Softmax **учитывает относительные оценки действий**, но требует настройки τ.  
- Обе политики можно комбинировать (например, уменьшать `ε` или `τ` со временем для баланса exploration/exploitation).

---

<img src="temp/temp_slides2/processed_frame_104325.jpg" width="795" height="445" alt="Frame 104325" />

### **Сравнение Q-Learning и SARSA**  

#### **Q-Learning (офф-политики)**  
- **Преимущества**:  
  - Может обучаться на **любых данных** (даже от других политик, например, наблюдение за "старшим братом").  
  - Использует **максимальное Q-значение** для следующего состояния (`max Q(S', a')`), что:  
    - Позволяет игнорировать фактическое действие, выбранное в `S'`.  
    - Ускоряет сходимость к оптимальной политике.  
  - **Не требует взаимодействия со средой** для сбора данных (можно использовать чужой опыт).  

- **Недостатки**:  
  - Склонен к **переоценке Q-значений** из-за максимизации.  
  - Менее стабилен при шумных данных.  

---

#### **SARSA (он-политики)**  
- **Преимущества**:  
  - Учитывает **реальные действия** политики в следующем состоянии (`Q(S', A')`, где `A'` выбирается текущей политикой).  
  - Более **консервативен** — учитывает exploration (например, ε-жадность).  
  - Устойчивее в стохастических средах.  

- **Недостатки**:  
  - Требует **сэмплы из своей политики** (нельзя учиться на чужом опыте).  
  - Может сходиться к **субоптимальной политике** из-за учета случайных действий (например, при высоком ε).  

---

### **Ключевые различия**  
1. **Источник данных**:  
   - Q-Learning: **офф-политики** (любые данные, включая чужие стратегии).  
   - SARSA: **он-политики** (только свои действия).  

2. **Обработка следующего действия**:  
   - Q-Learning: `max Q(S', a')` → оптимистичная оценка.  
   - SARSA: `Q(S', A')` → реалистичная оценка (учитывает exploration).  

3. **Пример использования**:  
   - Q-Learning: обучение на записях игр других агентов ("старшие братья").  
   - SARSA: обучение только на собственном опыте (требует постоянного взаимодействия со средой).  

4. **Сходимость**:  
   - Q-Learning → оптимальная политика.  
   - SARSA → безопасная, но возможно субоптимальная политика.  

---

### **Итог**  
- **Q-Learning** предпочтителен для **offline-обучения** и использования сторонних данных.  
- **SARSA** лучше подходит для **online-обучения** с акцентом на безопасность и учет стохастичности.

---

<img src="temp/temp_slides2/processed_frame_107825.jpg" width="795" height="446" alt="Frame 107825" />

### **Уточнение: On-Policy vs Off-Policy методы**  

#### **On-Policy (SARSA)**  
- **Суть**:  
  - Требует **данные только от текущей политики** (π), которую обучают.  
  - Пример: обучение на сэмплах, собранных **в реальном времени** через взаимодействие со средой (например, ε-жадная политика).  

- **Преимущества**:  
  - Учитывает **реальные exploration-стратегии** (например, случайные действия в ε-жадной политике).  
  - Более **стабилен** в обучении, так как политика и данные согласованы.  

- **Примеры**:  
  - Обучение безопасному вождению, где важно учитывать **текущие ошибки** агента.  
  - Обновление политики на основе **собственных пробных действий** (например, робот учится ходить).  

---

#### **Off-Policy (Q-Learning)**  
- **Суть**:  
  - Может использовать **данные от любой политики** (μ, *behavior policy*) для обучения целевой политики (π, *target policy*).  
  - Примеры данных:  
    - **Логи действий других агентов** (например, записи игр "старшего брата").  
    - **Смешанные стратегии** (например, агрегация данных от нескольких политик).  

- **Преимущества**:  
  - **Повторное использование данных** (например, *imitation learning* на исторических записях).  
  - Возможность обучать **оптимальную политику** (π) на основе **субоптимального поведения** (μ).  

- **Примеры**:  
  - Обучение жадной политики (π) на данных от ε-жадной политики (μ).  
  - Анализ логов пользователей для создания рекомендательной системы.  

---

### **Ключевые различия**  
1. **Гибкость данных**:  
   - **Off-Policy**: Может комбинировать данные из разных источников (даже неактуальные/устаревшие).  
   - **On-Policy**: Требует **актуальные данные** от текущей версии политики.  

2. **Сходимость и стабильность**:  
   - **On-Policy**: Менее эффективен по данным (*sample-inefficient*), но дает **консервативные и предсказуемые** результаты.  
   - **Off-Policy**: Более **sample-efficient**, но рискует переобучиться на "шумные" или несбалансированные данные.  

3. **Сценарии использования**:  
   - **On-Policy**: Онлайн-обучение, безопасные среды (робототехника, автономные системы).  
   - **Off-Policy**: Оффлайн-обучение, использование исторических данных, многозадачное обучение.  

---

### **Важный нюанс**  
- **On-Policy не всегда хуже**:  
  - Несмотря на ограничение данных, обеспечивает **высокую надежность** в динамических средах.  
  - Позволяет **адаптироваться к изменениям** в реальном времени (например, обновление политики при сдвиге распределения данных).  

- **Off-Policy challenges**:  
  - Риск **смещения распределения** (distribution shift) при несоответствии данных политики μ и целевой π.

---

<img src="temp/temp_slides2/processed_frame_112025.jpg" width="795" height="445" alt="Frame 112025" />

### **On-Policy vs Off-Policy: Детали реализации и теория**  

#### **Теоретические аспекты**  
- **Условия теоремы Робинсона-Монро**:  
  - Для сходимости алгоритмов требуется:  
    - Сумма коэффициентов обучения (α) **бесконечна**: ∑α = ∞.  
    - Сумма квадратов коэффициентов **конечна**: ∑α² < ∞.  
  - На практике: теория часто игнорирует ограничения (например, конечные данные).  

- **Q-learning (Off-Policy)**:  
  - **Цель**: Обучение оптимальной функции Q* через максимизацию ожидаемой награды.  
  - **Особенность**: Политика **не участвует явно** в обновлении Q-функции.  
  - **Сходимость**: При выполнении условий теоремы Q сходится к Q*, что позволяет восстановить оптимальную политику π*.  

- **SARSA (On-Policy)**:  
  - **Цель**: Обучение на данных, собранных **текущей политикой** (включая её exploration-стратегии).  
  - **Особенность**: Политика **явно влияет** на обновление Q-функции (чередование действия и наблюдения).  

---

#### **Практическая реализация**  
- **Инициализация**:  
  - Q-значения стартуют с **произвольных значений** (например, нулевых или случайных).  
  - Агент начинает обучение "с нуля", без предварительного опыта.  

- **Процесс обучения**:  
  1. **Сбор опыта**:  
     - Используется **ε-жадная политика** для баланса exploration/exploitation.  
     - Данные собираются в реальном времени (онлайн).  
  2. **Обновление Q-функции**:  
     - Для Q-learning:  
       ```python  
       Q(s, a) = Q(s, a) + α * (r + γ * max(Q(s', a')) - Q(s, a))  
       ```  
     - Для SARSA:  
       ```python  
       Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))  
       ```  

- **Онлайн vs Оффлайн**:  
  - **Онлайн**: Обучение происходит параллельно с взаимодействием со средой (пример: робот учится ходить).  
  - **Оффлайн**: Обучение на заранее собранных данных (пример: анализ логов игр).  

---

#### **Ключевые различия SARSA и Q-learning**  
| **Аспект**         | **SARSA (On-Policy)**                     | **Q-learning (Off-Policy)**              |  
|---------------------|-------------------------------------------|-------------------------------------------|  
| **Данные**          | Только от текущей политики (включая exploration). | Любые данные (даже от других политик). |  
| **Обновление Q**    | Учитывает следующее действие (a') из текущей политики. | Использует максимальное Q(s', a') (оптимальное действие). |  
| **Риски**           | Консервативен, учитывает exploration-шум. | Может переоценить Q-значения из-за максимизации. |  

---

### **Инженерные нюансы**  
- **Шаги взаимодействия**:  
  - `policy.sample_action()`: Выбор действия на основе текущей политики.  
  - `env.step(action)`: Получение награды (r) и следующего состояния (s').  
- **Важно**:  
  - В **Q-learning** политика используется только для сбора данных, но не для выбора действия при обновлении Q.  
  - В **SARSA** следующее действие (a') выбирается **текущей политикой**, что влияет на обновление.  

---

### **Проблемы на практике**  
- **Плохая инициализация**: Может привести к длительному периоду "слепого" исследования.  
- **Несбалансированные данные**: В off-policy возможно смещение распределения (distribution shift).  
- **Trade-off ε-жадности**: Слишком высокий ε замедляет обучение, слишком низкий — снижает exploration.

---

<img src="temp/temp_slides2/processed_frame_115925.jpg" width="795" height="445" alt="Frame 115925" />

### **Детали реализации SARSA и Q-learning**

#### **Ключевое отличие в сэмплировании действий**
- **SARSA (On-Policy)**:
  - Требует **два последовательных вызова политики** для одного обновления Q-функции:
    1. `a = policy.sample(s)` — действие в текущем состоянии `s`.
    2. `a' = policy.sample(s')` — действие в следующем состоянии `s'` (после перехода).
  - Оба действия (`a` и `a'`) выбираются **текущей политикой** (включая exploration, например, ε-жадность).

- **Q-learning (Off-Policy)**:
  - Использует **только один вызов политики** для действия `a` в состоянии `s`.
  - Действие `a'` в состоянии `s'` выбирается **жадно** (`a' = argmax Q(s', *)`), без учёта exploration-стратегии.

---

#### **Оптимизация и нюансы**
1. **Повторное сэмплирование в SARSA**:
   - В SARSA действие `a'` **нельзя кэшировать** из предыдущего шага, так как:
     - После обновления Q-функции политика **может измениться** (особенно при использовании нейросетей).
     - Для корректности требуется "свежий" сэмпл действия, отражающий **актуальную политику**.

2. **Практическая реализация**:
   - Даже при онлайн-обучении **второй вызов `policy.sample(s')` обязателен**:
     ```python
     # SARSA шаг
     a = policy.sample(s)
     s', r = env.step(a)
     a' = policy.sample(s')  # Обязательно!
     Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a') - Q(s, a))
     ```
   - В Q-learning действие `a'` вычисляется жадно, без взаимодействия со средой:
     ```python
     # Q-learning шаг
     a = policy.sample(s)
     s', r = env.step(a)
     a'_best = argmax(Q(s', *))
     Q(s, a) = Q(s, a) + α * (r + γ * Q(s', a'_best) - Q(s, a))
     ```

3. **Теоретическая корректность**:
   - Повторное сэмплирование в SARSA **не нарушает сходимость**, так как соответствует уравнению Беллмана для on-policy методов.
   - Игнорирование второго сэмпла (`a'`) приводит к смещению оценки Q-функции.

---

### **Ответ на вопрос: SARSA vs Q-learning**
- **SARSA** сходится к **оптимальной политике с учётом exploration** (например, ε-жадной), так как учится на данных, включающих "шум" исследования.
- **Q-learning** сходится к **абсолютно оптимальной политике** (π*), игнорируя exploration-стратегии в обновлениях.

| **Алгоритм** | **Сходится к**                          | **Учёт exploration** |
|--------------|-----------------------------------------|-----------------------|
| SARSA        | Оптимальной политике **с exploration** | Да (on-policy)        |
| Q-learning   | Абсолютно оптимальной политике (π*)     | Нет (off-policy)      |

---

#### **Дополнительные замечания**
- **Softmax/Entropy регуляризация**:
  - Позволяет балансировать exploration/exploitation через температуру (обсуждается отдельно).
  - В SARSA может быть интегрирована в политику через взвешенное сэмплирование действий.
- **Нейросетевые реализации**:
  - При использовании DQN (нейросетей) повторное сэмплирование в SARSA критично, так как Q-функция меняется **глобально** после каждого обновления.

---

<img src="temp/temp_slides2/processed_frame_127525.jpg" width="795" height="445" alt="Frame 127525" />

### **Сравнение SARSA и Q-learning в среде с риском**

#### **Ключевые различия в сходимости**
- **SARSA (On-Policy)**:  
  - Сходится к **безопасной политике**, избегающей рисков (например, обрывов).  
  - Причина: использует **ε-жадную стратегию** с ненулевой вероятностью случайных действий (риск падения в "лаву").  
  - Учитывает **стохастичность политики** при обновлении Q-функции, минимизируя потенциальные потери.  

- **Q-learning (Off-Policy)**:  
  - Сходится к **абсолютно оптимальной политике** (кратчайший путь).  
  - Игнорирует exploration-стратегии (например, ε-жадность) при обновлении Q-функции.  
  - Не учитывает риск случайных действий в процессе обучения.  

---

#### **Пример среды с риском**
- **Постановка задачи**:  
  - Агент должен добраться из точки **A** в терминальное состояние **B**.  
  - **Награды**:  
    - `-1` за каждый шаг в среде.  
    - `-100` за падение в "лаву" (сброс в начальное состояние).  
  - **Оптимальный путь**: Минимум 13 шагов (награда `-13`).  

- **Поведение алгоритмов**:  
  - **SARSA**: Выбирает более длинный, но безопасный путь, избегая зон с риском.  
  - **Q-learning**: Выбирает кратчайший путь, но может "упасть в лаву" из-за игнорирования exploration-шума.  

---

#### **Причины разной сходимости**
1. **SARSA**:  
   - Обновляет Q-функцию с учётом **текущей политики** (включая ε-жадность).  
   - Учитывает вероятность **случайных опасных действий** (например, 5% шаг в сторону обрыва).  
   - Формирует политику, **минимизирующую дисперсию наград** (безопасный маршрут).  

2. **Q-learning**:  
   - Обновляет Q-функцию через **жадную стратегию** (`argmax`).  
   - Не моделирует риски, возникающие в процессе exploration (например, ε-шаги).  
   - Оптимизирует **математическое ожидание наград** без учёта реальных траекторий.  

---

#### **Парадокс SARSA**
- **Эффект "избыточной осторожности"**:  
  - Даже при малом ε (например, 1%) SARSA избегает зон, где **потенциальный риск** превышает выгоду от кратчайшего пути.  
  - Пример: Обход обрыва, хотя прямой путь короче, но вероятность падения делает его менее выгодным в долгосрочной перспективе.  

---

#### **Практические выводы**
- **Выбор алгоритма**:  
  - SARSA — для сред с **высокими рисками** (робототехника, медицинские приложения).  
  - Q-learning — для сред с **детерминированными переходами** (игры, симуляции).  

- **Важность ε-стратегии**:  
  - В SARSA параметр ε влияет на баланс между **безопасностью** и **скоростью обучения**.  
  - В Q-learning ε используется только для сбора данных, но не влияет на обновления Q-функции.

---

<img src="temp/temp_slides2/processed_frame_134100.jpg" width="795" height="445" alt="Frame 134100" />

### **Последствия игнорирования рисков (на примере Q-learning)**
- **Неоптимальность в стохастических средах**:  
  — Если не учитывать **вероятность случайных действий** (ε-шаги), агент выберет рискованный кратчайший путь.  
  — Пример: В среде с обрывом агент будет падать в "лаву" в **5% случаев** из-за ε-исследования, но Q-learning этого не предусмотрит.  

- **Завышенная оценка наград**:  
  — Q-функция будет считать кратчайший путь идеальным (`-13`), игнорируя **реальные потери** от падений (`-100`).  
  — Фактическая награда: `0.95*(-13) + 0.05*(-113) ≈ -18` (хуже ожидаемого `-13`).  

- **Невозможность выявить риски**:  
  — Без анализа **стохастичности политики** (как в SARSA) агент не поймёт, что "безопасный" путь выгоднее в долгосрочной перспективе.  

---

### **Ключевой вывод**  
Игнорирование **вероятности негативных исходов** (отрицательных наград) приводит к:  
1. Переоценке эффективности политики.  
2. Выбору стратегий с **высокой дисперсией наград**.  
3. Невозможности обнаружить скрытые риски среды.  

SARSA решает эту проблему через **явное моделирование exploration-шума** в Q-функции.

---

