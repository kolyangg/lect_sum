{
  "processed_frame_014825.jpg": {
    "1": {
      "question": "Какое состояние здоровья упомянул лектор в начале занятия?\nа) Грипп\nб) Непродолжительная простуда\nв) Аллергия\nг) Переутомление\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какое по счету занятие проводит лектор в данном контексте?\nа) Первое\nб) Второе\nв) Третье\nг) Четвертое\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Какой тип среды стал главным объектом изучения на текущем занятии?\nа) Model-based\nб) Model-free\nв) Детерминированная\nг) Стохастическая\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_016150.jpg": {
    "1": {
      "question": "Какой компонент НЕ входит в состав марковского процесса принятия решений (MDP) согласно лекции?\nа) Пространство действий (action space)\nб) История состояний и действий\nв) Условная вероятность перехода\nг) Детерминированная награда\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какой пример среды используется для иллюстрации MDP в лекции?\nа) Змейка\nб) Тетрис\nв) Марио\nг) Пакман\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Почему в лекции выбрана детерминированная награда вместо стохастической?\nа) Для увеличения сложности модели\nб) Для упрощения выкладок\nв) Чтобы учесть историю состояний\nг) Для имитации реального мира\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_019925.jpg": {
    "1": {
      "question": "Как в лекции называется кумулятивная награда, обозначаемая как GT?\nа) Ожидаемая награда\nб) Дисконтированная награда\nв) Награда к получению (reward to go)\nг) Стохастическая награда\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Почему в лекции была введена Q-функция?\nа) Потому что V-функции недостаточно\nб) Для учета истории состояний\nв) Чтобы увеличить сложность модели\nг) Для имитации детерминированных действий\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "3": {
      "question": "Чему, согласно лекции, равен GT (кумулятивная награда)?\nа) RT\nб) V(s)\nв) Q(s, a)\nг) P(s'|s, a)\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    }
  },
  "processed_frame_021700.jpg": {
    "1": {
      "question": "Что происходит с вкладом будущих наград при гамме равной нулю согласно лекции?\nа) Все награды имеют одинаковый вес\nб) Максимизируется текущая награда\nв) Награды убывают экспоненциально\nг) Учитывается только конечная награда\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какой пример среды с эпизодическим взаимодействием приведен в лекции?\nа) Шахматная доска\nб) Муравей в continuous control задаче\nв) Гридворлд с лабиринтом\nг) Дерево решений с дисконтированием\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Как называются среды, где взаимодействие заканчивается через фиксированное количество шагов?\nа) Инфинитивные\nб) Стохастические\nв) Эпизодические\nг) Дисконтированные\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_027775.jpg": {
    "1": {
      "question": "Какое уравнение выводится при выполнении условий конечности state и action space согласно лекции?\nа) Уравнение Лагранжа\nб) Уравнение Беллмана\nв) Уравнение динамического программирования\nг) Уравнение Маркова\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Сколько ассамбляжей, помимо конечности space, упоминается в лекции как «не очень строгие»?\nа) Один\nб) Два\nв) Три\nг) Четыре\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Как оценивается реалистичность сценария с конечным state space в лекции?\nа) Очень реалистичный\nб) Не очень реалистичный\nв) Полностью нереалистичный\nг) Зависит от action space\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_029250.jpg": {
    "1": {
      "question": "Почему в уравнениях для value и Q-функций используются суммы, согласно лекции?\nа) Из-за непрерывности пространства\nб) Из-за замены интегралов на матожидания\nв) Из-за конечности носителя (state space)\nг) Из-за детерминированных действий\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Что утверждает теорема о конечных МДП, упомянутая в лекции?\nа) Существует стохастическая оптимальная политика\nб) Оптимальная политика зависит от начального состояния\nв) Существует детерминированная оптимальная политика\nг) Все политики сходятся к одной точке\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Сколько этапов объединяет алгоритм Value Iteration по описанию из лекции?\nа) Один\nб) Два\nв) Три\nг) Четыре\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_032875.jpg": {
    "1": {
      "question": "Какие два основных этапа включает алгоритм, описанный в лекции?\nа) Policy Evaluation и Policy Update\nб) Policy Evaluation и Policy Improvement\nв) Value Optimization и Policy Adjustment\nг) State Exploration и Reward Calculation\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Как выбирается действие на этапе Policy Improvement согласно лекции?\nа) Случайным образом\nб) Жадно (максимизируя награду)\nв) На основе нейронной сети\nг) С учетом энтропии\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "К чему сходится итеративная процедура Policy Evaluation по утверждению лекции?\nа) К случайному распределению\nб) К уравнению динамического программирования\nв) К неподвижной точке уравнения Беллмана\nг) К оптимальной политике\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_041500.jpg": {
    "1": {
      "question": "Что утверждается о новой политике после её улучшения согласно лекции?\nа) Она будет хуже предыдущей\nб) Её значение в любом состоянии не меньше, чем у предыдущей\nв) Она становится полностью случайной\nг) Требует больше вычислительных ресурсов\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Как представляется Q-функция согласно лекции?\nа) Вектор размерности S\nб) Таблица состояний и времени\nв) Матрица (количество состояний × количество действий)\nг) Граф переходов\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что необходимо сделать перед Policy Improvement при использовании V-функции?\nа) Обновить вектор состояний\nб) Восстановить Q-функцию\nв) Увеличить размер матрицы\nг) Провести случайный выбор действий\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_046350.jpg": {
    "1": {
      "question": "Чем славится алгоритм эволюционных стратегий согласно лекции?\nа) Высокой скоростью сходимости\nб) Неэффективностью по количеству используемых данных\nв) Использованием нейронных сетей\nг) Минимальным бюджетом взаимодействия\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "На что направлен тренд в разработке алгоритмов, упомянутый в лекции?\nа) Увеличение размера популяции\nб) Работа в рамках бюджета взаимодействия со средой\nв) Использование случайных траекторий\nг) Максимизация энтропии\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Как влияет уменьшение Inductive Biases на эффективность алгоритма?\nа) Повышает эффективность каждого сэмпла\nб) Снижает эффективность использования информации из среды\nв) Увеличивает размер популяции\nг) Делает алгоритм детерминированным\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_049350.jpg": {
    "1": {
      "question": "Что является достаточным условием остановки policy iteration согласно лекции?\nа) Проверка изменения политики\nб) Удовлетворение уравнению Беллмана с заданной точностью\nв) Достижение максимального бюджета взаимодействий\nг) Снижение шума в наградах\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какая характеристика реального мира упоминается как худший сценарий взаимодействия со средой?\nа) Динамика среды известна точно\nб) Среда недоступна для наблюдения\nв) Награды детерминированы\nг) Политика всегда сходится\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Почему в реальном мире обучение v-функции теряет смысл согласно лекции?\nа) Невозможно восстановить политику из неё\nб) Она требует слишком много данных\nв) Не связана с policy improvement\nг) Всегда даёт зашумлённые оценки\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    }
  },
  "processed_frame_053350.jpg": {
    "1": {
      "question": "Какой метод оценки значений упоминается в лекции?\nа) Максимальное правдоподобие\nб) Эмпирическое среднее\nв) Бутстрэп\nг) Скользящее среднее\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Что утверждается о применении v-функции в рассматриваемом сетапе?\nа) Она необходима для обновления политики\nб) Она используется для расчета наград\nв) Она не пригодится\nг) Она обеспечивает точные предсказания\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что упоминается о теоретической стороне v-функции?\nа) Существуют теоретические ограничения\nб) Есть теоретические гарантии\nв) Теоретические основы устарели\nг) Теоретические методы не применяются\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_054075.jpg": {
    "1": {
      "question": "Какой метод оценки математического ожидания Q-функции упоминается в лекции?\nа) Бутстрэп\nб) Эмпирическое среднее\nв) Скользящее окно\nг) Метод Монте-Карло\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какой основной недостаток алгоритма, связанный с неэпизодическими средами?\nа) Необходимость доигрывать траектории до конца\nб) Использование детерминированных действий\nв) Отсутствие теоретических гарантий\nг) Низкая скорость вычислений\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "3": {
      "question": "Что называется главным недостатком алгоритма оценки Q-функции через эмпирическое среднее?\nа) Смещение оценки\nб) Медленная скорость сходимости\nв) Высокая дисперсия\nг) Необходимость больших вычислительных ресурсов\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_064000.jpg": {
    "1": {
      "question": "Какое уравнение в представленном ряду является лишним по мнению лектора?\nа) Уравнение с матожиданием в правой части\nб) Уравнение с суммой наград\nв) Уравнение с максимумом матожидания\nг) Уравнение с распределением вероятностей\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Почему одно из уравнений считается «самым странным» в контексте лекции?\nа) Оно содержит сумму наград\nб) В нём используется максимум вместо матожидания\nв) Оно не имеет правой части\nг) В нём отсутствует распределение\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Что объединяет три уравнения из четырёх, согласно объяснению лектора?\nа) Наличие максимума в правой части\nб) Использование эмпирического среднего\nв) Правую часть в виде матожидания\nг) Отсутствие распределения\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_066600.jpg": {
    "1": {
      "question": "Какое значение αk упоминается в контексте для детерминированного пересчёта среднего?\nа) 1/k\nб) k/2\nв) 1/(k+1)\nг) 0.5\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "2": {
      "question": "Какой метод используется для обновления среднего при поступлении нового элемента в онлайн-режиме?\nа) Градиентный спуск\nб) Эмпирическое среднее\nв) Формула с весами для старой и новой аппроксимации\nг) Метод максимального правдоподобия\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "На что похож итерационный процесс, описанный в лекции?\nа) На метод Ньютона\nб) На алгоритм градиентного спуска\nв) На метод Монте-Карло\nг) На метод опорных векторов\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_068925.jpg": {
    "1": {
      "question": "Какая теорема упоминается в контексте для обоснования сходимости в L2?\nа) Теорема Робинсона-Ван Ро\nб) Теорема Байеса\nв) Теорема Больцано-Вейерштрасса\nг) Теорема Пифагора\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "2": {
      "question": "Какая сходимость следует из сходимости в L2 согласно лекции?\nа) Сходимость почти наверное\nб) Сходимость по распределению\nв) Сходимость по вероятности\nг) Сходимость в равномерной метрике\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что упоминается как дополнительное ограничение для действия «Аштрик»?\nа) Оно должно быть случайным\nб) Оно должно зависеть от состояния\nв) Оно должно прийти из определённой позиции\nг) Оно должно максимизировать награду\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_070275.jpg": {
    "1": {
      "question": "Какой алгоритм упоминается как похожий на описанную процедуру?\nа) Q-learning\nб) SARSA\nв) Монте-Карло политики\nг) Детерминированный градиентный спуск\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Что используется для построения следующей Q-функции согласно лекции?\nа) Уравнение Лагранжа\nб) Уравнение Больцмана\nв) Уравнение Беллмана\nг) Уравнение Эйлера\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что обеспечивает итерационная процедура согласно контексту?\nа) Случайную инициализацию\nб) Теоретические гарантии\nв) Минимизацию дисперсии\nг) Оптимальность за один шаг\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_071600.jpg": {
    "1": {
      "question": "Какой алгоритм упоминается в контексте как пример с использованием временной разности (temporal difference)?\nа) Q-learning\nб) SARSA\nв) TD0\nг) Монте-Карло\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Как называется процесс создания таргетов «на ходу» с использованием текущей аппроксимации?\nа) Experience replay\nб) Bootstrapping\nв) Policy gradient\nг) Value iteration\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "К чему сходится алгоритм при фиксированной политике согласно лекции?\nа) К среднему значению награды\nб) К оптимальной Q-функции (Q*)\nв) К функции V для всех состояний\nг) К детерминированной политике\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_081925.jpg": {
    "1": {
      "question": "Какое название алгоритма упоминается в контексте как «Cool Learning»?\nа) Q-learning\nб) SARSA\nв) TD0\nг) Монте-Карло\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "2": {
      "question": "Что обозначает «а штрих» в контексте лекции?\nа) Текущее действие\nб) Следующее действие (a_{t+1})\nв) Оптимальное действие\nг) Действие предыдущего шага\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "На что обращают внимание при сравнении алгоритмов согласно лекции?\nа) Скорость вычислений\nб) Теоретические гарантии\nв) Количество параметров\nг) Сложность реализации\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_087125.jpg": {
    "1": {
      "question": "Что означает «а штрих» в контексте лекции?\nа) Текущее действие (a_t)\nб) Следующее действие (a_{t+1})\nв) Оптимальное действие\nг) Действие предыдущего шага\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какое условие теоремы требует, чтобы сумма коэффициентов α была бесконечной?\nа) Инфинит визит\nб) Конечная дисперсия\nв) Линейная сходимость\nг) Стационарность среды\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "3": {
      "question": "Почему необходимо посещать пары (состояние, действие) бесконечное число раз?\nа) Для минимизации вычислительных затрат\nб) Чтобы гарантировать сходимость к оптимальным значениям\nв) Для ускорения обучения\nг) Чтобы избежать локальных оптимумов\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_091875.jpg": {
    "1": {
      "question": "Что подразумевается под «черным типом коз» в контексте лекции?\nа) Оптимальные действия\nб) Затраты на исследование (неоптимальные действия)\nв) Финансовые потери\nг) Технические ошибки\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какой пример используется для иллюстрации потенциальной выгоды исследования?\nа) Поиск нового маршрута\nб) Открытие более хорошего ресторана\nв) Оптимизация алгоритма\nг) Игра в шахматы\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Что необходимо соблюдать в контексте дилеммы исследования и эксплуатации?\nа) Строгий график действий\nб) Баланс между использованием знаний и открытостью к новому\nв) Максимальную скорость принятия решений\nг) Полный отказ от неоптимальных действий\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_094325.jpg": {
    "1": {
      "question": "Какой пример оптимальной политики приведен в контексте?\nа) Q со звездой\nб) Случайная политика\nв) Детерминированная политика\nг) Политика с энтропией\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "2": {
      "question": "С какой вероятностью предлагается «кидать монетку» в примере смешанной политики?\nа) 0,1\nб) 0,3\nв) 0,5\nг) 0,7\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Как называется политика, сочетающая жадные и случайные действия согласно контексту?\nа) Softmax-политика\nб) Эпсилон-жадный поликлип\nв) Детерминированный алгоритм\nг) Q-обучение\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_096500.jpg": {
    "1": {
      "question": "Какой метод предложен в лекции для стокастической политики, использующей Q-функцию?\nа) Эпсилон-жадный\nб) Жадная политика\nв) Softmax-политика\nг) Случайный поиск\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Как вычисляется вероятность выбора жадного действия в эпсилон-жадной политике согласно контексту?\nа) 1 − ε\nб) ε / n\nв) (1 − ε) + (ε / n)\nг) ε + (1 − ε) / n\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что используется для преобразования логитов нейронной сети в вероятности действий?\nа) Линейная регуляризация\nб) Функция активации ReLU\nв) Softmax-слой\nг) Сигмоидная функция\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_100800.jpg": {
    "1": {
      "question": "Как влияет увеличение параметра температуры (α) в softmax-политике согласно контексту?\nа) Политика становится детерминированной\nб) Политика стремится к равномерному распределению\nв) Увеличивается вероятность жадного действия\nг) Уменьшается энтропия политики\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Как вычисляется вероятность выбора жадного действия в эпсилон-жадной политике согласно объяснению в лекции?\nа) 1 − ε\nб) ε / n\nв) 1 − ε + ε / n\nг) (1 − ε) * (1 / n)\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что происходит с политикой при стремлении параметра температуры (α) к бесконечности?\nа) Политика становится детерминированной\nб) Вероятности действий пропорциональны Q-значениям\nв) Все действия выбираются равновероятно\nг) Увеличивается доля случайных действий\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_104325.jpg": {
    "1": {
      "question": "Какое преимущество первого алгоритма (Q-learning) упоминается в лекции по сравнению со вторым (SARSA)?\nа) Требует данных только из своей политики\nб) Может обучаться на данных из любых политик\nв) Использует шаблон из динамики среды\nг) Требует взаимодействия с двумя разными средами\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какое ограничение второго алгоритма (SARSA) подчеркивается в контексте?\nа) Не может использовать максимум Q-значений\nб) Требует данных, собранных только из своей политики\nв) Невозможно обучаться на чужом опыте\nг) Использует шаблон из динамики среды\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Какой пример используется в лекции для иллюстрации обучения на данных из разных политик?\nа) Старший брат, который не дает играть в компьютер\nб) Два старших брата, играющих в разные игры\nв) Обучение через взаимодействие с одной средой\nг) Использование шаблона из динамики среды\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_107825.jpg": {
    "1": {
      "question": "Какой пример имитационного обучения (imitation learning) приведен в лекции для off-policy методов?\nа) Обучение через взаимодействие с одной средой\nб) Использование логов, собранных другими политиками\nв) Применение эпсилон-жадной политики\nг) Обучение жадной политики без исследований\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "С помощью какой политики собираются сэмплы для обучения жадной политики в примере из лекции?\nа) Таргет-политики\nб) Эпсилон-жадной политики\nв) Детерминированной политики\nг) Поведенческой политики\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Какое преимущество off-policy методов упоминается в контексте?\nа) Возможность обучаться на данных из нескольких политик\nб) Более высокая sample-эффективность\nв) Не требуют данных из динамики среды\nг) Использование максимальных Q-значений\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    }
  },
  "processed_frame_112025.jpg": {
    "1": {
      "question": "Что используется в Q-learning для обучения оптимальной политике согласно лекции?\nа) Только текущая политика\nб) Сэмплы из другой политики\nв) Фиксированная динамика среды\nг) Логи из нескольких сред\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какое условие теоремы Роббинса-Монро упоминается в контексте?\nа) Сумма альфа бесконечна\nб) Сумма альфа конечна\nв) Сумма квадратов альфа конечна\nг) Альфа убывает экспоненциально\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "3": {
      "question": "Что характерно для онлайн-обучения в reinforcement learning по лекции?\nа) Агент начинает без опыта\nб) Используются заранее собранные логи\nв) Обучение происходит оффлайн\nг) Политика фиксирована на всех этапах\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    }
  },
  "processed_frame_115925.jpg": {
    "1": {
      "question": "Сколько раз вызывается policy.sample в SARSA согласно лекции?\nа) Один раз\nб) Два раза\nв) Три раза\nг) Ни разу\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Почему в лекции предлагают не посылать s' в среду повторно?\nа) Для экономии сэмплирования\nб) Из-за фиксированной политики\nв) Потому что Q-функция не меняется\nг) Это нарушает теоретические гарантии\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "а"
    },
    "3": {
      "question": "Что происходит с теоретическими гарантиями при повторном сэмплировании из политики?\nа) Гарантии полностью ломаются\nб) Требуется новая инициализация\nв) Гарантии не нарушаются\nг) Зависит от температуры софтмакса\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    }
  },
  "processed_frame_127525.jpg": {
    "1": {
      "question": "Какая оптимальная награда упоминается в примере с лабиринтом?\nа) -5\nб) -10\nв) -13\nг) -20\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "в"
    },
    "2": {
      "question": "Почему SARSA сходится к безопасной политике согласно контексту?\nа) Из-за использования нейронной сети\nб) Из-за отвращения к риску\nв) Из-за детерминированных действий\nг) Из-за максимальной энтропии\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Что получает агент за каждое время, проведенное в среде?\nа) Награду +1\nб) Награду -1\nв) Награду -10\nг) Нулевую награду\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  },
  "processed_frame_134100.jpg": {
    "1": {
      "question": "Что происходит при попадании в обрыв в примере из лекции?\nа) Награда +1\nб) Возврат в начальную точку и штраф\nв) Автоматический переход к терминальному состоянию\nг) Увеличение энтропии политики\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "2": {
      "question": "Какой штраф получает агент за каждый шаг в среде?\nа) -0.5\nб) -1\nв) -5\nг) -10\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    },
    "3": {
      "question": "Что ограничивает пространство политик в SARSA согласно контексту?\nа) Использование нейронных сетей\nб) Детерминированные действия и эпсилон-жадная стратегия\nв) Максимальная энтропия\nг) Отсутствие терминальных состояний\nд) Ответ на этой вопрос отсутствует в контексте",
      "answer": "б"
    }
  }
}