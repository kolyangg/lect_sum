{
  "processed_frame_014825.jpg": "* Второе занятие посвящено model-free сеттингам, когда информация о среде минимальна\n* Model-free сеттинги важны, так как соответствуют реальным задачам в жизни\n* Напоминание о прошлом занятии предшествует новой теме",
  "processed_frame_016150.jpg": "*",
  "processed_frame_019925.jpg": "* GT - кумулятивная награда, случайная величина\n  * Функция v(s, p) - ожидаемая награда, начиная из состояния s и действуя по политике p\n  * Функция q(s, a, p) - ожидаемая награда, начиная из состояния s, выполнив действие a и действуя по политике p\n  * Reward to go (GT) - кумулятивная награда, равная RT",
  "processed_frame_021700.jpg": "*",
  "processed_frame_027775.jpg": "* Говорим, что state space конечен, что удовлетворяет достаточно много сред реального мира, хотя не все.\n* Action space также считается конечным, что еще больше увеличивает количество удовлетворяющих этому условию сред.\n* Если оба условия выполнены, можно вывести уравнение Беллмана, связывающее value функции в текущем состоянии.",
  "processed_frame_029250.jpg": "*",
  "processed_frame_032875.jpg": "* Policy Evaluation и Policy Improvement - основные этапы алгоритмов в Reinforcement Learning.\n* Policy Evaluation: оценка текущей политики, определение ожидаемой награды.\n* Policy Improvement: улучшение политики с использованием детерминированного правила.\n* Value Iteration: алгоритм,",
  "processed_frame_041500.jpg": "* Policy Improvement: улучшение политики с использованием детерминированного правила, новая политика не хуже предыдущей.\n* Policy Iteration и Value Iteration: алгоритмы, которые можно применить как к V-функции, так и к Q-функции.\n* Value Iteration: сложность алгоритма увеличивается, так как обновляются пары состояния-действие, а не только состояния.\n* V-функция: представлена в виде вектора, Q-функция - в виде матрицы (количество состояний на количество действий).\n* При обучении V-функции перед Policy Improvement нужно восстановить Q-функцию, чтобы оценить качество действий.\n* При обучении Q-функции шаг восстановления Q-функции можно пропустить и сразу брать максимум.\n* Q и V функции двигают распределение в сторону элитных траекторий.",
  "processed_frame_046350.jpg": "* Эволюционные стратегии: вдохновение для алгоритмов, которые двигают распределение в сторону элитных траекторий (с высокой наградой).\n* Эффективность: алгоритмы эволюционных стратегий неэффективны с точки зрения количества сэмплов, но могут сойтись к хорошему решению.\n* Бюджет: тренд на создание алгоритмов с ограниченным бюджетом взаимодействия со средой.\n* Inductive Biases: чем меньше вкладывается в алгоритм, тем менее эффективен один сэмпл и меньше вклад в качество всего алгоритма.\n* Популяция: при большой популяции вклад отдельного индивидуума становится менее заметным.",
  "processed_frame_049350.jpg": "* Policy iteration: критерий остановы - проверка, что функция удовлетворяет уравнению Беллмана с определенной точностью. Проверка неизменности политики не обязательна, так как при одинаково хороших действиях политика может меняться.\n* Реальный мир: в отличие от идеализированной модели, в реальном мире среда может быть неизвестна и шумной, что усложняет понимание ее динамики.\n* V-функция: в реальном мире для проведения policy improvement v-функция больше не нужна, так как из нее нельзя восстановить политику. Однако, это не делает ее абсолютно бесполезной.",
  "processed_frame_053350.jpg": "* V-функция: в данном сетапе не используется для policy improvement, так как из нее нельзя восстановить политику. Однако, она все еще имеет значение и применяется в других контекстах, включая теоретические аспекты.",
  "processed_frame_054075.jpg": "* Q-функция как математическое ожидание может быть оценена сэмплами из распределения, используя эмпирическое среднее. Это подкрепляется теоретически законом больших чисел.\n* Для оценки Q-функции по траекториям: начинаем с состояния, совершаем действия, добегаем до конца траектории, считаем эмпирическую награду. Одно наблюдение требует доигрывания до конца эпизода.\n* Две размерности: размерность усреднения и время внутри траектории. Для одного наблюдения нужно доиграть до конца.\n* Политика может быть случайной, среда - стохастической. Оценивание Q-функции сэмплами R1, R2 и т.д. заменяет внутренние ожидания на несмещенные оценки.\n* Недостатки алгоритма: необходимость доигрывания до конца эпизода (неприемлемо в неэпизодических средах), большая дисперсия.\n* Достоинства: несмещенная оценка того, что нужно.\n* Альтернативы: уравнение Белмана, связывающее Q-функцию через один шаг.",
  "processed_frame_064000.jpg": "* В игре в ассоциативный ряд с четырьмя уравнениями, лишним является третье уравнение, так как оно содержит максимум от математического ожидания, в отличие от остальных, где правая часть представляет собой математическое ожидание по какому-то распределению.\n* Цель оценки математического ожидания важна для понимания и анализа поведения системы, особенно в контексте Reinforcement Learning, где оно используется для оценки Q-функции.\n* Третье уравнение выделяется, так как оно не соответствует остальным, где правая часть представляет собой математическое ожидание по распределению.\n* Оценка математического ожидания критична для правильного понимания и моделирования процессов в Reinforcement Learning, где оно используется для оценки Q-функции и принятия решений на основе ожидаемых наград.",
  "processed_frame_066600.jpg": "*",
  "processed_frame_068925.jpg": "* Теорема Робинсона-Ван Ро: если αk удовлетворяет определенным условиям, то последовательность θk сходится к θ* в L2, что означает сходимость ожидания квадрата разности к нулю.\n* Сходимость в L2 влечет за собой сходимость по вероятности и по распределению.\n* Пример алгоритма, похожего на процедуру оценки ожидания для одного распределения, но в общем случае распределения могут зависеть от состояния и действия.\n* Использование Bellman с дополнительными ограничениями на действие, которое должно приходить из определенной позиции.",
  "processed_frame_070275.jpg": "*",
  "processed_frame_071600.jpg": "* Используем итерационную процедуру для оценки среднего значения, с теоретическими гарантиями.\n* Уравнение Беллмана позволяет улучшить алгоритм, используя текущую оценку функции Q и информацию о среде.\n* TD-1 алгоритм использует один сэмпл для обновления Q-функции, не требуя доигрывания эпизодов до конца.\n* Таргеты в TD-1 алгоритме создаются на основе текущей аппроксимации, что называется bootstrapping.\n* Альфа-каты (learning rate) может зависеть от состояния и действия, что важно для эффективного обучения.\n* Использование Bellman optimality equation позволяет улучшить алгоритм, убирая ограничения на действия.\n* При фиксированной политике алгоритм сойдется к Q-функции, соответствующей этой политике.\n* Алгоритм, использующий максимум по действиям, сойдется к оптимальной Q-функции, независимо от политики.",
  "processed_frame_081925.jpg": "* Монте-Карло, Cool Learning и Saksa - различные алгоритмы, которые нужно сравнить и понять, в каких ситуациях каждый из них эффективнее.\n* Важно анализировать свойства каждого алгоритма, чтобы лучше понять их применение.\n* Условие, что A' (A_T+1) и S' (S_T+1) - это следующее действие и следующее состояние соответственно, важно для понимания алгоритмов.\n* Сравнение алгоритмов должно основываться на формальных условиях и теоретических гарантиях.",
  "processed_frame_087125.jpg": "*",
  "processed_frame_091875.jpg": "* В агенте RL а' (a prime) - это следующее действие, которое агент выбирает в состоянии s' (s prime).\n* Теоретические гарантии требуют бесконечного числа посещений пар состояния-действия (SA) для сходимости к оптимальному значению.\n* Для этого агент должен постоянно исследовать, не пренебрегая любыми парами состояния-действия.\n* В теории, для сходимости к оптимальному значению, каждая пара состояния-действия должна быть посещена бесконечное число раз.\n* В реальной жизни возникает дилемма exploration-exploitation: использовать текущие знания для максимизации награды или исследовать новые возможности.\n* Исследование может привести к неоптимальным действиям (черный тип коз), но может также привести к нахождению лучших решений.\n* Необходимо найти баланс между использованием текущих знаний и исследованием новых возможностей.",
  "processed_frame_094325.jpg": "* Exploration и exploitation - две стороны спектра в RL.\n* Exploration - случайная политика, выбирающая каждое действие с одинаковой вероятностью.\n* Exploitation - жадная политика, следующая оптимальной стратегии Q*.\n* Если текущая оптимизация Q близка к реальности, действия будут близки к оптимальным, иначе - неоптимальны.\n* Эпсилон-жадная политика лежит между exploration и exploitation: с вероятностью ε выбирает случайное действие, с вероятностью 1-ε - жадное действие.\n* Эпсилон-жадная политика позволяет балансировать между исследованием и использованием текущих знаний.",
  "processed_frame_096500.jpg": "* Эпсилон-жадная политика вводит параметр ε, определяющий вероятность выбора случайного действия (ε) и жадного действия (1-ε).\n* Вероятность выбора жадного действия в эпсилон-жадной политике равна 1-ε + ε/n, где n - количество возможных действий.\n* Можно придумать другие стократические политики, использующие Q-функцию, например, с использованием нейронной сети и функции softmax для получения вероятностей действий.\n* Такая политика будет зависеть от состояния и использовать Q-функцию для определения вероятностей действий, обеспечивая баланс между случайностью и оптимальностью.",
  "processed_frame_100800.jpg": "* В эпсилон-жадной политике вероятность выбора жадного действия равна 1-ε + ε/n, где n - количество возможных действий.\n* Это связано с тем, что жадное действие может быть выбрано двумя способами: с вероятностью 1-ε напрямую, и с вероятностью ε/n через случайный выбор.\n* Введение температуры (α) в функцию softmax позволяет регулировать стократичность политики: при α→∞ политика стремится к равномерному распределению.\n* Температура в функции softmax выступает как масштабирующий фактор, влияющий на распределение вероятностей действий.\n* В классе эпсилон-жадных политик мы будем использовать два алгоритма для обучения и оценки эффективности.",
  "processed_frame_104325.jpg": "* Сравниваем два алгоритма: Q-обучение и SARSA.\n* Q-обучение (off-policy) может учиться с чужого опыта, не требуя сэмплирования из своей политики, в то время как SARSA (on-policy) требует сэмплирования из текущей политики.\n* Q-обучение не имеет ограничений на пары состояний и действий (S, A), но S' должно приходить из динамики среды; SARSA требует, чтобы все данные приходили из текущей политики.\n* Q-обучение может использовать опыт, собранный из разных политик, что позволяет учиться, наблюдая за другими агентами, в то время как SARSA должна самостоятельно взаимодействовать со средой для обучения.\n* SARSA учитывает текущую политику при выборе действий, в то время как Q-обучение выбирает максимум Q-функции.\n* Q-обучение может учиться, наблюдая за другими агентами, в то время как SARSA должна самостоятельно взаимодействовать со средой.\n* Q-обучение может учиться с опыта, собранного другими политиками, что делает его более гибким, но SARSA учитывает текущую политику при обучении, что делает его более привязанным к текущей стратегии.\n* Q-обучение работает в off-policy режиме, используя опыт, собранный из разных политик, в то время как SARSA работает в on-policy режиме, используя только свой опыт.\n* Q-обучение может учиться, наблюдая за другими агентами, в то время как SARSA должна самостоятельно взаимодействовать со средой для обновления своей политики.\n* Q-обучение может быть более привлекательным, так как оно может извлекать знания из различных источников, в то время как SARSA учитывает только свой опыт.\n* Q-обучение позволяет учиться с чужого опыта, в то время как SARSA учитывает только свой опыт, что делает его более зависимым от текущей политики.\n* Q-обучение может быть более эффективным в ситуациях, где доступен опыт других агентов, в то время как SARSA требует собственного опыта для обновления своей политики.",
  "processed_frame_107825.jpg": "*",
  "processed_frame_112025.jpg": "*",
  "processed_frame_115925.jpg": "* Сарс и Q-learning имеют схожие параметры, но Сарс делает два вызова policy.sample для текущего состояния и следующего состояния.\n* В Сарс после первого вызова политики, агент получает награду и следующее состояние, но не отправляет его в среду, так как это уже детали оптимизации.\n* Вопрос о том, нужно ли сэмплировать второй раз из политики, зависит от того, изменится ли Q-функция после обновления. Если Q-функция изменится, то сэмплирование второй раз может быть необходимо.\n* Теоретические гарантии не сломаются, если просамплировать второй раз, но это может быть необходимо в конечной ситуации, когда Q-функция изменится.\n* В Q-learning и Сарс используются разные подходы к выбору действий: Q-learning использует argmax, а Сарс может использовать soft-max с температурой.\n* Уравнение Беллмана в Q-learning и Сарс представляет собой последовательность ожиданий по состояниям и действиям.\n* Вместо оценки по одному сэмплу, можно брать ожидание по честной политике, суммируя значения функции с определенными весами.\n* Вопрос о том, какой алгоритм подходит к какой политике, зависит от конкретной задачи и требований к решению.",
  "processed_frame_127525.jpg": "* Окулежник не подходит для данной задачи.\n* Кулерник сойдется к оптимальному пути, который ближе к лифу, так как он учит курсу звездой.\n* SARS-CoV-2 более своего уровня.\n* SARS сойдется к безопасной политике, так как он учится с помощью эпсилон-жадной политики, которая ограничивает пространство политик и имеет ненулевую вероятность прыгнуть в лаву или с обрыва.\n* Кулерник не имеет таких рисков, поэтому может выучить оптимальную политику.\n* В данной задаче оптимальная награда равна минус 13, так как нужно минимум 13 шагов, чтобы добраться из точки С в терминальное состояние.\n* Кулерник сойдется к кратчайшему пути, так как он может восстановить оптимальную политику.\n* SARS имеет отвращение к риску, поэтому выбирает самый безопасный путь, чтобы минимизировать риски.\n* Взаимодействие с оптимизированной политикой может привести к парадоксу, так как есть вероятность прыгнуть с обрыва, особенно если проходить близко к нему.",
  "processed_frame_134100.jpg": "* Эффективные последствия не указаны в данном контексте.\n* Если не будем виноваты в отчаянии, не сможем выяснить, что получили от него."
}