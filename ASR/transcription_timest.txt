[Chunk start 0.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 25.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 50.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 75.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 100.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 125.00s]: [0.00-29.98] Продолжение следует...
[Chunk start 150.00s]: [0.00-3.00] Появился в этом случае, как правило, неизвестный.
[Chunk start 175.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 200.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 225.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 250.00s]: [0.00-29.44] Всем привет! [29.44-0.00]  [1.00-NA] Привет!
[Chunk start 275.00s]: [0.00-4.00] Все, что мы видим, это не только те, кто не понимает, а те, кто не понимает. [4.00-5.00] Это не только те, кто не понимает, а те, кто не понимает. [5.00-6.00] Это не только те, кто не понимает, а те, кто не понимает. [6.00-7.00] Это не только те, кто не понимает, а те, кто не понимает. [7.00-8.00] Это не только те, кто не понимает, а те, кто не понимает. [8.00-9.00] Это не только те, кто не понимает, а те, кто не понимает. [9.00-10.00] Это не только те, кто не понимает, а те, кто не понимает.
[Chunk start 300.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 325.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 350.00s]: [0.00-29.98] Подписывайтесь на наш канал.
[Chunk start 375.00s]: [0.00-28.90] Да, давайте начнем. [28.90-0.00]  [1.10-NA] это не так.
[Chunk start 400.00s]: [0.00-4.00] Да, давайте начнем. [4.00-0.00]  [26.00-NA] Появился в этом случае, как правило, неудобно.
[Chunk start 425.00s]: [0.00-11.16] нас сегодня ждет не то чтобы сильно большая поэтому скорее всего мы пройдем ее без перерыва думаю мы [11.16-17.04] успеем где-то за час максимум все это дело рассказать но при этом заключительная лекция [17.04-27.24] в блоке первом лекции связанных с ломками такими именно больше коры историями там про модальности [27.24-0.00]  [2.76-NA] про то, как их обучают, какие там способы используют.
[Chunk start 450.00s]: [0.00-4.84] историями там про модальности и про то, как их обучают, какие там способы используют. [5.34-10.66] И сегодня мы, по большей части, как раз поговорим про какие-то оставшиеся модальности, [10.72-16.42] которые не успели затронуть на предыдущей лекции, и заодно немного поговорим о тех методах, [16.52-22.42] которые там используют для того, чтобы вообще сервис на базе лаб какой-то свой сделать, [23.28-26.08] почему эти методы вдруг актуальны и зачем их используют, [26.08-0.00]  [3.92-NA] и подведем некоторые вообще в целом итоги того, что
[Chunk start 475.00s]: [0.00-6.10] зачем их используют и подведем некоторые вообще в целом итоге того что мы там изучили на первом [6.10-15.04] блоке и зачем это сидел проходили да на прошлой лекции разбрали по вижу модальность такая [15.04-23.04] достаточно большая интересная сегодня будем рассматривать с вами и код и аудио и начнем [23.04-29.82] с кода вот как модальность она в целом достаточно уникальная потому что ее даже выделил модальность [29.82-0.00]  [1.00-NA] с.
[Chunk start 500.00s]: [0.00-6.10] Код как модальность, она в целом достаточно уникальная, потому что ее даже в отдел модальности в целом не всегда выносят. [6.60-11.38] Однако код сам по себе имеет достаточно много особенностей. [11.52-20.82] Во-первых, он написан на привычном для любой лоломки тексте, так или иначе, но при этом, естественно, не представляет собой естественный язык. [22.06-25.06] Для чего вообще нужен код как модальность? [25.06-0.00]  [4.94-NA] Ну, во-первых, любой разработчик в целом может сделать это.
[Chunk start 525.00s]: [0.00-7.00] Ну, во-первых, любой разработчик в целом, который особенно разрабатывает подобные модали, [7.14-10.82] если он так или иначе хотел иметь какого-то универсального помощника, [10.90-14.34] который поможет достаточно быстро решать какие-то задачи, связанные с кодом, [14.44-16.30] которые возникают в целом практически всегда, [16.78-20.86] если вы разрабатываете или занимаетесь дата-сайенсом или что-то подобное. [21.24-28.26] И таких задач достаточно много, которые возникают у вас там из раза в раз в вашей жизни. [29.34-0.00]  [1.74-NA] Иногда, когда мы говорим о том, что мы не можем сделать
[Chunk start 550.00s]: [0.00-3.26] у вас там из раза в раз в вашей жизни. [4.26-7.32] Иногда нужно посмотреть, где находится какой-то баг, [8.42-11.50] причем этот баг нужно найти и потом еще и понять, [11.72-14.82] каким образом нам надо сделать так, чтобы этот баг не работал, [15.28-16.54] точнее, исправить этот баг. [17.34-21.62] Нам нужно найти что-то в коде, либо своем, либо чужом. [22.76-25.76] Code-to-code retrieval достаточно часто тоже история, [25.90-27.90] которая позволяет нам решать кодовую модальность. [29.26-0.00]  [2.10-NA] Ну, естественно, в этом случае мы должны улучшить
[Chunk start 575.00s]: [0.00-7.08] история, которая позволяет нам решать кодовую модальность. Естественно, самая главная вещь, [7.08-13.02] для чего кодовая модальность нужна, это генерация кода того или иного. Это может быть полностью как [13.02-19.44] рерайтинг кода, который написали, либо завершение кода, то есть какие-то универсальные помощники, [19.44-25.40] которые позволяют на основании тех данных, которых они обучились, завершить ту или иную вещь. Причем [25.40-0.00]  [4.60-NA] Потом кодовые сервисы обычно, они сочетаются с кодовыми сервисами.
[Chunk start 600.00s]: [0.00-10.92] Причем кодовые сервисы обычно, они сочетают в себе не просто какую-то одну модельку, так они в целом сочетают множество моделей. [10.92-11.88] Сейчас секундочку. [17.38-29.36] Они сочетают множество моделей, начиная от single line модели, это как отдельная модель, где требуется завершить только одну маленькую строчку кода, [29.36-0.00]  [0.64-NA] что при этом не было.
[Chunk start 625.00s]: [0.00-4.34] как отдельная модель, где требуется завершить только одну маленькую строчку кода, [4.68-11.24] при этом контекст у нее должен быть связан либо с одной строчкой кода, [11.32-15.76] либо как раз таки с кодом предыдущим каким-то, который был написан. [16.36-21.40] Это может быть мультилайн история, когда мы хотим несколько строчек кода сгенерить, [21.54-23.82] которые нам помогут выполнить ту или иную функцию. [24.18-27.30] Либо это должен быть какой-то ассистент, который понимает код [27.30-0.00]  [2.70-NA] и может провести с вами диалог, как раз-таки связанный
[Chunk start 650.00s]: [0.00-5.22] какой-то ассистент который понимает код и может провести с вами диалог как раз таки связаны с [5.22-13.18] тем как вы должны там либо код свой построить либо поговорить в целом о коде либо дать задачку [13.18-22.00] комнату кодовому ассистенту на там работу сингла и мультилай модели вот что касается данных для [22.00-0.00]  [6.76-NA] обучения здесь как раз таки всей кодовой модальности безумно сильно повезло потому что года
[Chunk start 675.00s]: [0.00-9.60] всей кодовой модальности безумно сильно повезло потому что кода очень много почти весь в так или [9.60-14.94] иначе представлены в интернете коде он полезный несмотря на достаточно большое количество [14.94-22.32] дубликатов в коде обычно все бенчмарки ой бен датасеты которые там являются при троеном для [22.32-0.00]  [7.56-NA] для подобных моделей, они редусят с помощью дедубликации кода данных.
[Chunk start 700.00s]: [0.00-8.52] редусит с помощью дедубликации кода данных буквально свой размер раза в два но при этом [8.52-18.78] все этих данных сильно больше чем на естественном языке на удивление и самый такой распространенный [18.78-26.90] к примеру там при тройного их дата сета для кода это стак 2 недавно вышедший там порядка [26.90-0.00]  [3.10-NA] Однако, если мы не будем воплотиться в эти проблемы, то мы не
[Chunk start 725.00s]: [0.00-8.12] вышедший. Там порядка 900 миллиардов токенов. Как вы помните, там в российском интернете нам в целом [8.12-14.40] такое даже сниться там не может на естественном языке, да даже на англоязычном на самом деле. [14.66-21.52] Не то чтобы там прям имелись такие какие-то датасеты, где столько много токенов было бы представлено. [21.52-0.00]  [8.64-NA] Весит, конечно, эта махина достаточно мощно, но даже на этом, на самом деле, разработчики не осознают, что это не так.
[Chunk start 750.00s]: [0.00-19.68] Но даже на этом, на самом деле, разработчики не останавливаются, потому что у кода в целом, как у отдельной модальности, есть такое свойство, что мы всегда можем проверить правильность кода, который так или иначе нам встретится как кусок на какую-то истинность. [20.00-0.00]  [10.32-NA] Раз мы можем проводить такие проверки, мы можем написать какое-то определенное задание на сервис, связанный с генерацией подобных данных.
[Chunk start 775.00s]: [0.00-5.12] определенные задания на сервисы, связанные с генерацией подобных данных, [5.64-12.22] и заасертить какую-то историю так, чтобы мы хотели, чтобы все асерты проходили по данным, [12.30-17.56] которые нам сгенерировали тайная или иная модель, и добавить такой синтетический код у нас в обучение. [18.18-25.90] Сейчас тяжело даже назвать толком долю синтетических данных внутри современных моделей, [25.90-29.82] связанных с модальностью кода, однако их достаточно много.
[Chunk start 800.00s]: [0.00-2.64] моделей, связанные с модальностью кода. [3.24-6.86] Однако их достаточно много, потому что любая такая синтетика, [7.02-9.12] она так или иначе достаточно качественная. [9.54-12.38] Причем есть несколько подходов, связанные с тем, [12.56-16.40] как подобные данные можно генерить. [17.42-21.14] Есть модели достаточно эффективные, вот их тут список представленный [21.14-23.56] для генерации кода конкретно. [23.56-29.44] Это как маленькие модели, так и в целом достаточно большие некоторые представлены, [29.50-0.00]  [0.54-NA] но они эффективно не могут быть внести в себя.
[Chunk start 825.00s]: [0.00-5.10] как маленькие модели так и в целом достаточно там большие некоторые представлены но они эффективно [5.10-10.02] справляются с задачей генерации кода причем генерации кода у них может быть трех типов [10.02-17.10] это либо какая-то салфетка история когда мы даем самой какой-то ломки которые нам будет этот код [17.10-22.68] генерировать для тех или иных задач какое-то задание она нам на основании этого задания [22.68-29.76] генерит что-то это может быть какой-то evolution instruction когда мы хотим дать какую-то проблему [29.76-0.00]  [1.00-NA] нам.
[Chunk start 850.00s]: [0.00-4.90] это evolution instruction, когда мы хотим дать какую-то проблему, [5.22-8.28] которая нам не кажется достаточно серьезной или сложной, [8.70-13.94] и мы хотим каким-то образом с помощью какой-то инструкции для ломки ее усложнить. [14.38-16.56] Причем это может быть достаточно интеративный процесс. [17.18-19.58] Мы еще поговорим о усложнении задач, связанных с кодом, [20.06-23.58] но это позволяет нам сильно увеличить сложность данных, [24.58-28.90] коли мы можем контролировать на самом деле сложность данных в обучении подобных модальностей. [29.32-0.00]  [1.08-NA] Это открывает нам новые возможности.
[Chunk start 875.00s]: [0.00-4.38] можем контролировать на самом деле сложность данных в обучении подобных модальностей это [4.38-12.20] открывает нам достаточно большой простор в плане там построение каких-то эффективных методов обучения [12.20-18.46] нашей ломки который будет связан с кодом можно использовать тот же сирикульным ленин который [18.46-22.94] дает достаточно большой эффективности страдает от того что как раз у нас недостаточно данных [22.94-0.00]  [2.16-3.38] которые разбиты по когортам сложности. [7.22-NA] Либо это может быть OS-инструкция, когда у нас есть...
[Chunk start 900.00s]: [0.00-9.88] либо это может быть о с instruction когда у нас есть какой-то сниппет кода и на базе такого сниппета [9.88-15.48] на может генерироваться целое множество проблем которые до этого не встречалось особо сильно [15.48-28.38] извините пожалуйста да сейчас буду вот которые доселе нам не встречалось либо на который у нас [28.38-0.00]  [1.62-NA] у нас там изначального решения.
[Chunk start 925.00s]: [0.00-6.16] селе нам не встречалось, либо на которой у нас там изначального решения не было, но при этом модель [6.16-16.28] хорошо умеет генерировать подобные примеры. В целом у нас достаточно хорошо развитые алгоритмы, [16.28-22.80] связанные с фильтрацией подобного, точнее какого-то плохого хода, который мы можем встретить в нашем [22.80-0.00]  [7.18-NA] при трене, ровно как и история, связанная с дедупликацией данных, которые...
[Chunk start 950.00s]: [0.00-7.56] как и истории связанные с дедупликацией данных которые нам позволяют активно как-то отсеять [7.56-15.94] нежелательные какие-то вещи в притрагивании, потому что мы умеем хорошо как раз таки некоторые вещи [15.94-21.06] заасерсить, некоторые вещи мы семантически можем выискивать в наших данных чтобы понять [21.06-29.76] насколько код этот был похож, потому что либо по выходам каких-то кусков кода мы можем проводить [29.76-0.00]  [0.20-NA] дик.
[Chunk start 975.00s]: [0.00-6.32] что либо по выходам на какого-то каких-то кусков кода мы можем проводить как раз таки и симуляре [6.32-13.88] десерт для того чтобы эффективно там где дублицировать данные ну и самый важный на самом [13.88-19.36] деле для в плане обучения кода является контроль лицензий если мы видим какой-то копирают лицензию [19.36-23.52] мы к сожалению такое обычно должны фильтровать и не использовать нашем обучении потому что чреват [23.52-0.00]  [6.48-NA] каким-то последствиям. С токенизацией тут тоже ничего нового нет. Определенную токенизацию
[Chunk start 1000.00s]: [0.00-2.64] С токенизацией тут тоже ничего нового нет. [3.84-7.38] Определенную токенизацию проводят по коду в рамках обучения модели. [7.62-11.34] Здесь ровно такие же проблемы, как у обычных LLAM. [11.48-13.54] Тут ничего в целом нового нет. [14.08-19.20] Каким-то образом дезаблицируют код, используют ровно те же техники, которые для естественного языка. [19.82-20.54] В целом все понятно. [21.82-24.22] Что можно поменять в обучении LLAM? [24.42-26.86] На самом деле можно не менять ровным счетом ничего. [26.86-0.00]  [2.00-NA] просто взяв большой пайл...
[Chunk start 1025.00s]: [0.00-6.98] можно не менять ровным счетом ничего, просто взяв большой пайл естественного языка, [7.08-12.68] для того чтобы у нас модель понимала инструкции, и взять тоже кучу данных, связанных с кодом, [13.68-18.58] все это дело вместе как-то обучить, подружить, и у нас появится неплохой такой помощник. [19.96-25.86] Но можно, понятное дело, взять какую-то готовую лампу, которая у нас обучена на естественном языке, [26.86-0.00]  [4.14-4.16] Разморозить несколько слоев, также обучить на коде, тоже проблема.
[Chunk start 1050.00s]: [0.00-5.82] естественном языке, разморозить несколько слоев, также обучить на коде, тоже проблем не возникнет, [5.98-12.38] можно добавить какую-то голову, можно заадаптить под какую-то нашу историю, которая нам интересна. [12.66-17.08] И все эти методы, они достаточно стандартны в целом для обучения простых LLM, [17.08-23.72] просто новых задач на естественном языке, и при этом они достаточно эффективны для кода, [23.88-28.56] как для модальности, и мы тут как раз-таки сейчас каких-то определенных проблем не встречаем, [28.66-0.00]  [1.44-NA] пообщениям, хотя хотели бы иметь более конкретные
[Chunk start 1075.00s]: [0.00-13.40] И мы тут как раз-таки сейчас каких-то определенных проблем не встречаем, хотя хотели бы иметь какие-то определенные структуры, архитектуры моделей, которые могут эффективно на нас работать с кодом, об этом еще поговорим. [13.40-0.00]  [16.62-NA] Но важной вещью также является возможность селф-импрува таких моделей, как отдельный просто процесс обучения на кормежке данных, собственно, для модельки, так или иначе, из-за того, что мы...
[Chunk start 1100.00s]: [0.00-7.02] данных собственно для модельки так или иначе из за того что мы можем как раз таки проводить этап [7.02-16.72] эволюция кода которые сгенерируют наша моделька мы можем давать фидбэк самой модели это такое у нас [16.72-25.08] получается бесплатный рель где модельку как раз таки можно достаточно эффективно обучить чтобы [25.08-0.00]  [4.92-NA] чтобы она лишилась каких-то своих ошибок, причем можно давать...
[Chunk start 1125.00s]: [0.00-6.48] чтобы она лишилась каких-то своих ошибок, причем можно давать ассерты к написанию [6.48-11.24] либо как вручную, так и какой-то другой кодовой лампе, [11.56-14.34] которая выигрывает сейчас всех на бенчмарках, [14.34-19.18] таким образом достаточно быстро сходясь в хорошем качестве для моделей. [21.28-27.54] У нас может быть много связанных рисков, связанных с обучением модели, [28.14-0.00]  [1.86-NA] потому что
[Chunk start 1150.00s]: [0.00-7.98] рисков, связанных с обучением модели, потому что зачастую в любых проскрапленных данных, даже в [7.98-14.62] стейке 2, встречаются какие-то куски кода, которые могут быть связаны с лицензией, а это нарушение [14.62-22.86] каких-то авторских прав, достаточно много всяких маловаров, то есть там вирусов, либо какой-то [22.86-0.00]  [7.14-NA] нехороший ход, который мог быть даже выложен в рамках просто
[Chunk start 1175.00s]: [0.00-8.10] ход который мог быть даже выложен в рамках просто ознакомительных внутри там гитхаба в качестве там [8.10-14.70] учебного материала и так далее которые так или иначе может привести к тому что там помощник [14.70-22.18] там разработчика на его же мощностях может запустить какой-то вирус такие истории наверное [22.18-0.00]  [2.88-3.56] есть, я точно не знаю, но риск точно имеется. [7.74-NA] Можно потерять какие-то или слить.
[Chunk start 1200.00s]: [0.00-9.66] Можно потерять какие-то или слить, по крайней мере, данные. В кусках кода нередко, как минимум, [9.66-18.36] находятся какие-то сниппеты или примеры, которые могут содержаться в комментариях. Тех или иных [18.36-24.80] персональных данных зачастую тоже может являться большой проблемой. Мы должны уметь фильтровать [24.80-0.00]  [5.18-NA] такие куски кода, плюс весь код, который так...
[Chunk start 1225.00s]: [0.00-7.62] куски кода, плюс весь код, который так или иначе пошел в обучение, не факт, что является очень [7.62-13.50] эффективным, и таким образом у нас появляется проблема, связанная с тем, что у нас в лампе [13.50-19.62] можно учиться на неэффективных кусках кода. Эта история, опять же, у нас должна фикситься либо [19.62-23.74] какой-то другой, более мощной лампой, которая будет проверять весь код, который у нас при [23.74-29.98] тренинге находится, но это дорого, либо изначально мы должны иметь требования к тем кускам кода, [29.98-0.00]  [1.00-2.00] Но в этом не все. В основном, это не все. [2.00-3.00] В основном, это не все. [3.00-4.00] В основном, это не все. [4.00-5.00] В основном, это не все. [5.00-6.00] В основном, это не все. [6.00-7.00] В основном, это не все. [7.00-8.00] В основном, это не все. [8.00-9.00] В основном, это не все. [9.00-10.00] В основном, это не все. [10.00-11.00] В основном, это не все. [11.00-12.00] В основном, это не все. [12.00-13.00] В основном, это не все. [13.00-14.00] В основном, это не все. [14.00-15.00] В основном, это не все. [15.00-16.00] В основном, это не все. [16.00-17.00] В основном, это не все. [17.00-18.00] В основном, это не все. [18.00-19.00] В основном, это не все. [19.00-20.00] В основном, это не все. [20.00-21.00] В основном, это не все. [21.00-22.00] В основном, это не все. [22.00-23.00] В основном, это не все. [23.00-24.00] В основном, это не все. [24.00-25.00] В основном, это не все.
[Chunk start 1250.00s]: [0.00-6.30] либо изначально мы должны иметь требования к тем кускам кода, которые мы должны будем рисовывать [6.30-16.14] в притрейн. Да, множество всего, короче, может быть в целом связано с рисками, начиная от таких [16.14-21.54] плохих, которые могут привести к нашим каким-то галлюцинациям или нежелательным потерям, заканчивая [21.54-27.96] какой-то легальной историей. Что касается бичмарков, из-за того, что мы эффективно умеем генерировать [27.96-0.00]  [2.06-NA] в целом куске кода, у нас тут проблем вообще нет.
[Chunk start 1275.00s]: [0.00-27.12] Из-за того, что мы эффективно умеем генерировать в целом куски кода, у нас тут проблем вообще никаких нет. Бенчмаркам завались. Единственное, что на текущий момент, наверное, плохо, то, что нет сложных бенчмарков, потому что на текущий момент не то чтобы все решения, несмотря на то, что у них кода достаточно много, не умеют в сложной задаче, связанной с кодингом. Об этом тоже обсудим. [27.12-0.00]  [2.88-NA] И, наверное, из всех, которые были в Казахстане, я думаю,
[Chunk start 1300.00s]: [0.00-7.34] кодинга в этом тоже обсудим и наверное из всех бичмарков даже которые сейчас здесь перечислены [7.34-14.04] и которые развиваются даже на текущий момент там самым оптимальным самым лучшим решением является [14.04-21.42] просто посмотреть как на битком пенча у нас какая у нас там метрика посмотреть может быть какие-то [21.42-27.76] более специфичные истории типа если мы хотим помочь когда центиста обучить 1дс тысячу если [27.76-0.00]  [2.22-3.22] Если мы хотим, чтобы у нас там много языков знало, то, конечно, мы можем. [3.22-4.22] Мы можем сделать это, но мы не можем сделать это, [4.22-5.22] потому что это не так просто. [5.22-6.22] Мы можем сделать это, но мы не можем сделать это, [6.22-7.22] потому что это не так просто. [7.22-8.22] Мы можем сделать это, но мы не можем сделать это, [8.22-9.22] потому что это не так просто. [9.22-10.22] Мы можем сделать это, но мы не можем сделать это, [10.22-11.22] потому что это не так просто. [11.22-12.22] Мы можем сделать это, но мы не можем сделать это, [12.22-13.22] потому что это не так просто. [13.22-14.22] Мы можем сделать это, но мы не можем сделать это, [14.22-15.22] потому что это не так просто. [15.22-16.22] Мы можем сделать это, но мы не можем сделать это, [16.22-17.22] потому что это не так просто. [17.22-18.22] Мы можем сделать это, но мы не можем сделать это, [18.22-19.22] потому что это не так просто. [19.22-20.22] Мы можем сделать это, но мы не можем сделать это, [20.22-21.22] потому что это не так просто. [21.22-22.22] Мы можем сделать это, но мы не можем сделать это,
[Chunk start 1325.00s]: [0.00-6.00] обучить тот на ds-1000 если мы хотим чтобы у нас там много языков знала то что-то еще проверить [6.00-12.86] типа мультилингах и уменовал ну да сконцентрироваться на каких-то локальных проверках и какой какой-то [12.86-19.24] общий бенчмарк вообще в целом среди всех моделей взять то за основу там биткотт бенч руки у нас [19.24-25.68] поросли на самом деле всех бенчмарков начиная с как раз таки human ивала который вышел достаточно [25.68-0.00]  [4.22-NA] давно, где-то четыре года назад, по-моему, его сделали.
[Chunk start 1350.00s]: [0.00-2.76] вышел достаточно давно, где-то четыре года назад, [2.76-9.52] по-моему, его сделали как раз таки open, ребята из openA. [9.52-13.40] Задача достаточно простая, у нас есть какой-то код, [13.40-17.44] чаще всего это просто функция, причем внутри функции описан [17.44-21.28] док-стрингом примерно, как это должно выглядеть, либо [21.28-25.94] данные куски кода других функций, может быть даже [25.94-28.62] законченных, либо внутри самой функции уже несколько [28.62-0.00]  [1.36-NA] строк реализовано.
[Chunk start 1375.00s]: [0.00-4.50] может быть даже законченных, либо внутри самой функции уже несколько строк реализована. [4.92-8.54] Задача бенчмарка достаточно простая — продолжить код. [9.88-13.92] По какому-то определенному контексту связаны там либо с этой функцией, [14.04-15.60] либо с несколькими функциями одновременно. [16.88-19.60] И на основании того кода, который, собственно, сгенерится, [20.06-23.70] строится метрика под названием там path, это что-то там. [25.24-0.00]  [6.32-NA] В зависимости от того, за какое количество денег
[Chunk start 1400.00s]: [0.00-6.42] В зависимости от того, за какое количество попыток нас устроит, [6.42-14.84] что наша какая-то ламка пройдет тот или иной тест. [15.94-21.44] Здесь мы хотим как раз-таки учитывать вариативность нашей ламки. [21.66-24.20] Желательно, конечно, смотреть на метрику Passed-1. [24.64-28.24] То есть нас интересует первая генерация, и она должна быть суперидеальной. [28.76-0.00]  [1.76-NA] но иногда смотрят
[Chunk start 1425.00s]: [0.00-6.42] интересует первая генерация и она должна быть супер идеальный но иногда смотрит и пасы т.н. обычно [6.42-16.02] пасы т.п. то есть мы в целом допускаем что часть там генерации может быть плохая но у нас интересует [16.02-24.66] хотя бы чтобы в один раз из пяти было хорошо вот понятное дело что там бичмарком сейчас особо [24.66-0.00]  [1.62-5.34] сильно не следят. Не так давно там сам Human Eval под названием Code Eval.
[Chunk start 1450.00s]: [0.00-8.04] следят не так давно там сам humanовал под названием кодовал был адаптирован для русского языка поэтому [8.04-15.18] сейчас в россии у нас за этим следят но сейчас бичмарк сам по себе устарел на нем выбиваются [15.18-20.36] какие-то уже невероятные значения плюсом бичмарк особо не обновляется то есть там как джипе тишка [20.36-27.18] точнее какая-то там агентура на джипе тишка я нам победила в этом бичмарке так там сейчас только [27.18-0.00]  [2.50-NA] как одни GPT и в целом на такой бенчмарк особо не смотрят.
[Chunk start 1475.00s]: [0.00-2.90] победила в этом бичмарке, так там сейчас только одни GPT, [3.00-4.70] в целом такой бичмарк особо не смотрят. [5.78-12.12] В отличие от бигкот-бенча, который является таким наследником [12.12-15.56] Humanvala, они об этом напрямую пишут в своей статье, [15.98-18.82] вот как раз-таки бигкот-бенч является достаточно интересным [18.82-23.88] и достаточно сложным бичмарком, причем есть сразу несколько версий, [23.88-28.42] как и просто фулл бигкот-бенч, так и хард бигкот-бенч. [28.96-0.00]  [1.58-NA] Метрики на нем присутствуют.
[Chunk start 1500.00s]: [0.00-8.10] full bitcode bench так и hard bitcode bench метрики на нем пока что даже у самых классных моделей они [8.10-13.26] не сильно большие это не может не радовать значит есть куда расти и значит задачи действительно [13.26-21.60] сложные суть такого бенча заключается в том что у нас опять же есть какая-то функция нам эту функцию [21.60-28.94] как-то закончить но при этом все наши докстринги они оформлены определенным образом есть какие-то [28.94-0.00]  [3.00-NA] параметры
[Chunk start 1525.00s]: [0.00-8.26] оформлены определенным образом, есть какие-то параметры, которые тоже важно и правильно как-то задать, [8.38-13.36] либо прочитать с другого контекста, к примеру, с наших импортов, что это за параметры вообще могут быть. [14.00-17.72] Нам надо что-то вернуть, какую-то ошибку, возможно, поднять. [18.18-24.52] У нас есть определенные, да, реквайрменты, мы можем немножко зафишотить нашу всю историю. [24.52-0.00]  [5.48-NA] Ну и, естественно, как-то это все дело проверить с какими-то...
[Chunk start 1550.00s]: [0.00-7.16] Ну и, естественно, как-то это все дело проверить какими-то как раз таки ассертами, которые мы делаем. [7.96-16.24] Причем все примеры, которые в биткоин-бенче так или иначе находятся, они верифицируются трехэтапно. [17.14-22.36] Изначально генерируются вообще все эти примеры с помощью каких-то методов, связанных с генерацией кода. [22.36-0.00]  [7.64-NA] Затем все это дело перепрогоняется как людьми, так и какими-то более мощными моделями.
[Chunk start 1575.00s]: [0.00-5.10] как людьми, так и какими-то более мощными моделями. [5.58-10.36] И затем еще раз дополнительная проверка с помощью каких-то экспертов [10.36-13.08] в области программирования, какие-то кросс-чеки. [14.66-17.74] Ну, так или иначе, да, под людьми все курировано, [17.84-20.06] так что там какие-то суперидеальные примеры. [20.06-23.24] Таких примеров может быть не очень много, 1140 всего, [23.76-25.80] однако они хорошо разбиты по доменам, [26.32-29.30] они задействуют так или иначе большинство библиотек, [29.50-0.00]  [1.00-2.00] которые, как мы знаем, не могут быть в состоянии. В этом случае мы должны решить, как мы можем помочь. [2.00-3.00] В этом случае мы должны помочь. [3.00-4.00] В этом случае мы должны помочь. [4.00-5.00] В этом случае мы должны помочь. [5.00-6.00] В этом случае мы должны помочь. [6.00-7.00] В этом случае мы должны помочь. [7.00-8.00] В этом случае мы должны помочь. [8.00-9.00] В этом случае мы должны помочь. [9.00-10.00] В этом случае мы должны помочь. [10.00-11.00] В этом случае мы должны помочь. [11.00-12.00] В этом случае мы должны помочь. [12.00-13.00] В этом случае мы должны помочь. [13.00-14.00] В этом случае мы должны помочь. [14.00-15.00] В этом случае мы должны помочь. [15.00-16.00] В этом случае мы должны помочь. [16.00-17.00] В этом случае мы должны помочь. [17.00-18.00] В этом случае мы должны помочь. [18.00-19.00] В этом случае мы должны помочь. [19.00-20.00] В этом случае мы должны помочь. [20.00-21.00] В этом случае мы должны помочь. [21.00-22.00] В этом случае мы должны помочь. [22.00-23.00] В этом случае мы должны помочь. [23.00-24.00] В этом случае мы должны помочь.
[Chunk start 1600.00s]: [0.00-6.12] разбиты по доменам, они задействуют так или иначе большинство библиотек, которые завязаны на [6.12-13.92] измерения, ну, завязаны на деятельность, там, разработчиков, и с чем они чаще всего встречаются. [13.92-22.56] Однако, как вы могли заметить, все это дело только на питоне на текущий момент. По-моему, да. [23.38-0.00]  [3.32-4.22] Да, если я не ошибаюсь, это пока что только питон. [7.60-NA] Другие языки, сейчас даже посмотрим, по-моему, он где-то...
[Chunk start 1625.00s]: [0.00-11.34] что только питон другие языки и даже посмотрим по моему где-то тут есть да да он только на питоне [11.34-18.18] это огорчает потому что зачастую мы хотим от помощника разработчика чтобы она мне только [18.18-23.02] на питоне помогал однако на текущий момент это действительно сам лучший бенчмарк хотя [23.02-0.00]  [6.94-NA] Хотя трехэтапный ступенчатый какой-то анализ тех тасок, он дорогой.
[Chunk start 1650.00s]: [0.00-8.76] ступенчатый какой-то анализ тех таз и кону дорогой и только адаптируется на текущий момент под какие-то [8.76-17.70] другие языки ds1000 тоже очень похожий пример связанный с уже непосредственно работой там [17.70-27.38] датсайентистов у нас тоже есть какое-то описание возможно дата-фреймов там в пандасе и задачи и мы [27.38-0.00]  [2.40-NA] мы должны на основании как раз-таки этой задачи заняться.
[Chunk start 1675.00s]: [0.00-10.52] И мы должны на основании как раз-таки этой задачи заняться генерацией кода на основании какого-то кодового контекста и привести какое-то решение. [10.84-14.32] И в дальнейшем его как-то засертить, что-то провести. [16.34-18.12] Останавливаться особо сильно на нем не буду. [18.46-25.16] В нем достаточно представимость данных по всем библиотекам, так или иначе, которые используют дата-сиентисты. [25.62-0.00]  [4.86-NA] Ну и, да, современные, собственно, модели, которые там побивают все это, естественно, GPT-чемпионы.
[Chunk start 1700.00s]: [0.00-7.30] Ну и современные модели, которые побивают все это, естественно, GPT-Core, Cloud, DeepSeq. [8.10-9.78] В целом ничего удивительного тут нет. [11.56-15.06] Также хорошим примером бенчмарка является SVE-бенч. [15.36-22.62] Он тоже зачастую очень часто используется при скорингах всех моделей, которые так или иначе связаны с кодом. [23.10-0.00]  [7.26-NA] бенч является достаточно уникальным интересным ввиду того что он решает задачу может ли какие-то
[Chunk start 1725.00s]: [0.00-8.76] интересным ввиду того что он решает задачу может ли какие-то ломки достаточно хорошо справляться [8.76-19.18] с закрытием и шли с на гитхабе так у нас есть какие-то примеры и шьюз пропоршн и с самими [19.18-28.02] там разработчиками этого бенчмарка там порядка 90 тысяч пиаров было проанализировано так или [28.02-0.00]  [1.96-NA] иначе, и что с ним были все простые дела.
[Chunk start 1750.00s]: [0.00-3.50] 90 тысяч пиаров было проанализировано так или иначе, [3.94-5.50] иши с ним были все просмотрены, [6.02-8.68] иши обычно идут с какими-то кусками кода, [9.36-13.48] и мы должны на основании как раз-таки всех данных, [13.48-16.26] которые приведены в том или ином иши, [16.68-21.10] сгенерировать либо код, который сможет помочь решить это ишью, [21.46-23.60] и потом, собственно, сгенерировать тесты, [23.70-25.60] ну, точнее, да, провести тесты под него, [26.08-0.00]  [4.40-NA] потому что в этом бенчмарке они заранее все известны.
[Chunk start 1775.00s]: [0.00-9.84] под него потому что на в этом бенчмарке они заранее все известны ну вот да ты и мы должны сгенерировать [9.84-19.40] решение это конец вот на этом бенчмарке тоже не особо большие какие-то результаты и достаточно [19.40-27.48] затюнены как будто бы только подход модели но выбиваются вперед то есть там не встретишь просто [27.48-0.00]  [2.50-NA] как там, не знаю, в DS1000 с GPT-чем-то.
[Chunk start 1800.00s]: [0.00-6.54] то есть там не встретишь просто как там не знаю в дэс тысячи джипе течет клауд нет тут встреч [6.54-15.66] конкретно какие-то вещи типа у дикта которые как-то запромтировали клад там и свои агент [15.66-23.20] опять же на клауде что-то связанное с чепики я не знаю название этого приложения так или иначе вот [23.20-27.78] то есть созданы там людьми специальный промпт какой-то какая-то инструкция которая позволяет [27.78-0.00]  [2.18-NA] хорошо с этим печем как-то справляются.
[Chunk start 1825.00s]: [0.00-1.96] промпт, какая-то инструкция, [2.16-3.94] которая позволяет хорошо [3.94-5.48] с этим бенчмарком как-то справляться. [6.04-7.84] Можно профильтровать внутри самого [7.84-9.58] бенчмарка, найти там [9.58-11.82] непосредственно просто какие-то модельки. [12.90-14.00] Но хотел бы показать [14.00-15.94] именно вот это, плюс метрики не особо сильно [15.94-17.94] там, опять же, большие, что говорит [17.94-19.72] о сложности такого бенчмарка. [20.08-21.98] Но этот бенчмарк крайне полезен, потому что [21.98-24.06] мы хотим видеть в дальнейшем от наших [24.06-25.76] кодовых помощников [25.76-26.60] именно [26.60-29.86] такой пример при использовании.
[Chunk start 1850.00s]: [0.00-4.86] помощников именно такой пример использования. [7.50-10.02] Да, на бичмарках по коду все. [12.12-19.10] Мы хотим в целом в будущем от подобных помощников добиться как раз-таки генерацию сложного кода. [19.42-22.12] Мультилайн это в целом уже достаточно сложная история. [22.54-26.70] Это не просто закончить предложение, это еще и как-то продолжить его, [26.70-0.00]  [3.30-NA] написать несколько строчек кода, но мы хотим, чтобы такие помошники...
[Chunk start 1875.00s]: [0.00-3.26] как-то продолжить его, написать несколько строчек кода, [3.42-5.82] но мы хотим, чтобы такие помощники думали куда дальше, [7.22-12.18] возможно, написали как и целый скрипт, который поможет решить проблему внутри какого-то кода, [12.52-15.28] либо как-то оптимизирует целую библиотеку, [16.30-20.26] либо вообще сгенерирует целое репо под какую-то задачу, [21.62-24.46] которую можно будет из коробки запускать. [24.46-29.64] Пока таких решений, к сожалению, на рынке либо крайне мало, и они неэффективны, [29.82-0.00]  [0.36-NA] акта деятельности.
[Chunk start 1900.00s]: [0.00-6.14] таких решений, к сожалению, на рынке либо крайне малы, и они неэффективны, либо вообще в целом нет. [7.30-10.94] Мы хотим изобрести, естественно, специальную архитектуру, адаптированную под код, [11.22-15.68] ровно такую же, как мы это видели, к примеру, в модальности по видео, [15.84-21.28] когда мы делали какой-то проектор на токены, естественно, нового языка, [21.56-26.36] тот же проектор, к примеру, для кода, в целом, возможно, был бы хорошей идеей, [26.36-28.66] я таких исследований еще не видел, не находил. [29.68-0.00]  [1.00-2.00] А по поводу того, что мы не можем с этим разоружиться, мы должны решать это. [2.00-3.00] Мы должны решать это. [3.00-4.00] Мы должны решать это. [4.00-5.00] Мы должны решать это. [5.00-6.00] Мы должны решать это. [6.00-7.00] Мы должны решать это. [7.00-8.00] Мы должны решать это. [8.00-9.00] Мы должны решать это. [9.00-10.00] Мы должны решать это. [10.00-11.00] Мы должны решать это. [11.00-12.00] Мы должны решать это. [12.00-13.00] Мы должны решать это. [13.00-14.00] Мы должны решать это. [14.00-15.00] Мы должны решать это. [15.00-16.00] Мы должны решать это. [16.00-17.00] Мы должны решать это. [17.00-18.00] Мы должны решать это. [18.00-19.00] Мы должны решать это. [19.00-20.00] Мы должны решать это. [20.00-21.00] Мы должны решать это. [21.00-22.00] Мы должны решать это. [22.00-23.00] Мы должны решать это. [23.00-24.00] Мы должны решать это. [24.00-25.00] Мы должны решать это. [25.00-26.00] Мы должны решать это. [26.00-27.00] Мы должны решать это. [27.00-28.00] Мы должны решать это. [28.00-29.00] Мы должны решать это.
[Chunk start 1925.00s]: [0.00-9.00] был хорошей идеей, я таких исследований еще не видел и не находил. Возможно, мы хотим более умный [9.00-17.74] способ работы с данными, иметь связанных с кодом, как-то лучше использовать какой-то фильтринг, [17.74-27.30] понимать, какой код у нас может нести действительно большую ценность для обучения модели. Мы хотим [27.30-0.00]  [2.70-NA] будем изобрести крутые бенчмарки, связанные с
[Chunk start 1950.00s]: [0.00-1.90] модели. [1.90-5.24] Мы хотим изобрести крутые бенчмарки, связанные с [5.24-9.16] код-геном, потому что все текущие бенчмарки, они так [9.16-13.64] или иначе связаны на достаточно простые проблемы для текущих [13.64-16.16] помощников. [16.16-19.74] Помимо колд-бенча, там, SWE-бенча, мало что суперсложного [19.74-21.88] можно найти. [21.88-25.00] Мы хотим поддержку иметь не только там Python как языка, [25.00-29.42] мы хотим вообще в целом все языки, в том числе и низкоуровневые, [29.42-0.00]  [0.58-NA] представляют.
[Chunk start 1975.00s]: [0.00-3.46] хотим вообще в целом все языки, в том числе низкоуровневые, [3.86-9.26] которые представляются в текущем там ландшафте, [9.72-13.96] очень редко, очень мало, но так или иначе, [14.08-17.34] как будто бы лампка умеет ходить в грамматике, [17.34-22.50] поэтому why not, почему бы не обучить там какие-то сложные, тяжелые языки, [22.62-25.22] почему бы не синтезировать там данные, [25.64-0.00]  [4.66-NA] которые могут быть для подобного иската.
[Chunk start 2000.00s]: [0.00-9.36] которые могут быть для подобного иска релевантны. Мы хотим, естественно, иметь поддержку continuous [9.36-16.66] learning. Тут это супер сильно важно. Напоминаю, что такое это, когда мы продолжаем обучаться, [16.66-23.82] даже после тренинга, какой-то нашей очередной модельке. Мы хотим понимать, какие кодовые [23.82-28.74] фреймворки сейчас актуальны, что нам нужно сейчас обязательно, чтобы наша модель умела, [28.74-0.00]  [1.26-NA] было, чтобы она была в порядке.
[Chunk start 2025.00s]: [0.00-6.30] что нам нужно сейчас обязательно чтобы наша модель умела чтобы она подстраивалась под текущие какие-то [6.30-16.18] обстоятельства к примеру там не знаю выходит новая там статья про какую-то там оптимизацию и мы хотим [16.18-21.24] естественно чтобы есть она революционная была то наш там универсальный помощник поддерживал подобный [21.24-29.94] алгоритм которые были бы реализованы в этом новом алгоритме ну да естественно хотим решить проблему [29.94-0.00]  [1.00-2.00] Благодарю вас за внимание. До новых встреч. [2.00-3.00] До новых встреч. [3.00-4.00] До новых встреч. [4.00-5.00] До новых встреч. [5.00-6.00] До новых встреч. [6.00-7.00] До новых встреч. [7.00-8.00] До новых встреч. [8.00-9.00] До новых встреч. [9.00-10.00] До новых встреч. [10.00-11.00] До новых встреч. [11.00-12.00] До новых встреч. [12.00-13.00] До новых встреч. [13.00-14.00] До новых встреч. [14.00-15.00] До новых встреч. [15.00-16.00] До новых встреч. [16.00-17.00] До новых встреч. [17.00-18.00] До новых встреч. [18.00-19.00] До новых встреч. [19.00-20.00] До новых встреч. [20.00-21.00] До новых встреч. [21.00-22.00] До новых встреч. [22.00-23.00] До новых встреч. [23.00-24.00] До новых встреч. [24.00-25.00] До новых встреч. [25.00-26.00] До новых встреч. [26.00-27.00] До новых встреч. [27.00-28.00] До новых встреч. [28.00-29.00] До новых встреч.
[Chunk start 2050.00s]: [0.00-5.50] Ну да, естественно, хотим решить проблемы, связанные [5.50-6.50] с безопасностью. [6.50-11.66] На каком-то уровне такие ломки должны понимать, [11.66-14.52] где у нас находится там опасный код, где у нас находится [14.52-19.24] неопасный код, причем как-то предупреждать об этом пользователя, [19.24-21.86] предупреждать пользователь о возможных каких-то авторских [21.86-27.24] правах, либо об утечке данных, и как-то нивелировать это. [27.24-0.00]  [2.96-NA] Но это тоже будущий вызов, пока, естественно, не будет.
[Chunk start 2075.00s]: [0.00-5.92] и как-то нивелировать это, это тоже будущий вызов, пока естественное решение особо нет. [7.44-9.34] Теперь поговорим про аудио. [10.02-12.36] Олег, а можно вопрос про кодовую модальность? [13.06-17.54] У моделей будет такое же свойство, например, если, ну вот, когда обучают LLM, [17.74-21.38] большая часть данных на английском языке, добавляют немного языков, [21.38-26.60] например, русский, китайский, и модель начинает с меньшим количеством данных, [26.70-28.00] понимает уже другие языки. [28.18-0.00]  [2.00-NA] И такое же свойство есть на...
[Chunk start 2100.00s]: [0.00-5.44] с меньшим количеством данных понимает уже другие языки такое же свойство есть на там например весь [5.44-12.00] код на питоне практически весь да и сарказм то есть есть мы к примеру какую-то питоновскую чисто модель [12.00-20.34] засунем код связанный там все плюс плюс нам возможно выдастся код носит плюс плюс но возможно [20.34-27.50] он будет нерабочий объясняю вообще почему такое явление вдруг может произойти на самом деле все [27.50-0.00]  [2.22-NA] Все данные, которые вот тут были рассмотрены,
[Chunk start 2125.00s]: [0.00-7.50] на самом деле все данные которые вот тут были рассмотрены на самом первом слайде они так или [7.50-15.70] иначе используются в притринах но при этом почему у них такой большой размер не всегда это код во [15.70-22.36] время там процесса дедубликации порой мы фильтры все комментарии однако чаще всего даже при [22.36-29.48] разработке там гига кода нашего российского мы эти комментарии все оставляем комментариях у нас [29.48-0.00]  [1.00-NA] находится под основной целью.
[Chunk start 2150.00s]: [0.00-6.30] российского мы эти комментарии все оставляем комментариях у нас находится куча интересной [6.30-10.38] информации которая связана во первых с другими какими-то языками программирования зачастую [10.38-15.42] потому что вставляют примеру докстринговый сниппет там кода на си плюс плюс перепишет [15.42-22.60] на питон что не подобное так и в целом комментарии несут очень много технической информации которые [22.60-28.62] содержат как русский язык то китайский язык как такой английский какие-то такие большие [28.62-0.00]  [1.38-NA] вашей представленности.
[Chunk start 2175.00s]: [0.00-2.70] китайский язык, как такой английский, [2.84-4.56] какие-то такие большие представленности. [5.94-10.66] Поэтому представленность языков, она так или иначе есть, [10.86-13.20] даже, блин, вот жалко ее не привел на стеки, [14.62-19.04] она достаточно большая, и все в целом о лампе про нее шарят, [19.04-22.14] но в основном, конечно, питон, но несмотря на то, [22.24-26.00] что представимость, к примеру, какого-нибудь Котлина, [26.44-0.00]  [4.00-NA] Может быть, там 6% от всего, от всех данных для обучения.
[Chunk start 2200.00s]: [0.00-5.22] Kotlin, может быть, там 6% от всего, от всех данных для обучения, [5.42-11.86] но все равно у нас ломки достаточно хорошо запоминают подобные данные [11.86-16.88] и могут спокойно потом генерировать данные, там, с Kotlin связанные, [17.20-19.80] даже несмотря на то, что там достаточно мало было примеров. [20.18-23.68] Единственное, что проблема будет это хорошо как-то грамотно протестировать. [23.68-27.84] У нас из всех бенчмарков, которые вообще в целом есть там для кода, [27.94-0.00]  [2.14-NA] у нас только парочка связанных
[Chunk start 2225.00s]: [0.00-6.90] бенчмарков, которые вообще в целом есть там для кода, у нас только парочка связанных с мультилингл [6.90-14.66] историей. Так у нас есть только мультилингл human-to-vile, но human-to-vile как бенчмарк, он не слишком [14.66-21.48] сложный, поэтому нам тяжело будет сказать, насколько хорошо там наша моделька справляется с этим. [21.48-28.74] Вот, но если мы там разработаем какие-то определенные наши тесты, которые там нам [28.74-0.00]  [1.26-NA] нужны для проверки.
[Chunk start 2250.00s]: [0.00-7.98] Но если мы там разработаем какие-то определенные наши тесты, которые нам нужны для проверки эффективности работы на том или ином языке, [8.46-10.00] нам, возможно, этого будет достаточно. [11.20-15.20] Нужно ждать появления каких-то новых бенчмарков, которые нам помогут рассказать, [15.20-21.58] типа, мультилингву, бигконд-бенч, к примеру, о качествах подобных моделей, [22.20-26.94] которые будут ориентированы не только на Python, и было бы вообще суперславно. [27.16-0.00]  [3.08-NA] А так, в целом, все современные там помощники, они...
[Chunk start 2275.00s]: [0.00-4.42] И было бы вообще суперславно, а так в целом все современные помощники, [4.78-8.50] они так или иначе поддерживают практически все языки программирования, [9.08-11.86] как и любая в целом LLM. [12.18-16.88] Какую ни спроси, все в целом на каких-то даже около мертвых языках умеют говорить. [17.38-19.52] Но, понятное дело, с не самым большим качеством. [22.20-23.80] Да, вот как-то развернуто ответил. [24.38-25.38] Надеюсь, ответил. [25.76-26.42] Да, спасибо. [27.24-27.88] Да, супер. [27.88-0.00]  [2.12-NA] Да, давайте поговорим про то, что мы сейчас говорим.
[Chunk start 2300.00s]: [0.00-10.56] ответил да спасибо да супер да давайте говорим про модальность аудио честно это как одновременно [10.56-18.56] достаточно простая тема так и крольче нора ввиду того что зачастую мы от аудио моделек не хотим [18.56-25.36] добиваться того что мы просто положили какое-то текстовое описание наш какой-то аудио input и [25.36-0.00]  [3.38-3.80] И нам на выходе получился просто какой-то текст-инпут. [4.80-NA] Зачастую мы хотим...
[Chunk start 2325.00s]: [0.00-6.00] и нам на выходе получился просто какой-то текст импут зачастую мы хотим вот самую последнюю [6.00-13.26] историю которые реализованы на картинке это аудио input и текст prompting и на выходе мы получаем [13.26-19.20] зачастую нам даже текст аут тут не нужен мы хотим тоже получить аудио но про это рассказывать можно [19.20-25.50] очень долго там много подходов по большей части сегодня среды . просто на аудио как модальности [25.50-0.00]  [2.76-4.50] То есть мы добавляем, к примеру, какое-то аудио на вход, мы добавляем текст и получаем тематический эффект.
[Chunk start 2350.00s]: [0.00-4.98] модальности то есть мы добавляем примеру какой-то аудио на вход мы добавляем текст и получаем там [4.98-14.16] текста на выход добавление там в кодиров несет определенные и какие-то добавочные применения [14.16-20.46] похожих модальности к примеру мы можем генерировать музыку достаточно эффективно причем неплохо [20.46-0.00]  [8.98-NA] делают это современные модели но в основном мы хотим про суммаризировать какое-то видео
[Chunk start 2375.00s]: [0.00-8.34] мы хотим просуммаризировать какое-то видео к примеру на ютюбе по аудио транскрипции здесь [8.34-17.14] как раз нам помогают там спич плюс текст это текст истории вот что там нужно нам как-то поменять но [17.14-22.16] на самом деле я не стал сильно растягивать историю связан с этой модельностью ровно почему потому [22.16-29.32] что она сильно не отличается от вижен истории от слова совсем мы единственное что подменяем это [29.32-0.00]  [0.68-NA] то.
[Chunk start 2400.00s]: [0.00-8.10] от вижен истории от слова совсем мы единственное что подменяем это какой-то visual энкодер на [8.10-17.04] аудио энкодер и в целом все готово у нас есть одна небольшая проблема она знакома тем кто занимался [17.04-26.10] там как раз таки аудио моделями у нас у токенов аудио токенов сильно выше значение беден гав чем [26.10-0.00]  [3.90-4.90] чем у текстовых токенов, это связано там со многими проблемами.
[Chunk start 2425.00s]: [0.00-7.20] беден гав чем у текстовых токенов это связано там со многими причинами так или иначе и для [7.20-13.80] того чтобы нам когда мы делаем нашу там прожектор который нам будет это все вот одно как раз это [13.80-21.88] фаза ванная там пространство пихать нам необходимо и токи нормализовать и в целом но нормализация [21.88-29.10] это там не то чтобы какой-то супер интересный процесс про это на и можно найти тысячи тысяч [29.10-0.00]  [0.88-NA] одной из основных проблем.
[Chunk start 2450.00s]: [0.00-6.66] суперинтересный процесс, про это можно найти тысячи и тысячи, одну реализацию и статью в [6.66-13.86] интернете, поэтому тут тоже говорить об этом сильно не будут. Мы достаточно эффективно умеем это делать, [14.14-22.12] единственное, что об этом нужно знать, что там как раз-таки в нашем, когда мы подаем все в фьюзированный [22.12-28.10] там embedding space, у нас могут быть разные значения там у аудиотокенов и текст-токенов, а желательно, [28.10-0.00]  [1.56-NA] чтобы они были равно распределены.
[Chunk start 2475.00s]: [0.00-4.52] значения там у аудио токенов и текст токенов желательно чтобы они были равно распределены [4.52-12.30] потому что это все-таки должно слиться в какую-то одну интер лифт информацию когда мы должны [12.30-17.88] учитывать и то и другое одновременно и правильно так чтобы у нас там не было переобучение какой-то [17.88-27.56] из вот этих двух историй в плане бенчмаркирования люди сделали все очень просто на самом деле мы [27.56-0.00]  [2.44-NA] Мы проверяем любую модель, которая так или иначе сотрудничает.
[Chunk start 2500.00s]: [0.00-5.70] очень просто на самом деле мы проверяем любую модель который так или иначе затрагивает у нас [5.70-16.00] аудио input как обычно текстовую модель аудио input порой тестируем отдельно просто за инструктив [16.00-25.78] модель на задачу автоматик спички к внешне то есть у нас есть какая-то голосовое там сообщение [25.78-0.00]  [4.22-NA] Который мы подаем модель, мы промкнем ее, чтобы она
[Chunk start 2525.00s]: [0.00-7.26] сообщение, которым мы подаем модель, мы промтим ее, чтобы она написала нам транскрипцию этого звука, [7.26-15.36] модель выдает транскрипцию звука, и мы меряем по всем тем же как раз таки методам, которые современные [15.36-22.44] люди там меряют, automatic speech recognition сервиса. Основной метрикой во всех этих сервисах [22.44-0.00]  [7.50-NA] является там борд р рейд есть модификации там чартер р рейд есть сентенсер рейд и так далее так
[Chunk start 2550.00s]: [0.00-8.12] модификации, там, character rate, есть sentence rate и так далее, и так далее, но в основном вер, вер мерится [8.12-14.86] просто как количество замен, количество вставок и количество удаления тех или иных символов на [14.86-20.64] общее число символов в оригинальном как раз таки сообщении, то есть у нас есть, к примеру, вот [20.64-28.48] оригинал, у нас слово, и дата, и транскрипция, у нас слово quick поменялось на brown, а точнее [28.48-0.00]  [1.50-NA] Квик вообще убралось, осталось только выключить.
[Chunk start 2575.00s]: [0.00-6.64] quick поменялось на браун, а точнее quick вообще убралось, осталось только браун, и lazy dog там [6.64-15.98] добавилось. Пример какое-то слово могло там неправильно как-то транскрибироваться, да, и мы посчитаем [15.98-21.82] количество вот этих вставок там удалений и замен, разделим на общее число слов в оригинальном [21.82-28.66] предложении, получим значение вверх. У современных моделей оно очень хорошее, порядка трех процентов [28.66-0.00]  [1.34-NA] Он.
[Chunk start 2600.00s]: [0.00-10.60] хорошая порядка трех процентов то есть в целом очень редкие какие-то ошибки и зачастую мы никак [10.60-18.54] не хотим учитывать аудио составляющего таких моделей потому что ошибка там маленькая это [18.54-23.52] не картиночная история где ошибки могут быть достаточно большие и там вообще нет какой-то [23.52-0.00]  [0.92-2.92] определенной истины. [6.48-NA] Для аудиоданных в аудиоинкодерах в любом случае,
[Chunk start 2625.00s]: [0.00-6.10] Для аудио данных в аудиоэнкодерах в любом случае решил привести, [6.28-9.36] и конкретно для российского рынка, для нероссийского рынка, [9.36-12.76] можно найти где угодно и в большем количестве часов записи. [13.44-18.40] Основным височником данных является OpenCT на русском. [18.66-25.36] Там порядка 20 тысяч часов записи продиктованного текста в абсолютно различных доменах. [26.34-0.00]  [3.18-NA] переменах, это голос от Сбера.
[Chunk start 2650.00s]: [0.00-10.08] доменах это голос от сбера тоже очень очень много часов порядка 18 тысяч транскрибации [10.08-16.68] происходит обычно с каких-то радио эфиров потому что очень хороший источник данных для нас там [16.68-23.40] постоянно люди говорят поэтому давайте запишем большое количество там эфиров заставим людей там [23.40-0.00]  [6.54-NA] все это транскрибировать и потом на этом сидели обучимся вам public спичей youtube
[Chunk start 2675.00s]: [0.00-2.04] и потом на этом все деле обучимся. [2.58-3.96] Там паблик спичей, [4.30-5.24] ютуба, [6.32-7.92] аудиокниг, звонков [7.92-10.10] и прочие какие-то истории. [10.60-12.00] И есть еще небольшой [12.00-12.88] LibriSpeech [12.88-16.06] на 98 часов записи, [16.06-18.16] но его зачастую используют как какую-то [18.16-20.38] тестовую выборку для проверки [20.38-22.30] навыков. Как раз просто посчитать веру. [23.20-24.24] Единственным исключением, [24.34-26.00] наверное, из правил, больше [26.00-28.00] бенчей вы не найдете, наш любимый [28.00-0.00]  [1.98-NA] знакомый Биг Бенч, только не был в состоянии.
[Chunk start 2700.00s]: [0.00-8.44] больше бенча не найдете наш любимый и знакомый big bench только теперь с приставкой аудио тоже [8.44-13.98] тысячи там аудио каких-то вопросов мы хотим посмотреть на как раз таки какой-то спичи [13.98-21.08] лиза нинг связать аудио наше сообщение с сообщением на текстовом языке что-то померить но [21.08-27.38] больших отличий там от любого big bench натуральном языке на самом деле нет да тот же big bench только [27.38-0.00]  [2.62-NA] только вместе с голосовыми сообщениями.
[Chunk start 2725.00s]: [0.00-5.70] деле нет на тот же big bench только вместе с голосовыми сообщениями поэтому он достаточно [5.70-11.22] сильно там резаный но при этом учитывается несколько задач как тексту текст так спичку [11.22-18.90] спич тексту спички и спичку текст для моделей потому что там есть такая вот модификацию там [18.90-25.40] у этого бенчмарка так или иначе до закончили на самом деле с модальностями проуди больше [25.40-0.00]  [2.86-3.46] что углубляться не будем, про код рассказали про Vision тоже.
[Chunk start 2750.00s]: [0.00-9.96] больше углубляться не будем по прокат рассказали про vision тоже да теперь кратко давайте расскажем [9.96-16.30] как вообще в целом занимаются тем что собирают сервис на базе л.м. моделька у нас готово и есть [16.30-22.50] теперь нам необходимо ее обернуть в какой-то сервис который действительно нашем железе достаточно [22.50-28.18] эффективно работал причем мы предъявляем сразу несколько там требований к нашему сервису естественно [28.18-0.00]  [0.88-NA] он должен быть быстрый.
[Chunk start 2775.00s]: [0.00-2.78] сразу несколько там требований к нашему сервису. [2.90-4.08] Естественно, он должен быть быстрый. [5.68-7.90] Медленные сервисы нас не особо сильно интересуют. [8.02-9.06] Дожидаясь там ответа год, [9.30-13.52] любое физическое лицо, которое пользуется твоим сервисом, [13.94-14.68] просто уйдет. [15.48-19.22] У этого сервиса должна быть достаточно большая пропускная способность. [20.08-24.58] То есть мы хотим, чтобы наша скорость не сильно страдала [24.58-28.46] при огромном потоке пользователей [28.46-0.00]  [1.46-NA] с своими запросами.
[Chunk start 2800.00s]: [0.00-6.22] огромном потоке пользователей с своими запросами в наш сервис. [6.22-10.62] Естественно, фиар-сервис должен функционировать, [10.62-13.02] функционировать неправильно, без нарушения функциональности, [13.02-16.08] и он должен быть стабильный во времени, чтобы никогда не падал и так далее. [16.08-22.42] Но наши все основные требования как раз таки связаны со скоростью и с рутбутом. [23.34-28.90] Объясняю почему. Зачастую сервисы вообще делятся на два типа. [28.90-0.00]  [1.00-2.00] И это тут признается в том, что мы должны поддержать и поддержать, и поддержать, и поддержать, и поддержать, [2.00-3.00] и поддержать, и поддержать, и поддержать, и поддержать, [3.00-4.00] и поддержать, и поддержать, и поддержать, и поддержать, [4.00-5.00] и поддержать, и поддержать, и поддержать, и поддержать, [5.00-6.00] и поддержать, и поддержать, и поддержать, и поддержать, [6.00-7.00] и поддержать, и поддержать, и поддержать, и поддержать, [7.00-8.00] и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, [8.00-9.00] и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, [9.00-10.00] и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, [10.00-11.00] и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, [11.00-12.00] и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, и поддержать, и
[Chunk start 2825.00s]: [0.00-3.96] Зачастую сервисы вообще делятся на два типа. [4.30-7.06] Я тут презентацию не представил, поэтому голос там расскажу. [9.26-15.20] И у каждого есть какая-то своя метрика, которая является основной. [15.96-21.22] Так, к примеру, если мы делаем сервис по типу Нейра, который Яндексовская, [22.80-0.00]  [8.80-NA] Мы на самом деле не хотим добиться от нее...
[Chunk start 2850.00s]: [0.00-12.12] как мы не хотим чтобы потому что она очень часто занимается там к примеру кем-то задачами связаны [12.12-18.54] просумеризируем не это видео в это самый популярный запрос там у нейра берутся какое-то видео там на [18.54-25.00] ютюбе мы хотим чтобы она просумеризировалась нам не суть важно тут tokens per second на [25.00-0.00]  [4.20-NA] На самом деле, нам здесь очень будет важно, там как раз таки пропускная способность.
[Chunk start 2875.00s]: [0.00-7.64] самом деле нам здесь очень будет важно там как раз таки пропускная способность то есть мы хотим чтобы [7.64-15.94] при очень большой нагрузки у нас наш сервис все равно стабильно работал выдавая тот же токен [15.94-23.94] сперсик онско которым выдает в обычном режиме чтобы сильно не страдал когда в каких-то онлайн [23.94-0.00]  [5.78-NA] ассистента к примеру если вы там зайдете там сейчас
[Chunk start 2900.00s]: [0.00-10.56] к примеру если вы там зайдете там сейчас в telegram да но она напишите там гига чату что-то нам тут [10.56-17.24] целом будет важно конечно токен сперсек он супер важно но нам будет важно и тайм ту ферст окин это [17.24-23.46] самый первый шаг для любой ломки потому что нужно заниматься контекст декодингом нужно просто этом [23.46-0.00]  [5.90-NA] до какое-то время в очереди на запрос потому что там пропускная способность можно позволить
[Chunk start 2925.00s]: [0.00-6.16] очереди на запрос потому что там пропускная способность можно позволить да вы сильно [6.16-12.50] от таки не зация будет все это дело зависеть так из декодинг нам позволяет как раз таки ломки сначала [12.50-18.72] обработать ваш запрос сгенерировать на него ответ а затем уже как раз таки итеративно идти это будет [18.72-25.86] сильно быстрее поэтому зачастую у нас таким сперси конс как в метрике качества там работы сервиса не [25.86-0.00]  [4.14-NA] учитывается порой первый токен, его отбрасывают, потому что это как отдельный метод.
[Chunk start 2950.00s]: [0.00-6.90] не учитывается порой первый токен его отбрасывают потому что это как отдельная метрика у нас хотя [6.90-13.02] можно не отбрасывать в целом тогда будут там чуть другие но очень схожи все равно значения [13.02-23.82] современной модельки как-то распределены на этом графике по как раз таки двум этим метриком самый [23.82-0.00]  [6.12-NA] идеальный квадрант у нас очень маленькое время на time to first token.
[Chunk start 2975.00s]: [0.00-8.50] нас очень маленькое время на тайм ту ферз токен и очень большое количество токен спер секанс понятное [8.50-14.22] дело такого достигают обычно какие-то маленькие модельки либо моделька стамп с приставку flash [14.22-23.76] ну а какие-то очень мощные модели в них достаточно там мощный какой-то декоринг происходит типа [23.76-0.00]  [6.18-NA] дипсика у них очень большой обычно тайм тофер стокин но при этом можете
[Chunk start 3000.00s]: [0.00-12.06] У них очень большой обычно time-to-first токен, но при этом может сильно отличаться в зависимости от задачи, которые ставятся перед моделькой по tokens-per-sequence. [12.06-0.00]  [7.82-8.62] В tokens-per-sequence очень важно в целом учитывать то, с чем мы работаем. [17.90-NA] Так у нас может быть история связана с тем, что нам важно максимизировать TPS по питону, по математике.
[Chunk start 3025.00s]: [0.00-7.08] тем что нам важно максимизировать там тпс по питону по математике там по русскому языку или по [7.08-13.50] английскому и мы можем очень быстро заметить что токены на самом деле по разным доменом они сильно [13.50-19.80] разные поэтому нужно обращать на это внимание иногда производится замер там по тпс у сильно [19.80-0.00]  [9.78-NA] сказать смещенный ввиду того что не знаю мы там мерим там тпс джипе течет и мерим
[Chunk start 3050.00s]: [0.00-9.78] мы там мерим там тпс джипе течет и мерим тпс там не знаю код помощника как раз разработчика и мы [9.78-16.74] знаем что там фертильность и кинезации то есть в среднем размер токи на какой-то он на коде сильно [16.74-25.92] выше чем на русском языке так мы можем наблюдать здесь что у нас там здесь видно средний там длина [25.92-0.00]  [4.06-NA] цена токена в GPT-4 она там четыре с чем-то на русском языке,
[Chunk start 3075.00s]: [0.00-6.72] Средняя длина токена в GPT-4, она там 4 с чем-то на русском языке, это 2 и там 2. [8.26-14.26] То есть сгенерировать такой токен, сгенерировать другой на самом деле требует разных скоростей вдруг. [17.02-23.08] И о чем очень хочется сильно поговорить, что вообще так или иначе повлияет на риски использования LLAM, [23.08-28.02] это промерч методы оптимизации, которые в сервисах используют. [28.30-0.00]  [1.98-NA] Вдруг мы очень быстро, как будто бы, сжали все снаряды.
[Chunk start 3100.00s]: [0.00-7.68] которые в сервис используют вдруг мы очень быстро на самом деле начинает понимать что все наши [7.68-15.48] модели которые мы разработали безумно долгие и требует очень много памяти излишней памяти мы [15.48-21.50] это все можем сильно оптимизировать при этом никак не потеряв качестве зачастую оптимизации добавляя [21.50-27.68] самый легкий способ это просто добавить новых железяк понят дело может купить там со самые [27.68-0.00]  [2.32-NA] современные видеокарточки и за
[Chunk start 3125.00s]: [0.00-6.36] понятное дело может купить там самые современные видеокарточки и забыть о нашей текущей проблеме [6.36-12.12] но на самом деле мы можем куда лучше мы иногда можем заменить какие-то архитектурные способности [12.12-20.34] модели там маешки в целом микшеров экспорта они быстрее работают чем обычные там dance [20.34-29.28] до нейросети но зачастую объединяют методы связанные с какими-то архитектурными решениями [29.28-0.00]  [1.00-NA] внутримодельными.
[Chunk start 3150.00s]: [0.00-5.22] объединяют методы, связанные с какими-то архитектурными решениями внутри моделей, [5.82-10.08] и железную историю, то есть оптимизация именно хардвера, [10.72-12.74] и скрестили это все дело в мерч-методы, [14.34-19.22] когда мы достаточно эффективно в тех вещах, которые мы уже давно знаем, [19.80-26.86] можем использовать управление виртуальной памятью, управление видеопамятью. [26.86-0.00]  [29.98-NA] Так что у нас в целом все начнет считаться, что это не так.
[Chunk start 3175.00s]: [0.00-6.84] управление там видеопамятью так что у нас целом все начнет считаться в несколько раз быстрее при [6.84-11.86] этом это никак не потеряет в качестве потому что мы сохраняем ровно тоже логику работы так [11.86-18.50] примеру самый эффективный способ вообще там по оптимизации любого лом сервисом и подробно сегодня [18.50-26.74] разберем таковы кэш continuous бачинг идея в нем супер просто она на картинке и мне кажется даже [26.74-0.00]  [1.98-2.46] сильно объяснять как-то это не надо. [3.26-NA] Мы хотим...
[Chunk start 3200.00s]: [0.00-6.66] на картинке и мне кажется даже сильно там объяснять как-то это не надо мы хотим когда у нас вот такой [6.66-12.48] вот бач при этом он там заканчивается достаточно рано мы хотим в оставшуюся часть бача потому что [12.48-17.28] она у нас просто западе на и вставить какой-то другой бач который был там сильно меньше всего [17.28-24.42] нашего сэмпла и мы таких бачей скорее всего найдем так вот к примеру там в с1 вот эту всю историю до [24.42-0.00]  [5.04-NA] поместился батч S6, который состоял всего там из двух токенов условно.
[Chunk start 3225.00s]: [0.00-4.56] Вместился бач S6, который состоял всего из двух токенов условно. [6.12-10.84] Здесь вместился S5, который тоже из двух токенов состоял, при этом из трех. [12.50-14.72] Это из двух, наверное, а тут просто падинг. [15.48-18.98] И мы таким образом сильно скомпонуем наше пространство бачей, [19.14-21.88] которые мы подаем нашу ломку, и сильно быстрее посчитаем. [22.00-25.32] При этом мы будем очень хорошо знать, где у нас что заканчивается, [25.32-28.72] потому что мы специальные токены будем использовать, которые end-of-centers являются. [29.62-0.00]  [1.00-2.00] Это флешмобильный инструмент, который позволяет пользователям внести в себя вред. [2.00-3.00] Это флешмобильный инструмент, который позволяет пользователям [3.00-4.00] внести в себя вред. [4.00-5.00] Это флешмобильный инструмент, который позволяет пользователям [5.00-6.00] внести в себя вред. [6.00-7.00] Это флешмобильный инструмент, который позволяет пользователям [7.00-8.00] внести в себя вред. [8.00-9.00] Это флешмобильный инструмент, который позволяет пользователям [9.00-10.00] внести вред. [10.00-11.00] Это флешмобильный инструмент, который позволяет пользователям [11.00-12.00] внести вред. [12.00-13.00] Это флешмобильный инструмент, который позволяет пользователям [13.00-14.00] внести вред. [14.00-15.00] Это флешмобильный инструмент, который позволяет пользователям [15.00-16.00] внести вред. [16.00-17.00] Это флешмобильный инструмент, который позволяет пользователям [17.00-18.00] внести вред.
[Chunk start 3250.00s]: [0.00-5.04] потому что мы специальные токены будем использовать которые end of sentence являются это flash [5.04-11.82] attention их целое семейство тоже сегодня подробно не поговорим это page detention он ничего общего [11.82-17.98] с словом attention вообще не имеет но подход очень интересный я отдельно статью тут оставил на эту [17.98-26.16] всю историю но мы смотреть на него не будем там квантизованные лоры интересные методы квантизации [26.16-0.00]  [2.90-NA] лампок на int8.
[Chunk start 3275.00s]: [0.00-4.04] методы квантизации лмок на int8. [5.76-8.18] Это использование, там, k-bit precision, [8.70-14.10] для того, чтобы, там, как-то заквантить наши параметры, [14.22-15.78] при этом не сильно поменять в качестве. [16.34-18.08] Я, кстати, наверное, отдельную скину, [18.44-19.94] сегодня тоже на смотреть на это не будем. [20.64-21.72] Фьюзирование слоев. [21.96-24.80] Это больше архитектурная, конечно, история, нежели чем мерч. [24.80-27.60] Но зачастую это используется, [27.86-0.00]  [4.32-NA] когда несколько слоев у нас просто помощи
[Chunk start 3300.00s]: [0.00-6.24] но зачастую это используется, когда несколько слоев у нас просто по мощности обвиняются условно в один, [6.46-7.58] и быстрее считаться начинает. [8.06-12.68] И есть целые фреймворки для оптимизации, это там Petals и Swarm, [13.40-16.10] но давайте начнем с Flash Attention. [16.88-19.22] Здесь не сильно устали, но не так много осталось. [22.70-25.86] Нет, это тот бач, который мы подаем на вход, а ла-лам. [27.18-0.00]  [4.14-NA] Поэтому мы тут просто определенным образом...
[Chunk start 3325.00s]: [0.00-7.86] на вход и лалам поэтому мы тут просто определенным образом синие то паттинги насколько понимаю либо [7.86-14.22] какие-то специальные токены красные это end of sentence там желтые это собственно те данные [14.22-22.94] которые мы хотим подать просто это как пример компоновки бача когда мы место там весь у нас [22.94-0.00]  [2.00-6.16] у нас, к примеру, весь наш... вся наш... [6.16-7.12] весь наш бач заказов...
[Chunk start 3350.00s]: [0.00-8.46] Весь наш батч заканчивается, к примеру, на S6, то есть шестое там предложение. [8.82-12.62] Мы укомпоновали на самом деле всего в четыре, хотя их было изначально шесть, [12.72-16.02] просто было очень много паддингов, пробелов, которые нам особо не нужны, [16.46-20.42] и мы это все дело скомпоновали, Continuous Patching здесь хорошо работает. [22.24-23.50] Начнем с Flash Attention. [26.16-0.00]  [6.50-NA] Конечно, это такая супер база для всех LLM.
[Chunk start 3375.00s]: [0.00-7.32] Это такая супер база для всех LLM-ок, сервисов и так далее, которые используются. [7.32-16.26] Она по своим результатам в целом сильно меняет тренинг тайм, она меняет инференс тайм в том числе. [16.26-23.52] Какие-то модельки, обученные Flash Attention, обычно показывают рост производительности в три с половиной раза. [23.52-0.00]  [2.76-6.48] С чем это вдруг связано? С тем, что у нас подсчет нашего социального сообщества
[Chunk start 3400.00s]: [0.00-8.28] это вдруг связано с тем что у нас подсчет нашего салфеттен шина на самом деле он очень не [8.28-15.06] оптимизирован по памяти от слова совсем у него есть несколько операций операция там матричного [15.06-21.72] перемножения операция взять и софт макса и операции еще одного матричного там перемножения но не суть [21.72-28.96] важно какого потому что она нигде в целом особо не оптимизируется вот и на самом деле почет софт [28.96-0.00]  [1.04-NA] Макса и вот этот.
[Chunk start 3425.00s]: [0.00-6.20] И на самом деле почет softmax и вот это матричное приложение, [6.20-10.30] которое первое в наших коверах, очень затратное. [10.30-14.60] И обычно, как оно делается во всех фреймворках, они [14.60-22.50] ее пихают в так называемую hard-bound with memory внутри GPU. [22.50-28.54] Нашу GPU можно представить как какую-то consistent память, [28.54-0.00]  [1.44-NA] какой-то, как будто бы жесткий дебил.
[Chunk start 3450.00s]: [0.00-6.84] какую-то consistent память наш какой-то как будто бы жесткий диск внутри джипу и есть какая-то [6.84-14.62] оперативка внутри джипу несмотря на то что полтора терабайта в секунду кажется для гпухи это все равно [14.62-21.76] этого какие скорости но на самом деле на огромных там пайлов данных и так далее это сильно замедляет [21.76-27.40] процесс там и обучение всего остального когда у нас при этом есть очень маленькая потому что там [27.40-0.00]  [2.58-NA] там всего 20 мб обычно, но при этом очень много.
[Chunk start 3475.00s]: [0.00-5.58] при этом есть очень маленькая потому что там всего 20 гигабайт обычно но при этом очень пропускная по [5.58-16.02] своей способности виртуальная память внутри нашей гипухи и flash attention они очень эффективно [16.02-24.12] научились работать с этим небольшим кусочком как раз таки связанным с видео оперативной памятью они [24.12-0.00]  [5.40-NA] вся основная суть почему вдруг этот кусочек стал использоваться
[Chunk start 3500.00s]: [0.00-4.70] Вся основная суть, почему вдруг этот кусочек стал использоваться, [5.04-8.34] ведь на него вроде не положишь целую матрицу, [8.34-15.28] а это в рамках как раз-таки самой первой реализации Flash Attention, [15.74-20.74] в рамках перемножения матриц, это использование тайлинга, [21.82-27.92] когда мы вместо подсчетов, когда мы считаем Q на K, [28.16-0.00]  [1.98-NA] мы обычно там матрицу перемножаем.
[Chunk start 3525.00s]: [0.00-4.90] Когда мы считаем Q на K, мы обычно матрицу перемножаем, [5.04-6.66] как мы знаем, как мы их перемножаем. [6.66-9.32] Там используется просто более эффективный метод, [9.70-14.80] который резко сокращает количество операций, но это окей. [16.52-24.18] В рамках подсчета как раз-таки Softmax используется так называемый онлайн Softmax, [24.58-27.66] который считается у нас рекуррентно, [28.52-0.00]  [2.32-NA] И именно благодаря сфере развития, в основном, в области
[Chunk start 3550.00s]: [0.00-8.34] считается у нас рекуррентно, и именно благодаря свойству рекуррентности при подсчете онлайн софтмакса [8.34-17.78] мы очень хорошо и эффективно умеем хранить информацию как раз-таки о текущем рекуррентном состоянии этого софтмакса [17.78-26.04] в очень быстрой памяти, и это нам позволяет как раз-таки вот эти две операции тайлинга и онлайн софтмакса, [26.22-0.00]  [3.98-NA] во-первых, не просто быстро считать, так еще и...
[Chunk start 3575.00s]: [0.00-7.56] онлайн софтмакса во-первых не просто быстро считать так еще и производить вычисления на одном ядре гпу [7.56-16.40] при всем при этом прошлое все наши вычисления они слова парализовались сейчас это проявляются просто [16.40-23.24] прекрасно так что вот на одном там ядре гпу это все дело считается и виду как раз таки этого мы [23.24-0.00]  [6.56-NA] заметили там сильный прирост flash оттеншин 2 flash оттеншин 3 и по там
[Chunk start 3600.00s]: [0.00-7.80] Flash Attention 2, Flash Attention 3 и прочие какие-то модернизации подобных флешей, [8.18-14.58] они так или иначе продолжают идею авторов изначального Flash Attention. [15.24-20.38] Но Flash Attention сейчас в той или иной реализации нет ни одной, наверное, лампки, [20.38-21.84] которая бы не использовала его. [23.08-23.84] Не знаю, правда. [24.40-25.58] Поэтому он сейчас везде. [27.10-29.80] И при этом все мы не теряем качество от слова совсем никак, [29.96-0.00]  [1.00-NA] по.
[Chunk start 3625.00s]: [0.00-6.06] сейчас везде и при этом все мы не теряем качестве от слова совсем никак потому что мы получаем ровно [6.06-12.72] тот же результат то есть это действительно очень хорошая оптимизация работы алгоритма причем не по [12.72-18.32] там какой-то компетенции на класт то есть мы в целом имеем ту же сложность алгоритма который [18.32-24.42] была у нас до этого мы очень сильно имеем ниже требования по памяти которые требуются нам для [24.42-0.00]  [3.04-NA] для подсчета как раз-таки Attention's Carов.
[Chunk start 3650.00s]: [0.00-10.98] как раз таки то тэншин с коров ну и теперь кого кэш стараешь быстренько по нему пройти идея очень [10.98-16.68] простая как у нас работает лампка у нас есть изначальный какой-то запрос это примеру два [16.68-25.48] плюс два она отсылается лампки лампка генерирует будет теперь 2 плюс 2 будет равно 2 плюс 2 равно [25.48-0.00]  [4.54-NA] но четыре, ввиду того, что, ну, LLM-ка у нас обычно это декодирует.
[Chunk start 3675.00s]: [0.00-7.44] равно 4 по виду того что новая лампу у нас обычно это декодер она всегда берет какое-то предыдущее [7.44-13.60] свое состояние и на основании это предыдущее состояние генерирует следующий токен и так [13.60-23.28] итеративно недурно можно заметить что на самом деле у нас есть в достаточно повторяющиеся куски [23.28-0.00]  [6.46-NA] даже не так мы вот эти все куски
[Chunk start 3700.00s]: [0.00-2.82] Точнее, даже не так. [2.82-7.22] Мы вот эти все куски, которые у нас были там в качестве [7.22-11.16] запроса, в качестве ответа, мы можем очень эффективно [11.16-12.16] где-то хранить. [12.16-15.60] Уже не дурно как идея. [15.60-19.82] А теперь развеем эту идею совсем до крайностей. [19.82-22.96] А почему бы нам просто вот эти куски кода не хранить [22.96-26.40] в каком-то кэше, который у нас будет постоянно обновляться, [26.40-29.38] а именно этот кэш, который у нас всегда заложен как [29.38-0.00]  [1.00-NA] значения.
[Chunk start 3725.00s]: [0.00-5.76] постоянно обновляться а именно этот кэш который у нас всегда заложен как значение мы будем подавать [5.76-17.46] всегда в л.м. качестве входного какого-то контекста то есть просто базового хранить этот весь кэш [17.46-26.78] будет куда удобнее и куда более быстрой какой-то реализации чем мы будем заново скармливать модели [26.78-0.00]  [3.34-NA] или весь предыдущий какой-то контекст, ведь у нас в целом есть...
[Chunk start 3750.00s]: [0.00-6.60] заново скармливать модели весь предыдущий какой-то контекст ведь у нас в целом есть по киваю уже [6.60-16.08] какие-то данные от модели которые мы можем есть достаточно эффективно скармливать это особенно [16.08-24.96] важно для time to first о кино но точнее наоборот не сильно важно потому что у нас первый контекст [24.96-0.00]  [4.62-NA] текст декодинга у нас никуда не пойдет у нас пока вы кэш появится только после этого
[Chunk start 3775.00s]: [0.00-4.66] текст-декодинг у нас никуда не пойдет, у нас KV-кэш появится только после этого. [5.12-8.02] Однако этот способ сильно увеличивает ТПС, [10.28-16.42] но в целом особо сильно никак не влияет в своем первоначальном виде на использование памяти. [16.82-21.90] У нас все равно резервируется больше 30% на любой видюхе под KV-кэш. [21.90-0.00]  [8.10-NA] Однако, что очень важно, что вдруг научились его очень хорошо квантизовывать и оптимизировать.
[Chunk start 3800.00s]: [0.00-5.08] что вдруг научились его очень хорошо квантизовывать и оптимизировать. [5.88-10.74] У нас была проблема, что изначально веса модели при маленьких контекстах, [11.50-17.48] то есть при маленьком кавэкэше, они у нас занимали большую часть памяти, [17.88-22.26] понятное дело, но этот кавэкэш при больших контекстах сильно растет, [22.86-27.00] так что у нас не хватает памяти на эффективное хранение весов. [27.34-0.00]  [2.98-NA] И поэтому научились кавыкаш-квантинизм.
[Chunk start 3825.00s]: [0.00-8.58] памяти на эффективное хранение весов не поэтому научились кого кэш квантизовать причем с помощью [8.58-17.90] квантизации кого кэша можно во-первых сохранить те же самые результаты по модели которые используются [17.90-25.24] но при этом сильно сократив потребление памяти на хранение кого кэша и текущие как раз таки [25.24-0.00]  [4.76-NA] О реализации квантизации КВ-кавша, КВ-квант просто
[Chunk start 3850.00s]: [0.00-6.48] таки реализации там квантизации кавыка шаг там кого квант просто и называется они позволяют [6.48-12.90] им достичь как раз таки ломком на продакшене там контекстного окна в 10 миллионов токен ровно [12.90-18.24] потому что вот это оптимизировали кого кэш он хорошо поместится на любую железяку и [18.24-27.54] последнее что мы раз а мы просто типов к каждым на каждом шаге мы просто конка чем в последнем [27.54-0.00]  [2.46-NA] В конец добавляем...
[Chunk start 3875.00s]: [0.00-8.28] каждом шаге мы просто конка чем в последнем в конец добавляем имбединг токина да да все просто [8.28-15.26] очень просто все так то есть тут нет какого-то супер ношу стояли гениальной идеи просто обратили [15.26-21.00] внимание что он в целом какие-то куски текста они постоянно повторяются их решили отдельно в каком-то [21.00-28.74] кэше хранить значение там ключ значения по ним все тут ничего супер особенного нету вот этот [28.74-0.00]  [1.24-NA] КВКшум просто отвечает за все, что нужно.
[Chunk start 3900.00s]: [0.00-3.00] Тут ничего супер-особенного нету. [3.00-6.70] Вот этот KVCash просто отвечает за какой-то контекст, который [6.70-9.10] на каждом шаге модель сама себе дает. [9.10-11.62] Вот она может к нему очень эффективно быстро обратиться. [11.62-15.50] С помощью квантизации он еще и весить мало начинает. [15.50-22.68] Да, и последняя история тоже, она просто более умная [22.68-25.08] квантизация, нежели чем там просто заквантить весы [25.08-27.72] и найти какой-то фактор квантизации. [27.72-0.00]  [2.28-NA] Там LLMint8.
[Chunk start 3925.00s]: [0.00-10.82] и найти какой-то фактор квантизации это там ллм-инт8 там прям так называется вот что делают смотрят [10.82-19.26] на значение в любой на самом деле матрицы находят как и какие-то условные обычные значения внутри [19.26-25.98] матрицы которые как-то равномерно распределены между самими собой но матрицы мы также еще и [25.98-0.00]  [2.06-2.24] и чаще всего находим какие-то аутлайеры. [4.00-NA] Чаще всего из-за того, что у нас...
[Chunk start 3950.00s]: [0.00-4.62] мы также еще и чаще всего находим какие-то аутлайеры. Чаще всего из-за того, что у нас [4.62-12.00] достаточно спорсированы порой бывают эмбеддинги, у нас эти аутлайеры действительно хранят какую-то [12.00-19.38] супер ключевую информацию для лмп. Поэтому было предложено, давайте мы на самом деле наши аутлайеры, [19.38-26.16] в виду того, что они несут достаточно большую смысловую нагрузку для наших моделей, не будем [26.16-0.00]  [3.84-NA] никак трогать в рамках квантизации, потому что из-за, там, квантизации
[Chunk start 3975.00s]: [0.00-7.96] Не будем никак трогать в рамках квантизации, потому что из-за квантизации подобных параметров сильно потом может пострадать качество. [8.58-9.72] Мы оставим их как есть. [11.52-19.92] И действительно, можно эти вещи оставить как есть, но при этом заняться квантизацией не аутлайеров, [20.44-24.50] а просто каких-то значений, которые так или иначе в нашем алгоритме распределены. [24.50-0.00]  [5.50-NA] со стандартным алгоритмом квантизации, и потом все это дело...
[Chunk start 4000.00s]: [0.00-6.46] По стандартному алгоритму квантизации и потом все это дело уметь эффективно объединять. [7.66-15.34] Подробнее про алгоритм, то, как он выбирает аутлайеры, то, как он выбирает регулярные значения, тут решил не касаться. [15.66-18.06] Но идея тоже достаточно простая, интуитивная. [19.94-27.72] И на самом деле очень весомая, потому что зачастую все современные реализации, которые можно скачать с Hugging Face, [27.72-0.00]  [2.28-NA] они поддерживают LMint 8.
[Chunk start 4025.00s]: [0.00-6.38] Современные реализации, которые можно скачать с Hugging Face, они поддерживают LMN8 внутри себя, [6.38-13.30] и это дает еще меньшую просадку по качеству, чем при обычной квантизации полноценной. [14.64-24.70] И при этом все позволяет сильно меньше памяти потреблять модели на инференции, на обучении и так далее. [26.54-0.00]  [5.28-NA] Нам осталось поговорить по поводу фреймворка.
[Chunk start 4050.00s]: [0.00-10.02] Нам осталось поговорить по поводу фреймворков, на которых работают, собственно, разработчики, [10.02-16.20] которые выводят модели в прод. У нас есть несколько фреймворков, которые точно хотелось бы затронуть, [16.20-22.48] не суперподробно, но просто хотя бы рассказать, которые там все open-source-ники так или иначе [22.48-30.00] используют. Это tensorrtlm, это больше такой production на самом деле фреймворк. Эти фреймворки
[Chunk start 4075.00s]: [0.00-3.00] Это больше такой продакшн, на самом деле, фреймворк. [4.36-6.26] Эти фреймворки все в целом зачем-то нужны. [6.60-11.86] Они объединяют все то, что мы там обсудили до этого, [14.42-18.94] то есть какие-то методы оптимизации, какие-то ускорения инференса, [19.42-23.16] поддержка стабильности и функциональности нашего сервиса, [23.16-0.00]  [6.84-NA] как раз-таки внутри себя имеют хорошую поддержку на каком-то...
[Chunk start 4100.00s]: [0.00-6.34] имеют хорошую поддержку на каком-то более низком уровне, [6.44-8.24] то есть по общению с железяками. [8.42-12.40] К примеру, TensorRT, это непосредственно разработано NVIDIA, [12.90-14.88] оно поддерживает самые современные ядра. [14.88-19.24] Если выкатывается драйвер на какой-нибудь H100 GPU, [19.62-21.72] на который вы будете учить свою модель, [21.82-24.68] то, скорее всего, TensorRT обновится сиюсекундно, [25.06-27.98] и у вас будет самая современная поддержка без багов и так далее. [28.24-0.00]  [2.00-NA] В отличие от всех других фреймворков, которые мы
[Chunk start 4125.00s]: [0.00-2.98] И у вас будет там самая современная поддержка без багов и так далее. [3.24-8.38] В отличие от всех других фреймворков, потому что они зачастую просто реализованы китайцами, [8.56-11.96] у которых хоть и есть какие-то свои GPU, они не так распространены, [11.96-17.38] и все равно все метятся в как раз-таки использовании GPU от NVIDIA. [17.98-23.20] При этом все, TensorRT, он production фреймворк в первую очередь, [23.70-25.62] и у него очень сложный порог входа. [26.14-0.00]  [4.38-NA] Не каждая лаборатория может позволить себе вдруг взять специалиста.
[Chunk start 4150.00s]: [0.00-6.66] порог входа. Не каждая лаборатория может позволить себе вдруг взять специалиста по этому фреймву [6.66-14.92] и как-то на нем работать, продолжать. Зачастую самым популярным это VLLM используется. [15.04-23.80] У него очень простой сам по себе. У него хорошая скорость как раз-таки, [23.80-28.90] и инференс модели после определенных операций, которые этот фреймворк делает. [29.52-0.00]  [1.10-NA] появляет.
[Chunk start 4175.00s]: [0.00-6.60] там после определенных операций которые этот фреймворк делает авторы вы как раз реализовали [6.60-15.80] по 5 шт attention патча теншин нам очень эффективно позволяет работать с кого кишон в удивление вот [15.80-23.44] блочно как-то реализуют там какую-то структуру там по хранению этого к вкша в не особо разбираюсь [23.44-0.00]  [6.16-NA] честно вот очень популярный это можно по звездочкам увидеть его
[Chunk start 4200.00s]: [0.00-8.70] очень популярный это можно по звездочкам увидеть его зачастую используют все современные там ломки [8.70-17.22] которые не супер большие они там на в лами так иначе написаны есть еще и ломде плой первые [17.22-24.18] ребята которые там запустили там ламу один ламу 2 сделали там от авторы континент бачинга тоже [24.18-0.00]  [5.76-NA] простой но гениальной идеи, тоже какой-то фреймворк. Вот они есть.
[Chunk start 4225.00s]: [0.00-10.14] гениальные идеи, тоже какой-то фреймворк. Вот они есть такие. И заключительно, что хотелось бы сказать, [10.14-17.02] а именно сделать какой-то определенный рекап, зачем мы это вообще вдруг все прошли на протяжении [17.02-24.84] всех этих пяти лекций, ведь что нас дальше ждет. Мы в целом поговорили на самой первой лекции, что [24.84-0.00]  [5.06-NA] там генеративный искусственный интеллект это круто есть определенные понятно дело грехи
[Chunk start 4250.00s]: [0.00-2.66] что там генеративный искусственный интеллект это круто, [3.10-5.78] есть определенные, понятное дело, грехи на текущий момент, [5.92-7.42] есть какие-то нерешенные у него проблемы, [8.02-13.06] но что немаловажно, то, что есть определенные риски, [13.38-18.56] которые являются не просто рисками, что мы там денежку какую-то потеряем, [18.56-25.98] но это там топ-2 рисков по версии там Международного экономического форума, [26.58-28.34] такой самой большой, наверное, организации, [28.48-0.00]  [1.64-NA] которые там доклялись.
[Chunk start 4275.00s]: [0.00-8.58] форума такой самый большой организации которые там так или иначе занимается тем что подсвечивает [8.58-12.48] какие-то риски мировые именно к чему все прислушиваются там самой большой компании [12.48-19.86] так далее то есть это очень авторитетный источник да и эти риски связаны с галлюцинациями считают [19.86-26.70] сильно не просто галлюцинациями но и дезинформации считают очень опасными и очень важно нам [26.70-0.00]  [3.30-NA] нам добросовестно и очень качественно мерить в таком случае алан.
[Chunk start 4300.00s]: [0.00-5.10] и очень важно нам добросовестно и очень качественно мерить в таком случае LLM. [5.10-11.40] Мы поговорили на второй лекции про то, что используют вообще в рамках обучения LLM, [11.40-19.26] и какие модификации делают над LLM-ками, которые так или иначе влияют на работу самой LLM, [19.26-25.62] и их нужно учитывать. Вообще в целом, что нужно уметь все правильно измерять, [25.62-0.00]  [4.36-NA] нужно не просто вслепую бросаться на первый попавшийся бичмарк, но уметь
[Chunk start 4325.00s]: [0.00-5.40] измерять нужно не просто вслепую бросаться на первый попавшийся бичмарк но уметь как-то его [5.40-12.34] оценить оценить особенности нашей лампки что она может что не может не используйте где она не может [12.34-21.00] она там обучить ее тоже определенным образом и до рассмотрели модальности как следующий шаг развития [21.00-28.20] общих целых всех и лампок некоторые нюансы связанных с их обучениями о том что модальность [28.20-0.00]  [1.80-NA] это не просто какой-то блокбокс.
[Chunk start 4350.00s]: [0.00-5.04] нюансы связанных с их обучениями о том что модальность это не просто какой-то блокбокс [5.04-12.72] но это все-таки состоящий из каких-то различных энкодеров проекторов там кросса теншин в истории [12.72-19.56] это что их качество измерить это достаточно большой челлендж но и сегодня поговорили в [19.56-28.50] целом о каких-то методах оптимизации которые так или иначе используются и которые могут [28.50-0.00]  [1.50-NA] могут нам так или иначе помочь.
[Chunk start 4375.00s]: [0.00-8.64] и которые могут нам так или иначе повлиять на картину что мы можем увидеть что там хайф модельки [8.64-13.82] которые мы загружаем там с ген фейса они могут отличаться того что мы это можем увидеть на [13.82-22.26] сервисе и поэтому от этого нам собственно правильно как надо строить наше тестирование все дальнейшие [22.26-0.00]  [5.12-7.74] Следующие лекции проведут Ваня Подпружников и Степан Пономарев, мои коллеги.
[Chunk start 4400.00s]: [0.00-4.10] Степан Подпружников и Степан Пономарев, мои коллеги. [4.68-10.02] В дальнейшем вас ждет достаточно увлекательное и долгое путешествие мир раков, агентов, [10.44-18.16] и не просто ЛЛМ, а так таковой, но и их применений, как они правильно используются, [19.24-23.00] как их правильно использовать, как узнать, что они правильно используются. [25.40-0.00]  [5.88-NA] и проговорим про какие-то реальные истории жизни.
[Chunk start 4425.00s]: [0.00-3.90] проговорим про какие-то реальные истории жизни. [5.34-11.22] Степан, как раз-таки по большей части сконцентрированный на диффузионных моделях, [11.32-12.38] расскажет ровно про них. [13.42-19.58] Скорее всего, там самый современный доклад будет связан с текстом видео, [19.86-22.08] той вещью, которая развивается меньше года. [23.70-26.50] Посмотрим на методы как раз-таки оценки подобных моделей, [26.50-29.68] какие риски там тоже могут быть, что с этим делать.
[Chunk start 4450.00s]: [0.00-2.56] раз такие оценки подобных моделей, какие риски там [2.56-4.68] тоже могут быть, что с этим делать. [4.98-7.54] И если все пойдет гладко, то, возможно, проведем бонусную [7.54-8.42] лекцию, о которой говорили. [9.60-12.60] Сразу предвкушая вопрос про домашнее здание, появится [12.60-13.16] ли оно сегодня. [13.54-15.64] На самом деле, нет, не появится, появится оно в субботу. [16.18-16.44] Вот. [17.40-18.66] На все, что хотел сказать. [19.20-20.64] Так, всем спасибо. [21.02-22.70] Если есть вопросы, буду рад слышать. [26.24-28.14] А есть какое-то понимание, про что домашнее здание [28.14-28.30] будет? [29.18-29.84] Да, есть.
[Chunk start 4475.00s]: [0.00-3.32] А есть какое-то понимание, про что домашка-то будет? [4.14-4.86] Да, есть. [6.44-10.00] Обычно домашка будет состоять из... [11.88-14.16] У вас будет, скорее всего, инференция ллмки, [15.20-18.38] достаточно быстрой, очень надеюсь на это. [19.44-25.44] И нужно будет эту ллмку уметь прогнать на бенчмарках, [25.44-29.36] которые мы обсуждали как раз-таки в рамках этих лекций.
[Chunk start 4500.00s]: [0.00-6.90] бенчмарках, которые мы обсуждали как раз-таки в рамках этих лекций, возможно, в различных режимах, [8.58-9.84] и написать по этому какие-то выгоды. [10.38-12.50] В целом, это будет ровно про это. [13.84-14.26] Спасибо. [15.02-16.02] Сверхъестественного, да. [21.28-24.18] Здесь сейчас еще есть вопросы, буду рад ответить. [24.36-27.24] Но если вдруг нет, то пишите в чат. [28.64-0.00]  [2.76-NA] И тогда всем спасибо.
[Chunk start 4525.00s]: [0.00-3.46] вдруг нет, то пишите в чат. [3.46-6.06] И тогда всем спасибо. [6.06-10.10] Получается, Ваня начнёт с четверга, а по поводу первого [10.10-15.02] домашнего задания сброшу как раз-таки на неделю информацию. [15.02-16.02] Всем хорошего вечера. [16.02-21.94] — Спасибо, пока.
[Chunk start 4550.00s]: [0.00-29.98] Подписывайтесь на наш канал.