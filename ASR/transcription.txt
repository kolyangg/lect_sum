Подписывайтесь на наш канал. Подписывайтесь на наш канал. Подписывайтесь на наш канал. Подписывайтесь на наш канал. Подписывайтесь на наш канал. Появился в этом случае, как раз, в том числе в том, что в этом случае, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том числе, в том чис Подписывайтесь на наш канал. Подписывайтесь на наш канал. Подписывайтесь на наш канал. Все, что мы видим, это не только те, кто не понимает, но и те, кто не понимает. Подписывайтесь на наш канал. Подписывайтесь на наш канал. Подписывайтесь на наш канал. Да, давайте начнем. У нас сегодня ждет не то чтобы сильно большая, поэтому скорее всего мы проведем ее без перерыва. Думаю, мы успеем где-то за час максимум все это дело рассказать. Но при этом заключительная лекция в блоке первом лекции связанных с лоломками, такими именно больше коры. историями про модальности и про то, как их обучают, какие там способы используют. И сегодня мы, по большей части, как раз поговорим про какие-то оставшиеся модальности, которые не успели затронуть на предыдущей лекции, и заодно немного поговорим о тех методах, которые используют для того, чтобы вообще сервис на базе лаб какой-то свой сделать, почему эти методы вдруг актуальны и зачем их используют, и подвинем некоторые вообще в целом итоги того, мы там изучили на первом блоке и зачем это все дело проходили. Да, на прошлой лекции разбрали в Vision модальность. Она такая достаточно большая и интересная. Сегодня будем рассматривать с вами и код, и аудио, и начнем с кода. Код как модальность, она в целом достаточно уникальная, потому что ее даже в отделу модальности в целом не всегда выносят. достаточно много особенностей. Во-первых, он написан на привычном для любой лоломки тексте, так или иначе, но при этом, естественно, не представляет собой естественный язык. Для чего вообще нужен код как модальность? Ну, во-первых, любой разработчик в целом, который особенно разрабатывает подобные модальности, он так или иначе хотел иметь какого-то универсального помощника, который поможет достаточно быстро решать какие-то задачи, связанные с кодом, которые возникают. в целом практически всегда, если вы разрабатываете или занимаетесь дата-сайенсом или что-то подобное. И таких задач достаточно много, которые возникают у вас там из раза в раз в вашей жизни. Иногда нужно посмотреть, где находится какой-то баг, причем этот баг нужно найти и потом еще и понять, каким образом нам надо сделать так, чтобы этот баг не работал, точнее, исправить этот баг. либо своем, либо чужом. Code-to-code retrieval достаточно часто тоже история, которая позволяет нам решать кодовую модальность. Естественно, самая главная вещь, для чего кодовая модальность нужна, это генерация кода того или иного. Это может быть полностью как рерайтинг кода, который написали, либо завершение кода, то есть какие-то универсальные помощники, которые позволяют на основании тех данных, которых они обучились, завершить ту или иную вещь. Причем кодовые сервисы обычно сочетают в себе не просто какую-то одну модельку, так они в целом сочетают множество моделей. Сейчас, секундочку. Они сочетают множество моделей, начиная от single line модели, это как отдельная модель, где требуется завершить только одну маленькую строчку кода, Там контекст у нее должен быть связанный либо с одной этой строчкой кода, либо как раз таки с кодом предыдущим каким-то, который был написан. Это может быть мультилайн история, когда мы хотим несколько строчек кода сгенерить, которые нам помогут выполнить ту или иную функцию. Либо это должен быть какой-то ассистент, который понимает код и может провести с вами диалог, как раз таки связанный с тем, как вы должны там либо код свой поставить, либо поговорить в целом о коде, либо дать задачку какому-то кодовому ассистенту на работу сингла и мультилай модели. Что касается данных для обучения, здесь как раз-таки всей кодовой модальности безумно сильно повезло, потому что кода очень много, почти весь так или иначе представленный в интернете коде, он полезный, несмотря на достаточно большой код. дубликатов в коде обычно все бенчмарки ой бен датасеты которые там являются при троеном для подобных моделей они редусят с помощью дедубликации кода данных буквально свой размер раза в два но но при этом все этих данных сильно больше чем на естественном языке на удивление и самый такой распространенный к примеру там пред и новый датасет для кода это стак 2 недавно вышедший там порядка 900 миллиардов токенов как вы помните там в российском интернете нам в целом такое даже снится там не может на естественном языке в даже на англоязычном на самом деле не то чтобы там прям вы имелись такие какие-то дата сета где столько много токенов было бы представлено весит конечно это махина достаточно мощно но Но даже на этом, на самом деле, разработчики не останавливаются, потому что у кода в целом, как у отдельной модальности, есть такое свойство, что мы всегда можем проверить правильность кода, который так или иначе нам встретится как кусок на какую-то истинность. Раз мы можем проводить такие проверки, мы можем написать какое-то определенное задание на сервисе, связанное с генерацией подобных данных, И засертить какую-то историю так, чтобы мы хотели, чтобы все ассерты проходили по данным, которые нам сгенерировали тайная или иная модель, и добавить такой синтетический код у нас в обучение. Сейчас тяжело даже назвать толком долю синтетических данных внутри современных моделей, связанных с модальностью кода. Однако их достаточно много, потому что любая такая синтетика, она так или иначе достаточно качественная. несколько подходов связаны с тем как подобные данные можно генерить есть модели достаточно эффективные плоды их тут список представленный для генерации кода конкретно это как маленькие модели так и в целом достаточно там большие некоторые представлены но они эффективно справляются с задачей генерации кода причем генерации кода у них может быть трех типов это либо какая-то салфетка какой-то ломки которые нам будет этот код генерировать для тех или иных задач какое-то задание она нам на основании этого задания генерит что-то это может быть какой-то evolution instruction когда мы хотим дать какую-то проблему которая нам не кажется достаточно серьезно или сложный и мы хотим каким-то образом с помощью какой-то инструкции для ломки и и усложнить причем это может быть достаточно интерактивный процесс мы еще поговорим усложнение задачи связанных с кодом Но это позволяет нам сильно увеличить сложность данных. Коли мы можем контролировать на самом деле сложность данных в обучении подобных модальностей, это открывает нам достаточно большой простор в плане построения каких-то эффективных методов обучения нашей лампе, которая будет связана с кодом. Можно использовать тот же Circulum Learning, который дает достаточно большую эффективность и страдает от того, что как раз у нас недостаточно данных, которые разбиты по когортам сложностей. либо это может быть о с instruction когда у нас есть какой-то сниппет кода и на базе такого сниппета на может генерироваться целое множество проблем которые до этого не встречалось особо сильно извините пожалуйста да сейчас вот которые доселе нам не встречалось либо на который у нас там изначального решения не было но при этом модель хорошо уметь генерировать подобные примеры в целом у нас достаточно хорошо развитые алгоритмы связанные с фильтрацией подобного то но точнее какого-то плохого входа который мы можем встретить нашем при трене ровно как и история связанные с деда плика ций данных которые нам позволяют активно когда отсеять вещи в притрении, ровно потому что мы умеем хорошо как раз-таки некоторые вещи заосердить, некоторые вещи мы семантически можем выискивать в наших данных, чтобы понять, насколько код этот был похож, потому что либо по выходам каких-то кусков кода мы можем проводить как раз-таки similarity search для того, чтобы эффективно дублицировать данные. Ну и самое важное, на самом деле, в плане обучения, Возможно, это не так, как в предыдущих видео. все понятно что можно поменять в обучении л.м. на самом деле можно не менять ровным счетом ничего просто взяв большой пайл там естественного языка для того чтобы у нас модель понимал инструкции и взять тоже кучу данных связанных с кодом все это дело вместе как-то обучить подружить и у нас появится неплохой такой помощник но можно понятное дело взять какую-то готовую лампу которую у нас обучена на естественном языке, разморозить несколько слоев, также обучить на коде, тоже проблем не возникнет, можно добавить какую-то голову, можно заадаптить под какую-то нашу историю, которая нам интересна. И все эти методы, они достаточно стандартны в целом для обучения простых LLM, просто новых задач на естественном языке, и при этом они достаточно эффективны для кода, как для модальности. И мы тут как раз-таки сейчас каких-то определенных проблем не встречаем, хотя хотели бы, какие-то определенные структуры, архитектуры моделей, которые могут эффективно работать с кодом, об этом еще поговорим. Но важной вещью также является возможность self-improve таких моделей, как отдельный просто процесс обучения на кормежке данных, собственно, для модельки, так или иначе, из-за того, что мы можем как раз-таки проводить этап эволюэйшена кода, генерирует наша моделька мы можем давать фидбэк самой модели это такое у нас получается бесплатный рель где модельку как раз таки можно достаточно эффективно обучить чтобы она лишилась каких-то своих ошибок причем можно давать ассерты к написанию либо как вручную так и какой-то другой кодовые ломки там которые выигрывают сейчас всех на бенчмарках таким образом достаточно быстро сходить в хорошем качестве для моделей. У нас может быть много связанных рисков, связанных с обучением модели, потому что зачастую в любых проскрапленных данных, даже в стеке 2, встречаются какие-то куски кода, которые могут быть связаны с лицензией, а это нарушение каких-то авторских прав. Достаточно много всяких маловаров, то есть там вирусов, либо какой-то нехороший ход, который мог быть даже выложен в рамках просто ознакомительных внутри гитхаба в качестве учебного материала и так далее, который так или иначе может привести к тому, что там помощник разработчика на его же мощностях может запустить какой-то вирус. Такие истории, наверное, есть, я точно не знаю, но риск точно имеется. Можно потерять какие-то или слить, по крайней мере, данные. В кусках кода нередко, как минимум, находятся какие-то сниппеты или примеры, которые могут содержаться в комментариях тех или иных персональных данных. Зачастую это тоже может являться большой проблемой. Мы должны уметь фильтровать такие куски кода. или иначе пошел обучение не факт что является очень эффективным и таким образом у нас появляется проблема связана с тем что у нас поломка может научиться на неэффективных кусков кода эта история опять же у нас должна фикситься либо какой-то другой более мощные лампы который будет проверять есть код который у нас при трене находится на это дорого либо изначально и мы должны иметь требования к тем кускам кода которые мы должны будем совать при трей да Множество всего, короче, может быть в целом связано с рисками, начиная от каких-то плохих, которые могут привести к нашим галлюцинациям или нежелательным потерям, заканчивая какой-то легальной историей. Что касается бенчмарков, из-за того, что мы эффективно умеем генерировать в целом куски кода, у нас тут проблем вообще никаких нет. Бенчмаркам завались. Единственное, что на текущий момент, наверное, плохо, то, что нет сложных бенчмарков. Потому что на текущий момент не то чтобы все решения, несмотря на то, что у них кода достаточно много, не умеют в сложной задаче, связанной с кодингом. Об этом тоже обсудим. И, наверное, из всех бичмарков, которые сейчас здесь перечислены и которые развиваются даже на текущий момент, самым оптимальным, самым лучшим решением является просто посмотреть, как на биткомп-бенче у нас, какая у нас там метрика, посмотреть может быть какие-то более специфичные истории типа если мы хотим помочь когда-то scientist обучить то на ds1000 если мы хотим чтобы у нас там много языков знала то что-то еще проверить типа мультилингах и уменовал ну да сконцентрироваться на каких-то локальных проверках и какой какой-то общий бенчмарк вообще в целом среди всех моделей взять то за основу там биткотт бенч руки у нас поросли на самом деле из всех бенчмарков начиная с как раз таки human ивала который вышел достаточно давно, где-то четыре года назад, по-моему, его сделали как раз таки ребята из OpenAE. Задача достаточно простая. У нас есть какой-то код, чаще всего это просто функция, причем внутри функции описан док-стрингом примерно, как это должно выглядеть, либо данные куски кода других функций, может быть, даже законченных, либо внутри самой функции уже несколько строк реализовано. Задача бенчмарка достаточно простая — продолжить код по какому-то определенному контексту, связанному либо с этой функцией, либо с несколькими функциями одновременно. И на основании того кода, который, собственно, сгенерится, строится метрика под названием pass это что-то там, в зависимости от того, за какое количество попыток нас устроит, что наша какая-то лампка пройдет. тот или иной тест. Здесь мы хотим как раз-таки учитывать вариативность нашей лампы. Желательно, конечно, смотреть на метрику Passed1. То есть нас интересует первая генерация, и она должна быть суперидеальной. Но иногда смотрят Passed10 обычно, Passed5. То есть мы в целом допускаем, что часть генерации может быть плохая. Но нас интересует хотя бы, чтобы один раз из пяти было хорошо. Понятное дело, что бенчмаркам сейчас особо сильно не следят. Не так давно сам Human Eval под названием Code Eval был адаптирован для русского языка, поэтому сейчас в России у нас за этим следят. Но сейчас бенчмарк сам по себе устарел, на нем выбиваются какие-то уже невероятные значения, плюс сам бенчмарк особо не обновляется. Точнее, какая-то там агентура над GPT-шкой победила в этом бичмарке. Так там сейчас только одни GPT, в целом такой бичмарк особо не смотрят. В отличие от BigCodeBench, который является таким наследником HumanVala, они об этом напрямую пишут в своей статье, вот как раз-таки BigCodeBench является достаточно интересным и достаточно сложным бичмарком, причем есть сразу несколько версий, как и просто бичмарка. full bitcode bench так и hard bitcode bench. Метрики на нем пока что даже у самых классных моделей они не сильно большие. Это не может не радовать, значит, есть куда расти, значит, задачи действительно сложные. Суть такого бенча заключается в том, что у нас опять же есть какая-то функция, нам эту функцию надо как-то закончить. Но при этом все наши докстринги, они оформлены определенным образом, есть какие-то параметры, которые тоже важны и правильно как-то задать, либо прочитать с другого контекста, например, с наших импортов, что это за параметры вообще могут быть. Нам надо что-то вернуть, какую-то ошибку, возможно, поднять. У нас есть определенные реквайрменты. Мы можем немножко зафишотить нашу всю историю. Ну и, естественно, как-то это все дело проверить какими-то как раз таки ассертами, которые мы делаем. Причем все, Да. иначе, да, под людьми все курировано, так что там какие-то суперидеальные примеры. Таких примеров может быть не очень много, 1140 всего, однако они хорошо разбиты по доменам, они задействуют так или иначе большинство библиотек, которые завязаны на измерениях, ну, завязаны на деятельность разработчиков, и с чем они чаще всего встречаются, однако, как вы могли заметить, все это дело только на питоне на текущий момент. по-моему, да. Если я не ошибаюсь, это пока что только питон. Другие языки, сейчас даже посмотрим, по-моему, он где-то тут есть. Да, да, он только на питоне. Это огорчает, потому что зачастую мы хотим от помощника-разработчика, чтобы он нам не только на питоне помогал, однако на текущий момент это действительно самый лучший бичмарк, хотя вот этот трехэтапный, ступенчатый какой-то анализ тех тасок, он дорогой, и он только адаптируется на текущий момент под какие-то другие языки. DS1000 тоже очень похожий пример, связанный с уже непосредственной работой дата-сайентистов. У нас тоже есть какое-то описание, возможно, дата-фреймов в пандасе, и мы должны на основании как раз-таки этой задачи заняться, генерации кода на основании какого-то кодового контекста и привести какое-то решение. И в дальнейшем его как-то засертить, что-то провести. Вот. Останавливаться особо сильно на нем не буду. В нем достаточно представимость данных по всем библиотекам, так или иначе, которые используют дата-сиентисты. Ну и, да, современные, собственно, модели, которые там побивают все это, естественно, GPT-чо, Cloud, DeepSeq. В целом ничего удивительного тут нет. Также хорошим примером бенчмарка является SVE-бенч. Он тоже зачастую очень часто используется при скорингах всех моделей, которые так или иначе связаны с кодом. Бенч является достаточно уникальным, интересным, ввиду того, что он решает задачу, может ли какие-то LN достаточно хорошо справляться с закрытием issues на GitHub. Примеры issues пропаршены с самими разработчиками этого бенчмарка. Там порядка 90 тысяч пиаров было проанализировано так или иначе. Иши с ним были все просмотрены. Иши обычно идут с какими-то кусками кода. И мы должны на основании как раз-таки всех данных, которые приведены в том или ином мышле, сгенерировать либо код, который сможет помочь решить это ишью и потом собственно сгенерировать тесты, ну точнее провести тесты под него, потому что в этом бенчмарке они заранее все известны. Вот. Да, и мы должны сгенерировать решение. Это конец. На этом бенчмарке тоже не особо большие какие-то результаты и достаточно затюденные, как будто бы только под код модели, но выбиваются вперед то есть там не встретишь просто как там не знаю в дэс тысячи джипе течет клауд нет тут встреч конкретно какие-то вещи типа у дикта которые как-то запромтировали клад там и свои агент опять же на клауде что-то связанное с чепики я не знаю название этого приложения так или иначе вот то есть созданы там людьми специальный промпт какой-то какая-то инструкция которая позволяет хорошо с этим бичом как-то Можно профильтровать внутри самого бенчмарка, найти непосредственно просто какие-то модельки. Но хотел бы показать именно вот это, плюс метрики не особо сильно, опять же, большие, что говорит о сложности такого бенчмарка. Но этот бенчмарк крайне полезен, потому что мы хотим видеть в дальнейшем от наших кодовых помощников именно такой пример использования. Мы хотим в целом в будущем от подобных помощников добиться как раз генерации сложного кода. Мультилайн это в целом уже достаточно сложная история. Это не просто закончить предложение, это еще и как-то продолжить его, написать несколько строчек кода. Но мы хотим, чтобы такие помощники думали куда дальше. Возможно, написали как и целый скрипт, который поможет решить проблему внутри какого-то кода, либо как-то оптимизирует целую библиотеку. либо вообще сгенерирует целую репо под какую-то задачу, которую можно будет из коробки запускать. Пока таких решений, к сожалению, на рынке либо крайне мало и они неэффективны, либо вообще в целом нет. Мы хотим изобрести, естественно, специальную архитектуру, адаптированный подход, ровно такую же, как мы это видели, к примеру, в модальности по видео, когда мы делали какой-то проектор на токены, естественного языка, тот же проектор, к примеру, для кода, в целом, возможно, был бы хорошей идеей. Я таких исследований еще не видел, не находил. Возможно, мы хотим более умный способ работы с данными, иметь связанных с кодом, как-то лучше использовать какой-то фильтринг, понимать, какой код у нас может нести действительно большую ценность для обучения в области кодов. Мы хотим изобрести крутые бенчмарки, связанные с код-геном, потому что все текущие бенчмарки, они так или иначе связаны на достаточно простые проблемы для текущих помощников. Помимо колд-бенча, там, SWE-бенча, мало что суперсложного можно найти. Мы хотим поддержку иметь не только там бетон как языка, мы хотим вообще в целом все языки, в том числе и низкоуровневые, которые представляют в текущем ландшафте очень редко, очень мало, но так или иначе, как будто бы лампка умеет ходить в грамматике, поэтому why not, почему бы не обучить какие-то сложные, тяжелые языки, почему бы не синтезировать данные, которые могут быть для подобного языка релевантны. Мы хотим, естественно, иметь поддержку continuous learning, тут он супер сильно важно напоминаю что такое это когда мы продолжаем обучаться даже после тренинга какой-то нашей очередной модельки мы хотим понимать какие там кодовые фреймворки сейчас актуальны и что нам нужно сейчас обязательно чтобы наша модель умела чтобы она подстраивалась по текущие какие-то обстоятельства примеру там не знаю выходит новая там статья про какой-то там оптимизации И мы хотим, естественно, чтобы, если она революционная была, то наш универсальный помощник поддерживал подобный алгоритм, который были бы реализованы в этом новом алгоритме. в возможных каких-то авторских правах, либо об утечке данных, и как-то нивелировать это. Это тоже будущий вызов, пока естественного решения особо нет. Теперь поговорим про аудио. Олег, а можно вопрос про кодовую модальность? У моделей будет такое же свойство, например, когда обучают LLM, большая часть данных на английском языке, добавляют немного языков, например, русский и китайский, и модель начинает... с меньшим количеством данных понимает уже другие языки такое же свойство есть на там например весь код на питоне практически весь да и сарказм то есть есть мы к примеру в какую-то питоновскую чисто модель засунем код связанный там все плюс плюс нам возможно выдастся код носит плюс плюс но возможно он будет нерабочий объясняю вообще почему такое явление вдруг может произойти на самом деле все данные которые вот тут были рассмотрены самом первом слайде. Они и так или иначе используются в притринах, но при этом, почему у них такой большой размер? Не всегда это код. Во время процесса дедупликации порой мы фильтрируем все комментарии, однако чаще всего, даже при разработке гигакода нашего российского, мы эти комментарии все оставляем. В комментариях у нас находится куча интересной информации, которая связана, во-первых, с другими какими-то языками программирования, зачастую, потому что вставляют, к примеру, докстринговый сниппет, там, кода на C++, перепиши это на Python или что-нибудь подобное. Так и в целом комментарии несут очень много технической информации, которая содержит как русский язык, так и китайский язык, так и английский, какие-то такие большие представленности. Поэтому представленность языков, она так или иначе есть, даже, блин, вот жалко ее не привел на стеки. Она достаточно большая, и все в целом ломки про нее шарят. В основном, конечно, питон, но несмотря на то, что представимость, к примеру, Котлина может быть 6% от всех данных для обучения, все равно у нас ломки достаточно хорошо запоминают подобные данные и могут спокойно потом генерировать данные, связанные с Котлином, даже несмотря на то, что там достаточно мало было примеров. Единственное, что проблема будет это хорошо как-то грамотно протестировать. У нас из всех бенчмарков, которые вообще в целом есть там для кода, у нас только парочка связанных с мультилингл истории. Так у нас есть только мультилингл HumanVal, но HumanVal как бенчмарк, он не слишком сложный, поэтому нам тяжело будет сказать, насколько хорошо там наша моделька справляется с этим. Но если мы там разработаем какие-то определенные наши тесты, которые нам нужны для проверки эффективности работы на том или ином языке, нам, возможно, этого будет достаточно. Нужно ждать появления каких-то новых бенчмарков, которые нам помогут рассказать, типа, мультилингву, бигконд-бенч, к примеру, о качествах подобных моделей, которые будут ориентированы не только на Python, и было бы вообще суперславно, а так в целом все современные помощники, Да, спасибо. одновременно достаточно простая тема, так и крольче нора, ввиду того, что зачастую мы от аудиомоделек не хотим добиваться того, что мы просто положили какое-то текстовое описание, наш какой-то аудиоинпут, и нам на выходе получился просто какой-то текст-инпут. Зачастую мы хотим вот самую последнюю историю, которая реализована на картинке, это аудиоинпут и текст-промптинг, и на выходе мы получаем зачастую нам даже текст-алкоголь. тут не нужен, мы хотим тоже получить аудио. Но про это рассказывать можно очень долго, там много подходов. По большей части сегодня сосредоточить просто на аудио как модальности, то есть мы добавляем, к примеру, какое-то аудио на вход, мы добавляем текст и получаем там текст на выход. Добавление там вукодеров несет определенные и какие-то добавочные применения похожих модальностей. К примеру, мы можем генерировать музыку достаточно эффективно, причем, неплохо делают это современные модели но в основном мы хотим про суммаризировать какое-то видео к примеру на ютюбе по аудио транскрипции здесь как раз нам помогают там спич плюс текст это текст истории вот что там нужно нам как-то поменять но на самом деле я не стал сильно растягивать историю связан с этой модельностью ровно почему потому что она сильно не отличается от вижен истории от слова совсем мы единственное что подменяем это какой-то visual энкодер на аудио энкодер и в целом все готово у нас есть одна небольшая проблема она знакома тем кто занимался там как раз таки аудио моделями у нас у токенов аудио токенов сильно выше значение причинами так или иначе и для того чтобы нам когда мы делаем нашу там прожектор который нам будет это все вот одно как раз это фаза ванная там пространство пихать нам необходимо и токи нормализовать и в целом но нормализация это там не то чтобы какой-то супер интересный процесс про это на и можно найти тысячи тысячи одну реализацию и статью в интернете поэтому тут тоже говорить об этом сильно не буду Мы достаточно эффективно умеем это делать. Единственное, что об этом нужно знать, что там как раз-таки в нашем, когда мы подаем все в фьюзированный там embedding space, у нас могут быть разные значения там у аудиотокенов и текст-токенов, а желательно, чтобы они были равно распределены, потому что это все-таки должно слиться в какую-то одну интерлифт информацию, когда мы должны учитывать и то, и другое одновременно и правильно. так, чтобы у нас там не было переобучения какой-то из вот этих двух историй. В плане бенчмаркирования люди сделали все очень просто. На самом деле мы проверяем любую модель, которая так или иначе затрагивает у нас аудиоинпут, как обычную текстовую модель. Аудиоинпут порой тестируем отдельно, просто за инструктив модель на задачу автоматик спич рекогнишена. То есть у нас есть какое-то голосовое сообщение, которому мы подаем модель, мы промтим ее, чтобы она написала нам транскрипцию этого звука, модель выдает транскрипцию звука, и мы меряем по всем тем же как раз таки методам, которые современные люди меряют, Automatic Speech Recognition Service. Основной метрикой во всех этих сервисах является Board Error Rate. модификации, там, character rate, есть sentence rate, и так далее, и так далее, но в основном ver. Ver мерится просто как количество замен, количество вставок и количество удаления тех или иных символов на общее число символов в оригинальном как раз-таки сообщении. То есть у нас есть, к примеру, вот оригинал, у нас слово, и дата, и транскрипция, у нас слово quick поменялось на brown, а точнее quick вообще убралось, осталось только, И лэйзи док там добавилось. Пример какое-то слово могло неправильно транскрибироваться. И мы посчитаем количество вот этих вставок, удалений и замен, разделим на общее число слов в оригинальном предложении, получим значение веры. У современных моделей оно очень хорошее, порядка 3%. То есть в целом очень редкие какие-то ошибки. Мы никак не хотим учитывать аудиосоставляющую таких моделей, потому что ошибка там маленькая. Это не картиночная история, где ошибки могут быть достаточно большие, и там вообще нет какой-то определенной истины. Для аудиоданных в аудиоинкодерах в любом случае решили перевести конкретно для российского рынка. Для нероссийского рынка можно найти где угодно и в большем количестве часов записи. является OpenCT на русском. Там порядка 20 тысяч часов записи продиктованного текста в абсолютно различных доменах. Это голос от Сбера тоже очень-очень много часов, порядка 18 тысяч. Транскрибации происходят обычно с каких-то радиоэфиров, потому что очень хороший источник данных для нас. Там постоянно люди говорят, поэтому давайте запишем, большое количество эфиров, заставим людей все это транскрибировать, и потом на этом все деле обучимся. Паблик спичей, ютуба, аудиокниг, звонков и прочие какие-то истории. И есть еще небольшой либриспич на 98 часов записи, но его зачастую используют как какую-то тестовую выборку для проверки навыков. Как раз просто посчитать веру. больше бенча не найдете наш любимый и знакомый big bench только теперь с приставкой аудио тоже тысячи там аудио каких-то вопросов мы хотим посмотреть на как раз таки какой-то спичи лиза нинг связать аудио наше сообщение с сообщением на текстовом языке что-то померить но больших отличий там от любого big bench натуральном языке на самом деле нет да тот же big bench только вместе с голосовыми сообщениями Поэтому он достаточно сильно там резаный. Но при этом учитывается несколько задач, как текст-то текст, так спич-то спич, текст-то спич и спич-то текст для моделей, потому что там есть такая вот модификация у этого бенчмарка, так или иначе. Да, закончили на самом деле с модальностями. Про аудио больше углубляться не будем. Про код рассказали, про вижн тоже. как вообще в целом занимаются тем, что собирают сервис на базе LLAM. Моделька у нас готовая есть. Теперь нам необходимо ее обернуть в какой-то сервис, который действительно на нашем железе достаточно бы эффективно работал. Причем мы предъявляем сразу несколько требований к нашему сервису. Естественно, он должен быть быстрый. Медленные сервисы нас не особо сильно интересуют. Дожидаясь ответа год, любое физическое лицо, которое пользуется твоим сервисом, просто уйдет. У этого сервиса должна быть достаточно большая пропускная способность, то есть мы хотим, чтобы наша скорость не сильно страдала при огромном потоке пользователей со своими запросами в наш сервис. Естественно, сервис должен функционировать, функционировать неправильно, без нарушения функциональности, и он должен быть стабильный во времени, чтобы никогда не падал и так далее. скоростью и с рутбутом вот объясняю почему зачастую сервисы вообще делятся на два типа это тут презентации не представил поэтому голос там расскажу и у каждого есть какая-то своя метрика который является основной так к примеру если мы делаем сервис по типа нейро который яндексовская мы на самом деле не хотим добиться от него как мы не хотим чтобы потому что она очень часто занимается там к примеру кем-то задачами связаны просумеризируем не это видео в это самый популярный запрос там у нейра берутся какой-то видео там на ютюбе мы хотим чтобы она просумеризировалась нам не суть важно тут tokens per second на самом деле нам здесь очень будет важно там как раз таки пропускная способность То есть мы хотим, чтобы при очень большой нагрузке у нас наш сервис все равно стабильно работал, выдавая тот же токен сперсеконс, который он выдает в обычном режиме, чтобы это сильно не страдало. Когда в каких-то онлайн ассистентах, к примеру, если вы зайдете сейчас в Telegram, напишите там гигачату что-то, Нам тут в целом будет важно, конечно, токен с персеконсом супер важно, но нам будет важно и time-to-first токен. Это самый первый шаг для любой ломки, потому что нужно заниматься контекст-декодингом, нужно простоять там какое-то время в очереди на запрос, потому что там пропускная способность, можно позволить. Да, ну и сильно от токенизации будет все это дело зависеть. Токен с декодингом нам позволяет как раз-таки ломки сначала обработать ваш запрос, сгенерировать на него ответ, а затем уже как раз-таки итеративно идти, и это будет сильно быстрее. Поэтому зачастую у нас токен сперсеконс, как в метрике качества работы сервиса, не учитывается порой первый токен, его отбрасывают, потому что это как отдельная метрика у нас. Хотя можно и не отбрасывать в целом, тогда будут чуть другие, но очень схожие все равно значения. Современные модельки как-то распределены на этом графике, таки двум этим метриком самый идеальный квадрант у нас очень маленькое время на тайм ту ферст окин и очень большое количество talking персик онс понятное дело такого достигают обычно какие-то маленькие модельки либо моделька стамс приставку flash ну а какие-то очень мощные модели в них достаточно там мощный какой-то декоринг происходит типа дерзки дипсика У них очень большой обычно time-to-first токен, но при этом может сильно отличаться в зависимости от задач, которые ставятся перед моделькой по tokens-per-sequence. В tokens-per-sequence очень важно в целом учитывать то, с чем мы работаем. Так у нас может быть история связана с тем, что нам важно максимизировать TPS по питону, по математике, по русскому языку или по английскому, и мы можем очень быстро заметить, что токены на самом деле по разным доменам, они сильно разные, поэтому нужно обращать на это внимание. Иногда производится замер по ТПСу сильно, как сказать, смещенный, ввиду того, что, не знаю, мы там меряем ТПС GPT-ЧО и меряем ТПС, не знаю, какого-то помощника, как раз разработчика. что там фертильность токенизации то есть в среднем размер токена какой-то он на коде сильно выше чем на русском языке так мы можем наблюдать здесь что у нас там здесь видно средний там длина токена g5 и 4 она там четыре с чем-то на русском языке это два и там два да то есть ну сгенерировать такой токен сгенерировать другой на самом деле требует разных скоростей вдруг И о чем очень хочется сильно поговорить, что вообще так или иначе повлияет на риски использования LLAM, это промерч-методы оптимизации, которые в сервисах используют. Вдруг мы очень быстро на самом деле начинаем понимать, что все наши модели, которые мы разработали, безумно долгие и требуют очень много памяти, излишней памяти. Мы это все можем сильно оптимизировать, при этом никак не потеряв качество. Зачастую оптимизация добавляет... Самый легкий способ — это просто добавить новых железяк. Понятное дело, можно купить там самые современные видеокарточки и забыть о нашей текущей проблеме, но на самом деле мы можем куда лучше. Мы иногда можем заменить какие-то архитектурные способности моделей, там, маешки в целом, микшеров экспорта, они быстрее работают, чем обычные там Dense, да, нейросети. Но зачастую, объединяют методы, связанные с какими-то архитектурными решениями внутри моделей, и железную историю, то есть оптимизация именно хардвера, и скрестили это все дело в мерч-методы, когда мы достаточно эффективно в тех вещах, которые мы уже давно знаем, можем использовать управление виртуальной памятью, управление видеопамятью, так что у нас в целом все начнет считаться в несколько раз быстрее при этом это никак не потеряет в качестве потому что мы сохраняем ровно тоже логику работы так примеру самый эффективный способ вообще там по оптимизации любого лом сервисом и подробно сегодня разберем таковы кэш continuous бачинг идея в нем супер просто она на картинке и мне кажется даже сильно там объяснять как-то это не надо мы хотим когда у нас вот такой вот бач при этом он там заканчивается достаточно рано мы хотим в оставшуюся часть бача, потому что она у нас просто западенная, вставить какой-то другой бач, который был там сильно меньше всего нашего сэмпла. И мы таких бачей, скорее всего, найдем. Так вот, к примеру, там в S1, вот эту всю историю, да, вместился бач S6, который состоял всего там из двух токенов условно. Здесь вместился там S5, который тоже из двух токенов состоял, при этом, ну, из трех, да. Ну, это из двух, наверное, а тут просто падинг. И мы таким образом сильно скомпонуем наше пространство бачей, к которым мы подаем нашу ломку, и сильно быстрее посчитаем. При этом мы будем очень хорошо знать, где у нас что заканчивается, потому что мы специальные токены будем использовать, которые end-of-sentence являются. Это Flash Attention, их целое семейство, тоже сегодня подробно о них поговорим. Это Page Detention, он ничего общего с словом Attention вообще не имеет, но подход очень интересный. Я отдельно статью тут оставил на эту всю историю, но мы смотреть на него не будем. Там квантизованные лоры, интересные методы квантизации ллмок на int8. Это использование kbit-прецижена для того, чтобы как-то заквантить наши параметры, при этом не сильно поменять в качестве. Я, кстати, наверное, отдельную скину, сегодня тоже насмотреть на это не будем. Фьюзирование слоев. Это больше архитектурная, конечно, история, нежели чем мерч. но зачастую это используется, когда несколько слоев у нас просто по мощности обвиняются условно в один, и быстрее считаться начинает. И есть целые фреймворки для оптимизации, это там Petals и Swarm, но давайте начнем с Flash Attention. Здесь не сильно устали, но не так много осталось. Нет, это тот бач, который мы подаем на вход, а ла-лам. Поэтому мы тут просто определенным образом, Синие — это паддинги, насколько я понимаю, либо какие-то специальные токены. Красные — это end-of-sentence, желтые — это, собственно, те данные, которые мы хотим подать. Просто это как пример компоновки бача, когда мы вместо... Если у нас, к примеру, весь наш... Весь наш бач заканчивается, к примеру, на S6, то есть шестое предложение. деле всего в четыре хотя их было изначально 6 просто было очень много паддингов пробелов которые нам особо не нужны и мы это все дело скомпоновали континент патчем здесь хорошо работает до начнем с ваша тэншина это такая супер база для всех ломок сервисов так далее которые используются она по своим результатам в целом сильно меняет тренинг тайм она меняет inference time в том числе. Какие-то модельки, обученные Flash Attention, обычно показывают рост производительности в 3,5 раза. С чем это вдруг связано? С тем, что у нас подсчет нашего Self-Attention, на самом деле, он очень не оптимизирован по памяти. От слова совсем. У него есть несколько операций. Операция матричного перемножения, операция взятия Softmax, и операция еще одного матричного. перемножения но не суть важно какого потому что она нигде в целом особо не оптимизируется вот и на самом деле почет софтмакса и вот это матричное приложение которое первое там наших ковырился кейса но очень затратная и обычно как она делается во всех фреймворках они и опихают называемую там кардбэнд виз мемори но внутри джипы юхи наши джипы их можно представить как какую-то consistent память, наш какой-то как будто бы жесткий диск внутри GPU и есть какая-то оперативка внутри GPU. Несмотря на то, что полтора терабайта в секунду кажется для GPU-хи это все равно, это вау, какие скорости, но на самом деле на огромных там пайлах данных и так далее это сильно замедляет процесс там и обучения и всего остального, когда у нас при этом есть очень маленькая, потому что там всего 20 гигабайт обычно, но при этом очень пропускная по своей способности виртуальная память внутри нашей гэпухи и flash attention они очень эффективно научились работать с этим небольшим кусочком как раз таки связанным с видео оперативной памятью они вся основная суть почему вдруг этот кусочек стал использоваться ведь него вроде не положишь там целую матрицу Это в рамках как раз-таки самой первой реализации Flash Attention, в рамках перемножения матриц, это использование тайлинга, когда мы вместо подсчетов, как раз-таки, когда мы считаем Q на K, мы обычно матрицу перемножаем, как мы знаем, как мы их перемножаем. Там используется просто более эффективный метод, который резко сокращает количество операций, но это окей. В рамках подсчета как раз-таки софтмакса используется так называемый онлайн софтмакс, который считается у нас рекуррентным. И именно благодаря свойству рекуррентности при подсчете онлайн софтмакса мы очень хорошо и эффективно умеем хранить информацию как раз-таки о текущем рекуррентном состоянии этого софтмакса в очень быстрой памяти. И это нам позволяет как раз-таки вот эти две операции, там, тайдинга и онлайн софтмакса, во-первых, не просто быстро считать, так еще и производить вычисления на одном ядре GPU при всем при этом. Прошлые все наши вычисления, они слабо параллелизовались, а сейчас это параллелизуется просто прекрасно. Так что вот на одном ядре GPU это все дело считается. И ввиду как раз-таки этого мы заметили там сильный прирост. Flash Attention 2, Flash Attention 3 и прочие какие-то модернизации подобных флешей, они так или иначе продолжают идею авторов изначального Flash Attention. Но Flash Attention сейчас в той или иной реализации нет ни одной, наверное, лампки, которая бы не использовала его. Не знаю, правда. Поэтому он сейчас везде. И при этом все мы не теряем качество от слова совсем никак, потому что мы получаем ровно тот же результат. То есть это действительно очень хорошая оптимизация работы алгоритма, причем не по какой-то computational cost, то есть мы в целом имеем ту же сложность алгоритма, которая и была у нас до этого. Мы очень сильно имеем ниже требования по памяти, которые требуются нам для подсчета как раз-таки attention scores. Ну и теперь KVCache. Постараюсь быстренько по нему пройти. Идея очень простая. Как у нас работает LLM? У нас есть изначальный какой-то запрос, это, к примеру, 2 плюс 2. Оно отсылается к LLM, LLM генерирует будет, теперь 2 плюс 2 будет равно 2 плюс 2 равно 4. Ввиду того, что LLM у нас обычно это декодер, оно всегда берет какое-то предыдущее свое состояние и на основании этого предыдущего состояния генерирует следующий токен. И так это ративно. Недурно можно заметить, что на самом деле у нас есть достаточно повторяющиеся куски, точнее даже не так. Мы вот эти все куски, которые у нас были там в качестве запроса, в качестве ответа, мы можем очень эффективно где-то хранить. Уже недурно как идея. А теперь развеем эту идею совсем до крайностей, Почему бы нам просто вот эти куски кода не хранить в каком-то кэше, который у нас будет постоянно обновляться, а именно этот кэш, который у нас всегда заложен как значение, мы будем подавать всегда в LLM в качестве входного какого-то контекста. То есть просто базово хранить этот весь кэш будет куда удобнее и куда более быстрой какой-то реализацией, будем заново скармливать модели весь предыдущий какой-то контекст, ведь у нас в целом есть по key value уже какие-то данные от модели, которые мы можем ей достаточно эффективно скармливать. Это особенно важно для time-to-first токена, точнее наоборот не сильно важно, потому что у нас первый контекст декодинга у нас никуда не пойдет, у нас кавэкэш появится только после этого, Однако этот способ сильно увеличивает ТПС, но в целом особо сильно никак не влияет в своем первоначальном виде на использование памяти. У нас все равно резервируется больше 30% на любой видюхе под KV-кэш. Однако, что очень важно, что вдруг научились его очень хорошо квантизовывать и оптимизировать. В этом случае, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, в основном, самые результаты по модели которые используются но при этом сильно сократив потребление памяти на хранение кого каша и текущие как раз таки реализации там квантизации кого каша там кого квант просто и называется они позволяют им достичь как раз таки ломком на продакшене там контекстного окна в 10 миллионов токен ровно потому что вот это оптимизировали кого кэш он хорошо поместится на любую железяку И последнее, что мы рассмотрели. А мы просто на каждом шаге мы просто конкатим в конец, добавляем имбединг токена, да? Да. Все просто, очень просто. Все так. То есть тут нет какого-то суперношества или гениальной идеи, просто обратили внимание, что у нас в целом какие-то куски текста, они постоянно повторяются, их решили отдельно в каком-то каше хранить, ключ значения по ним, и все. Тут ничего супер особенного нету. Вот этот KVCash просто отвечает за какой-то контекст, который на каждом шаге модель сама себе дает. Вот она может к нему очень эффективно быстро обратиться. С помощью квантизации он еще и весить мало начинает. Да, и последняя история тоже, она просто более умная квантизация, нежели чем там просто заквантить весы и найти какой-то фактор квантизации. Это там LLMint8. Там прям так называется. Что делают? Смотрят на значения любой на самом деле матрицы, находят как и какие-то условные обычные значения внутри матрицы, которые как-то равномерно распределены между самими собой, но в матрице мы также еще и чаще всего находим какие-то аутлайеры. Чаще всего из-за того, что у нас достаточно спорсированы порой бывают эмбеддинги, там действительно хранят какую-то супер ключевую информацию для ломки поэтому было предложено давайте мы на самом деле нашего от лаера ввиду того что они несут достаточно большую смысловую нагрузку для наших там моделей не будем никак трогать в рамках монти зации потому что из-за там квантизации подобных параметров сильно потом может пострадать качество мы оставим их как есть и действительно можно действительно эти вещи оставить как есть но Но при этом заняться квантизацией не аутлайеров, а просто каких-то значений, которые так или иначе алгоритмами распределены, по стандартному алгоритму квантизации, и потом все это дело уметь эффективно объединять. Подробнее про алгоритм, то, как он выбирает аутлайеры, то, как он выбирает регулярные значения, тут решил не касаться, но идея тоже достаточно простая, интуитивная. И на самом деле очень весомое, потому что зачастую все современные реализации, которые можно скачать с Hagenface, они поддерживают LMN8 внутри себя, и это дает еще меньшую просадку по качеству, чем при обычной квантизации полноценной. И при этом все позволяет сильно меньше памяти потреблять модели на инференции, на обучении и так далее. Нам осталось поговорить по поводу фреймворков, на которых работают разработчики, которые выводят модели в прод. У нас есть несколько фреймворков, которые точно хотелось бы затронуть, не суперподробно, но просто хотя бы рассказать, которые все open-source-ники так или иначе используют. Это TensorFlow, это больше такой production на самом деле фреймворк. все в целом зачем нужны они объединяют все то что мы там обсудили до этого то есть какие-то методы там оптимизации какие-то ускорения там инференции поддержка стабильности там функционально и функциональность нашего сервиса как раз таки внутри себя имеют хорошую поддержку там на каком-то более низком уровне то есть по общению там с железяками к примеру тендер рт Это непосредственно разработано NVIDIA, оно поддерживает самые современные ядра. Если выкатывается драйвер на какой-нибудь H100 GPU, на который вы будете учить свою модель, то, скорее всего, TensorRT обновится сиюсекундно, и у вас будет самая современная поддержка без багов и так далее. В отличие от всех других фреймворков, потому что они зачастую просто реализованы китайцами, у которых хоть и есть какие-то свои GPU, они не так распространены, использования гпу от nvidia при этом всем танцор рт он продакшен фреймворк в первую очередь и у него очень сложно порог входа не каждая лаборатория может позволить себе вдруг там взять и специалисты потом по этому фрейму и как-то работать продолжать зачастую самым популярным это в л л м используется у него очень простой сам по себе но у него Хорошая скорость как раз-таки и inference-модели после определенных операций, которые этот фреймворк делает. Авторы как раз реализовали patched attention. Пэтч-детеншн нам очень эффективно позволяет работать с KVCache, на удивление. Блочно как-то реализует какую-то структуру по хранению этого KVCache, не особо разбираюсь, если честно. очень популярный это можно по звездочкам увидеть его зачастую используют все современные там ломки которые не супер большие они там на в лами так иначе написано есть еще и ломде плой первые ребята которые там запустили там ламу один ламу 2 то сделали там от авторы континент бачинга тоже простой но гениальной идеи на тоже какой-то фреймворк вот они есть И, заключительно, что хотелось бы сказать, а именно сделать какой-то определенный рекап, зачем мы это вообще вдруг все прошли на протяжении всех этих пяти лекций, ведь что нас дальше ждет. Мы в целом поговорили на самой первой лекции, что генеративный искусственный интеллект это круто, есть определенные, понятное дело, грехи на текущий момент, есть какие-то нерешенные у него проблемы, но что немаловажно, То, что есть определенные риски, которые являются не просто рисками, что мы там денежку какую-то потеряем, но это там топ-2 рисков по версии там Международного экономического форума, такой самой большой, наверное, организации, которая там так или иначе занимается тем, что подсвечивает какие-то риски мировые именно, к чему все прислушиваются, там самые большие компании и так далее, то есть это очень авторитетный источник. Да, и эти риски, связанные с галлюцинациями, считают сильно, не просто галлюцинациями, но и дезинформацией считают очень опасными, и очень важно нам добросовестно и очень качественно мерить в таком случае LLM. Мы поговорили на второй лекции про то, что используют вообще в рамках обучения LLM, и какие модификации делают над LLM, которые так или иначе влияют на работу самой LLM, учитывать вообще в целом что нужно уметь все правильно измерять нужно не просто вслепую бросаться на первый попавшийся бенчмарк но уметь как-то его оценить оценить особенности нашей лампки что она может что не может не используйте где она не может она там обучить ее тоже определенным образом и до рассмотрели модальности как следующий шаг развития общих целых всех и лампок некоторые нюансы, связанных с их обучениями, о том, что модальность — это не просто какой-то blackbox, но это все-таки состоящий из каких-то различных энкодеров, проекторов, там, cross-attention в истории, и то, что их качество измерить — это достаточно большой челлендж. Ну и сегодня поговорили в целом о каких-то методах оптимизации, которые так или иначе используются, и которые могут нам так или иначе повлиять на картину, что мы можем увидеть, что там HF-модельки, которые мы загружаем там с хоггинфейса, они могут отличаться от того, что мы можем увидеть на сервисе. И поэтому от этого нам, собственно, правильно надо строить наше тестирование. Все дальнейшие лекции проведет Ваня Подпружников и Степан Пономарев, мои коллеги. В дальнейшем вас ждет достаточно увлекательное и долгое путешествие в мир раков, агентов, и не просто там ллм так таковой но их применений как они правильно используются как правильно использовать как узнать что они правильно используются и поговорим про какие-то реальные истории жизни степан как раз таки по большей части сконцентрированный на диффузионных моделях расскажет ровно про них скорее всего Появится ли на сегодня? Спасибо. Если есть вопросы, буду рад слышать. бенчмарках, которые мы обсуждали как раз-таки в рамках этих лекций, возможно, в различных режимах, и написать по этому какие-то выгоды. В целом, это будет ровно про это. Спасибо. Сверхъестественного. Здесь сейчас еще есть вопросы, буду рад ответить. Но если вдруг нет, то пишите в чат. И тогда всем спасибо. Получается, Ваня начнет с четверга, а по поводу первого домашнего задания сброшу как раз-таки на неделю информацию. Всем хорошего вечера.