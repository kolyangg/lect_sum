{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/home/kolyangg/anaconda3/envs/stylegan3 --cc=/opt/conda/conda-bld/ffmpeg_1597178665428/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "Input #0, matroska,webm, from '../LLM_valid_ru.mkv':\n",
      "  Metadata:\n",
      "    COMPATIBLE_BRANDS: iso6avc1mp41\n",
      "    MAJOR_BRAND     : dash\n",
      "    MINOR_VERSION   : 0\n",
      "    ENCODER         : Lavf58.45.100\n",
      "  Duration: 01:15:55.28, start: -0.007000, bitrate: 390 kb/s\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(tv, smpte170m/bt470bg/bt709, progressive), 1854x1080 [SAR 1:1 DAR 103:60], 25 fps, 25 tbr, 1k tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      HANDLER_NAME    : ISO Media file produced by Google Inc.\n",
      "      DURATION        : 01:15:55.240000000\n",
      "    Stream #0:1(eng): Audio: opus, 48000 Hz, stereo, fltp (default)\n",
      "    Metadata:\n",
      "      DURATION        : 01:15:55.281000000\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (opus (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '../LLM_valid_ru.wav':\n",
      "  Metadata:\n",
      "    COMPATIBLE_BRANDS: iso6avc1mp41\n",
      "    MAJOR_BRAND     : dash\n",
      "    MINOR_VERSION   : 0\n",
      "    ISFT            : Lavf58.45.100\n",
      "    Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      DURATION        : 01:15:55.281000000\n",
      "      encoder         : Lavc58.91.100 pcm_s16le\n",
      "size=  142352kB time=01:15:55.27 bitrate= 256.0kbits/s speed= 794x        \n",
      "video:0kB audio:142352kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000054%\n"
     ]
    }
   ],
   "source": [
    "# create audio track\n",
    "!ffmpeg -i ../LLM_valid_ru.mkv -vn -acodec pcm_s16le -ar 16000 -ac 1 ../LLM_valid_ru.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "/home/kolyangg/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 16000\n",
      "Подписывайтесь на наш канал.Подписывайтесь на наш канал.Мы не хотим, чтобы в мире были такие проблемы, как в мире, в котором есть такие проблемы.Подписывайтесь на наш канал.Подписывайтесь на наш канал.Все привет, давайте подождем и начнем.Подписывайтесь на наш канал.Да, давайте начнем. У нас сегодня ждет не то чтобы сильно большая, поэтому скорее всего мы проведем ее без перерыва. Думаю, мы успеем где-то за час максимум все это дело рассказать. Но при этом заключительная лекция в блоке первом лекции связанных с лоломками, такими именно больше коры историями про модальности и про то, как их обучают, какие там способы используют. И сегодня мы, по большей части, как раз поговорим про какие-то оставшиеся модальности, которые не успели затронуть на предыдущей лекции, и заодно немного поговорим о тех методах, которые используют для того, чтобы вообще сервис на базе лампы какой-то свой сделать, почему эти методы вдруг актуальны и зачем их используют, и подвернем некоторые вообще в целом итоги того, что мы там изучили на первом блоке и зачем мы это все дело проходили. Да, на прошлой лекции разбрали в Vision модальность. Она такая достаточно большая и интересная. Сегодня будем рассматривать с вами и код, и аудио, и начнем с кода. Код как модальность, она в целом достаточно уникальная, потому что ее даже в отдел модальности в целом не всегда выносят. Однако код сам по себе имеет достаточно много особенностей. Во-первых, он написан на привычном для любой лоломки тексте, так или иначе, но при этом, естественно, не представляет собой естественный язык. Для чего вообще нужен код как модальность? Ну, во-первых, любой разработчик в целом, который особенно разрабатывает подобные модальности, он так или иначе хотел иметь какого-то универсального помощника, который поможет достаточно быстро решать какие-то задачи, связанные с кодом, которые возникают в целом практически всегда, если вы разрабатываете или занимаетесь дата-сайенсом или что-то подобное. И таких задач достаточно много, которые возникают у вас там из раза в раз в вашей жизни. Иногда нужно посмотреть, где находится какой-то баг, причем этот баг нужно найти и потом еще и понять, каким образом нам надо сделать так, чтобы этот баг не работал, точнее, исправить этот баг. Нам нужно найти что-то в коде, либо своем, либо чужом. Code-to-code retrieval достаточно часто тоже история, которая позволяет нам решать кодовую модальность. Ну, естественно, самая главная вещь, для чего кодовая модальность нужна, это генерация кода того или иного. Это может быть полностью как рерайтинг кода, который написали, либо завершение кода, то есть какие-то универсальные помощ они в целом сочетают множество моделей. Сейчас, секундочку. Они сочетают множество моделей, начиная от single line модели, это как отдельная модель, где требуется завершить только одну маленькую строчку кода, при этом контекст у нее должен быть связанный либо с одной строчкой кода, либо как раз таки с кодом предыдущим каким-то, который был написан. Это может быть multi-line история, когда мы хотим несколько строчек кода сгенерить, которые нам помогут выполнить ту или иную функцию. Либо это должен быть какой-то ассистент, который понимает код и может провести с вами диалог, как раз таки связанный с тем, как вы должны там либо код свой построить, либо поговорить в целом о коде, либо дать задачку какому-то кодовому ассистенту на работу сингла и мультилай модели. Что касается данных для обучения, здесь как раз-таки всей кодовой модальности безумно сильно повезло, потому что кода очень много, почти весь так или иначе представленный в интернете код, он полезный, несмотря на достаточно большое количество дубликатов в коде, обычно все бенчмарки, датасеты, которые являются притроеном для подобных моделей, они редусят с помощью дедубликации кода данных буквально свой размер раза в два, но при этом все этих данных сильно больше, чем на естественном языке, на удивление. И самый такой распространенный, к примеру, там, претрейновый датасет для кода — стак 2 недавно вышедший там порядка 900 миллиардов токенов как вы помните там в российском интернете нам в целом такое даже снится там не может на естественном языке в даже на англоязычном на самом деле не то чтобы там прям вы имелись такие какие-то дата сета где столько много токенов было бы представлено. Весит, конечно, эта махина достаточно мощно, но даже на этом, на самом деле, разработчики не останавливаются, потому что у кода в целом, как у отдельной модальности, есть такое свойство, что мы всегда можем проверить правильность кода, который так или иначе нам встретится как кусок на какую-то истинность. Раз мы можем проводить такие проверки, мы можем написать какое-то определенное задание на сервисе, связанное с генерацией подобных данных, и заасертить какую-то историю так, чтобы мы хотели, чтобы все асерты проходили по данным, которые нам сгенерировали тайная или иная модель, и добавить такой синтетический код у нас в обучение. Сейчас тяжело даже назвать толком долю синтетических данных внутри современных моделей, связанных с модальностью кода. Однако их достаточно много, потому что любая такая синтетика, она так или иначе достаточно качественная. Причем есть несколько подходов, связанные с тем, как подобные данные можно генерить. Есть модели достаточно эффективные, вот их тут список представленный для генерации кода конкретно. Это как маленькие мод так и в целом достаточно там большие некоторые представлены но они эффективно справляются с задачей генерации кода причем генерации кода у них может быть трех типов это либо какая-то салфетка история когда мы даем самой какой-то ломки которые нам будет откуда генерировать для тех или иных задач какое-то задание она нам на основании этого задания генерит что-то это может быть какой-то ломки которые нам будет этот код генерировать для тех или иных задач какое-то задание она нам на основании этого задания генерит что-то это может быть какой-то evolution instruction когда мы хотим дать какую-то проблему которая нам не кажется достаточно серьезно или сложный и мы хотим каким-то образом с помощью какой-то инструкции для ломки и и усложнить это может быть достаточно интерактивный процесс. Мы еще поговорим о усложнении задач, связанных с кодом. Но это позволяет нам сильно увеличить сложность данных. Коли мы можем контролировать, на самом деле, сложность данных в обучении подобных модальностей, это открывает нам достаточно большой простор в плане построения каких-то эффективных методов обучения нашей ломки который будет связан с кодом можно использовать тот же сирикульным ленин который дает достаточно большой эффективности страдает от того что как раз у нас недостаточно данных которые разбиты по когортам сложности либо это может быть о с instruction когда у нас есть какой-то сниппет кода и на базе такого сниппета на может генерироваться целое множество проблем которые до этого не встречалось особо сильно извините пожалуйста да сейчас вот которые доселе нам не встречалось либо либо на которой у нас изначального решения не было, но при этом модель хорошо умеет генерировать подобные примеры. В целом у нас достаточно хорошо развитые алгоритмы, связанные с фильтрацией подобного, точнее какого-то плохого хода, который мы можем встретить в нашем притрейне, ровно как и истории, связанные с дедупликацией данных, которые нам позволяют активно как-то отсеять нежелательные какие-то вещи в притрейне, ровно потому, что мы умеем хорошо как раз-таки некоторые вещи заосердить, некоторые вещи мы семантически можем выискивать в наших данных, чтобы понять, насколько код этот был похож, потому что либо по выходам каких-то кусков кода мы можем проводить как раз-таки similarity search для того, чтобы эффективно дублицировать данные. Ну и самое важное, на самом деле, в плане обучения кода является контроль лицензий. Если мы видим какую-то копируют лицензию, мы, к сожалению, такое обычно должны фильтровать и не использовать в нашем обучении, потому что чревато какими-то последствиями. С токенизацией тут тоже ничего нового нет. Определенную токенизацию проводят по коду в рамках обучения модели. Здесь ровно такие же проблемы, как у обычных LLAM. Тут ничего в целом нового нет. Каким-то образом дезаблюцируют код, используют ровно те же техники, которые для естественного языка. В целом все понятно. Что можно поменять в обучении LLAM? на самом деле можно не менять ровным счетом ничего просто взяв большой пайл там естественного языка для того чтобы у нас модель понимал инструкции и взять тоже кучу данных связанных с кодом все это дело вместе как-то обучить подружить и у нас появится неплохой такой помощник но можно понятное дело взять какую-то готовую лампу которую у нас обучить, подружить, и у нас появится неплохой такой помощник. Но можно, понятное дело, взять какую-то готовую лампу, которая у нас обучена на естественном языке, разморозить несколько слоев, также обучить на коде, тоже проблем не возникнет, можно добавить какую-то голову, можно заадаптить под какую-то нашу историю, которая нам интересна. И все эти методы, они достаточно стандартны в целом для обучения простых LLM, просто новых задач на естественном языке. И при этом они достаточно эффективны для кода, как для модальности. И мы тут как раз-таки сейчас каких-то определенных проблем не встречаем. Хотя хотели бы иметь какие-то определенные структуры, архитектуры, модели которые могут эффективно работать с кодом, об этом еще поговорим. Но важной вещью также является возможность self-improve таких моделей, как отдельный просто процесс обучения на кормежке данных, собственно, для модельки, так или иначе, из-за того, что мы можем как раз таки проводить этап эволюция кода которые сгенерируют наша моделька мы можем давать фидбэк самой модели это такое у нас получается бесплатный рель где модельку как раз таки можно достаточно эффективно обучить чтобы она лишилась каких-то своих ошибок причем можно давать ассерты к написанию либо как вручную так и какой-то другой кодовые ломки там которые выигрывают сейчас всех на бенчмарках таким образом достаточно быстро сходясь в хорошем качестве для моделей. У нас может быть много связанных рисков, связанных с обучением модели, потому что зачастую в любых проскрапленных данных, даже в стеке 2, встречаются какие-то куски кода, которые могут быть связаны с лицензией, а это нарушение каких-то авторских прав. Достаточно много всяких маловеров, то есть вирусов, либо какой-то нехороший ход, который мог быть даже выложен в рамках просто ознакомительных внутри гитхаба в качестве учебного материала и так далее, который так или иначе могут привести к тому, что помощник разработчика на его же мощностях может запустить какой-то вирус. Такие истории, наверное, есть, я о них точно не знаю, но риск точно имеется. Можно потерять какие-то или слить, по крайней мере, данные. В кусках кода нередко, как минимум, находятся какие-то сниппеты или примеры, которые могут содержаться в комментариях тех или иных персональных данных. Зачастую это тоже может являться большой проблемой. Мы должны уметь фильтровать такие куски кода. Плюс весь код, который так или иначе пошел в обучение, не факт, что является очень эффективным. И таким образом у нас появляется проблема, связанная с тем, что у нас выломка может научиться на неэффективных кусках кода. Эта история, опять же, у нас должна фикситься либо какой-то другой, более мощной лнмкой, которая будет проверять весь код, который у нас при трене находится, но это дорого, либо изначально мы должны иметь требования к тем кускам кода, которые мы должны будем рисовывать при трене. Да, множество всего, короче, может быть в целом связано с рисками, начиная от каких-то плохих, которые могут привести к нашим галлюцинациям или нежелательным потерям, заканчивая какой-то легальной историей. Что касается бенчмарков, из-за того, что мы эффективно умеем генерировать в целом куски кода, у нас тут проблем вообще никаких нет. Бенчмаркам завались. Единственное, что на текущий момент, наверное, плохо, то, что нет сложных бенчмарков, потому что на текущий момент не то чтобы все решения, несмотря на то, что у них кода достаточно много, не умеют в сложные задачи, связанные с кодингом. Об этом тоже обсудим. И, наверное, из всех б бичмарков даже которые сейчас здесь перечислены и которые развиваются даже на текущий момент там самым оптимальным самым лучшим решением является просто посмотреть как на битком бенчи у нас какая у нас там метрика посмотреть может быть какие-то более специфичные истории типа если мы хотим помочь когда-то scientist обучить то на ds1000 если мы хотим чтобы у нас там много языков знала то что-то еще проверить типа мультилингах и уменовал ну да сконцентрироваться на каких-то локальных проверках и какой какой-то общий бенчмарк вообще в целом среди всех моделей взять то за основу там биткотт бенч руки у нас поросли на самом деле из всех бенчмарков начиная с как раз таки human ивала который вышел достаточно давно где-то четыре года назад по моему его сделали как раз таки open ребята из упыная задача достаточно простая у нас есть какой-то код чаще всего это просто функция причем внутри функции описано док стрингом примерно как это должно выглядеть либо данные куски кода других функций может быть даже законченных либо внутри самой функции уже несколько строк реализована задача бенчмарка достаточно простая продолжить код по кому-то опредто определенному контексту, связанному либо с этой функцией, либо с несколькими функциями одновременно. И на основании того кода, который, собственно, сгенерится, строится метрика под названием pass это что-то там, в зависимости от того, за какое количество попыток нас устроит, что наша какая-то лампка пройдет тот или иной тест. Здесь мы хотим как раз-таки учитывать вариативность нашей лампки. Желательно, конечно, смотреть на метрику Passed-1, то есть нас интересует первая генерация, и она должна быть суперидеальной, но иногда смотрят Passed 10 обычно, Passed 5, то есть мы в целом допускаем, что часть генерации может быть плохая, но нас интересует хотя бы, чтобы один раз из пяти было хорошо. Понятное дело, что бенчмаркам сейчас особо сильно не следят. Не так давно сам Human Eval под названием Code Eval был адаптирован для русского языка, поэтому сейчас в России у нас за этим следят. Но сейчас бенчмарк сам по себе устарел, на нем выбиваются какие-то уже невероятные значения плюсом бичмарк особо не обновляется то есть там как же птичка точнее какая-то там агентура на джи птичка я нам победила в этом бичмаркетах там сейчас только одни джебите и в целом такой бичмарк особо не смотрит в отличие от дубик код бенча наследником хуманувала, они об этом напрямую пишут в своей статье, вот как раз-таки биткот-бенч является достаточно интересным и достаточно сложным бичмарком, причем есть сразу несколько версий как и просто фулл биткот-бенч, так и хард биткот-бенч. Метрики на нем пока что даже у самых классных моделей они не сильно большие. Это не может не радовать, значит, есть куда расти, значит, задачи действительно сложные. Суть такого бенча заключается в том, что у нас опять же есть какая-то функция, нам эту функцию надо как-то закончить. Но при этом все наши докстринги, они оформлены определенным образом. Есть какие-то параметры, которые тоже важны правильно как-то задать, либо прочитать с другого контекста, например, с наших импортов, что это за параметры вообще могут быть. Нам надо что-то вернуть, какую-то ошибку, возможно, поднять. У нас есть определенные рек реквайрменты мы можем немножко зафишотить нашу всю историю ну естественно как-то это все дело проверить какими-то как раз таки ассертами которые мы делаем причем все примеры которые в битком бенча я так или иначе находятся они верифицируются трехэтапно изначально генерируются вообще все эти примеры с помощью каких-то Да. И затем еще раз дополнительная проверка с помощью каких-то экспертов в области программирования, какие-то кросс-чеки. Ну, так или иначе, да, под людьми все курировано, так что там какие-то и суперидеальные примеры. Таких примеров может быть не очень много, 1140 всего, однако они хорошо разбиты по доменам, они задействуют так или иначе большинство библиотек, которые завязаны на деятельность разработчиков, и с чем они чаще всего встречаются. Однако, как вы могли заметить, все это дело только на питоне на текущий момент. По-моему, да. Если я не ошибаюсь, это пока что только питон. Другие языки, сейчас даже посмотрим, по-моему, он где-то тут есть. Да, да, он только на питоне. Это огорчает, потому что зачастую мы хотим от помощника-разработчика, чтобы он нам не только на питоне помогал, однако на текущий момент это действительно сам лучший бенчмарк хотя вот этот трехэтапный ступенчатый какой-то анализ тех таз и кону дорогой и только адаптируется на текущий момент под какие-то другие языки ds1000 тоже очень похожий пример связанный с уже непосредственной работой дата-сайентистов. У нас тоже есть какое-то описание, возможно, дата-фреймов в пандасе. И мы должны на основании как раз-таки этой задачи заняться генерацией кода на основании какого-то кодового контекста и привести какое-то решение. И в дальнейшем его как-то засертить, что-то провести. Вот. Останавливаться особо сильно на нем не буду. В нем достаточно представимость данных по всем библиотекам, так или иначе, которые используют дата-сиентисты. Ну и, да, современные, собственно, модели, которые там побивают все это, естественно, GPT-Core, Cloud, DeepSeq. В целом ничего удивительного тут нет. Также хорошим примером бенчмарка является SVE-бенч. Он тоже зачастую очень часто используется при скорингах всех моделей, которые так или иначе связаны с кодом. Бенч является достаточно уникальным, интересным, ввиду того, что он решает задачи, он может ли какие-то, Спасибо. Примеры issues пропаршены с самими разработчиками этого бенчмарка. Там порядка 90 тысяч пиаров было проанализировано так или иначе. Иши с ним были все просмотрены. Иши обычно идут с какими-то кусками кода. И мы должны на основании как раз-таки всех данных, которые приведены в том или ином мышью сгенерировать либо код который сможет помочь решить это ищу и потом собственно сгенерировать тесты но не до провести тесты под него потому что на в этом бенчмарке они заранее все известны ну вот да и мы должны сгенерировать решение это конец вот на этом бенчмарке тоже не особо большие какие-то результаты и достаточно затюнены как будто бы только под код модели но выбиваются вперед то есть там не встретишь просто как там не знаю в дэсси джипе течет клауд нет тут встреч конкретно какие-то вещи типа у дикта которые как-то запромтировали клад там и свои агент опять же на клауде что-то связанное с чепики я не знаю название этого приложения так или иначе вот то есть созданы там людьми специальный промпт какой-то какая-то инструкция которая позволяет хорошо с этим печем как-то справляться можно профильтровать внутри самого бенчмарка найти там непосредственно просто какие-то модельки но хотел показать именно вот это плюс метрики не особо сильно там опять же большие что говорит о сложности такого бенчмарка но это бенчмарк крайне полезен потому что мы хотим видеть дальнейшем от наших там кодовых помощников именно такой пример по использованию да там на бенчмарках по коду все мы хотим в целом в будущем от подобных помощников добиться как раз генерации сложного кода. Мультилайн это в целом уже достаточно сложная история. Это не просто закончить предложение, это еще и как-то продолжить его, написать несколько строчек кода. Но мы хотим, чтобы такие помощники думали куда дальше. Возможно, написали как и целый скрипт, который поможет решить проблему внутри какого-то кода, либо как-то оптимизирует целую библиотеку, либо вообще сгенерирует целую репо под какую-то задачу, которую можно будет из коробки запускать. Пока таких решений, к сожалению, на рынке либо крайне мало и они неэффективны, либо вообще в целом нет. Мы хотим изобрести, естественно, специальную архитектуру, адаптированную под код, ровно такую же, как мы это видели, к примеру, в модальности по видео, когда мы делали какой-то проектор на токены, естественно, нового языка, тот же проектор, к примеру, для кода. В целом, возможно, был бы хорошей идеей. Я таких исследований еще не видел, не находил. Возможно, мы хотим более умный способ работы с данными, иметь связанных с кодом, как-то лучше использовать какой-то фильтринг, понимать, какой код у нас может нести действительно большую ценность для обучения модели. Мы хотим изобрести крутые бенчмарки, связанные с код-геном, потому что все текущие бенчмарки, они так или иначе связаны на достаточно простые проблемы для текущих помощников. Помимо coldben чатам и свои обещал мало что супер сложного можно найти мы хотим поддержку иметь не только там питон как языка мы хотим вообще в целом все языки в том числе низкоуровневые которые представляются в текущем там ландшафте очень редко, очень мало, но так или иначе, как будто бы лампка умеет ходить в грамматике, поэтому why not, почему бы не обучить какие-то сложные, тяжелые языки, почему бы не синтезировать данные, которые могут быть для подобного языка релевантны. Мы хотим, естественно, иметь поддержку continuous learning. Тут суперсильно важно, напоминаю, что такое это, когда мы продолжаем обучаться даже после тренинга какой-то нашей очередной модельке. Мы хотим понимать, какие кодовые фреймворки сейчас актуальны, что нам нужно сейчас обязательно, чтобы наша модель умела, чтобы она подстраивалась под текущие какие-то обстоятельства. К примеру, выходит новая статья про какую-то оптимизацию, и мы хотим, чтобы если она революционная была, то наш универсальный помощник поддерживал подобный алгоритм, который были предупреждать об этом пользователя предупреждая пользовательства возможных каких-то авторских правах либо об утечке данных и как-то нивелировать это это тоже будущий вызов как естественно решение особо нет теперь поговорим про аудио олег а можно вопрос про прокуда модельность у нас и у модели будет такое же свойство например если но когда обучают лнп больше часть данных на английском языке добавляет немного языков там пример русский китайский вот и модель начинаем но с меньшим количеством данных понимает уже другие языки такое же свойство есть на там например весь код на питоне практически весь да и сарказм то есть есть мы к примеру в какую-то питоновскую чисто модель засунем код связанный там все плюс плюс нам возможно выдастся код носит плюс плюс но возможно он будет нерабочий объясняю вообще почему такое явление вдруг может произойти. На самом деле, все данные, которые вот тут были рассмотрены на самом первом слайде, они и так или иначе используются в притринах, но при этом, почему у них такой большой размер? Не всегда это код. Во время процесса дедупликации порой мы фильтрируем все комментарии, однако чаще всего, даже при разработке гигакода нашего российского, мы эти комментарии все оставляем. В комментариях у нас находится куча интересной информации, которая связана, во-первых, с другими какими-то языками программирования зачастую, потому что вставляют, к примеру, докстринговый сниппет, там, кода на C++, перепиши это на Python или что-нибудь подобное. Так и в целом комментарии несут очень много технической информации, которая содержит как русский язык, так и китайский язык, так и английский, какие-то такие большие представленности. Поэтому представленность языков, она так или иначе есть, даже, блин, вот жалко ее не привел на стеки. Она достаточно большая, и все в целом ломки про нее шарят, но в основном, конечно, питон, но несмотря на то, что представимость, к примеру, какого-нибудь Котлина может от всего, от всех данных для обучения, но все равно у нас ломки достаточно хорошо запоминают подобные данные и могут спокойно потом генерировать данные, там, с Котлином связанные, даже несмотря на то, что там достаточно мало было примеров. Единственное, что проблема будет это хорошо как-то грамотно протестировать. У нас из всех бенчмарков, которые вообще в целом есть там для кода, у нас только парочка связанных с мультилингл истории. Так у нас есть только мультилингл HumanVal, но HumanVal как бенчмарк, он не слишком сложный, поэтому нам тяжело будет сказать, насколько хорошо наша модель справляется с этим. Но если мы разработаем какие-то определенные наши тесты, которые нам нужны для проверки эффективности работы на том или ином языке, нам, возможно, этого будет достаточно. Нужно ждать появления каких-то новых бенчмарков, которые нам помогут рассказать, типа, мультилингвулу, там, бигконд-бенч. К примеру, о качествах подобных моделей, которые будут ориентированы не только на питон, и было бы вообще суперславно, а так, в целом, все современные помощники, они так или иначе поддерживают практически все языки программирования, как и любая в целом LLM-ка, какую ни спроси, все в целом на каких-то, Да, спасибо. ответил да спасибо да супер да давайте говорим про модальность аудио честно это как одновременно достаточно простая тема так и крольче нора ввиду того что зачастую мы от аудио моделек не хотим добиваться того что мы просто положили какое-то текстовое описание наш какой-то аудио input и нам на выходе получился просто какой-то текстовое описание, наш какой-то аудиоинпут, и нам на выходе получился просто какой-то текст-инпут. Зачастую мы хотим, вот самую последнюю историю, которая реализована на картинке, это аудиоинпут и текст-промптинг, и на выходе мы получаем, зачастую нам даже текст-аутпут не нужен, мы хотим тоже получить аудио. Но про это рассказывать можно очень долго, там много подходов. По большей части сегодня сосредоточить просто на аудио как модальности, то есть мы добавляем, к примеру, какое-то аудио на вход, мы добавляем текст и получаем там текст на выход. Добавление там вукодеров несет определенные и какие-то добавочные применения похожих модальностей к примеру мы можем генерировать музыку достаточно эффективно причем неплохо делают это в современные модели но в основном мы хотим просумеризировать какое-то видео к примеру на ютюбе по аудио транскрипции здесь как как раз нам помогают speech плюс текст, то текст и story. Что там нужно нам как-то поменять? Но на самом деле я не стал сильно растягивать историю, связанную с этой модальностью. Ровно почему? Потому что она сильно не отличается от Vision и Story от слова совсем. Мы единственное, что подменменяем это какой-то visual энкодер на аудио энкодер и в целом все готово у нас есть одна небольшая проблема она знакома тем кто занимался там как раз таки аудио моделями у нас у токенов аудиудиотокенов, сильно выше значение эмбеддингов, чем у текстовых токенов. Это связано там со многими причинами, так или иначе. И для того, чтобы нам, когда мы делаем нашу там, прожектор, который нам будет это все одно как раз-то фьюзованное там пространство пихать, нам необходимо этот токен нормализовать, и в в целом но нормализация это там не то чтобы какой-то супер интересный процесс про это на и можно найти тысячи тысячи одну реализацию и статью в интернете поэтому тут тоже говорить об этом сильно не будут мы достаточно эффективно умеем это делать единственноеинственное, что об этом нужно знать, что там как раз-таки в нашем, когда мы подаем все в фьюзированный там embedding space, у нас могут быть разные значения там у аудиотокенов и текст-токенов, а желательно, чтобы они были равно распределены, потому что это все-таки должно слиться в какую-то одну интерлифт информацию, когда мы должны учитывать и то, и другое одновременно и правильно, так, чтобы у нас там не было переобучения какой-то из вот этих двух историй. В плане бенчмаркирования люди сделали все очень просто. На самом деле мы проверяем любую модель, которая так или иначе затрагивает у нас аудиоинпут, как обычно текстовую модель. Аудиоинпут порой тестируем отдельно, просто за инструктив модель на задачу automatic speech recognition, то есть у нас есть какое-то голосовое сообщение, которому мы подаем модель, мы промтим ее, чтобы она написала нам транскрипцию этого звука, модель выдает транскрипцию звука, и мы меряем по всем тем же как раз таки методам, которые современные люди меряют, Automatic Speech Recognition Service. Основной метрикой во всех этих сервисах является Word Error Rate, есть модификации Character Error Rate, есть Sentence Error Rate и так далее, и так далее, но в основном вер. Вер меряется просто как количество замен, количество вставок и количество удаления тех или иных символов на общее число символов в оригинальном сообщении. То есть у нас есть, к примеру, оригинал, у нас слово и транскрипция, у нас слово «квик» поменялось на «браун», а точнее «квик» вообще убралось, осталось только «браун», и «лэйзи док» там добавилось. К примеру, какое-то слово могло неправильно транскрибироваться. И мы посчитаем количество вот этих вставок, удалений и замен, разделим на общее число слов в оригинальном предложении, получим значение веры. У современных моделей оно очень хорошее, порядка 3%. То есть в целом очень редкие какие-то ошибки, и зачастую мы никак не хотим учитывать аудиосоставляющую таких моделей, потому что ошибка там маленькая, это не картиночная история, где ошибки могут быть достаточно большие, и там вообще нет какой-то определенной истины вот для аудио данных в аудио энкодера в любом случае решил привести конкретно для российского рынка для не российского рынка можно найти где угодно и большем количестве часов записи основным источником данных является области ти на русском там порядка двадцати тысяч часов записи продиктованного текста в абсолютно различных доменах. Это голос от Сбера тоже очень-очень много часов, порядка 18 тысяч. Транскрибации происходят обычно с каких-то радиоэфиров, потому что очень хороший источник данных для нас, там постоянно люди говорят, поэтому давайте запишем большое количество эфиров, заставим людей там все это транскрибировать, и потом на этом все деле обучимся. Паблик спичей, ютуба, аудиокниг, звонков и прочие какие-то истории. И есть еще небольшой LibreSpeech на 98 часов записи, но его зачастую используют как какую-то тестовую выборку для проверки навыков. Как раз просто посчитать веру. Единственным исключением, наверное, из правил, больше бенчей вы не найдете, наш любимый и знакомый BigBench, только теперь с приставкой аудио тоже тысячи там аудио каких-то вопросов мы хотим посмотреть на как раз таки какой-то спичи лиза нинг связать аудио наше сообщение с сообщением на текстовом языке что-то померить но больших отличий там от любого big bench натуральнома на натуральном языке на самом деле нет. Это тот же бигбенч, только вместе с голосовыми сообщениями. Поэтому он достаточно сильно там урезанный. Но при этом учитывается несколько задач, как текст-то текст, так спич-то спич, текст-то спич и спич-то текст для моделей, потому что там есть такая вот модификация у этого бенчмарка, так или иначе до закончили на самом деле с модальностями проуди больше углубляться не будем по прокат рассказали про vision тоже да теперь кратко давайте расскажем как вообще в целом занимаются тем что собирают сервис на базе л.м. Моделька у нас готовая есть. Теперь нам необходимо ее обернуть в какой-то сервис, который действительно на нашем железе достаточно бы эффективно работал. Причем мы предъявляем сразу несколько требований к нашему сервису. Естественно, он должен быть быстрый. Медленные сервисы нас не особо сильно интересуют. Дожидаясь ответаод» любое физическое лицо, которое пользуется твоим сервисом, просто уйдет. У этого сервиса должна быть достаточно большая пропускная способность, то есть мы хотим, чтобы наша скорость не сильно страдала при огромном потоке пользователей со своими запросами в наш сервис. Ну, естественно, фиар-сервис должен функционировать, функционировать неправильно, без нарушения функциональности, и он должен быть стабильный во времени, чтобы никогда не падал и так далее. Но наши все основные требования как раз таки связаны со скоростью и с рутпутом. Объясняю, почему. почему зачастую сервисы вообще делятся на два типа это тут презентации не представил поэтому голос там расскажу и у каждого есть какая-то своя метрика который является основной так к примеру если мы делаем сервис по типа нейро который яндексовская мы на самом деле не хотим добиться от нее как мы не хотим чтобы потому что она очень часто занимается там к примеру кем-то задачами связаны про суммаризируем не это видео в это самый популярный запрос там у Нейра, берутся какое-то видео там на Ютьюбе, мы хотим, чтобы оно просуммеризировалось. Нам не суть важна тут tokens per second, на самом деле, нам здесь очень будет важно там как раз таки пропускная способность, то есть мы хотим, чтобы при очень большой нагрузке у нас наш сервис все равно стабильно работал, выдавая тот же токен сперсеконс, который он выдает в обычном режиме, чтобы это сильно не страдало. Когда в каких-то онлайн ассистентах, к примеру, если вы зайдете сейчас в Telegram, напишите там гигачату что-то, нам тут в целом будет важно, конечно, токен с Persecon super важно, но нам будет важно и Time-to-First токен. Это самый первый шаг для любой ломки, потому что нужно заниматься контекст-декодингом, нужно простоять там какое-то время в очереди на запрос потому что там пропускная способность можно позволить да вы сильно от токенизации будет все это дело зависеть только с декодинг нам позволяет как раз таки ломки сначала обработать ваш запрос сгенерировать на него ответ а затем уже как раз таки итеративно идти, и это будет сильно быстрее. Поэтому зачастую у нас токен сперсеконс, как в метрике качества работы сервиса, не учитывается порой первый токен, его отбрасывают, потому что это как отдельная метрика у нас. Хотя можно и не отбрасывать в целом, тогда будут чуть другие, но очень схожие все равно значения. Современные модельки как-то распределены на этом графике по как раз таки двум этим метрикам. Самый идеальный квадрант у нас очень маленькое время на time to first token и очень большое количество tokens per seconds. Понятное дело, такого достигают обычно какие-то маленькие модельки, либо модельки с приставкой Flash, ну а какие-то очень мощные модели, у них достаточно мощный какой-то декодинг происходит, типа DeepSeq, у них очень большой обычно Time-to-First токен, но при этом может сильно отличаться в зависимости от задач, которые ставятся перед моделькой по tokens-per-sequence. В tokens-per-sequence очень важно в целом учитывать то, с чем мы работаем. Так у нас может быть история связана с тем, что нам важно максимизировать TPS по питону, по математике, по русскому языку или по английскому. И мы можем очень быстро заметить, что токены на самом деле по разным доменам, они сильно разные. Поэтому нужно обращать на это внимание. Иногда производится замер по TPS сильноиленно сказать смещенный виду того что не знаю мы там мерим там тпс джипе течет и мерим тпс там не знаю код помощника как раз разработчика и мы знаем что там фертильность токенизации то есть в среднем размер токена какой-то он на коде сильно выше чем на русском языке так мы можем наблюдать здесь что у нас там здесь видно средний там длина токена g5 и 4 она там четыре с чем-то на русском языке это два и там два да то есть ну сгенерировать такой токен сгенерировать другой на самом деле требует разных скоростей вдруг и о чем очень хочется сильно поговорить что вообще так или иначе повлияет на риски использования лам это промерч методы оптимизации которые в сервис используют вдруг мы очень быстро на самом деле начинает понимать что все наши модели которые мы разработали безумно долгие и требует очень много памяти излишней памяти мы это все можем сильно оптимизировать при этом никак не потеряв качестве зачастую оптимизации добавляя самый легкий способ это просто добавить новых железяк. Понятное дело, можно купить там самые современные видеокарточки и забыть о нашей текущей проблеме, но на самом деле мы можем куда лучше. Мы иногда можем заменить какие-то архитектурные способности моделей, там, маешки в целом, микшеров экспорта, они быстрее работают, чем обычные там Dense, да, нейросети. Но зачастую объединяют методы, связанные с какими-то архитектурными решениями внутри моделей и железную историю, то есть оптимизация именно хардвера, и скрестили это все дело в мерч-методы, когда мы достаточно эффективно в тех вещах, которые мы уже давно знаем, можем использовать управление виртуальной памятью, управление видеопамятью, так что у нас в целом все начнет считаться в несколько раз быстрее, при этом это никак не потеряет в качестве, что мы сохраняем ровно тоже логику работы так примеру самый эффективный способ вообще там по оптимизации любого лом сервисом и подробно сегодня разберем таковы кэш continuous бачинг идея в нем супер просто она на картинке и мне кажется даже сильно там объяснять как-то это не надо мы Мы хотим, когда у нас вот такой вот бач, при этом он там заканчивается достаточно рано, мы хотим в оставшуюся часть бача, потому что она у нас просто западенная, вставить какой-то другой бач, который был там сильно меньше всего нашего сэмпла. И мы таких бачей скорее всего найдем. Так вот, к примеру, там в S1, вот эту всю историю, да, вместестился бач с 6 который состоял всего там из двух токенов условно здесь вместился там с 5 который тоже из двух токенов состоял при этом ну то есть 3 до едет из двух наверное тут просто парень и мы таким образом сильно скомпонуем наш пространство бачей куда который мы подаем нашу ломку, и сильно быстрее посчитаем. При этом мы будем очень хорошо знать, где у нас что заканчивается, потому что мы специальные токены будем использовать, которые end-of-sentence являются. Это Flash Attention, их целое семейство, тоже сегодня подробно о них поговорим. Это Page Detention, он ничего общего с словом Attention вообще не имеет, но подход очень интересный. Я отдельно статью тут оставил на эту всю историю но мы смотреть на него не будем там квантизованные лоры интересные методы квантизации лмк на int8 это использование там keybit precision для того чтобы там как-то заквантить наши параметры, при этом не сильно поменять в качестве. Я, кстати, наверное, отдельную скину, сегодня тоже насмотреть на это не будем. Фьюзирование слоев. Это больше архитектурная, конечно, история, нежели чем мерч. Но зачастую это используется, когда несколько слоев у нас просто по мощности обвиняются условно в один, и быстрее считаться начинает. И есть целые фреймворки для оптимизации, это там Petals и Swarm, но давайте начнем с Flash Attention. Здесь не сильно устали, но не так много осталось. Нет, это тот бач, который мы подаем на вход,alam, поэтому мы тут просто определенным образом... Синий — это паддинги, насколько я понимаю, либо какие-то специальные токены. Красный — это end of sentence, желтый — это собственно те данные, которые мы хотим подать. Просто это как пример компоновки бача, когда мыесто там весь у нас примеру весь наш вся наш весь наш бач заканчивается к примеру на из 6 до то есть что это мы предложение мукам полновали на самом деле всего в четыре хотя их было изначально 6 просто было очень много паддингов пробелов которые нам особо не нужны и мы это все дело скомпоновали континент патчем здесь хорошо работает до начнем с ваша тэншина это такая супер база для всех ломок сервисов так далее которые используются она по своим результатам в целом сильно меняет тренинг тайм, она меняет инференс тайм в том числе. Какие-то модельки, обученные Flash Attention, обычно показывают рост производительности в 3,5 раза. С чем это вдруг связано? С тем, что у нас подсчет нашего Self-тен шина на самом деле он очень не оптимизирован по памяти от слова совсем у него есть несколько операций операция там матричного перемножения операция взять и софт макса и операции еще одного матричного там перемножения но не суть важно какого потому что она нигде в целом особо не оптимизируется вот и на самом деле почет софтмакса и вот это матричное приложение которое первое там наших ковырился кейса но очень затратная и обычно как она делается во всех фреймворках они и опихают называемую там кардбэнд виз мемори но внутри джипы юхи GPU. Наши GPU можно представить как какую-то consistent память, наш какой-то как будто бы жесткий диск внутри GPU, и есть какая-то оперативка внутри GPU. Несмотря на то, что полтора терабайта в секунду, кажется, для GPU это все равно, это вау, какие скорости, но на самом деле на огромных пайлах данных и так далее это сильно замедляет процесс обучения и всего остального, когда у нас при этом есть очень маленькая, потому что там всего 20 гигабайт обычно, но при этом очень пропускная по своей способности виртуальная память внутри нашей гэпухи и flash attention они очень эффективно научились работать с этим небольшим кусочком как раз таки связанным с видео оперативной памятью они вся основная суть почему вдруг этот кусочек стал использоваться, ведь на него вроде не положишь целую матрицу, это в рамках как раз-таки самой первой реализации Flash Attention, в рамках перемножения матриц, это использование тайлинга, когда мы вместо подсчетов, когда мыи там когда мы считаем куб на кинока мы обычно там матрицу перемножаем как мы знаем как мы их перемножаем вот там используется просто более эффективный метод который резко сокращает там и количество операций но это окей то в рамках подсчета как раз таки софтмакса используется так называемый онлайн софтмакс, который считается у нас рекуррентным. И именно благодаря свойству рекуррентности при подсчете онлайн софтмакса мы очень хорошо и эффективно умеем хранить информацию как раз-таки о текущем рекуруррентном состоянии там этого softmax в очень быстрой памяти и это нам позволяет как раз таки вот эти две операции там тайлинга и онлайн софтмакса во-первых не просто быстро считать так еще и производить вычисления на одном ядре гпу при всем при этом прошлые все наши вычисления, они слабо параллелизовались, а сейчас это параллелизуется просто прекрасно. Так что вот на одном ядре GPU это все дело считается, и ввиду как раз-таки этого мы заметили там сильный прирост. Flash Attention 2, Flash Attention 3 и прочие какие-то модернизации подобных флешей, они так или иначе продолжают идею авторов изначального Flash Attention. Но Flash Attention сейчас в той или иной реализации нет ни одной, наверное, лампки, которая бы не использовала его. Не знаю, правда. Поэтому он сейчас везде. И при этом все мы не теряем качество от слова совсем никак, потому что мы получаем ровно тот же результат. То есть это действительно очень хорошая оптимизация работы алгоритма, причем не по какой-то computational cost. То есть мы в целом имеем ту же сложность алгоритма, которая была Как у нас работает LLAM-ка? У нас есть изначальный какой-то запрос, это, к примеру, 2 плюс 2. Оно отсылается к LLM, LLM генерирует будет, теперь 2 плюс 2 будет равно 2 плюс 2 равно 4. Ввиду того, что LLM у нас обычно это декодер, оно всегда берет какое-то предыдущее свое состояние и на основании этого предыдущее состояние генерирует следующий токен и так итеративно недурно можно заметить что на самом деле у нас есть в достаточно повторяющиеся куски даже не так бы вот эти все куски которые которые у нас были в качестве запроса, в качестве ответа, мы можем очень эффективно где-то хранить. Уже не дурно как идея. А теперь развеем эту идею совсем до крайностей. А почему бы нам просто вот эти куски кода не хранить в каком-то кэше, который у нас будет постоянно обновляться, а именно этот кэш, который у нас всегда заложен как значение, мы будем подавать всегда в LLM в качестве входного какого-то контекста. То есть просто базово хранить этот весь кэш будет куда удобнее и куда более быстрой какой-то реализацией, чем мы будем заново скармливать модели весь предыдущий какой-то контекст. Ведь у нас в целом есть по key value уже какие-то данные от модели, которые мы можем достаточно эффективно скармливать. Это особенно важно для Time-to-First токена. Точнее, наоборот, не сильно важно, потому что у нас первый контекст декодинга, он у нас никуда не пойдет, у нас KVCash появится только после этого. Однако этот способ сильно увеличивает ТПС, но в целом особо сильно никак не влияет в своем первоначальном виде на использование памяти. У нас все равно резервируется больше 30% на любой видюхе под KV-кэш. Однако, что очень важно, что вдруг научились его очень хорошо квантизовывать и оптимизировать. У нас была проблема, что изначально веса модели при маленьких контекстах, то есть при маленьком KV-кэше, они у нас занимали большую часть памяти, понятное дело, но этот KV-кэш при больших контекстах сильно растет, так что у нас не хватает памяти на эффективное хранение весов. И поэтому научились KVCash квантизовать. Причем с помощью квантизации KVCash можно, во-первых, сохранить те же самые результаты по модели, которые используются, но при этом сильно сократив потребление памяти на хранение кого каша и текущие как раз таки реализации там квантизации кого каша там кого квант просто и называется они позволяют им достичь как раз таки ломком на продакшене там контекстного окна в 10 миллионов токен ровно потому что вот это опимизированный KVCache, он хорошо поместится на любую железяку. И последнее, что мы рассмотрим. А мы просто на каждом шаге мы просто конкатим в конец добавляем имбединг токена, да? Да. Все просто. Очень просто. Все так. То есть тут нет какого-то супер ношества или гениальной идеи, просто обратили внимание, что у нас в целом какие-то куски текста, они постоянно повторяются, их решили отдельно в каком-то каше хранить, значение там, ключ значения по ним, все. Тут ничего супер особенного нету. Вот этот KVCash просто отвечает за какой-то контекст, который на каждом шаге модель сама себе дает. Вот она может к нему очень эффективно быстро обратиться. С помощью квантизации он еще и весить мало начинает. Да, и последняя история тоже, она просто более умная квантизация, нежели чем там просто заквантить весы и найти какой-то фактор квантизации. Это там LLMint8, там прям так называется. Что делают? Смотрят на значения любой на самом деле матрицы, находят как и какие-то условные обычные значения внутри матрицы, которые как-то равномерно распределены между самими собой, но в матрице мы также еще и чаще всего находим какие-то аутлайеры. Чаще всего из-за того, что у нас достаточно спорсированные порой бывают имбеддинги, у нас эти аутлайеры действительно хранят какую-то суперключевую информацию для лмки. было предложено давайте мы на самом деле нашего от лаера ввиду того что они несут достаточно большую смысловую нагрузку для наших там моделей не будем никак трогать в рамках монти зации потому что из-за там квантизации подобных параметров сильно потом может пострадать качество мы оставим их как есть и действительно можно действительно эти вещи оставить кисть но при этом заняться квантизации не аутлаеров просто каких-то значений которые так или иначе нашим там алгоритме распределены по стандартам алгоритм там квантизации и потом все это дело уметь эффективно объединять. Подробнее про алгоритм, то, как он выбирает аутлайеры, то, как он выбирает регулярные значения, тут решил не касаться, но идея тоже достаточно простая, интуитивная и на самом деле очень весомая, потому что зачастую все современные реализации, которые можно скачать с Hagenface, они поддерживают LMN8 внутри себя, и это дает еще меньшую просадку по качеству, чем при обычной квантизации полноценной. И при этом все позволяет сильно меньше памяти потреблять модели на инференсе, на обучении и так далее. Нам осталось поговорить по поводу фреймворков, на которых работают разработчики, которые выводят модели в прот. У нас есть несколько фреймворков, которые точно хотелось бы затронуть, не суперподробно, но просто хотя бы рассказать, которые там все open-source-ники так или иначе используют. Это TensorFlow, это больше такой production на самом деле фреймворк. Эти фреймворки все в целом зачем-то нужны. Они объединяют все все то что мы там обсудили до этого то есть какие-то методы там оптимизации какие-то ускорения там инференции поддержка стабильности там функционально и функциональность нашего сервиса как раз таки внутри себя имеют хорошую поддержку там на каком-то более низком уровне то то есть по общению с железяками. К примеру, TensorRT, это непосредственно разработано NVIDIA, оно поддерживает самые современные ядра. Если выкатывается драйвер на какой-нибудь H100 GPU, на который вы будете учить свою модель, то, скорее всего, TensorRT обновится сиюсекундно, и у вас будет самая современная поддержка без багов и так далее. В отличие от всех других фреймворков, потому что они зачастую просто реализованы китайцами, у которых хоть и есть какие-то свои GPU, они не так распространены, и все равно все метятся в использовании GPU от NVIDIA. При этом все, TensorRT,шен фреймворк в первую очередь и у него очень сложно порог входа не каждая лаборатория может позволить себе вдруг там взять и специалисты потом по этому фрейму и как-то работать продолжать зачастую самым популярным это в л л муется. У него очень простой сам по себе. У него хорошая скорость, как раз-таки, и инференс модели после определенных операций, которые этот фреймворк делает. Авторы VLLM как раз реализовали patched attention. Патчшин нам очень эффективно позволяет работать с кого кишон в удивление вот блочно как-то реализуют там какую-то структуру там по хранению этого кого каша в не особо разбираюсь честно вот очень популярный это можно по звездочкам увидеть его зачастую используют все современные там ломки которые не супер большие они там на в лами так иначе написано есть еще и ломде плой первые ребята которые там запустили там ламу один ламу 2 то сделали там от авторы континент бачинга тоже простой но гениальной идеи на тоже какой-то фреймворк вот они есть такие и заключительно что хотелось бы сказать а именно сделать какой-то определенный рекап зачем мы это вообще вдруг все прошли протяжении всех этих пяти лекций без что нас дальше ждет мы в целом поговорили на самой первой лекции что там генеративный искусственный интеллект это круто есть определенный понятно дело грехи на текущий момент есть какие-то нерешенные у него проблемы но что немаловажно то что есть определенные риски которые яв являются не просто рисками, что мы там денежку какую-то потеряем, но это там топ-2 рисков по версии там Международного экономического форума, такой самой большой, наверное, организации, которая там так или иначе занимается тем, что подсвечивает какие-то риски мировые именно, к чему вселушиваются там самой большой компании так далее то есть это очень авторитетный источник да и эти риски связаны с галлюцинациями считают сильно не просто галлюцинациями но и дезинформации считают очень опасными и очень важно нам добросовестно и очень качественно мерить в таком случае LLM. Мы поговорили на второй лекции про то, что используют вообще в рамках обучения LLM и какие модификации делают над LLM, которые так или иначе влияют на работу самой LLM, и их нужно учитывать. Вообще в целом, что нужно уметь все правильно измерять, нужно не просто вслепую бросаться на первый попавшийся бенчмарк но уметь как-то его оценить оценить особенности нашей лампки что она может что не может не используйте где она не может она там обучить ее тоже определенным образом и до рассмотрели модальности как следующий шаг развития вообще в целом всех LLM. Некоторые нюансы, связанные с их обучениями, о том, что модальность — это не просто какой-то black box, но это все-таки состоящий из каких-то различных энкодеров, проекторов, там, cross-attention в истории. И то, что их качество измерить, это достаточно большой челлендж. Ну и сегодня поговорили в целом о каких-то методах оптимизации, которые так или иначе используются, и которые могут нам так или иначе повлиять на картину, что мы можем увидеть, что там, хайф-модельки, которые мы загружаем там с хоггинфейса, они могут отличаться от того, что мы можем увидеть на сервисе. И поэтому от этого нам, собственно, правильно надо строить наше тестирование. Все дальнейшие лекции проведет Ваня Подпружников и Степан Пономарев, мои коллеги. В дальнейшем вас ждет достаточно увлекательное и долгое путешествие в мир раков, агентов, Спасибо. использовать, как узнать, что они правильно используются, и поговорим про какие-то реальные истории жизни. Степан как раз-таки по большей части сконцентрированный на диффузионных моделях расскажет ровно про них. Скорее всего, там самый современный доклад будет связан с текстом видео, той вещью, которая развивается меньше года.Появится ли на сегодня на самом деле нет не появится появится на субботу все что хотел сказать так всем спасибо есть есть вопросы буду рад слышать а искать понимание просто дамажка то быть да есть обычно домашка будет состоять из вас У вас будет, скорее всего, инференция ллмки, достаточно быстрой, очень надеюсь на это. И нужно будет эту ллмку уметь прогнать на бенчмарках, которые мы обсуждали как раз-таки в рамках этих лекций, возможно, в различных режимах, и написать по этому какие-то выгоды. В целом, это будет ровно про это. Спасибо. Сверхъестественного, да. Здесь сейчас еще есть вопросы, буду рад ответить. Но если вдруг нет, то пишите в чат. И тогда всем спасибо. Получается, Ваня начнет с четверга, а по поводу первого домашнего задания сброшу как раз-таки на неделю информацию. Всем хорошего вечера.\n"
     ]
    }
   ],
   "source": [
    "# HF code to use Whisper\n",
    "\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, pipeline\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16 # set your preferred type here \n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    setattr(torch.distributed, \"is_initialized\", lambda : False) # monkey patching\n",
    "device = torch.device(device)\n",
    "\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"antony66/whisper-large-v3-russian\", torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True,\n",
    "    # add attn_implementation=\"flash_attention_2\" if your GPU supports it\n",
    ")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"antony66/whisper-large-v3-russian\")\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=whisper,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=256,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# read your wav file into variable wav. For example:\n",
    "\n",
    "# wav = BytesIO()\n",
    "# with open('../LLM_valid_ru.wav', 'rb') as f:\n",
    "#     wav.write(f.read())\n",
    "# wav.seek(0)\n",
    "\n",
    "# # get the transcription\n",
    "# asr = asr_pipeline(wav, generate_kwargs={\"language\": \"russian\", \"max_new_tokens\": 256}, return_timestamps=False)\n",
    "\n",
    "# print(asr['text'])\n",
    "\n",
    "\n",
    "# Read the WAV file into a NumPy array (and get its sample rate)\n",
    "import soundfile as sf\n",
    "wav, sr = sf.read('../LLM_valid_ru.wav')\n",
    "\n",
    "# Optionally, check that your sample rate is what you expect (e.g. 16000 Hz)\n",
    "print(\"Sample rate:\", sr)\n",
    "\n",
    "# Now pass the numpy array to the ASR pipeline\n",
    "asr = asr_pipeline(wav, generate_kwargs={\"language\": \"russian\", \"max_new_tokens\": 256}, return_timestamps=False)\n",
    "print(asr['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 16000\n",
      "Audio duration: 4555.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b0cd1e49a4433d92059ee9cbb80ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription_timest.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def transcribe_audio(\n",
    "    input_file, \n",
    "    output_file, \n",
    "    batch_size=16, \n",
    "    cpu_cores=1, \n",
    "    language=\"russian\", \n",
    "    return_timestamps=False,\n",
    "    chunk_length=30,       # chunk length in seconds\n",
    "    overlap_seconds=5      # overlap in seconds between consecutive chunks\n",
    "):\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import soundfile as sf\n",
    "    from datasets import Dataset\n",
    "    from transformers import (\n",
    "        WhisperForConditionalGeneration, \n",
    "        WhisperProcessor, \n",
    "        pipeline,\n",
    "        WhisperTimeStampLogitsProcessor\n",
    "    )\n",
    "\n",
    "    torch_dtype = torch.bfloat16\n",
    "\n",
    "    # --- Load audio and split into overlapping chunks ---\n",
    "    wav, sr = sf.read(input_file)\n",
    "    print(\"Sample rate:\", sr)\n",
    "    duration = len(wav) / sr\n",
    "    print(f\"Audio duration: {duration:.2f} seconds\")\n",
    "    \n",
    "    # Compute step size for sliding window\n",
    "    step = chunk_length - overlap_seconds\n",
    "    chunks = []\n",
    "    start_times = []\n",
    "    current_start = 0.0\n",
    "    while current_start < duration:\n",
    "        current_end = current_start + chunk_length\n",
    "        start_idx = int(current_start * sr)\n",
    "        end_idx = int(min(current_end * sr, len(wav)))\n",
    "        chunks.append(wav[start_idx:end_idx])\n",
    "        start_times.append(current_start)\n",
    "        current_start += step\n",
    "\n",
    "    data = {\"audio\": chunks, \"start_time\": start_times}\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    # --- Helper function to process a batch of audio ---\n",
    "    def transcribe_batch(batch, asr_pipeline_local, language):\n",
    "        audio_list = batch[\"audio\"]\n",
    "        # Ensure each sample is a 1D float32 numpy array; pad them to the same length in the batch.\n",
    "        max_len = max(len(a) for a in audio_list)\n",
    "        padded_list = []\n",
    "        for a in audio_list:\n",
    "            a = np.array(a, dtype=np.float32).squeeze()\n",
    "            if a.ndim != 1:\n",
    "                raise ValueError(\"Expected a single channel (1D) audio array.\")\n",
    "            if len(a) < max_len:\n",
    "                a = np.pad(a, (0, max_len - len(a)), mode=\"constant\")\n",
    "            padded_list.append(a)\n",
    "        # Include the WhisperTimeStampLogitsProcessor in generate_kwargs.\n",
    "        results = asr_pipeline_local(\n",
    "            padded_list,\n",
    "            generate_kwargs={\n",
    "                \"language\": language, \n",
    "                \"max_new_tokens\": 256,\n",
    "                \"logits_processor\": [WhisperTimeStampLogitsProcessor(asr_pipeline_local.model.generation_config)]\n",
    "            }\n",
    "        )\n",
    "        if return_timestamps:\n",
    "            transcriptions = []\n",
    "            for r in results:\n",
    "                if \"chunks\" in r and r[\"chunks\"]:\n",
    "                    chunk_strs = []\n",
    "                    for ch in r[\"chunks\"]:\n",
    "                        # Get the timestamp list; if the ending timestamp is missing, output \"NA\"\n",
    "                        ts = ch.get(\"timestamp\", [None, None])\n",
    "                        start_time_chunk = ts[0] if ts[0] is not None else 0.0\n",
    "                        end_time_chunk = ts[1]\n",
    "                        if end_time_chunk is None:\n",
    "                            end_time_str = \"NA\"\n",
    "                        else:\n",
    "                            end_time_str = f\"{end_time_chunk:.2f}\"\n",
    "                        chunk_strs.append(f\"[{start_time_chunk:.2f}-{end_time_str}] {ch['text'].strip()}\")\n",
    "                    transcriptions.append(\" \".join(chunk_strs))\n",
    "                else:\n",
    "                    transcriptions.append(r[\"text\"].strip())\n",
    "            batch[\"transcription\"] = transcriptions\n",
    "        else:\n",
    "            batch[\"transcription\"] = [r[\"text\"].strip() for r in results]\n",
    "        return batch\n",
    "\n",
    "    # --- Worker function for multiprocessing on CPU ---\n",
    "    def transcribe_worker(batch, language, batch_size):\n",
    "        local_device = torch.device(\"cpu\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            \"antony66/whisper-large-v3-russian\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        processor = WhisperProcessor.from_pretrained(\"antony66/whisper-large-v3-russian\")\n",
    "        asr_pipeline_local = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=256,\n",
    "            chunk_length_s=chunk_length,\n",
    "            batch_size=batch_size,\n",
    "            return_timestamps=return_timestamps,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=local_device,\n",
    "        )\n",
    "        return transcribe_batch(batch, asr_pipeline_local, language)\n",
    "\n",
    "    # --- Decide processing mode (GPU or CPU multiprocess) ---\n",
    "    if torch.cuda.is_available() or cpu_cores == 1:\n",
    "        device_main = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            \"antony66/whisper-large-v3-russian\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        processor = WhisperProcessor.from_pretrained(\"antony66/whisper-large-v3-russian\")\n",
    "        asr_pipeline_global = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=256,\n",
    "            chunk_length_s=chunk_length,\n",
    "            batch_size=batch_size,\n",
    "            return_timestamps=return_timestamps,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=device_main,\n",
    "        )\n",
    "\n",
    "        def transcribe_batch_main(batch):\n",
    "            return transcribe_batch(batch, asr_pipeline_global, language)\n",
    "\n",
    "        dataset = dataset.map(transcribe_batch_main, batched=True, batch_size=batch_size)\n",
    "    else:\n",
    "        dataset = dataset.map(\n",
    "            transcribe_worker,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            num_proc=cpu_cores,\n",
    "            fn_kwargs={\"language\": language, \"batch_size\": batch_size}\n",
    "        )\n",
    "\n",
    "    # --- Combine transcriptions and write to output file ---\n",
    "    # Write each chunk's transcription on a new line, along with its start time.\n",
    "    lines = []\n",
    "    for start, text in zip(dataset[\"start_time\"], dataset[\"transcription\"]):\n",
    "        lines.append(f\"[Chunk start {start:.2f}s]: {text}\")\n",
    "    full_transcription = \"\\n\".join(lines)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_transcription)\n",
    "\n",
    "    print(\"Transcription saved to\", output_file)\n",
    "    return full_transcription\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    transcribe_audio(\n",
    "        input_file=\"../LLM_valid_ru.wav\",\n",
    "        output_file=\"transcription_timest.txt\",\n",
    "        batch_size=16,\n",
    "        cpu_cores=16,  # adjust as desired\n",
    "        language=\"russian\", \n",
    "        return_timestamps=True,\n",
    "        chunk_length=30,\n",
    "        overlap_seconds=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incl. hot words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(\n",
    "    input_file, \n",
    "    output_file, \n",
    "    batch_size=16, \n",
    "    cpu_cores=1, \n",
    "    language=\"russian\", \n",
    "    return_timestamps=False,\n",
    "    chunk_length=30,       # chunk length in seconds\n",
    "    overlap_seconds=5,     # overlap in seconds between consecutive chunks\n",
    "    hot_words=None,        # list of hot words to bias (e.g. [\"ООО\", \"ИП\", \"РФ\"])\n",
    "    hot_word_bias=5.0      # bias value added to logits for each hot word token\n",
    "):\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import soundfile as sf\n",
    "    from datasets import Dataset\n",
    "    from transformers import (\n",
    "        WhisperForConditionalGeneration, \n",
    "        WhisperProcessor, \n",
    "        pipeline,\n",
    "        WhisperTimeStampLogitsProcessor,\n",
    "        LogitsProcessor\n",
    "    )\n",
    "    \n",
    "    torch_dtype = torch.bfloat16\n",
    "    \n",
    "    # --- Load audio and split into overlapping chunks ---\n",
    "    wav, sr = sf.read(input_file)\n",
    "    print(\"Sample rate:\", sr)\n",
    "    duration = len(wav) / sr\n",
    "    print(f\"Audio duration: {duration:.2f} seconds\")\n",
    "    \n",
    "    # Compute step size for sliding window\n",
    "    step = chunk_length - overlap_seconds\n",
    "    chunks = []\n",
    "    start_times = []\n",
    "    current_start = 0.0\n",
    "    while current_start < duration:\n",
    "        current_end = current_start + chunk_length\n",
    "        start_idx = int(current_start * sr)\n",
    "        end_idx = int(min(current_end * sr, len(wav)))\n",
    "        chunks.append(wav[start_idx:end_idx])\n",
    "        start_times.append(current_start)\n",
    "        current_start += step\n",
    "    \n",
    "    data = {\"audio\": chunks, \"start_time\": start_times}\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    # --- Define a custom logits processor for hot words ---\n",
    "    class HotWordsBiasProcessor(LogitsProcessor):\n",
    "        def __init__(self, hotwords_ids, bias_value=5.0):\n",
    "            self.hotwords_ids = hotwords_ids\n",
    "            self.bias_value = bias_value\n",
    "        def __call__(self, input_ids, scores):\n",
    "            # scores: tensor of shape (batch_size, vocab_size)\n",
    "            for token_id in self.hotwords_ids:\n",
    "                scores[:, token_id] += self.bias_value\n",
    "            return scores\n",
    "    \n",
    "    # --- Helper function to process a batch of audio ---\n",
    "    def transcribe_batch(batch, asr_pipeline_local, language):\n",
    "        audio_list = batch[\"audio\"]\n",
    "        # Ensure each sample is a 1D float32 numpy array; pad them to the same length.\n",
    "        max_len = max(len(a) for a in audio_list)\n",
    "        padded_list = []\n",
    "        for a in audio_list:\n",
    "            a = np.array(a, dtype=np.float32).squeeze()\n",
    "            if a.ndim != 1:\n",
    "                raise ValueError(\"Expected a single channel (1D) audio array.\")\n",
    "            if len(a) < max_len:\n",
    "                a = np.pad(a, (0, max_len - len(a)), mode=\"constant\")\n",
    "            padded_list.append(a)\n",
    "        \n",
    "        # Build the list of logits processors.\n",
    "        logits_processors = []\n",
    "        if return_timestamps:\n",
    "            logits_processors.append(WhisperTimeStampLogitsProcessor(asr_pipeline_local.model.generation_config))\n",
    "        if hot_words:\n",
    "            # Convert each hot word to token id(s) using the pipeline's tokenizer.\n",
    "            hotwords_ids = []\n",
    "            for word in hot_words:\n",
    "                token_ids = asr_pipeline_local.tokenizer.encode(word, add_special_tokens=False)\n",
    "                if token_ids:\n",
    "                    # Here we simply take the first token id.\n",
    "                    hotwords_ids.append(token_ids[0])\n",
    "            if hotwords_ids:\n",
    "                logits_processors.append(HotWordsBiasProcessor(hotwords_ids, bias_value=hot_word_bias))\n",
    "        \n",
    "        results = asr_pipeline_local(\n",
    "            padded_list,\n",
    "            generate_kwargs={\n",
    "                \"language\": language, \n",
    "                \"max_new_tokens\": 256,\n",
    "                \"logits_processor\": logits_processors\n",
    "            }\n",
    "        )\n",
    "        if return_timestamps:\n",
    "            transcriptions = []\n",
    "            for r in results:\n",
    "                if \"chunks\" in r and r[\"chunks\"]:\n",
    "                    chunk_strs = []\n",
    "                    for ch in r[\"chunks\"]:\n",
    "                        ts = ch.get(\"timestamp\", [None, None])\n",
    "                        start_time_chunk = ts[0] if ts[0] is not None else 0.0\n",
    "                        end_time_chunk = ts[1]\n",
    "                        if end_time_chunk is None:\n",
    "                            end_time_str = \"NA\"\n",
    "                        else:\n",
    "                            end_time_str = f\"{end_time_chunk:.2f}\"\n",
    "                        chunk_strs.append(f\"[{start_time_chunk:.2f}-{end_time_str}] {ch['text'].strip()}\")\n",
    "                    transcriptions.append(\" \".join(chunk_strs))\n",
    "                else:\n",
    "                    transcriptions.append(r[\"text\"].strip())\n",
    "            batch[\"transcription\"] = transcriptions\n",
    "        else:\n",
    "            batch[\"transcription\"] = [r[\"text\"].strip() for r in results]\n",
    "        return batch\n",
    "    \n",
    "    # --- Worker function for multiprocessing on CPU ---\n",
    "    def transcribe_worker(batch, language, batch_size):\n",
    "        local_device = torch.device(\"cpu\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            \"antony66/whisper-large-v3-russian\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        processor = WhisperProcessor.from_pretrained(\"antony66/whisper-large-v3-russian\")\n",
    "        asr_pipeline_local = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=256,\n",
    "            chunk_length_s=chunk_length,\n",
    "            batch_size=batch_size,\n",
    "            return_timestamps=return_timestamps,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=local_device,\n",
    "        )\n",
    "        return transcribe_batch(batch, asr_pipeline_local, language)\n",
    "    \n",
    "    # --- Decide processing mode (GPU or CPU multiprocess) ---\n",
    "    if torch.cuda.is_available() or cpu_cores == 1:\n",
    "        device_main = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            \"antony66/whisper-large-v3-russian\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        processor = WhisperProcessor.from_pretrained(\"antony66/whisper-large-v3-russian\")\n",
    "        asr_pipeline_global = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=256,\n",
    "            chunk_length_s=chunk_length,\n",
    "            batch_size=batch_size,\n",
    "            return_timestamps=return_timestamps,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=device_main,\n",
    "        )\n",
    "    \n",
    "        def transcribe_batch_main(batch):\n",
    "            return transcribe_batch(batch, asr_pipeline_global, language)\n",
    "    \n",
    "        dataset = dataset.map(transcribe_batch_main, batched=True, batch_size=batch_size)\n",
    "    else:\n",
    "        dataset = dataset.map(\n",
    "            transcribe_worker,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            num_proc=cpu_cores,\n",
    "            fn_kwargs={\"language\": language, \"batch_size\": batch_size}\n",
    "        )\n",
    "    \n",
    "    # --- Combine transcriptions and write to output file ---\n",
    "    # Each chunk's transcription is written on a new line with its start time.\n",
    "    lines = []\n",
    "    for start, text in zip(dataset[\"start_time\"], dataset[\"transcription\"]):\n",
    "        lines.append(f\"[Chunk start {start:.2f}s]: {text}\")\n",
    "    full_transcription = \"\\n\".join(lines)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_transcription)\n",
    "    \n",
    "    print(\"Transcription saved to\", output_file)\n",
    "    return full_transcription\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    transcribe_audio(\n",
    "        input_file=\"../LLM_valid_ru.wav\",\n",
    "        output_file=\"transcription_timest.txt\",\n",
    "        batch_size=16,\n",
    "        cpu_cores=16,  # adjust as desired\n",
    "        language=\"russian\", \n",
    "        return_timestamps=True,\n",
    "        chunk_length=30,\n",
    "        overlap_seconds=5,\n",
    "        hot_words=[\"cross attention\", \"flash attention\", \"LLM\", \"throughput\",\"swe bench\", \"human eval\",  \"vision\", \"kv-cache\", \n",
    "                   \"bigbenchaudio\", \"multi-line\"]  # example hot words to bias the transcription\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
