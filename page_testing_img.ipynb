{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_nanodet_model():\n",
    "    \"\"\"\n",
    "    Loads and returns YOLOv5 model for person detection.\n",
    "    \"\"\"\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def detect_person(image, detection_model, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Runs person detection and returns bounding boxes.\n",
    "    \"\"\"\n",
    "    results = detection_model(image)\n",
    "    detections = []\n",
    "    for det in results.xyxy[0].cpu().numpy():\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        if int(cls) == 0 and conf >= conf_threshold:  # class 0 is person\n",
    "            bbox = (int(x1), int(y1), int(x2), int(y2))\n",
    "            detections.append((bbox, float(conf), \"person\"))\n",
    "    return detections\n",
    "\n",
    "def determine_corner(image_shape, bbox, corner_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Determines if person is in a corner.\n",
    "    \"\"\"\n",
    "    height, width = image_shape[:2]\n",
    "    center_x = (bbox[0] + bbox[2]) / 2\n",
    "    center_y = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "    is_right = center_x > width * (1 - corner_threshold)\n",
    "    is_left = center_x < width * corner_threshold\n",
    "    is_top = center_y < height * corner_threshold\n",
    "    is_bottom = center_y > height * (1 - corner_threshold)\n",
    "    \n",
    "    if is_top and is_right:\n",
    "        return 'top_right', True\n",
    "    elif is_top and is_left:\n",
    "        return 'top_left', True\n",
    "    elif is_bottom and is_right:\n",
    "        return 'bottom_right', True\n",
    "    elif is_bottom and is_left:\n",
    "        return 'bottom_left', True\n",
    "    \n",
    "    return None, False\n",
    "\n",
    "def draw_detection(image, bbox, corner_type=None):\n",
    "    \"\"\"\n",
    "    Draws detection bbox and info on image.\n",
    "    \"\"\"\n",
    "    img_draw = image.copy()\n",
    "    cv2.rectangle(img_draw, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    \n",
    "    if corner_type:\n",
    "        text = f\"{corner_type}\"\n",
    "        cv2.putText(img_draw, text, (bbox[0], bbox[1] - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return img_draw\n",
    "\n",
    "def analyze_folder(folder_path, output_folder=\"debug_output\", min_det_conf=0.5):\n",
    "    \"\"\"\n",
    "    Analyzes all images in a folder for person detection and corners.\n",
    "    \"\"\"\n",
    "    # Create output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading detection model...\")\n",
    "    detection_model = load_nanodet_model()\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(folder_path) \n",
    "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # keep 50 random images\n",
    "    # random.shuffle(image_files)\n",
    "    # image_files = image_files[:50]\n",
    "    \n",
    "    # Process each image\n",
    "    corner_detections = defaultdict(list)  # Store detections by corner type\n",
    "    \n",
    "    print(f\"\\nProcessing {len(image_files)} images...\")\n",
    "    for idx, img_file in enumerate(image_files):\n",
    "        print(f\"\\nProcessing image {idx + 1}/{len(image_files)}: {img_file}\")\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Failed to load image: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect person\n",
    "        detections = detect_person(image_rgb, detection_model, min_det_conf)\n",
    "        \n",
    "        if not detections:\n",
    "            print(\"No person detected\")\n",
    "            continue\n",
    "        \n",
    "        # Get first person detection\n",
    "        bbox, conf, _ = detections[0]\n",
    "        \n",
    "        # Check if person is in corner\n",
    "        corner_type, is_corner = determine_corner(image.shape, bbox)\n",
    "        \n",
    "        if is_corner:\n",
    "            print(f\"Found person in {corner_type} corner\")\n",
    "            print(f\"bbox: ({bbox[0]}, {bbox[1]}, {bbox[2]}, {bbox[3]})\")\n",
    "            \n",
    "            # Draw and save detection\n",
    "            img_draw = draw_detection(image_rgb, bbox, corner_type)\n",
    "            \n",
    "            # Save annotated image\n",
    "            output_path = os.path.join(output_folder, f\"detected_{img_file}\")\n",
    "            Image.fromarray(img_draw).save(output_path)\n",
    "            \n",
    "            # Store detection\n",
    "            corner_detections[corner_type].append({\n",
    "                'file': img_file,\n",
    "                'bbox': bbox\n",
    "            })\n",
    "        else:\n",
    "            print(\"Person not in corner\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nDetection Summary:\")\n",
    "    for corner_type, detections in corner_detections.items():\n",
    "        print(f\"\\n{corner_type} corner:\")\n",
    "        print(f\"Found {len(detections)} images\")\n",
    "        if detections:\n",
    "            print(\"Bounding boxes:\")\n",
    "            for det in detections:\n",
    "                print(f\"File: {det['file']}\")\n",
    "                print(f\"bbox: {det['bbox']}\")\n",
    "    \n",
    "    return corner_detections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_bbox(detections):\n",
    "    \"\"\"\n",
    "    Calculate maximum bounding box that covers all detections.\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return None\n",
    "        \n",
    "    # Initialize with first bbox\n",
    "    first_bbox = detections[0]['bbox']\n",
    "    max_bbox = list(first_bbox)  # Convert to list for modification\n",
    "    \n",
    "    # Find max bounds across all detections\n",
    "    for det in detections[1:]:\n",
    "        bbox = det['bbox']\n",
    "        max_bbox[0] = min(max_bbox[0], bbox[0])  # min x1  # max top\n",
    "        max_bbox[1] = min(max_bbox[1], bbox[1])   # min y1  # min bottom - 30 pixels\n",
    "        max_bbox[2] = max(max_bbox[2], bbox[2])  # max x2  # max left + 30 pixels\n",
    "        max_bbox[3] = max(max_bbox[3], bbox[3])  # max y2  # min right\n",
    "        \n",
    "    print(f\"Max bbox pre_adjustments: {max_bbox}\")\n",
    "    \n",
    "    # add safety margin\n",
    "    margin = 30\n",
    "    \n",
    "    max_bbox[0] = max_bbox[0] - margin\n",
    "    max_bbox[3] = max_bbox[3] + margin\n",
    "    \n",
    "    print(f\"Max bbox post_adjustments: {max_bbox}\")\n",
    "    \n",
    "        \n",
    "    return tuple(max_bbox)\n",
    "\n",
    "\n",
    "\n",
    "def find_black_border(image, side='top'):\n",
    "    \"\"\"\n",
    "    Find where black border ends for a given side.\n",
    "    Returns the coordinate where color changes significantly from black.\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    threshold = 30  # threshold for black color\n",
    "    \n",
    "    if side == 'top':\n",
    "        for y in range(height):\n",
    "            line = image[y, :]\n",
    "            if np.mean(line) > threshold:\n",
    "                return y\n",
    "    elif side == 'bottom':\n",
    "        for y in range(height-1, -1, -1):\n",
    "            line = image[y, :]\n",
    "            if np.mean(line) > threshold:\n",
    "                return y\n",
    "    elif side == 'left':\n",
    "        for x in range(width):\n",
    "            line = image[:, x]\n",
    "            if np.mean(line) > threshold:\n",
    "                return x\n",
    "    elif side == 'right':\n",
    "        for x in range(width-1, -1, -1):\n",
    "            line = image[:, x]\n",
    "            if np.mean(line) > threshold:\n",
    "                return x\n",
    "    \n",
    "    return 0 if side in ['top', 'left'] else (height-1 if side == 'bottom' else width-1)\n",
    "\n",
    "def find_borders_from_samples(images, n_samples=3):\n",
    "    \"\"\"\n",
    "    Find borders from random sample images.\n",
    "    Returns maximum border coordinates with added safety margin.\n",
    "    \"\"\"\n",
    "    if len(images) > n_samples:\n",
    "        sample_images = random.sample(images, n_samples)\n",
    "    else:\n",
    "        sample_images = images\n",
    "    \n",
    "    borders = []\n",
    "    for image in sample_images:\n",
    "        # Convert to grayscale for border detection\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        top = find_black_border(gray, 'top')\n",
    "        bottom = find_black_border(gray, 'bottom')\n",
    "        left = find_black_border(gray, 'left')\n",
    "        right = find_black_border(gray, 'right')\n",
    "        \n",
    "        borders.append((top, bottom, left, right))\n",
    "        print(f\"Found borders: top={top}, bottom={bottom}, left={left}, right={right}\")\n",
    "    \n",
    "    # Get maximum borders and add safety margin\n",
    "    max_borders = (\n",
    "        max(b[0] for b in borders),      # max top\n",
    "        min(b[1] for b in borders), # min bottom \n",
    "        max(b[2] for b in borders), # max left\n",
    "        min(b[3] for b in borders)       # min right\n",
    "    )\n",
    "    \n",
    "    return max_borders\n",
    "\n",
    "def process_images_with_bbox(folder_path, corner_detections, output_folder, debug_print = False):\n",
    "    \"\"\"\n",
    "    Process images with border removal and bbox handling.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Calculate max bbox for each corner type\n",
    "    max_bboxes = {}\n",
    "    for corner_type, detections in corner_detections.items():\n",
    "        max_bbox = find_max_bbox(detections)\n",
    "        if max_bbox:\n",
    "            print(f\"Max bbox for {corner_type}: {max_bbox}\")\n",
    "            max_bboxes[corner_type] = max_bbox\n",
    "    \n",
    "    # Step 1: First remove bbox areas (make them black)\n",
    "    print(\"\\nStep 1: Removing bbox areas...\")\n",
    "    images_without_bbox = []\n",
    "    filenames = []\n",
    "    \n",
    "    for corner_type, detections in corner_detections.items():\n",
    "        if corner_type not in max_bboxes:\n",
    "            continue\n",
    "            \n",
    "        max_bbox = max_bboxes[corner_type]\n",
    "        \n",
    "        for det in detections:\n",
    "            img_path = os.path.join(folder_path, det['file'])\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"Failed to load image: {det['file']}\")\n",
    "                continue\n",
    "            \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Make bbox area black\n",
    "            x1, y1, x2, y2 = max_bbox\n",
    "            image_rgb[y1:y2, x1:x2] = [0, 0, 0]\n",
    "            \n",
    "            images_without_bbox.append(image_rgb)\n",
    "            filenames.append(det['file'])\n",
    "    \n",
    "    # Step 2: Find borders from sample images\n",
    "    print(\"\\nStep 2: Finding borders from samples...\")\n",
    "    max_borders = find_borders_from_samples(images_without_bbox)\n",
    "    print(f\"Maximum borders: top={max_borders[0]}, bottom={max_borders[1]}, left={max_borders[2]}, right={max_borders[3]}\")\n",
    "    \n",
    "    \n",
    "    # Step 3: Process all images\n",
    "    print(\"\\nStep 3: Processing all images...\")\n",
    "    for i, (image, filename) in enumerate(zip(images_without_bbox, filenames)):\n",
    "        # Get original dimensions\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Convert to grayscale for border detection\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Find actual borders for this image\n",
    "        top = find_black_border(gray, 'top')\n",
    "        bottom = find_black_border(gray, 'bottom')\n",
    "        left = find_black_border(gray, 'left')\n",
    "        right = find_black_border(gray, 'right')\n",
    "        \n",
    "        # Cut out the non-black part of the image\n",
    "        cropped = image[top:bottom, left:right]\n",
    "        \n",
    "        # Create output image of the cropped size\n",
    "        processed = cropped.copy()\n",
    "        \n",
    "        # Get the max_bbox coordinates in original image space\n",
    "        max_bbox = max_bboxes[corner_type]\n",
    "        \n",
    "        # Transform max_bbox coordinates to cropped image space\n",
    "        # Ensure we get the full overlapping region\n",
    "        bbox_left = max(0, max_bbox[0] - left)  # Shift left by border amount\n",
    "        bbox_top = max(0, max_bbox[1] - top)    # Shift top by border amount\n",
    "        bbox_right = min(right - left, max_bbox[2] - left)  # Adjust right to cropped width\n",
    "        bbox_bottom = min(bottom - top, max_bbox[3] - top)  # Adjust bottom to cropped height\n",
    "        \n",
    "        # Fill the overlapping region with white\n",
    "        # Only fill if we have a valid region\n",
    "        if (bbox_right > bbox_left and bbox_bottom > bbox_top and\n",
    "            bbox_left < processed.shape[1] and bbox_top < processed.shape[0]):\n",
    "            processed[bbox_top:bbox_bottom, bbox_left:bbox_right] = [255, 255, 255]\n",
    "        \n",
    "        # Save processed image\n",
    "        output_path = os.path.join(output_folder, f\"processed_{filename}\")\n",
    "        Image.fromarray(processed).save(output_path)\n",
    "        \n",
    "        # Save debug version showing the transformations\n",
    "        if debug_print:\n",
    "            debug = image.copy()\n",
    "            # Draw original borders in green\n",
    "            cv2.rectangle(debug, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            # Draw max_bbox in original coordinates in red\n",
    "            cv2.rectangle(debug, \n",
    "                         (max_bbox[0], max_bbox[1]), \n",
    "                         (max_bbox[2], max_bbox[3]), \n",
    "                         (0, 0, 255), 2)\n",
    "            debug_path = os.path.join(output_folder, f\"debug_{filename}\")\n",
    "            Image.fromarray(debug).save(debug_path)\n",
    "        \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Processed {len(images_without_bbox)} images\")\n",
    "    print(f\"Output saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kolyangg/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-2-13 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 50 images...\n",
      "\n",
      "Processing image 1/50: frame_87600.png\n",
      "Found person in top_right corner\n",
      "bbox: (1566, 0, 1848, 167)\n",
      "\n",
      "Processing image 2/50: frame_90950.png\n",
      "Found person in top_right corner\n",
      "bbox: (1560, 0, 1849, 169)\n",
      "\n",
      "Processing image 3/50: frame_99350.png\n",
      "Found person in top_right corner\n",
      "bbox: (1579, 0, 1846, 172)\n",
      "\n",
      "Processing image 4/50: frame_20300.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1853, 169)\n",
      "\n",
      "Processing image 5/50: frame_25200.png\n",
      "Found person in top_right corner\n",
      "bbox: (1575, 0, 1848, 170)\n",
      "\n",
      "Processing image 6/50: frame_14550.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 1, 1851, 168)\n",
      "\n",
      "Processing image 7/50: frame_54925.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1849, 168)\n",
      "\n",
      "Processing image 8/50: frame_40125.png\n",
      "Found person in top_right corner\n",
      "bbox: (1560, 0, 1849, 167)\n",
      "\n",
      "Processing image 9/50: frame_41525.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1851, 171)\n",
      "\n",
      "Processing image 10/50: frame_12775.png\n",
      "Found person in top_right corner\n",
      "bbox: (1562, 0, 1851, 169)\n",
      "\n",
      "Processing image 11/50: frame_108375.png\n",
      "Found person in top_right corner\n",
      "bbox: (1574, 0, 1803, 168)\n",
      "\n",
      "Processing image 12/50: frame_36500.png\n",
      "Found person in top_right corner\n",
      "bbox: (1557, 0, 1853, 168)\n",
      "\n",
      "Processing image 13/50: frame_72550.png\n",
      "Found person in top_right corner\n",
      "bbox: (1584, 1, 1819, 169)\n",
      "\n",
      "Processing image 14/50: frame_40425.png\n",
      "Found person in top_right corner\n",
      "bbox: (1559, 0, 1848, 169)\n",
      "\n",
      "Processing image 15/50: frame_40325.png\n",
      "Found person in top_right corner\n",
      "bbox: (1559, 0, 1848, 168)\n",
      "\n",
      "Processing image 16/50: frame_86625.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1847, 167)\n",
      "\n",
      "Processing image 17/50: frame_63900.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1850, 171)\n",
      "\n",
      "Processing image 18/50: frame_33125.png\n",
      "Found person in top_right corner\n",
      "bbox: (1560, 0, 1849, 168)\n",
      "\n",
      "Processing image 19/50: frame_59775.png\n",
      "Found person in top_right corner\n",
      "bbox: (1561, 0, 1851, 168)\n",
      "\n",
      "Processing image 20/50: frame_107975.png\n",
      "Found person in top_right corner\n",
      "bbox: (1570, 0, 1809, 167)\n",
      "\n",
      "Processing image 21/50: frame_36400.png\n",
      "Found person in top_right corner\n",
      "bbox: (1557, 0, 1852, 168)\n",
      "\n",
      "Processing image 22/50: frame_43925.png\n",
      "Found person in top_right corner\n",
      "bbox: (1581, 0, 1825, 168)\n",
      "\n",
      "Processing image 23/50: frame_106650.png\n",
      "Found person in top_right corner\n",
      "bbox: (1571, 0, 1807, 169)\n",
      "\n",
      "Processing image 24/50: frame_11525.png\n",
      "Found person in top_right corner\n",
      "bbox: (1615, 9, 1803, 169)\n",
      "\n",
      "Processing image 25/50: frame_99900.png\n",
      "Found person in top_right corner\n",
      "bbox: (1561, 0, 1851, 168)\n",
      "\n",
      "Processing image 26/50: frame_59700.png\n",
      "Found person in top_right corner\n",
      "bbox: (1561, 0, 1851, 170)\n",
      "\n",
      "Processing image 27/50: frame_22975.png\n",
      "Found person in top_right corner\n",
      "bbox: (1570, 0, 1820, 171)\n",
      "\n",
      "Processing image 28/50: frame_63225.png\n",
      "Found person in top_right corner\n",
      "bbox: (1562, 0, 1848, 168)\n",
      "\n",
      "Processing image 29/50: frame_66375.png\n",
      "Found person in top_right corner\n",
      "bbox: (1557, 0, 1851, 169)\n",
      "\n",
      "Processing image 30/50: frame_49400.png\n",
      "Found person in top_right corner\n",
      "bbox: (1586, 3, 1817, 169)\n",
      "\n",
      "Processing image 31/50: frame_60850.png\n",
      "Found person in top_right corner\n",
      "bbox: (1579, 1, 1825, 169)\n",
      "\n",
      "Processing image 32/50: frame_75000.png\n",
      "Found person in top_right corner\n",
      "bbox: (1569, 0, 1850, 169)\n",
      "\n",
      "Processing image 33/50: frame_61600.png\n",
      "Found person in top_right corner\n",
      "bbox: (1555, 0, 1852, 171)\n",
      "\n",
      "Processing image 34/50: frame_27800.png\n",
      "Found person in top_right corner\n",
      "bbox: (1565, 0, 1849, 169)\n",
      "\n",
      "Processing image 35/50: frame_104050.png\n",
      "Found person in top_right corner\n",
      "bbox: (1596, 1, 1818, 169)\n",
      "\n",
      "Processing image 36/50: frame_41850.png\n",
      "Found person in top_right corner\n",
      "bbox: (1578, 0, 1826, 167)\n",
      "\n",
      "Processing image 37/50: frame_91100.png\n",
      "Found person in top_right corner\n",
      "bbox: (1561, 0, 1848, 169)\n",
      "\n",
      "Processing image 38/50: frame_61750.png\n",
      "Found person in top_right corner\n",
      "bbox: (1555, 0, 1850, 167)\n",
      "\n",
      "Processing image 39/50: frame_61850.png\n",
      "Found person in top_right corner\n",
      "bbox: (1556, 0, 1851, 168)\n",
      "\n",
      "Processing image 40/50: frame_41700.png\n",
      "Found person in top_right corner\n",
      "bbox: (1610, 2, 1794, 169)\n",
      "\n",
      "Processing image 41/50: frame_73850.png\n",
      "Found person in top_right corner\n",
      "bbox: (1586, 0, 1819, 170)\n",
      "\n",
      "Processing image 42/50: frame_29525.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1848, 169)\n",
      "\n",
      "Processing image 43/50: frame_94700.png\n",
      "Found person in top_right corner\n",
      "bbox: (1571, 0, 1813, 170)\n",
      "\n",
      "Processing image 44/50: frame_104600.png\n",
      "Found person in top_right corner\n",
      "bbox: (1579, 2, 1826, 170)\n",
      "\n",
      "Processing image 45/50: frame_27050.png\n",
      "Found person in top_right corner\n",
      "bbox: (1564, 0, 1851, 172)\n",
      "\n",
      "Processing image 46/50: frame_70725.png\n",
      "Found person in top_right corner\n",
      "bbox: (1569, 0, 1819, 167)\n",
      "\n",
      "Processing image 47/50: frame_95925.png\n",
      "Found person in top_right corner\n",
      "bbox: (1551, 1, 1845, 172)\n",
      "\n",
      "Processing image 48/50: frame_35975.png\n",
      "Found person in top_right corner\n",
      "bbox: (1573, 1, 1827, 170)\n",
      "\n",
      "Processing image 49/50: frame_29275.png\n",
      "Found person in top_right corner\n",
      "bbox: (1562, 0, 1850, 169)\n",
      "\n",
      "Processing image 50/50: frame_88075.png\n",
      "Found person in top_right corner\n",
      "bbox: (1558, 0, 1849, 169)\n",
      "\n",
      "Detection Summary:\n",
      "\n",
      "top_right corner:\n",
      "Found 50 images\n",
      "Bounding boxes:\n",
      "File: frame_87600.png\n",
      "bbox: (1566, 0, 1848, 167)\n",
      "File: frame_90950.png\n",
      "bbox: (1560, 0, 1849, 169)\n",
      "File: frame_99350.png\n",
      "bbox: (1579, 0, 1846, 172)\n",
      "File: frame_20300.png\n",
      "bbox: (1564, 0, 1853, 169)\n",
      "File: frame_25200.png\n",
      "bbox: (1575, 0, 1848, 170)\n",
      "File: frame_14550.png\n",
      "bbox: (1564, 1, 1851, 168)\n",
      "File: frame_54925.png\n",
      "bbox: (1564, 0, 1849, 168)\n",
      "File: frame_40125.png\n",
      "bbox: (1560, 0, 1849, 167)\n",
      "File: frame_41525.png\n",
      "bbox: (1564, 0, 1851, 171)\n",
      "File: frame_12775.png\n",
      "bbox: (1562, 0, 1851, 169)\n",
      "File: frame_108375.png\n",
      "bbox: (1574, 0, 1803, 168)\n",
      "File: frame_36500.png\n",
      "bbox: (1557, 0, 1853, 168)\n",
      "File: frame_72550.png\n",
      "bbox: (1584, 1, 1819, 169)\n",
      "File: frame_40425.png\n",
      "bbox: (1559, 0, 1848, 169)\n",
      "File: frame_40325.png\n",
      "bbox: (1559, 0, 1848, 168)\n",
      "File: frame_86625.png\n",
      "bbox: (1564, 0, 1847, 167)\n",
      "File: frame_63900.png\n",
      "bbox: (1564, 0, 1850, 171)\n",
      "File: frame_33125.png\n",
      "bbox: (1560, 0, 1849, 168)\n",
      "File: frame_59775.png\n",
      "bbox: (1561, 0, 1851, 168)\n",
      "File: frame_107975.png\n",
      "bbox: (1570, 0, 1809, 167)\n",
      "File: frame_36400.png\n",
      "bbox: (1557, 0, 1852, 168)\n",
      "File: frame_43925.png\n",
      "bbox: (1581, 0, 1825, 168)\n",
      "File: frame_106650.png\n",
      "bbox: (1571, 0, 1807, 169)\n",
      "File: frame_11525.png\n",
      "bbox: (1615, 9, 1803, 169)\n",
      "File: frame_99900.png\n",
      "bbox: (1561, 0, 1851, 168)\n",
      "File: frame_59700.png\n",
      "bbox: (1561, 0, 1851, 170)\n",
      "File: frame_22975.png\n",
      "bbox: (1570, 0, 1820, 171)\n",
      "File: frame_63225.png\n",
      "bbox: (1562, 0, 1848, 168)\n",
      "File: frame_66375.png\n",
      "bbox: (1557, 0, 1851, 169)\n",
      "File: frame_49400.png\n",
      "bbox: (1586, 3, 1817, 169)\n",
      "File: frame_60850.png\n",
      "bbox: (1579, 1, 1825, 169)\n",
      "File: frame_75000.png\n",
      "bbox: (1569, 0, 1850, 169)\n",
      "File: frame_61600.png\n",
      "bbox: (1555, 0, 1852, 171)\n",
      "File: frame_27800.png\n",
      "bbox: (1565, 0, 1849, 169)\n",
      "File: frame_104050.png\n",
      "bbox: (1596, 1, 1818, 169)\n",
      "File: frame_41850.png\n",
      "bbox: (1578, 0, 1826, 167)\n",
      "File: frame_91100.png\n",
      "bbox: (1561, 0, 1848, 169)\n",
      "File: frame_61750.png\n",
      "bbox: (1555, 0, 1850, 167)\n",
      "File: frame_61850.png\n",
      "bbox: (1556, 0, 1851, 168)\n",
      "File: frame_41700.png\n",
      "bbox: (1610, 2, 1794, 169)\n",
      "File: frame_73850.png\n",
      "bbox: (1586, 0, 1819, 170)\n",
      "File: frame_29525.png\n",
      "bbox: (1564, 0, 1848, 169)\n",
      "File: frame_94700.png\n",
      "bbox: (1571, 0, 1813, 170)\n",
      "File: frame_104600.png\n",
      "bbox: (1579, 2, 1826, 170)\n",
      "File: frame_27050.png\n",
      "bbox: (1564, 0, 1851, 172)\n",
      "File: frame_70725.png\n",
      "bbox: (1569, 0, 1819, 167)\n",
      "File: frame_95925.png\n",
      "bbox: (1551, 1, 1845, 172)\n",
      "File: frame_35975.png\n",
      "bbox: (1573, 1, 1827, 170)\n",
      "File: frame_29275.png\n",
      "bbox: (1562, 0, 1850, 169)\n",
      "File: frame_88075.png\n",
      "bbox: (1558, 0, 1849, 169)\n",
      "Max bbox pre_adjustments: [1551, 0, 1853, 172]\n",
      "Max bbox post_adjustments: [1521, 0, 1853, 202]\n",
      "Max bbox for top_right: (1521, 0, 1853, 202)\n",
      "\n",
      "Step 1: Removing bbox areas...\n",
      "\n",
      "Step 2: Finding borders from samples...\n",
      "Found borders: top=85, bottom=1023, left=0, right=1668\n",
      "Found borders: top=85, bottom=1023, left=0, right=1668\n",
      "Found borders: top=85, bottom=1023, left=0, right=1668\n",
      "Maximum borders: top=85, bottom=1023, left=0, right=1668\n",
      "\n",
      "Step 3: Processing all images...\n",
      "\n",
      "Processing complete:\n",
      "Processed 50 images\n",
      "Output saved to: processed_output\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "folder_path = \"LLM_valid_ru_cv\"\n",
    "corner_detections = analyze_folder(folder_path)\n",
    "\n",
    "output_folder = \"processed_output\"\n",
    "process_images_with_bbox(folder_path, corner_detections, output_folder, debug_print = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "import multiprocessing\n",
    "\n",
    "def compare_images(image1, image2, min_diff=10, corner_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Compares two frames using the non-corner region.\n",
    "    Returns True if frames are significantly different.\n",
    "    \"\"\"\n",
    "    # Get the region excluding the top-right corner\n",
    "    height, width = image1.shape[:2]\n",
    "    region1 = image1[int(height * corner_threshold):, :int(width * (1 - corner_threshold))]\n",
    "    region2 = image2[int(height * corner_threshold):, :int(width * (1 - corner_threshold))]\n",
    "    \n",
    "    # Convert regions to PIL Images for hash comparison\n",
    "    pil_region1 = Image.fromarray(region1)\n",
    "    pil_region2 = Image.fromarray(region2)\n",
    "    \n",
    "    # Compare using perceptual hash\n",
    "    hash1 = imagehash.average_hash(pil_region1)\n",
    "    hash2 = imagehash.average_hash(pil_region2)\n",
    "    \n",
    "    return (hash1 - hash2) > min_diff\n",
    "\n",
    "\n",
    "# def extract_unique_frames(video_path, frame_interval=1, min_diff=10):\n",
    "#     \"\"\"\n",
    "#     Extract frames from video, filter unique ones, and save them.\n",
    "#     Args:\n",
    "#         video_path: Path to video file\n",
    "#         frame_interval: Time interval between frames in seconds\n",
    "#         min_diff: Minimum difference threshold for frame comparison\n",
    "#     Returns:\n",
    "#         temp_folder: Path to folder with extracted frames\n",
    "#         filenames: List of saved frame filenames\n",
    "#     \"\"\"\n",
    "#     # Create temporary folder for frames\n",
    "#     temp_folder = os.path.join(\n",
    "#         os.path.dirname(video_path),\n",
    "#         f\"temp_frames_{os.path.splitext(os.path.basename(video_path))[0]}\"\n",
    "#     )\n",
    "#     os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "#     # Open video\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     frame_step = int(fps * frame_interval)\n",
    "#     frame_count = 0\n",
    "    \n",
    "#     # For storing unique frames\n",
    "#     unique_frames = []\n",
    "#     previous_frame = None\n",
    "#     filenames = []\n",
    "    \n",
    "#     print(\"Extracting and filtering frames...\")\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "            \n",
    "#         if frame_count % frame_step == 0:\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "#             # Compare with previous frame (excluding top-right corner)\n",
    "#             if previous_frame is None or compare_images(frame_rgb, previous_frame, min_diff):\n",
    "#                 # Save unique frame\n",
    "#                 filename = f\"frame_{frame_count:06d}.jpg\"\n",
    "#                 output_path = os.path.join(temp_folder, filename)\n",
    "#                 Image.fromarray(frame_rgb).save(output_path)\n",
    "                \n",
    "#                 filenames.append(filename)\n",
    "#                 previous_frame = frame_rgb\n",
    "                \n",
    "#                 if len(filenames) % 10 == 0:\n",
    "#                     print(f\"Extracted {len(filenames)} unique frames...\")\n",
    "        \n",
    "#         frame_count += 1\n",
    "    \n",
    "#     cap.release()\n",
    "#     print(f\"Extracted {len(filenames)} unique frames from {frame_count} total frames\")\n",
    "    \n",
    "#     return temp_folder, filenames\n",
    "\n",
    "\n",
    "\n",
    "# def process_video_segment(args):\n",
    "#     \"\"\"\n",
    "#     Process a segment of video frames.\n",
    "#     Args:\n",
    "#         args: tuple of (video_path, start_frame, end_frame, temp_folder, frame_step, min_diff)\n",
    "#     Returns:\n",
    "#         List of (frame_count, frame_rgb, filename) for unique frames in this segment\n",
    "#     \"\"\"\n",
    "#     video_path, start_frame, end_frame, temp_folder, frame_step, min_diff = args\n",
    "    \n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "#     segment_frames = []\n",
    "#     previous_frame = None\n",
    "#     frame_count = start_frame\n",
    "    \n",
    "#     while frame_count < end_frame and cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "            \n",
    "#         if frame_count % frame_step == 0:\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "#             # Compare with previous frame\n",
    "#             if previous_frame is None or compare_images(frame_rgb, previous_frame, min_diff):\n",
    "#                 filename = f\"frame_{frame_count:06d}.jpg\"\n",
    "#                 segment_frames.append((frame_count, frame_rgb, filename))\n",
    "#                 previous_frame = frame_rgb\n",
    "        \n",
    "#         frame_count += 1\n",
    "    \n",
    "#     cap.release()\n",
    "#     return segment_frames\n",
    "\n",
    "# def extract_unique_frames(video_path, frame_interval=1, min_diff=10, num_workers=6):\n",
    "#     \"\"\"\n",
    "#     Extract frames from video using multiple processes, filter unique ones, and save them.\n",
    "#     Args:\n",
    "#         video_path: Path to video file\n",
    "#         frame_interval: Time interval between frames in seconds\n",
    "#         min_diff: Minimum difference threshold for frame comparison\n",
    "#         num_workers: Number of worker processes (default: CPU count - 1)\n",
    "#     Returns:\n",
    "#         temp_folder: Path to folder with extracted frames\n",
    "#         filenames: List of saved frame filenames\n",
    "#     \"\"\"\n",
    "#     if num_workers is None:\n",
    "#         num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "#     # Create temporary folder\n",
    "#     temp_folder = os.path.join(\n",
    "#         os.path.dirname(video_path),\n",
    "#         f\"temp_frames_{os.path.splitext(os.path.basename(video_path))[0]}\"\n",
    "#     )\n",
    "#     os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "#     # Get video info\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     frame_step = int(fps * frame_interval)\n",
    "#     cap.release()\n",
    "    \n",
    "#     # Split video into segments\n",
    "#     frames_per_worker = total_frames // num_workers\n",
    "#     segments = []\n",
    "    \n",
    "#     for i in range(num_workers):\n",
    "#         start_frame = i * frames_per_worker\n",
    "#         end_frame = start_frame + frames_per_worker if i < num_workers - 1 else total_frames\n",
    "#         segment = (video_path, start_frame, end_frame, temp_folder, frame_step, min_diff)\n",
    "#         segments.append(segment)\n",
    "    \n",
    "#     print(f\"Processing video with {num_workers} workers...\")\n",
    "    \n",
    "#     # Process segments in parallel\n",
    "#     with multiprocessing.Pool(num_workers) as pool:\n",
    "#         segment_results = pool.map(process_video_segment, segments)\n",
    "    \n",
    "#     # Combine results and eliminate duplicates between segments\n",
    "#     all_frames = []\n",
    "#     previous_frame = None\n",
    "#     filenames = []\n",
    "    \n",
    "#     # Flatten and sort by frame count\n",
    "#     all_segment_frames = []\n",
    "#     for segment in segment_results:\n",
    "#         all_segment_frames.extend(segment)\n",
    "#     all_segment_frames.sort(key=lambda x: x[0])  # Sort by frame count\n",
    "    \n",
    "#     print(\"Combining results and eliminating duplicates...\")\n",
    "    \n",
    "#     # Process combined frames\n",
    "#     for frame_count, frame_rgb, filename in all_segment_frames:\n",
    "#         if previous_frame is None or compare_images(frame_rgb, previous_frame, min_diff):\n",
    "#             output_path = os.path.join(temp_folder, filename)\n",
    "#             Image.fromarray(frame_rgb).save(output_path)\n",
    "#             filenames.append(filename)\n",
    "#             previous_frame = frame_rgb\n",
    "            \n",
    "#             if len(filenames) % 10 == 0:\n",
    "#                 print(f\"Processed {len(filenames)} unique frames...\")\n",
    "    \n",
    "#     print(f\"Extracted {len(filenames)} unique frames from {total_frames} total frames\")\n",
    "    \n",
    "#     return temp_folder, filenames\n",
    "\n",
    "def process_video_segment(args):\n",
    "    \"\"\"\n",
    "    Process a segment of video frames.\n",
    "    Args:\n",
    "        args: tuple of (video_path, start_frame, end_frame, temp_folder, frame_step, min_diff, fps)\n",
    "    Returns:\n",
    "        List of (frame_count, frame_rgb, filename, time_sec) for unique frames in this segment\n",
    "    \"\"\"\n",
    "    video_path, start_frame, end_frame, temp_folder, frame_step, min_diff, fps = args\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    segment_frames = []\n",
    "    previous_frame = None\n",
    "    frame_count = start_frame\n",
    "    \n",
    "    while frame_count < end_frame and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % frame_step == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            time_sec = frame_count / fps\n",
    "            \n",
    "            # Compare with previous frame\n",
    "            if previous_frame is None or compare_images(frame_rgb, previous_frame, min_diff):\n",
    "                filename = f\"frame_{frame_count:06d}.jpg\"\n",
    "                segment_frames.append((frame_count, frame_rgb, filename, time_sec))\n",
    "                previous_frame = frame_rgb\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return segment_frames\n",
    "\n",
    "def extract_unique_frames(video_path, frame_interval=1, min_diff=10, num_workers=None):\n",
    "    \"\"\"\n",
    "    Extract frames from video using multiple processes, filter unique ones, and track timing.\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        frame_interval: Time interval between frames in seconds\n",
    "        min_diff: Minimum difference threshold for frame comparison\n",
    "        num_workers: Number of worker processes (default: CPU count - 1)\n",
    "    Returns:\n",
    "        temp_folder: Path to folder with extracted frames\n",
    "        frame_info: Dictionary with frame timing {filename: (start_time, end_time)}\n",
    "    \"\"\"\n",
    "    if num_workers is None:\n",
    "        num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    # Create temporary folder\n",
    "    temp_folder = os.path.join(\n",
    "        os.path.dirname(video_path),\n",
    "        f\"temp_frames_{os.path.splitext(os.path.basename(video_path))[0]}\"\n",
    "    )\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "    # Get video info\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_step = int(fps * frame_interval)\n",
    "    cap.release()\n",
    "    \n",
    "    # Split video into segments\n",
    "    frames_per_worker = total_frames // num_workers\n",
    "    segments = []\n",
    "    \n",
    "    for i in range(num_workers):\n",
    "        start_frame = i * frames_per_worker\n",
    "        end_frame = start_frame + frames_per_worker if i < num_workers - 1 else total_frames\n",
    "        segment = (video_path, start_frame, end_frame, temp_folder, frame_step, min_diff, fps)\n",
    "        segments.append(segment)\n",
    "    \n",
    "    print(f\"Processing video with {num_workers} workers...\")\n",
    "    \n",
    "    # Process segments in parallel\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        segment_results = pool.map(process_video_segment, segments)\n",
    "    \n",
    "    # Combine results and eliminate duplicates between segments\n",
    "    previous_frame = None\n",
    "    frame_info = {}  # Dictionary to store frame timing information\n",
    "    current_frame_start = None\n",
    "    current_frame_name = None\n",
    "    \n",
    "    # Flatten and sort by frame count\n",
    "    all_segment_frames = []\n",
    "    for segment in segment_results:\n",
    "        all_segment_frames.extend(segment)\n",
    "    all_segment_frames.sort(key=lambda x: x[0])  # Sort by frame count\n",
    "    \n",
    "    print(\"Combining results and eliminating duplicates...\")\n",
    "    \n",
    "    # Process combined frames\n",
    "    for frame_count, frame_rgb, filename, time_sec in all_segment_frames:\n",
    "        if previous_frame is None or compare_images(frame_rgb, previous_frame, min_diff):\n",
    "            # If we had a previous frame, set its end time\n",
    "            if current_frame_name:\n",
    "                frame_info[current_frame_name] = (current_frame_start, time_sec)\n",
    "            \n",
    "            # Save the new frame\n",
    "            output_path = os.path.join(temp_folder, filename)\n",
    "            Image.fromarray(frame_rgb).save(output_path)\n",
    "            \n",
    "            # Update tracking info\n",
    "            current_frame_start = time_sec\n",
    "            current_frame_name = filename\n",
    "            previous_frame = frame_rgb\n",
    "            \n",
    "            if len(frame_info) % 10 == 0:\n",
    "                print(f\"Processed {len(frame_info)} unique frames...\")\n",
    "    \n",
    "    # Handle the last frame's end time\n",
    "    if current_frame_name:\n",
    "        frame_info[current_frame_name] = (current_frame_start, total_frames / fps)\n",
    "    \n",
    "    \n",
    "    return temp_folder, frame_info\n",
    "\n",
    "\n",
    "def process_video(video_path, output_folder, frame_files_file, frame_interval=1, min_diff=10, debug_print=False, num_workers = 2):\n",
    "    \"\"\"\n",
    "    Process video file using existing image processing pipeline.\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        output_folder: Where to save final processed frames\n",
    "        frame_interval: Time between frames in seconds\n",
    "        min_diff: Threshold for frame difference\n",
    "        debug_print: Whether to save debug visualizations\n",
    "    \"\"\"\n",
    "    # Extract unique frames\n",
    "    frames_folder, frame_files = extract_unique_frames(video_path, frame_interval, min_diff, num_workers=num_workers)\n",
    "    \n",
    "    try:\n",
    "        # Analyze frames for person detection\n",
    "        corner_detections = analyze_folder(frames_folder, output_folder, min_det_conf=0.5)\n",
    "        \n",
    "        # Process detected frames\n",
    "        if corner_detections:\n",
    "            process_images_with_bbox(frames_folder, corner_detections, output_folder, debug_print)\n",
    "        else:\n",
    "            print(\"No corner detections found in video frames\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(frames_folder):\n",
    "            import shutil\n",
    "            shutil.rmtree(frames_folder)\n",
    "            print(f\"Cleaned up temporary folder: {frames_folder}\")\n",
    "\n",
    "    # save frame_files as txt file\n",
    "    with open(os.path.join(output_folder, frame_files_file), \"w\") as f:\n",
    "        for item in frame_files:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "    # remove detected frames from output folder\n",
    "    for file in os.listdir(output_folder):\n",
    "        if file.startswith(\"detected_\"):\n",
    "            os.remove(os.path.join(output_folder, file))\n",
    "\n",
    "# Example usage:\n",
    "# process_video(\"your_video.mp4\", \"output_folder\", frame_interval=1, min_diff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video with 1 workers...\n",
      "Combining results and eliminating duplicates...\n",
      "Processed 0 unique frames...\n",
      "Processed 10 unique frames...\n",
      "Processed 20 unique frames...\n",
      "Processed 30 unique frames...\n",
      "Processed 40 unique frames...\n",
      "Processed 50 unique frames...\n",
      "Processed 60 unique frames...\n",
      "Processed 70 unique frames...\n",
      "Processed 80 unique frames...\n",
      "Processed 90 unique frames...\n",
      "Loading detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kolyangg/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-2-13 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16376MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 96 images...\n",
      "\n",
      "Processing image 1/96: frame_067425.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1065, 0, 1253, 116)\n",
      "\n",
      "Processing image 2/96: frame_053250.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1054, 0, 1253, 116)\n",
      "\n",
      "Processing image 3/96: frame_084675.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1072, 0, 1252, 114)\n",
      "\n",
      "Processing image 4/96: frame_010750.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1096, 5, 1226, 115)\n",
      "\n",
      "Processing image 5/96: frame_007175.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 6/96: frame_112225.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 7/96: frame_003575.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 8/96: frame_037025.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1067, 0, 1254, 116)\n",
      "\n",
      "Processing image 9/96: frame_075300.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1255, 116)\n",
      "\n",
      "Processing image 10/96: frame_094675.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1064, 0, 1227, 115)\n",
      "\n",
      "Processing image 11/96: frame_062400.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1063, 0, 1254, 114)\n",
      "\n",
      "Processing image 12/96: frame_083050.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1079, 0, 1235, 113)\n",
      "\n",
      "Processing image 13/96: frame_052175.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 14/96: frame_112700.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 15/96: frame_091350.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1065, 0, 1252, 115)\n",
      "\n",
      "Processing image 16/96: frame_041500.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1070, 0, 1253, 113)\n",
      "\n",
      "Processing image 17/96: frame_053200.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1055, 0, 1236, 115)\n",
      "\n",
      "Processing image 18/96: frame_068675.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1066, 0, 1226, 114)\n",
      "\n",
      "Processing image 19/96: frame_053225.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1053, 1, 1239, 116)\n",
      "\n",
      "Processing image 20/96: frame_082875.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1253, 115)\n",
      "\n",
      "Processing image 21/96: frame_113050.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 22/96: frame_113600.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 23/96: frame_112175.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 24/96: frame_112075.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 25/96: frame_111950.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 26/96: frame_010425.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "\n",
      "Processing image 27/96: frame_045150.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1251, 116)\n",
      "\n",
      "Processing image 28/96: frame_052650.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 29/96: frame_057575.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 30/96: frame_057525.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 31/96: frame_052675.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 32/96: frame_018950.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1083, 0, 1241, 115)\n",
      "\n",
      "Processing image 33/96: frame_096925.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 34/96: frame_057500.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1067, 0, 1252, 115)\n",
      "\n",
      "Processing image 35/96: frame_046400.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1070, 0, 1253, 116)\n",
      "\n",
      "Processing image 36/96: frame_003600.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 37/96: frame_084650.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1074, 0, 1253, 115)\n",
      "\n",
      "Processing image 38/96: frame_084300.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1072, 0, 1221, 113)\n",
      "\n",
      "Processing image 39/96: frame_031850.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1071, 0, 1233, 115)\n",
      "\n",
      "Processing image 40/96: frame_003675.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 41/96: frame_076850.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1254, 116)\n",
      "\n",
      "Processing image 42/96: frame_105825.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1071, 0, 1237, 114)\n",
      "\n",
      "Processing image 43/96: frame_097950.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1253, 114)\n",
      "\n",
      "Processing image 44/96: frame_040925.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1066, 0, 1252, 115)\n",
      "\n",
      "Processing image 45/96: frame_016750.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1073, 0, 1252, 116)\n",
      "\n",
      "Processing image 46/96: frame_076975.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1254, 116)\n",
      "\n",
      "Processing image 47/96: frame_052050.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1252, 116)\n",
      "\n",
      "Processing image 48/96: frame_055525.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "\n",
      "Processing image 49/96: frame_096775.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1064, 0, 1252, 113)\n",
      "\n",
      "Processing image 50/96: frame_112200.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 51/96: frame_007100.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 52/96: frame_007050.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 53/96: frame_065650.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1067, 0, 1252, 115)\n",
      "\n",
      "Processing image 54/96: frame_096875.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 55/96: frame_059725.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1067, 0, 1253, 115)\n",
      "\n",
      "Processing image 56/96: frame_075425.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1072, 0, 1255, 117)\n",
      "\n",
      "Processing image 57/96: frame_077275.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1253, 115)\n",
      "\n",
      "Processing image 58/96: frame_010175.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 59/96: frame_042775.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "\n",
      "Processing image 60/96: frame_025550.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1067, 0, 1253, 115)\n",
      "\n",
      "Processing image 61/96: frame_112100.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 62/96: frame_112000.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 63/96: frame_054925.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1252, 115)\n",
      "\n",
      "Processing image 64/96: frame_076825.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1254, 117)\n",
      "\n",
      "Processing image 65/96: frame_045100.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1071, 0, 1251, 116)\n",
      "\n",
      "Processing image 66/96: frame_003650.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 67/96: frame_112675.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 68/96: frame_028700.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1066, 0, 1254, 116)\n",
      "\n",
      "Processing image 69/96: frame_077050.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1254, 116)\n",
      "\n",
      "Processing image 70/96: frame_010125.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 71/96: frame_010400.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1066, 0, 1252, 115)\n",
      "\n",
      "Processing image 72/96: frame_000000.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 73/96: frame_054900.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1252, 115)\n",
      "\n",
      "Processing image 74/96: frame_055550.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "\n",
      "Processing image 75/96: frame_062675.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1064, 0, 1253, 115)\n",
      "\n",
      "Processing image 76/96: frame_111925.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 77/96: frame_112125.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 78/96: frame_007000.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 79/96: frame_112850.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 80/96: frame_112250.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 81/96: frame_010225.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 82/96: frame_101250.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1254, 115)\n",
      "\n",
      "Processing image 83/96: frame_012100.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1066, 0, 1253, 116)\n",
      "\n",
      "Processing image 84/96: frame_062625.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1062, 0, 1253, 115)\n",
      "\n",
      "Processing image 85/96: frame_097525.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1065, 0, 1254, 116)\n",
      "\n",
      "Processing image 86/96: frame_077075.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1070, 0, 1255, 116)\n",
      "\n",
      "Processing image 87/96: frame_040700.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1253, 115)\n",
      "\n",
      "Processing image 88/96: frame_084975.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1074, 0, 1253, 116)\n",
      "\n",
      "Processing image 89/96: frame_112725.jpg\n",
      "Person not in corner\n",
      "\n",
      "Processing image 90/96: frame_085000.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1072, 0, 1253, 115)\n",
      "\n",
      "Processing image 91/96: frame_077100.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1255, 117)\n",
      "\n",
      "Processing image 92/96: frame_007125.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 93/96: frame_075400.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1068, 0, 1255, 117)\n",
      "\n",
      "Processing image 94/96: frame_003625.jpg\n",
      "No person detected\n",
      "\n",
      "Processing image 95/96: frame_077000.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1069, 0, 1254, 116)\n",
      "\n",
      "Processing image 96/96: frame_084625.jpg\n",
      "Found person in top_right corner\n",
      "bbox: (1074, 0, 1252, 113)\n",
      "\n",
      "Detection Summary:\n",
      "\n",
      "top_right corner:\n",
      "Found 59 images\n",
      "Bounding boxes:\n",
      "File: frame_067425.jpg\n",
      "bbox: (1065, 0, 1253, 116)\n",
      "File: frame_053250.jpg\n",
      "bbox: (1054, 0, 1253, 116)\n",
      "File: frame_084675.jpg\n",
      "bbox: (1072, 0, 1252, 114)\n",
      "File: frame_010750.jpg\n",
      "bbox: (1096, 5, 1226, 115)\n",
      "File: frame_037025.jpg\n",
      "bbox: (1067, 0, 1254, 116)\n",
      "File: frame_075300.jpg\n",
      "bbox: (1069, 0, 1255, 116)\n",
      "File: frame_094675.jpg\n",
      "bbox: (1064, 0, 1227, 115)\n",
      "File: frame_062400.jpg\n",
      "bbox: (1063, 0, 1254, 114)\n",
      "File: frame_083050.jpg\n",
      "bbox: (1079, 0, 1235, 113)\n",
      "File: frame_091350.jpg\n",
      "bbox: (1065, 0, 1252, 115)\n",
      "File: frame_041500.jpg\n",
      "bbox: (1070, 0, 1253, 113)\n",
      "File: frame_053200.jpg\n",
      "bbox: (1055, 0, 1236, 115)\n",
      "File: frame_068675.jpg\n",
      "bbox: (1066, 0, 1226, 114)\n",
      "File: frame_053225.jpg\n",
      "bbox: (1053, 1, 1239, 116)\n",
      "File: frame_082875.jpg\n",
      "bbox: (1069, 0, 1253, 115)\n",
      "File: frame_010425.jpg\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "File: frame_045150.jpg\n",
      "bbox: (1069, 0, 1251, 116)\n",
      "File: frame_018950.jpg\n",
      "bbox: (1083, 0, 1241, 115)\n",
      "File: frame_057500.jpg\n",
      "bbox: (1067, 0, 1252, 115)\n",
      "File: frame_046400.jpg\n",
      "bbox: (1070, 0, 1253, 116)\n",
      "File: frame_084650.jpg\n",
      "bbox: (1074, 0, 1253, 115)\n",
      "File: frame_084300.jpg\n",
      "bbox: (1072, 0, 1221, 113)\n",
      "File: frame_031850.jpg\n",
      "bbox: (1071, 0, 1233, 115)\n",
      "File: frame_076850.jpg\n",
      "bbox: (1068, 0, 1254, 116)\n",
      "File: frame_105825.jpg\n",
      "bbox: (1071, 0, 1237, 114)\n",
      "File: frame_097950.jpg\n",
      "bbox: (1068, 0, 1253, 114)\n",
      "File: frame_040925.jpg\n",
      "bbox: (1066, 0, 1252, 115)\n",
      "File: frame_016750.jpg\n",
      "bbox: (1073, 0, 1252, 116)\n",
      "File: frame_076975.jpg\n",
      "bbox: (1068, 0, 1254, 116)\n",
      "File: frame_052050.jpg\n",
      "bbox: (1068, 0, 1252, 116)\n",
      "File: frame_055525.jpg\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "File: frame_096775.jpg\n",
      "bbox: (1064, 0, 1252, 113)\n",
      "File: frame_065650.jpg\n",
      "bbox: (1067, 0, 1252, 115)\n",
      "File: frame_059725.jpg\n",
      "bbox: (1067, 0, 1253, 115)\n",
      "File: frame_075425.jpg\n",
      "bbox: (1072, 0, 1255, 117)\n",
      "File: frame_077275.jpg\n",
      "bbox: (1068, 0, 1253, 115)\n",
      "File: frame_042775.jpg\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "File: frame_025550.jpg\n",
      "bbox: (1067, 0, 1253, 115)\n",
      "File: frame_054925.jpg\n",
      "bbox: (1069, 0, 1252, 115)\n",
      "File: frame_076825.jpg\n",
      "bbox: (1068, 0, 1254, 117)\n",
      "File: frame_045100.jpg\n",
      "bbox: (1071, 0, 1251, 116)\n",
      "File: frame_028700.jpg\n",
      "bbox: (1066, 0, 1254, 116)\n",
      "File: frame_077050.jpg\n",
      "bbox: (1069, 0, 1254, 116)\n",
      "File: frame_010400.jpg\n",
      "bbox: (1066, 0, 1252, 115)\n",
      "File: frame_054900.jpg\n",
      "bbox: (1068, 0, 1252, 115)\n",
      "File: frame_055550.jpg\n",
      "bbox: (1068, 0, 1252, 114)\n",
      "File: frame_062675.jpg\n",
      "bbox: (1064, 0, 1253, 115)\n",
      "File: frame_101250.jpg\n",
      "bbox: (1069, 0, 1254, 115)\n",
      "File: frame_012100.jpg\n",
      "bbox: (1066, 0, 1253, 116)\n",
      "File: frame_062625.jpg\n",
      "bbox: (1062, 0, 1253, 115)\n",
      "File: frame_097525.jpg\n",
      "bbox: (1065, 0, 1254, 116)\n",
      "File: frame_077075.jpg\n",
      "bbox: (1070, 0, 1255, 116)\n",
      "File: frame_040700.jpg\n",
      "bbox: (1068, 0, 1253, 115)\n",
      "File: frame_084975.jpg\n",
      "bbox: (1074, 0, 1253, 116)\n",
      "File: frame_085000.jpg\n",
      "bbox: (1072, 0, 1253, 115)\n",
      "File: frame_077100.jpg\n",
      "bbox: (1068, 0, 1255, 117)\n",
      "File: frame_075400.jpg\n",
      "bbox: (1068, 0, 1255, 117)\n",
      "File: frame_077000.jpg\n",
      "bbox: (1069, 0, 1254, 116)\n",
      "File: frame_084625.jpg\n",
      "bbox: (1074, 0, 1252, 113)\n",
      "Max bbox pre_adjustments: [1053, 0, 1255, 117]\n",
      "Max bbox post_adjustments: [1023, 0, 1255, 147]\n",
      "Max bbox for top_right: (1023, 0, 1255, 147)\n",
      "\n",
      "Step 1: Removing bbox areas...\n",
      "\n",
      "Step 2: Finding borders from samples...\n",
      "Found borders: top=20, bottom=719, left=22, right=1134\n",
      "Found borders: top=56, bottom=682, left=22, right=1134\n",
      "Found borders: top=56, bottom=682, left=22, right=1134\n",
      "Maximum borders: top=56, bottom=682, left=22, right=1134\n",
      "\n",
      "Step 3: Processing all images...\n",
      "\n",
      "Processing complete:\n",
      "Processed 59 images\n",
      "Output saved to: processed_output2\n",
      "Cleaned up temporary folder: temp_frames_llm_v2\n"
     ]
    }
   ],
   "source": [
    "video = video_file = \"llm_v2.webm\"\n",
    "output_folder = \"processed_output2\"\n",
    "frame_files_file = \"frame_files.txt\"\n",
    "\n",
    "process_video(video, output_folder, frame_files_file, frame_interval=1, min_diff=2.5, num_workers = 1, debug_print = False) # min_diff = 10 default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random 50 images from the folder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
