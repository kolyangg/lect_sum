# Конспект по обучению с подкреплением 



## Аннотация

Современные алгоритмы глубокого обучения с подкреплением способны решать задачи искусственного интеллекта методом проб и ошибок без использования каких-либо априорных знаний о решаемой задаче. В этом конспекте собраны принципы работы основных алгоритмов, достигших прорывных результатов во многих задачах от игрового искусственного интеллекта до робототехники. Вся необходимая теория приводится с доказательствами, использующими единый ход рассуждений, унифицированные обозначения и определения. Основная задача этой работы - не только собрать информацию из разных источников в одном месте, но понять разницу между алгоритмами различного вида и объяснить, почему они выглядят именно так, а не иначе.

Предполагается знакомство читателя с основами машинного обучения и глубокого обучения. Об ошибках и опечатках в тексте можно сообщать в репозитории проекта.

---

# Оглавление 

1 Задача обучения с подкреплением ..... 6
1.1 Модель взаимодействия агента со средой ..... 6
1.1.1 Связь с оптимальным управлением ..... 6
1.1.2 Марковская цепь ..... 7
1.1.3 Среда ..... 8
1.1.4 Действия ..... 9
1.1.5 Траектории ..... 9
1.1.6 Марковский процесс принятия решений (MDP) ..... 10
1.1.7 Эпизодичность ..... 11
1.1.8 Дисконтирование ..... 12
1.2 Алгоритмы обучения с подкреплением ..... 14
1.2.1 Условия задачи RL ..... 14
1.2.2 Сравнение с обучением с учителем ..... 14
1.2.3 Концепция model-free алгоритмов ..... 15
1.2.4 On-policy vs Off-policy ..... 16
1.2.5 Классификация RL-алгоритмов ..... 17
1.2.6 Критерии оценки RL-алгоритмов ..... 17
1.2.7 Сложности задачи RL ..... 18
1.2.8 Дизайн функции награды ..... 20
1.2.9 Бенчмарки ..... 21
2 Мета-эвристики ..... 23
2.1 Бэйзлайны ..... 23
2.1.1 Задача безградиентной оптимизации ..... 23
2.1.2 Случайный поиск ..... 24
2.1.3 Hill Climbing ..... 25
2.1.4 Имитация отжига ..... 26
2.1.5 Эволюционные алгоритмы ..... 27
2.1.6 Weight Agnostic Neural Networks (WANN) ..... 28
2.1.7 Видовая специализация ..... 29
2.1.8 Генетические алгоритмы ..... 30
2.2 Эволюционные стратегии ..... 32
2.2.1 Идея эволюционных стратегий ..... 32
2.2.2 Оценка вероятности редкого события ..... 32
2.2.3 Метод Кросс-Энтропии для стохастической оптимизации ..... 34
2.2.4 Метод Кросс-Энтропии для обучения с подкреплением (CEM) ..... 35
2.2.5 Натуральные эволюционные стратегии (NES) ..... 35
2.2.6 OpenAI-ES ..... 36
2.2.7 Адаптация матрицы ковариации (CMA-ES) ..... 37
3 Классическая теория ..... 39
3.1 Оценочные функции ..... 39
3.1.1 Свойства траекторий ..... 39
3.1.2 V-функция ..... 41
3.1.3 Уравнения Беллмана ..... 42
3.1.4 Оптимальная стратегия ..... 43
3.1.5 Q-функция ..... 43
3.1.6 Принцип оптимальности Беллмана ..... 44
3.1.7 Отказ от однородности ..... 45
3.1.8 Вид оптимальной стратегии (доказательство через отказ от однородности) ..... 46
3.1.9 Уравнения оптимальности Беллмана ..... 47
3.1.10 Критерий оптимальности Беллмана ..... 48

---

3.2 Улучшение политики ..... 49
3.2.1 Advantage-функция ..... 49
3.2.2 Relative Performance Identity (RPI) ..... 50
3.2.3 Policy Improvement ..... 52
3.2.4 Вид оптимальной стратегии (доказательство через PI) ..... 53
3.3 Динамическое программирование ..... 54
3.3.1 Метод простой итерации ..... 54
3.3.2 Policy Evaluation ..... 55
3.3.3 Value Iteration ..... 57
3.3.4 Policy Iteration ..... 58
3.3.5 Generalized Policy Iteration ..... 59
3.4 Табличные алгоритмы ..... 62
3.4.1 Монте-Карло алгоритм ..... 62
3.4.2 Экспоненциальное сглаживание ..... 63
3.4.3 Стохастическая аппроксимация ..... 64
3.4.4 Temporal Difference ..... 66
3.4.5 Q-learning ..... 67
3.4.6 Exploration-exploitation дилемма ..... 68
3.4.7 Реплей буфер ..... 69
3.4.8 SARSA ..... 70
3.5 Bias-Variance Trade-Off ..... 73
3.5.1 Дилемма смещения-разброса ..... 73
3.5.2 N-step Temporal Difference ..... 74
3.5.3 Интерпретация через Credit Assingment ..... 75
3.5.4 Backward View ..... 76
3.5.5 Eligibility Trace ..... 78
3.5.6 $\operatorname{TD}(\boldsymbol{\lambda})$ ..... 78
3.5.7 $\operatorname{Retrace}(\boldsymbol{\lambda})$ ..... 81
4 Value-based подход ..... 85
4.1 Deep Q-learning ..... 85
4.1.1 Q-сетка ..... 85
4.1.2 Переход к параметрической Q-функции ..... 85
4.1.3 Таргет-сеть ..... 87
4.1.4 Декорреляция сэмплов ..... 88
4.1.5 DQN ..... 89
4.2 Модификации DQN ..... 91
4.2.1 Overestimation Bias ..... 91
4.2.2 Twin DQN ..... 93
4.2.3 Double DQN ..... 93
4.2.4 Dueling DQN ..... 93
4.2.5 Шумные сети (Noisy Nets) ..... 94
4.2.6 Приоритизированный реплей (Prioritized DQN) ..... 96
4.2.7 Multi-step DQN ..... 97
4.2.8 Retrace ..... 99
4.3 Distributional RL ..... 100
4.3.1 Идея Distributional подхода ..... 100
4.3.2 Z-функция ..... 101
4.3.3 Distributional-форма уравнения Беллмана ..... 103
4.3.4 Distributional Policy Evaluation ..... 103
4.3.5 Distributional Value Iteration ..... 108
4.3.6 Категориальная аппроксимация Z-функций ..... 110
4.3.7 Categorical DQN ..... 111
4.3.8 Квантильная аппроксимация Z-функций ..... 114
4.3.9 Quantile Regression DQN ..... 115
4.3.10 Implicit Quantile Networks ..... 117
4.3.11 Rainbow DQN ..... 118
5 Policy Gradient подход ..... 120
5.1 Policy Gradient Theorem ..... 120
5.1.1 Вывод первым способом ..... 120
5.1.2 Вывод вторым способом ..... 121
5.1.3 Физический смысл ..... 124

---

5.1.4 REINFORCE ..... 125
5.1.5 State visitation frequency ..... 125
5.1.6 Расцепление внешней и внутренней стохастики ..... 126
5.1.7 Связь с policy improvement ..... 128
5.1.8 Бэйзлайн ..... 129
5.2 Схемы «Актёр-критик» ..... 131
5.2.1 Введение критика ..... 131
5.2.2 Bias-variance trade-off ..... 132
5.2.3 Generalized Advantage Estimation (GAE) ..... 134
5.2.4 Обучение критика ..... 135
5.2.5 Advantage Actor-Critic (A2C) ..... 136
5.3 Продвинутые Policy Gradient ..... 138
5.3.1 Суррогатная функция ..... 138
5.3.2 Нижняя оценка ..... 140
5.3.3 Trust Region Policy Optimization (TRPO) ..... 142
5.3.4 Proximal Policy Loss ..... 145
5.3.5 Clipped Value Loss ..... 147
5.3.6 Proximal Policy Optimization (PPO) ..... 148
6 Continuous control ..... 150
6.1 DDPG ..... 150
6.1.1 Вывод из Deep Q-learning ..... 150
6.1.2 Вывод из Policy Gradient ..... 151
6.1.3 Связь между схемами ..... 152
6.1.4 Ornstein-Uhlenbeck Noise ..... 152
6.1.5 Deep Deterministic Policy Gradient (DDPG) ..... 153
6.1.6 Twin Delayed DDPG (TD3) ..... 154
6.1.7 Обучение стохастичных политик ..... 156
6.2 Soft Actor-Critic ..... 158
6.2.1 Maximum Entropy RL ..... 158
6.2.2 Soft Policy Evaluation ..... 159
6.2.3 Soft Policy Improvement ..... 160
6.2.4 Soft Actor-Critic (SAC) ..... 162
6.2.5 Аналоги других алгоритмов ..... 163
7 Model-based ..... 166
7.1 Бандиты ..... 166
7.1.1 Задача многоруких бандитов ..... 166
7.1.2 Простое решение ..... 167
7.1.3 Теорема Лан-Роббинса ..... 168
7.1.4 Upper Confidence Bound (UCB) ..... 169
7.1.5 Сэмплирование Томпсона ..... 170
7.1.6 Обобщение на табличные MDP ..... 172
7.2 Обучаемые модели окружения ..... 173
7.2.1 Планирование ..... 173
7.2.2 Модели мира (World Models) ..... 174
7.2.3 Модель прямой динамики ..... 175
7.2.4 Сновидения ..... 176
7.3 Планирование для дискретного управления ..... 176
7.3.1 Monte-Carlo Tree Search (MCTS) ..... 176
7.3.2 Применение MCTS ..... 178
7.3.3 Дистилляция MCTS ..... 180
7.3.4 AlphaZero ..... 180
7.3.5 $\boldsymbol{\mu}$-Zero ..... 181
7.4 Планирование для непрерывного управления ..... 183
7.4.1 Прямое дифференцирование ..... 183
7.4.2 Linear Quadratic Regulator (LQR) ..... 184
7.4.3 Случай шумной функции перехода ..... 186
7.4.4 Iterative LQR (iLQR) ..... 187
8 Next Stage ..... 189
8.1 Имитационное обучение ..... 189
8.1.1 Клонирование поведения ..... 189

---

8.1.2 Обратное обучение с подкреплением (Inverse RL) ..... 190
8.1.3 Guided Cost Learning ..... 192
8.1.4 Generative Adversarial Imitation Learning (GAIL) ..... 193
8.1.5 Generative Adversarial Imitation from Observation (GAIO) ..... 195
8.2 Внутренняя мотивация ..... 195
8.2.1 Вспомогательные задачи ..... 195
8.2.2 Совмещение мотиваций ..... 196
8.2.3 Exploration Bonuses ..... 198
8.2.4 Дистилляция случайной сетки (RND) ..... 199
8.2.5 Любопытство ..... 200
8.2.6 Модель обратной динамики ..... 201
8.2.7 Внутренний модуль любопытства (ICM) ..... 202
8.3 Multi-task RL ..... 203
8.3.1 Многозадачность ..... 203
8.3.2 Мета-контроллеры ..... 204
8.3.3 Переразметка траекторий ..... 205
8.3.4 Hindsight Experience Replay (HER) ..... 205
8.3.5 Hindsight Relabeling ..... 206
8.4 Иерархическое обучение с подкреплением ..... 208
8.4.1 Опции ..... 208
8.4.2 Semi-MDP ..... 209
8.4.3 Оценочная функция по прибытию (U-функция) ..... 210
8.4.4 Intra-option обучение ..... 211
8.4.5 Обучение стратегий рабочих ..... 211
8.4.6 Обучение функций терминальности ..... 212
8.4.7 Феодальный RL ..... 213
8.4.8 Feudal Networks (FuN) ..... 214
8.4.9 HIRO ..... 215
8.5 Частично наблюдаемые среды ..... 216
8.5.1 Частично наблюдаемые MDP ..... 216
8.5.2 Belief MDP ..... 216
8.5.3 Рекуррентные сети в Policy Gradient ..... 218
8.5.4 R2D2 ..... 218
8.5.5 Neural Episodic Control (NEC) ..... 219
8.6 Мульти-агентное обучение с подкреплением ..... 220
8.6.1 Связь с теорией игр ..... 220
8.6.2 Централизация обучения ..... 220
8.6.3 Self-play в антагонистических играх ..... 221
8.6.4 QMix в кооперативных играх ..... 221
8.6.5 Multi-Agent DDPG (MADDPG) в смешанных играх ..... 223
8.6.6 Системы коммуникации (DIAL) ..... 224
А Приложение ..... 226
А. 1 Натуральный градиент ..... 226
А.1.1 Проблема параметризации ..... 226
А.1.2 Матрица Фишера ..... 227
А.1.3 Натуральный градиент ..... 228
А. 2 Обоснование формул CMA-ES ..... 229
А.2.1 Вычисление градиента ..... 229
А.2.2 Произведение Кронекера ..... 230
А.2.3 Вычисление матрицы Фишера ..... 231
A.2.4 Covariance Matrix Adaptation Evolution Strategy (CMA-ES) ..... 233
А. 3 Сходимость Q-learning ..... 234
А.3.1 Action Replay Process ..... 234
А.3.2 Ключевые свойства ARP ..... 235
А.3.3 Схожесть ARP и настоящего MDP ..... 236

---

# ГЛАВА 1 

## Задача обучения с подкреплением

Ясно, что обучение с учителем это не та модель «обучения», которая свойственна интеллектуальным сущностям. Термин «обучение» здесь подменяет понятие интерполяции или, если уж на то пошло, построение алгоритмов неявным образом («смотрите, оно само обучилось, я сам ручками не прописывал, как кошечек от собачек отличать»). К полноценному обучению, на которое способен не только человек, но и в принципе живые организмы, задачи классического машинного обучения имеют лишь косвенное отношение. Значит, нужна другая формализация понятия «задачи, требующей интеллектуального решения», в которой обучение будет проводится не на опыте, заданном прецедентно, в виде обучающей выборки.

Термин подкрепление (reinforcement) пришёл из поведенческой психологии и обозначает награду или наказание за некоторый получившийся результат, зависящий не только от самих принятых решений, но и внешних, не обязательно подконтрольных, факторов. Под обучением здесь понимается поиск способов достичь желаемого результата методом проб и ошибок (trial and error), то есть попыток решить задачу и использование накопленного опыта для усовершенствования своей стратегии в будущем.

В данной главе будут введены основные определения и описана формальная постановка задачи. Под желаемым результатом


мы далее будем понимать максимизацию некоторой скалярной величины, называемой наградой (reward). Интеллектуальную сущность (систему/робота/алгоритм), принимающую решения, будем называть агентом (agent). Агент взаимодействует с миром (world) или средой (environment), которая задаётся зависящим от времени состоянием (state). Агенту в каждый момент времени в общем случае доступно только некоторое наблюдение (observation) текущего состояния мира. Сам агент задаёт процедуру выбора действия (action) по доступным наблюдениям; эту процедуру далее будем называть стратегией или политикой (policy). Процесс взаимодействия агента и среды задаётся динамикой средь (world dynamics), определяющей правила смены состояний среды во времени и генерации награды.

Буквы $s, a, r$ зарезервируем для состояний, действий и наград соответственно; буквой $t$ будем обозначать время в процессе взаимодействия.

## §1.1. Модель взаимодействия агента со средой

### 1.1.1. Связь с оптимальным управлением

Математика поначалу пришла к очень похожей формализации следующим образом. Для начала, нужно ввести какую-то модель мира; в рамках лапласовского детерминизма можно предположить, что положение, скорости и ускорения всех атомов вселенной задают её текущее состояние, а изменение состояния происходит согласно некоторым дифференциальным уравнениям:

$$
\dot{s}=f(s, t)
$$

Коли агент может как-то взаимодействовать со средой или влиять на неё, мы можем промоделировать взаимодействие следующим образом: скажем, что в каждый момент времени $t$ агент в зависимости от текущего состояния среды $s$ выбирает некоторое действие («управление») $a(s, t)$ :

$$
\dot{s}=f(s, a(s, t), t)
$$

---

Для моделирования награды положим, что в каждый момент времени агент получает наказание или штраф (cost) ${ }^{1}$ в объёме $\boldsymbol{L}(\boldsymbol{s}, \boldsymbol{a}(\boldsymbol{s}, \boldsymbol{t}), \boldsymbol{t})$. Итоговой наградой агента полагаем суммарную награду, то есть интеграл по времени:

$$
\left\{\begin{array}{l}
-\int \boldsymbol{L}(\boldsymbol{s}, \boldsymbol{a}(\boldsymbol{s}, \boldsymbol{t}), \boldsymbol{t}) \mathrm{d} \boldsymbol{t} \rightarrow \max _{\boldsymbol{a}(\boldsymbol{s}, \boldsymbol{t})} \\
\dot{\boldsymbol{s}}=\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a}(\boldsymbol{s}, \boldsymbol{t}), \boldsymbol{t})
\end{array}\right.
$$

За исключением того, что для полной постановки требуется задать ещё начальные и конечные условия, поставленная задача является общей формой задачи оптимального управления (optimal control). При этом обратим внимание на сделанные при постановке задачи предположения:

- время непрерывно;
- мир детерминирован;
- мир нестационарен (функция $\boldsymbol{f}$ напрямую зависит от времени $\boldsymbol{t}$ );
- модель мира предполагается известной, то есть функция $\boldsymbol{f}$ задана явно в самой постановке задачи;

Принципиально последнее: теория рассматривает способы для заданной системы дифференциальных уравнений поиска оптимальных $\boldsymbol{a}(\boldsymbol{s}, \boldsymbol{t})$ аналитически. Проводя аналогию с задачей максимизации некоторой, например, дифференцируемой функции $\boldsymbol{f}(\boldsymbol{x}) \rightarrow \max _{\boldsymbol{x}}$, теория оптимального управления ищет необходимые условия решения: например, что в экстремуме функции обязательно $\nabla_{\boldsymbol{x}} \boldsymbol{f}(\boldsymbol{x})=\mathbf{0}$. Никакого «обучения методом проб и ошибок» здесь не предполагается.

В обучении с подкреплением вместо поиска решения аналитически мы будем строить итеративные методы оптимизации, искать аналоги, например, градиентного спуска. В такой процедуре неизбежно появится цепочка приближений решений: мы начнём с какой-то функции, выбирающей действия, возможно, не очень удачной и не способной набрать большую награду, но с ходом работы алгоритма награда будет оптимизироваться и способ выбора агентом действий будет улучшаться. Так будет выглядеть обучение.

Также RL исходит из других предположений: время дискретно, а среда - стохастична, но стационарна. В частности, в рамках этой теории можно построить алгоритмы для ситуации, когда модель мира агенту неизвестна, и единственный способ поиска решений - обучение на собственном опыте взаимодействия со средой.

# 1.1.2. Марковская цепь 

В обучении с подкреплением мы зададим модель мира следующим образом: будем считать, что существуют некоторые «законы физики», возможно, стохастические, которые определяют следующее состояние среды по предыдущему. При этом предполагается, что в текущем состоянии мира содержится вся необходимая информация для выполнения перехода, или, иначе говоря, выполняется свойство Марковости (Markov property): процесс зависит только от текущего состояния и не зависит от всей предыдущей истории.

Определение 1: Марковской цепью (Markov chain) называется пара $(\mathcal{S}, \mathcal{P})$, где:

- $\mathcal{S}$ - множество состояний.
- $\mathcal{P}$ - вероятности переходов $\left\{\boldsymbol{p}\left(\boldsymbol{s}_{\boldsymbol{t}+1} \mid \boldsymbol{s}_{\boldsymbol{t}}\right) \mid \boldsymbol{t} \in\{\mathbf{0}, \mathbf{1}, \ldots\}, \boldsymbol{s}_{\boldsymbol{t}}, \boldsymbol{s}_{\boldsymbol{t}+1} \in \mathcal{S}\right\}$.

Если дополнительно задать стартовое состояние $\boldsymbol{s}_{\mathbf{0}}$, можно рассмотреть процесс, заданный марковской цепью. В момент времени $\boldsymbol{t}=\mathbf{0}$ мир находится в состоянии $\boldsymbol{s}_{\mathbf{0}}$; сэмплируется случайная величина $\boldsymbol{s}_{\mathbf{1}} \sim \boldsymbol{p}\left(\boldsymbol{s}_{\mathbf{1}} \mid \boldsymbol{s}_{\mathbf{0}}\right)$, и мир переходит в состояние $\boldsymbol{s}_{\mathbf{1}}$; сэмплируется $\boldsymbol{s}_{\mathbf{2}} \sim \boldsymbol{p}\left(\boldsymbol{s}_{\mathbf{2}} \mid \boldsymbol{s}_{\mathbf{1}}\right)$, и так далее до бесконечности.

Мы далее делаем важное предположение, что законы мира не изменяются с течением времени. В аналогии с окружающей нас действительностью это можно интерпретировать примерно как «физические константы не изменяются со временем». ${ }^{2}$

Определение 2: Марковская цепь называется однородной (time-homogeneous) или стационарной (stationary), если вероятности переходов не зависят от времени:

$$
\forall \boldsymbol{t}: \boldsymbol{p}\left(\boldsymbol{s}_{\boldsymbol{t}+1} \mid \boldsymbol{s}_{\boldsymbol{t}}\right)=\boldsymbol{p}\left(\boldsymbol{s}_{\mathbf{1}} \mid \boldsymbol{s}_{\mathbf{0}}\right)
$$

По определению, переходы $\mathcal{P}$ стационарных марковских цепей задаются единственным условным распределением $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}\right)$. Апостроф ${ }^{\prime}$ канонично используется для обозначения «следующих» моментов времени, мы будем активно пользоваться этим обозначением.

[^0]
[^0]:    ${ }^{1}$ теория оптимального управления строилась в СССР в формализме минимизации штрафов (потерь, убытков и наказаний), когда построенная в США теория обучения с подкреплением - в формализме максимизации награды (прибыли, счастья, тортиков). Само собой, формализмы эквивалентны, и любой штраф можно переделать в награду домножением на минус, и наоборот.
    ${ }^{2}$ вопрос на засыпку для любителей философии: а это вообще правда? Вдруг там через миллион лет гравитационная постоянная или скорость света уже будут другими в сорок втором знаке после запятой?..

---

Пример 1 - Канечные марковские цепи: Марковские цепи с конечным числом состояний можно задать при помощи ориентированного графа, дугам которого поставлены в соответствие вероятности переходов (отсутствие дуги означает нулевую вероятность перехода).

На рисунке приведён пример стационарной марковской цепи с 4 состояниями. Такую цепь можно также задать в виде матрицы переходов:



# 1.1.3. Среда 

Как и в оптимальном управлении, для моделирования влияния агента на среду в вероятности переходов достаточно добавить зависимость от выбираемых агентом действий. Итак, наша модель среды - это «управляемая» марковская цепь.

Определение 3: Средой (environment) называется тройка ( $\mathcal{S}, \mathcal{A}, \mathcal{P}$ ), где:

- $\mathcal{S}$ - пространство состояний (state space), некоторое множество.
- $\mathcal{A}$ - пространство деи́ствий (action space), некоторое множество.
- $\mathcal{P}$ - функиия переходов (transition function) или динамика среды (world dynamics): вероятности $p\left(s^{\prime} \mid s, a\right)$.

В таком определении среды заложена марковость (независимость переходов от истории) и стационарность (независимость от времени). Время при этом дискретно, в частности, нет понятия «времени принятия решения агентом»: среда, находясь в состоянии $s$, ожидает от агента действие $a \in \mathcal{A}$, после чего совершает шаг, сэмплируя следующее состояние $s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$.

По дефолту, среда также предполагается полностью наблюдаемой (fully observable): агенту при выборе $a_{t}$ доступно всё текущее состояние $s_{t}$ в качестве входа. Иными словами, в рамках данного предположения понятия состояния и наблюдения для нас будут эквивалентны. Мы будем строить теорию в рамках этого упрощения; если среда не является полностью наблюдаемой, задача существенно усложняется, и необходимо переходить к формализму частично наблюдаемых MDP (partially observable MDP, PoMDP). Обобщение алгоритмов для PoMDP будет рассмотрено отдельно в разделе 8.5 .

Пример 2 - Канечные среды: Среды с конечным числом состояний и конечным числом действий можно задать при помощи ориентированного графа, где для каждого действия задан свой комплект дуг.

На рисунке приведён пример среды с 2 действиями $\mathcal{A}=\{0, \square\}$; дуги для разных действий различаются цветом. Возле каждого состояния выписана стратегия агента.


---

Пример 3 - Кубик-Рубик: Пространство состояний - пространство конфигураций Кубика-Рубика. Пространство действий состоит из 12 элементов (нужно выбрать одну из 6 граней, каждую из которых можно повернуть двумя способами). Следующая конфигурация однозначно определяется текущей конфигурацией и действием, соответственно среда Кубика-Рубика детерминирована: задаётся вырожденным распределением, или, что тоже самое, обычной детерминированной функцией $s^{\prime}=f(s, a)$.


# 1.1.4. Действия 

Нас будут интересовать два вида пространства действий $\mathcal{A}$ :
a) конечное, или дискретное пространство действий (discrete action space): $|\mathcal{A}|<+\infty$. Мы также будем предполагать, что число действий $|\mathcal{A}|$ достаточно мало.
б) непрерывное пространство действий (continuous domain): $\mathcal{A} \subseteq[-1,1]^{m}$. Выбор именно отрезков $[-1,1]$ является не ограничивающем общности распространённым соглашением. Задачи с таким пространством действий также называют задачами непрерывного управления (continuous control).

Заметим, что множество действий не меняется со временем и не зависит от состояния. Если в практической задаче предполагается, что множество допустимых действий разное в различных состояний, в «законы физики» прописывается реакция на некорректное действие, например, случайный выбор корректного действия за агента.

В общем случае процесс выбора агентом действия в текущем состоянии может быть стохастичен. Таким образом, объектом поиска будет являться распределение $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}), \boldsymbol{a} \in \mathcal{A}, \boldsymbol{s} \in \mathcal{S}$. Заметим, что факт, что нам будет достаточно искать стратегию в классе стационарных (не зависящих от времени), вообще говоря, потребует обоснования.

### 1.1.5. Траектории

I Определение 4: Набор $\mathcal{T}:=\left(s_{0}, a_{0}, s_{1}, a_{1}, s_{2}, a_{2}, s_{3}, a_{3} \ldots\right)$ называется траекторией.

Пример 4: Пусть в среде состояния описываются одним вещественным числом, $\mathcal{S} \equiv \mathbb{R}$, у агента есть два действия $\mathcal{A}=\{+1,-1\}$, а следующее состояние определяется как $s^{\prime}=s+a+\varepsilon$, где $\varepsilon \sim \mathcal{N}(0,1)$. Начальное состояние полагается равным нулю $s_{0}=0$. Сгенерируем пример траектории для случайной стратегии (вероятность выбора каждого действия равна 0.5 ):


Поскольку траектории - это случайные величины, которые заданы по постановке задачи конкретным процессом порождения (действия генерируются из некоторой стратегии, состояния - из функции переходов), можно расписать распределение на множестве траекторий:

Определение 5: Для данной среды, политики $\pi$ и начального состояния $s_{0} \in \mathcal{S}$ распределение, из которого приходят траектории $\mathcal{T}$, называется trajectory distribution:

$$
p(\mathcal{T})=p\left(a_{0}, s_{1}, a_{1} \ldots\right)=\prod_{t \geq 0} \pi\left(a_{t} \mid s_{t}\right) p\left(s_{t+1} \mid s_{t}, a_{t}\right)
$$

Мы часто будем рассматривать мат.ожидания по траекториям, которые будем обозначать $\mathbb{E}_{\mathcal{T}}$. Под этим подразумевается бесконечная цепочка вложенных мат.ожиданий:

$$
\mathbb{E}_{\mathcal{T}}(\cdot):=\mathbb{E}_{\pi\left(a_{0} \mid s_{0}\right)} \mathbb{E}_{p\left(s_{1} \mid s_{0}, a_{0}\right)} \mathbb{E}_{\pi\left(a_{1} \mid s_{1}\right)} \ldots(\cdot)
$$

---

Поскольку часто придётся раскладывать эту цепочку, договоримся о следующем сокращении:

$$
\mathbb{E}_{\boldsymbol{T}}(\cdot)=\mathbb{E}_{\boldsymbol{a}_{0}} \mathbb{E}_{\boldsymbol{s}_{1}} \mathbb{E}_{\boldsymbol{a}_{1}} \ldots(\cdot)
$$

Однако в такой записи стоит помнить, что действия приходят из некоторой зафиксированной политики $\boldsymbol{\pi}$, которая неявно присутствует в выражении. Для напоминания об этом будет, где уместно, использоваться запись $\mathbb{E}_{\boldsymbol{T} \sim \pi}$.

# 1.1.6. Марковский процесс принятия решений (MDP) 

Для того, чтобы сформулировать задачу, нам необходимо в среде задать агенту цель - некоторый функционал для оптимизации. По сути, марковский процесс принятия решений - это среда плюс награда. Мы будем пользоваться следующим определением:

```
    Определение 6: Марковский процесс принятия решений (Markov Decision Process, MDP) - это четвёрка
    \((\mathcal{S}, \mathcal{A}, \mathcal{P}, \boldsymbol{r})\), где:
```

- $\mathcal{S}, \mathcal{A}, \mathcal{P}-$ среда.
- $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}-\boldsymbol{\text { функиия награды (reward function). }}$

Сам процесс выглядит следующим образом. Для момента времени $\boldsymbol{t}=\mathbf{0}$ начальное состояние мира полагается $\boldsymbol{s}_{\mathbf{0}}$; будем считать, оно дано дополнительно и фиксировано ${ }^{3}$. Агент наблюдает всё состояние целиком и выбирает действие $\boldsymbol{a}_{0} \in \boldsymbol{\mathcal { A }}$. Среда отвечает генерацией награды $\boldsymbol{r}\left(\boldsymbol{s}_{0}, \boldsymbol{a}_{0}\right)$ и смплирует следующее состояние $\boldsymbol{s}_{\mathbf{1}} \sim p\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}_{0}, \boldsymbol{a}_{0}\right)$. Агент выбирает $\boldsymbol{a}_{1} \in \boldsymbol{\mathcal { A }}$, получает награду $\boldsymbol{r}\left(\boldsymbol{s}_{1}, \boldsymbol{a}_{1}\right)$, состояние $\boldsymbol{s}_{2}$, и так далее до бесконечности.


Пример 5: Для среды из примера 4 зададим функцию награды как $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})=-\mid \boldsymbol{s}+\mathbf{4 . 2} \mid$. Независимость награды от времени является требованием стационарности к рассматриваемым MDP:


Формальное введение MDP в разных источниках чуть отличается, и из-за различных договорённостей одни и те же утверждения могут выглядеть совсем непохожим образом в силу разных исходных обозначений. Важно, что суть и все основные теоретические результаты остаются неизменными.

Теорема 1 - Эквивалентные определения MDP: Эквивалентно рассматривать MDP, где

- функция награды зависит только от текущего состояния;
- функция награды является стохастической;
- функция награды зависит от тройки $\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}\right)$;
- переходы и генерация награды задаётся распределением $\boldsymbol{p}\left(\boldsymbol{r}, \boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$;
- начальное состояние стохастично и генерируется из некоторого распределения $\boldsymbol{s}_{\mathbf{0}} \sim \boldsymbol{p}\left(\boldsymbol{s}_{\mathbf{0}}\right)$.

Скетч доказательства. Покажем, что всю стохастику можно «засовывать» в $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Например, стохастичность начального состояния можно «убрать», создав отдельное начальное состояние, из которого на первом шаге агент вне зависимости от выбранного действия перейдёт в первое по стохастичному правилу.

[^0]
[^0]:    ${ }^{3}$ формально его можно рассматривать как часть заданного MDP.

---

Покажем, что от самого общего случая (генерации награды и состояний из распределения $\boldsymbol{p}\left(\boldsymbol{r}, \boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ ) можно перейти к детерминированной функции награды только от текущего состояния. Добавим в описание состояний информацию о последнем действии и последней полученной агентом награде, то есть для каждого возможного (имеющего ненулевую вероятность) перехода ( $s, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$ ) размножим $\boldsymbol{s}^{\prime}$ по числу* возможных исходов $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a})$ и по числу действий. Тогда можно считать, что на очередном шаге вместо $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ агенту выда-


ётся $\boldsymbol{r}\left(s^{\prime}\right)$, и вся стохастика процесса формально заложена только в функции переходов.
*которое в худшем случае континуально, так как награда - вещественный скаляр.
Считать функцию награды детерминированной удобно, поскольку позволяет не городить по ним мат. ожидания (иначе нужно добавлять сэмплы наград в определение траекторий). В любом формализме всегда принято считать, что агент сначала получает награду и только затем наблюдает очередное состояние.
| Определение 7: MDP называется конечным (finite MDP) или табличным, если пространства состояний и действий конечны: $|\mathcal{S}|<\infty,|\mathcal{A}|<\infty$.

Пример 6: Для конечных MDP можно над дугами в графе среды указать награду за переход по ним или выдать награду состояниям (в рамках нашего формализма награда «за состояние» будет получена, давайте считать, при выполнении любого действия из данного состояния).


# 1.1.7. Эпизодичность 

Во многих случаях процесс взаимодействия агента со средой может при определённых условиях «закаичиваться», причём факт завершения доступен агенту.
| Определение 8: Состояние $\boldsymbol{s}$ называется терминалъным (terminal) в MDP, если $\forall \boldsymbol{a} \in \mathcal{A}$ :

$$
\mathbf{P}\left(s^{\prime}=s \mid s, a\right)=1 \quad r(s, a)=0
$$

то есть с вероятностью 1 агент не сможет покинуть состояние.
Пример 7: В примере 6 самое правое состояние является терминальным.
Считается, что на каждом шаге взаимодействия агент дополнительно получает для очередного состояния $\boldsymbol{s}$ значение предиката done $(\boldsymbol{s}) \in\{0,1\}$, является ли данное состояние терминальным. По сути, после попадания в терминальное состояние дальнейшее взаимодействие бессмысленно (дальнейшие события тривиальны), и, считается, что возможно произвести reset среды в $\boldsymbol{s}_{\mathbf{0}}$, то есть начать процесс взаимодействия заново. Введение терминальных состояний именно таким способом позволит всюду в теории писать суммы по времени до бесконечности, не рассматривая отдельно случай завершения за конечное время.

Для агента все терминальные состояния в силу постановки задачи неразличимы, и их описания среда обычно не возвращает (вместо этого на практике она обычно проводит ресет и возвращает $\boldsymbol{s}_{\mathbf{0}}$ следующего эпизода).

---

Определение 9: Один цикл процесса от стартового состояния до терминального называется эпизодом (episode).

Продолжительности эпизодов (количество шагов взаимодействия) при этом, конечно, могут различаться от эпизода к эпизоду.
Определение 10: Среда называется эпизодичной (episodic), если для любой стратегии процесс взаимодействия гарантированно завершается не более чем за некоторое конечное $\boldsymbol{T}^{\text {max }}$ число шагов.

Теорема 2 - Граф эпизодичных сред есть дерево: В эпизодичных средах вероятность оказаться в одном и том же состоянии дважды в течение одного эпизода равна нулю.

Доказательство. Если для некоторого состояния $\boldsymbol{s}$ при некоторой комбинации действий через $\boldsymbol{T}$ шагов агент с вероятностью $\boldsymbol{p}>\mathbf{0}$ пернётся в $\boldsymbol{s}$, при повторении такой же комбинации действий в силу марковости с вероятностью $p^{n}>\mathbf{0}$ эпизод будет длиться не менее $\boldsymbol{n} \boldsymbol{T}$ шагов для любого натурального $\boldsymbol{n}$. Иначе говоря, эпизоды могут быть неограниченно долгими.

Пример 8 - Cartpole: K тележке на шарнире крепится стержень с грузиком. Два действия позволяют придать тележке ускорение вправо или влево. Состояние описывается двумя числами: х-координатой тележки и углом, на которой палка отклонилась от вертикального положения.

Состояние считается терминальным, если х-координата стала слишком сильно отличной от нуля (тележка далеко уехала от своего исходного положения), или если палка отклонилась на достаточно большой угол.

Агент получает +1 каждый шаг и должен как можно дольше избегать терминальных состояний. Гарантии завершения эпизодов в этом MDP нет: агент в целом может справляться с задачей бесконечно долго.


На практике, в средах обычно существуют терминальные состояния, но нет гарантии завершения эпизодов за ограниченное число шагов. Это лечат при помощи таймера - жёсткого ограничения, требующего по истечении $T^{\max }$ шагов проводить в среде ресет. Чтобы не нарушить теоретические предположения, необходимо тогда заложить информацию о таймере в описание состояний, чтобы агент знал точное время до прерывания эпизода. Обычно так не делают; во многих алгоритмах RL будет возможно использовать только «начала» траекторий, учитывая, что эпизод не был доведён до конца. Формально при этом нельзя полагать done $\left(s_{T^{\max }}\right)=1$, но в коде зачастую так всё равно делают. Например, для Cartpole в реализации OpenAI Gym по умолчанию по истечении 200 шагов выдаётся флаг done, что формально нарушает марковское свойство.

# 1.1.8. Дисконтирование 

Наша задача заключается в том, чтобы найти стратегию $\boldsymbol{\pi}$, максимизирующую среднюю суммарную награду. Формально, нам явно задан функционал для оптимизации:

$$
\mathbb{E}_{\boldsymbol{T} \sim \pi} \sum_{t \geq 0} r_{t} \rightarrow \max _{\boldsymbol{\pi}}
$$

где $\boldsymbol{r}_{\boldsymbol{t}}:=\boldsymbol{r}\left(\boldsymbol{s}_{\boldsymbol{t}}, \boldsymbol{a}_{\boldsymbol{t}}\right)-$ награда на шаге $\boldsymbol{t}$.
Мы хотим исключить из рассмотрения MDP, где данный функционал может улететь в бесконечность ${ }^{4}$ или не существовать вообще. Во-первых, введём ограничение на модуль награды за шаг, подразумевая, что среда не может поощрять или наказывать агента бесконечно сильно:

$$
\forall s, a:|r(s, a)| \leq r^{\max }
$$

Чтобы избежать парадоксов, этого условия нам не хватит ${ }^{5}$. Введём дисконтирование (discounting), коэффициент которого традиционно обозначают $\boldsymbol{\gamma}$ :
Определение 11: Дисконтированной кумулятивной наградой (discounted cumulative reward) или total

[^0]
[^0]:    ${ }^{4}$ если награда может быть бесконечной, начинаются всякие парадоксы, рассмотрения которых мы хотим избежать. Допустим, в некотором MDP без терминальных состояний мы знаем, что оптимальная стратегия способна получать +1 на каждом шаге, однако мы смогли найти стратегию, получающую +1 лишь на каждом втором шаге. Формально, средняя суммарная награда равна бесконечности у обоих стратегий, однако понятно, что найденная стратегия «неоптимальна».
    ${ }^{5}$ могут возникнуть ситуации, где суммарной награды просто не существует (например, если агент в бесконечном процессе всегда получает +1 на чётных шагах и -1 на нечётных).

---

return для траектории $\mathcal{T}$ с коэффициентом $\gamma \in(0,1]$ называется

$$
R(\mathcal{T}):=\sum_{t \geq 0} \gamma^{t} r_{t}
$$

У дисконтирования есть важная интерпретация: мы полагаем, что на каждом шаге с вероятностью $1-\gamma$ взаимодействие обрывается, и итоговым результатом агента является та награда, которую он успел собрать до прерывания. Это даёт приоритет получению награды в ближайшее время перед получением той же награды через некоторое время. Математически смысл дисконтирования, во-первых, в том, чтобы в совокупности с требованием (1.3) гарантировать ограниченность оптимизируемого функционала, а во-вторых, выполнение условий некоторых теоретических результатов, которые явно требуют $\gamma<1$. В силу последнего, гамму часто рассматривают как часть MDP.

Определение 12: Скором (score или регformance) стратегии $\boldsymbol{\pi}$ в данном MDP называется

$$
J(\pi):=\mathbb{E}_{\mathcal{T} \sim \pi} R(\mathcal{T})
$$

Итак, задачей обучения с подкреплением является оптимизация для заданного MDP средней дисконтированной кумулятивной награды:

$$
J(\pi) \rightarrow \max _{\pi}
$$

Пример 9: Посчитаем $J(\pi)$ для приведённого на рисунке MDP, $\gamma=\frac{10}{11}$ и начального состояния А. Ясно, что итоговая награда зависит только от стратегии агента в состоянии А, поэтому можно рассмотреть все стратегии, обозначив $\pi(a=\square \mid s=A)$ за параметр стратегии $\theta \in[0,1]$.

С вероятностью $\boldsymbol{\theta}$ агент выберет действие $\square$, после чего попадёт в состояние В с вероятностью 0.8 . Там вне зависимости от стратегии он начнёт крутится и получит в пределе

$$
\gamma+\gamma^{2}+\cdots+=\sum_{t \geq 1} \gamma^{t}=\frac{\gamma}{1-\gamma}=10
$$

Ещё с вероятностью $1-\theta$ агент выберет $\square$ получит +3 и попадёт в терминальное состояние, после чего эпизод завершится. Итого:


Видно, что оптимально выбрать $\boldsymbol{\theta}=\mathbf{1}$, то есть всегда выбирать действие $\square$. Заметим, что если бы мы рассмотрели другое $\gamma$, мы бы могли получить другую оптимальную стратегию; в частности, при $\gamma=\frac{15}{10}$ значение $J(\pi)$ было бы константным для любых стратегий, и оптимальными были бы все стратегии.

Всюду далее подразумевается ${ }^{6}$ выполнение требования ограниченности награды (1.3), а также или дисконтирования $\gamma<1$, или эпизодичности среды.

Утверждение 1: При сделанных предположениях скор ограничен.
Доказательство. Если $\gamma<1$, то по свойству геометрической прогрессии для любых траекторий $\mathcal{T}$ :

$$
R(\mathcal{T})=\left|\sum_{t \geq 0} \gamma^{t} r_{t}\right| \leq \frac{1}{1-\gamma} r^{\max }
$$

Если же $\gamma=1$, но эпизоды гарантированно заканчиваются не более чем за $T^{\max }$ шагов, то суммарная награда не превосходит по модулю $\boldsymbol{T}^{\max } \boldsymbol{r}^{\max }$. Следовательно, скор как мат.ожидание от ограниченной величины также удовлетворяет этим ограничениям.

[^0]
[^0]:    ${ }^{6}$ в качества акта педантичности оговоримся, что также всюду подразумевается измеримость всех функций, необходимая для существования всех рассматриваемых интегралов и мат.ожиданий.

---

# §1.2. Алгоритмы обучения с подкреплением 

### 1.2.1. Условия задачи RL

Основной постановкой в обучении с подкреплением является задача нахождения оптимальной стратегии на основе собственного опыта взаимодействия. Это означает, что алгоритму обучения изначально доступно только:

- вид пространства состояний - количество состояний в нём в случае конечного числа, или размерность пространства $\mathbb{R}^{d}$ в случае признакового описания состояний.
- вид пространства действий - непрерывное или дискретное. Некоторые алгоритмы будут принципиально способны работать только с одним из этих двух видов.
- взаимодействие со средой, то есть возможность для предоставленной алгоритмом стратегии $\pi$ генерировать траектории $\mathcal{T} \sim \pi$; иными словами, принципиально доступны только сэмплы из trajectory distribution $(1.1)$.

Примерно так выглядит MDP для начинающего обучение агента:


Итак, в отличие от обучения с учителем, где датасет «дан алгоритму на вход», здесь агент должен сам собрать данные. Находясь в некотором состоянии, обучающийся агент обязан выбрать ровно одно действие, получить ровно один сэмпл $s^{\prime}$ и продолжить взаимодействие (накопление опыта - сбор сэмплов) из $s^{\prime}$. Собираемые в ходе взаимодействия данные и представляют собой всю доступную агенту информацию для улучшения стратегии.
| Определение 13: Пятёрки $\mathbb{T} \equiv\left(s, a, r, s^{\prime}\right.$, done $)$, где $r \equiv r(s, a), s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$, done $\equiv \operatorname{done}\left(s^{\prime}\right)$, называются переходами (transitions).

Таким образом, в RL-алгоритме должна быть прописана стратегия взаимодействия со средой во время обучения (behavior policy), которая может отличаться от «итоговой» стратегии (target policy), предназначенной для использования в среде по итогам обучения.

### 1.2.2. Сравнение с обучением с учителем

Необходимость собирать данные внутри самого алгоритма - одно из ключевых отличий задачи RL от обучения с учителем, где выборка подаётся алгоритму на вход. Здесь же «сигнал» для обучения предоставляется дизайном функции награды; на практике, чтобы говорить о том, что встретилась задача RL, необходимо задать MDP (предоставить как среду в виде симулятора или интерфейс для взаимодействия настоящего робота в реальном мире, так и функцию награды). Зачастую если какую-то «репрезентативную» выборку собрать можно, всегда проще и лучше обратиться к обучению с учителем как менее общей постановке задачи.

Утверждение 2: Задача обучения с учителем (с заданной функцией потерь) является частным случаем задачи RL.

Доказательство. Пусть дана обучающая выборка в виде пар $(\boldsymbol{x}, \boldsymbol{y}), \boldsymbol{x}$ - входной объект, $\boldsymbol{y}$ - целевая переменная. Скажем, что начальное состояние есть описание объекта $\boldsymbol{x}$, случайно сэмплированного из обучающей выборки (начальное состояние будет случайно). Агент выбирает метку класса $\hat{\boldsymbol{y}} \sim \boldsymbol{\pi}(\hat{\boldsymbol{y}} \mid \boldsymbol{x})$. После этого он получает награду за шаг в в размере $-\operatorname{Loss}(\hat{\boldsymbol{y}}, \boldsymbol{y})$ и игра завершается. Оптимизация такой функции награды в среднем будет эквивалентно минимизации функции потерь в обучении с учителем.

---

Поскольку RL всё-таки предполагает, что в общем случае эпизоды длиннее одного шага, агент будет своими действиями влиять на дальнейшие состояния, которые он встречает. «Управление» марковской цепью куда более существенная часть оптимизации RL алгоритмов, нежели максимизация наград за шаг, именно из этого свойства возникает большинство специфических именно для RL особенностей. И зачастую именно в таких задачах появляется такая непреодолимая для обучения с учителем проблема, как то, что правильный ответ никакому человеку, в общем-то, неизвестен. Не знаем мы обычно оптимальный наилучший ход в шахматах или как правильно поворачивать конечности роботу, чтобы начать ходить.

Да, доступной помощью для алгоритма могут быть данные от эксперта, то есть записи взаимодействия со средой некоторой стратегии (или разных стратегий), не обязательно, вообще говоря, оптимальной. Алгоритм RL, возможно, сможет эти данные как-то использовать, или хотя бы как-либо на них предобучиться. В простейшем случае предобучение выглядит так: если в алгоритме присутствует параметрически заданная стратегия $\boldsymbol{\pi}_{\boldsymbol{\theta}}$, можно клинировать поведение (behavior cloning), т.е. восстанавливать по парам $\boldsymbol{s}, \boldsymbol{a}$ из всех собранных экспертом траекторий функцию $\mathcal{S} \rightarrow \mathcal{A}$ (это обычная задача обучения с учителем), учиться воспроизводить действия эксперта. Задача обучения по примерам её решения около-оптимальным экспертом называется имитационным обучением (imitation learning); её мы обсудим отдельно в разделе 8.1, однако, если эксперт не оптимален, обученная стратегия вряд ли будет действовать хоть сколько-то лучше. Здесь можно провести прямую аналогию с задачей обучения с учителем, где верхняя граница качества алгоритма определяется качеством разметки; если разметка зашумлена и содержит ошибки, обучение вряд ли удастся.

В общем же случае мы считаем, что на вход в алгоритм никаких данных эксперта не поступает. Поэтому говорят, что в обучении с подкреплением нам не даны «правильные действия», и, когда мы будем каким-либо образом сводить задачу к задачам регрессии и классификации, нам будет важно обращать внимание на то, как мы «собираем себе разметку» из опыта взаимодействия со средой и какое качество мы можем от этой разметки ожидать. В идеале, с каждым шагом алгоритм сможет собирать себе всё более и более «хорошую» разметку (получать примеры траекторий с всё большей суммарной наградой), за счёт неё выбирать всё более и более оптимальные действия, и так «вытягивать сам себя из болота».

Здесь важно задать следующий вопрос: а если у нас есть данные из очень плохой стратегии, что можно рассчитывать с них выучить? То есть если мы можем собирать лишь примеры траекторий, в которых агент совершенно случайно себя ведёт (и редко получает набирает положительную награду), можно ли с таких данных выучить, например, оптимальную стратегию? Оказывается, как мы увидим дальше, за счёт структуры задачи RL и формализма MDP ответ положительный. Само собой, такое обучение только будет страшно неэффективным как по времени работы, так и по требуемым объёмам данных.

# 1.2.3. Концепция model-free алгоритмов 

Ещё одним возможным существенным изменением сеттинга задачи является наличие у агента прямого доступа к функции переходов $\boldsymbol{p (} \boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}$ ) и функции награды.
| Определение 14: Будем говорить, что у агента есть симулятор или «доступ к функции переходов», если он знает функцию награды и может в любой момент процесса обучения сэмплировать произвольное число сэмплов из $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ для любого набора пар $\boldsymbol{s}, \boldsymbol{a}$.

Симулятор - по сути копия среды, которую агент во время обучения может откатывать к произвольному состоянию. Симулятор позволяет агенту строить свою стратегию при помощи планирования (planning) - рассмотрения различных вероятных версий предстоящего будущего и использования их для текущего выбора действия. При использовании планирования в идеальном симуляторе никакого процесса непосредственного обучения в алгоритме может не быть, поскольку если на руках есть симулятор, то данные собирать не нужно: можно из симулятора сколько захотим данных получить.

Пример 10: Примером задач, в которых у алгоритма есть симулятор, являются пятнашки или кубик-рубик. То есть, пытаясь создать алгоритм, собирающий кубик-рубик, мы, естественно, можем пользоваться знаниями о том, в какую конфигурацию переводят те или иные действия текущее положение, и таким образом напрашиваются какие-то алгоритмы разумного перебора - «планирование».


Пример 11: Любые задачи, в которых среда реализована виртуально, можно рассматривать как задачи, где у агента есть симулятор. Например, если мы хотим обучить бота в Марио, мы можем сказать: да у нас есть исходники кода Марио, мы можем взять любую игровую ситуацию (установить симулятор в любое состояние) и для любого действия посмотреть, что будет дальше. В RL по умолчанию считается, что в видеоиграх такого доступа нет: динамика среды изначально агенту неизвестна.

---

Пример 12: Примером задач, в которых у алгоритма принципиально нет симулятора, являются любые задачи реальной робототехники. Важно, что даже если окружение реального робота симулируется виртуально, такая симуляция неточна - отличается от реального мира. В таких ситуациях можно говорить, что имеется неидеальный симулятор. Отдельно стоит уточнить, доступен ли симулятор реальному роботу в момент принятия решения (возможно, симулятор реализован на куда более вычислительно мощной отдельной системе) - тогда он может использоваться во время обучения, но не может использоваться в итоговой стра-


тегии.

По умолчанию всегда считается, что доступа к динамике среды нет, и единственное, что предоставляется алгоритму - среда, с которой возможно взаимодействовать. Можно ли свести такую задачу к планированию? В принципе, алгоритм может пытаться обучать себе подобный симулятор - строить генеративную модель, по $\boldsymbol{s}, \boldsymbol{a}$ выдающую $\boldsymbol{s}^{\prime}, \boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a}), \operatorname{done}\left(\boldsymbol{s}^{\prime}\right)$ - и сводить таким образом задачу к планированию. Приближение тогда, естественно, будет неидеальным, да и обучение подобного симулятора сопряжено с рядом других нюансов. Например, в сложных средах в описании состояний может хранится колоссальное количество информации, и построение моделей, предсказывающих будущее, может оказаться вычислительно неподъёмной и неоправданно дорогой задачей.

Одна из фундаментальных парадигм обучения с подкреплением, вероятно, столь же важная, как парадигма end-to-end обучения для глубокого обучения - идея model-free обучения. Давайте не будем учить динамику среды и перебирать потенциальные варианты будущего для поиска хороших действий, а выучим напрямую связь между текущим состоянием и оптимальными действиями.
| Определение 15: Алгоритм RL классифицируется как model-free, если он не использует и не пытается выучить модель динамики среды $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$.

Пример 13: Очень похоже, что когда мы учимся кататься на велосипеде, мы обучаемся как-то в model-free режиме. Мы, находясь в некотором состоянии, не планируем, в какой конфигурации окажется велосипед при разных возможных поворотах наших рук, и вместо этого учимся в точности методом проб и ошибок: мы просто запоминаем, при каком ощущении положения тела как нужно поворачивать руль.

Здесь очень любопытно пофилософствовать на тему того, почему в таких задачах мы зачастую умеем «запоминать» на всю жизнь полученные знания, не разучиваясь ездить на велосипеде после долгих лет без тренировок. Например, нейросетевые модели, которые мы дальше будем обучать для решения задач RL, таким свойством не обладают, и, обучаясь на одном опыте, они старый забывают; если агента RL обучать кататься на велосипеде, а потом долго учить кататься на самокате, стратегия для велосипеда крайне вероятно «сломается».

Оказывается, человек тоже может разучиться ездить на велосипеде при помощи очень хитрой процедуры: нужно придумать такой «самокат», обучение езде на котором требует противоположных навыков, чем велосипед. В качестве такого «самоката» можно взять «обратный велосипед» («The Backwards Bicycle»): велосипед, в котором поворот руля влево отклоняет колесо вправо, и наоборот. Подробнее про этот эксперимент можно посмотреть в этом видео. Интересно, что обе стратегии - и для езды на велосипеде, и для езды на «обратном велосипеде» - восстанавливаются после некоторой тренировки (причём как-то подозрительно резко, с каким-то «фазовым переходом») и в конечном счёте уживаются вместе.

# 1.2.4. On-policy vs Off-policy 

B model-free алгоритмах сбор данных становится важной составной частью: определяя политику взаимодействия со средой (behavior policy), мы влияем на то, для каких состояний $\boldsymbol{s}, \boldsymbol{a}$ мы получим сэмпл $\boldsymbol{s}^{\prime}$ из функции переходов. Собираемые данные - траектории - алгоритм может запоминать, например, в памяти. Но не каждый алгоритм RL сможет пользоваться такими сохранёнными данными, и поэтому возникает ещё одна важная классификация RL алгоритмов.
| Определение 16: Алгоритм RL называется off-policy, если он может использовать для обучения опыт взаимодействия произвольной стратегии.

Определение 17: Алгоритм RL называется on-policy, если для очередной итерации алгоритма ему требуется опыт взаимодействия некоторой конкретной, предоставляемой самим алгоритмом, стратегии.

Некоторое пояснение названия этих терминов: обычно в нашем алгоритме явно или неявно будет присутствовать некоторая «текущая», «наилучшая» найденная стратегия $\boldsymbol{\pi}$ : та самая целевая политика (target policy), которую алгоритм выдаст в качестве ответа, если его работу прервать. Если алгоритм умеет улучшать её (проводить очередную итерацию обучения), используя сэмплы любой другой произвольной стратегии $\boldsymbol{\mu}$, то мы проводим «off-policy» обучение: обучаем политику «не по ней же самой». On-policy алгоритмам будет необходимо «отправлять в среду конкретную стратегию», поскольку они будут способны улучшать стратегию $\boldsymbol{\pi}$ лишь

---

по сэмплам из неё же самой, будут «привязаны к ней»; это существенное ограничение. Другими словами, если обучение с подкреплением - обучение на основе опыта, то on-policy алгоритмы обучаются на своих ошибках, когда off-policy алгоритмы могут учиться как на своих, так и на чужих ошибках.

Off-policy алгоритм должен уметь проводить очередной шаг обучения на произвольных траекториях, сгенерированных произвольными (возможно, разными, возможно, неоптимальными) стратегиями. Понятие принципиально важно тем, что алгоритм может потенциально переиспользовать траектории, полученные старой версией стратегии со сколь угодно давних итераций. Если алгоритм может переиспользовать опыт, но с ограничениями (например, только с недавних итераций, или только из наилучших траекторий), то мы всё равно будем относить его к on-policy, поскольку для каждой новой итерации алгоритма нужно будет снова собирать сколько-то данных. Это не означает, что для on-policy алгоритма совсем бесполезны данные от (возможно, неоптимального) эксперта; почти всегда можно придумать какую-нибудь эвристику, как воспользоваться ими для хотя бы инициализации (при помощи того же клонирования поведения). Важно, что off-policy алгоритм сможет на данных произвольного эксперта провести «полное» обучение, то есть условно сойтись к оптимуму при достаточном объёме и разнообразии экспертной информации, не потребовав вообще никакого дополнительного взаимодействия со средой.

# 1.2.5. Классификация RL-алгоритмов 

При рассмотрении алгоритмов RL мы начнём с рассмотрения именно model-free алгоритмов и большую часть времени посвятим им. Их часто делят на следующие подходы:

- мета-эвристики (metaheuristic) никак не используют внутреннюю структуру взаимодействия среды и агента, и рассматривают задачу максимизации $\boldsymbol{J}(\boldsymbol{\pi})$ (1.5) как задачу «black box оптимизации»: можно примерно оценивать, чему равно значение функционала для разных стратегий, а структура задачи - формализм MDP - не используется; мы рассмотрим мета-эвристики в главе 2 как не требующие построения особой теории.
- value-based алгоритмы получают оптимальную стратегию неявно через теорию оценочных функций, которую мы рассмотрим в главе 3. Эта теория позволит нам построить value-based алгоритмы (глава 4) и будет использоваться всюду далее.
- policy gradient алгоритмы максимазируют $\boldsymbol{J}(\boldsymbol{\pi})$, используя оценки градиента функционала по параметрам стратегии; мы сможем помочь процессу оптимизации, правильно воспользовавшись оценочными функциями (глава 5).

Затем в главе 6 мы отдельно обсудим несколько алгоритмов специально для непрерывных пространств действий, находящихся на стыке value-based и policy gradient подхода, и увидим, что между ними довольно много общего. Наконец, model-based алгоритмы, которые учат или используют предоставленную модель среды $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, и которые обычно выделяют в отдельную категорию, будут рассмотрены после в главе 7.

### 1.2.6. Критерии оценки RL-алгоритмов

При оценивании алгоритмов принципиально соотношение трёх критериев:

- performance: Монте-Карло оценка значения $\boldsymbol{J}(\boldsymbol{\pi})$;
- wall-clock time: реальное время работы, потребовавшееся алгоритму для достижения такого результата (в полной аналогии с классическими методами оптимизации);
- sample efficiency: количество сэмплов (или шагов) взаимодействия со средой, потребовавшихся алгоритму. Этот фактор может быть ключевым, если взаимодействие со средой дорого (например, обучается реальный робот);

Поскольку победами над кожаными мешками в дотах уже никого не удивишь, вторые два фактора начинают играть всю большую роль.

Конечно, на практике мы хотим получить как можно больший performance при наименьших затратах peсурсов. Время работы алгоритма (wall-clock time) и эффективность по сэмплам связаны между собой, но их следует различать в силу того, что в разных средах сбор данных - проведение одного шага взаимодействия в среде - может быть как относительно дешёвым, так и очень дорогим. Например, если мы говорим о реальных роботах, то один шаг взаимодействия со средой - сверхдорогая операция, по сравнению с которыми любые вычисления на компьютере можно считать очень дешёвыми. Тогда нам выгодно использовать алгоритм, который максимально эффективен по сэмплам. Если же среда задана симулятором, а у нас ещё и есть куча серверов, чтобы параллельно запустить много таких симуляторов, то сбор данных для нас становится дешёвой процедурой. Тогда выгодно не гнаться за sample efficiency и выбирать вычислительно дешёвые алгоритмы.

---

Очень условно классы RL алгоритмов располагаются по sample efficiency в следующем порядке: самыми неэффективными по требуемым объёмам данных алгоритмами являются мета-эвристики. Зато в них практически не будет никаких вычислений: очень условно, на каждое обновление весов модели будет приходиться сбор нескольких сотен переходов в среде. Их будет иметь смысл применять, если есть возможность параллельно собирать много данных.

Далее, в Policy Gradient подходе мы будем работать в on-policy режиме: текущая, целевая, политика будет отправляться в среду, собирать сколько-то данных (например, мини-батч переходов, условно проводить порядка 32-64 шагов в среде) и затем делать одно обновление модели. Очень приближённо и с массой условностей говорят, что эффективность по сэмплам будет на порядок выше, чем у эволюционных алгоритмов, за счёт проведения на порядок большего количества вычислений: на одно обновление весов приходится сбор 30-60 переходов.

B value-based подходе за счёт off-policy режима работа можно будет контролировать соотношение числа собираемых переходов к количеству обновлений весов, но по умолчанию обычно полагают, что алгоритм собирает 1 переход и проводит 1 итерацию обучения. Таким образом, вычислений снова на порядок больше, как и (потенциально) sample efficiency.

Наконец, model-based вычислительно страшно тяжеловесные алгоритмы, но за счёт этого можно попытаться добиться максимальной эффективности по сэмплам.

Отразить четыре класса алгоритмов можно на условной картинке:


В зависимости от скорости работы среды, то есть времени, затрачиваемой на сбор данных, а также от особенностей среды (связанных непосредственно со спецификой этих четырёх классов алгоритмов), оптимальное соотношение ресурсы-качество может достигаться на разных классах алгоритмов. Но, к сожалению, на практике редко бывает очевидно, алгоритмы какого класса окажутся наиболее подходящими в конкретной ситуации.

В отличие от классических методов оптимизации, речи о критерии останова идти не будет, поскольку адекватно разумно проверить около-оптимальность текущей стратегии не представляется возможным в силу слишком общей постановки задачи. Считается, что оптимизация (обучение за счёт получения опыта взаимодействия) происходит, пока доступны вычислительные ресурсы; в качестве итога обучения предоставляется или получившаяся стратегия, или наилучшая встречавшаяся в ходе всего процесса.

# 1.2.7. Сложности задачи RL 

Обсудим несколько «именованных» проблем задачи обучения с подкреплением, с которыми сталкивается любой алгоритм решения.

Проблема застревания в локальных оптимумах приходит напрямую из методов оптимизации. В оптимизируемом функционале (1.5) может существовать огромное количество стратегий $\boldsymbol{\pi}$, для которых его значение далеко не максимально, но все в некотором смысле «соседние» стратегии дают в среднем ещё меньшую награду.

Пример 14: Часто агент может выучить тривиальное «пассивное» поведение, которое не приносит награды, но позволяет избегать каких-то штрафов за неудачи. Например, агент, который хочет научиться перепрыгивать через грабли, чтобы добраться до тортика ( +1 ), может несколько раз попробовать отправиться за призом, но наступить на грабли (-1), и выучить ничего не делать $(+0)$. Ситуация весьма типична: например, Марио может пару раз попробовать отправиться покорять первый уровень, получить по башке и решить не ходить вправо, а тупить в стену и получать «безопасный» +0 . Для нашей задачи оптимизации это типичнейшие локальные экстремумы: «похожие стратегии» набирают меньше текущей, и, чтобы добраться до большей награды, нужно как-то найти совершенно новую область в пространстве стратегий.


Другие проблемы куда более характерны именно для RL. Допустим, агент совершает какое-то действие, которое запускает в среде некоторый процесс. Процесс протекает сам по себе без какого-либо дальнейшего вмешательства агента и завершается через много шагов, приводя к награде. Это проблема отложенного сигнала (delayed reward) - среда даёт фидбэк агенту спустя какое-то (вообще говоря, неограниченно длительное) время.

---

Пример 15: Пример из видеовгр - в игре Atari Space Invaders очки даются в момент, когда удачный выстрел попал во вражеское НЛО, а не когда агент этот выстрел, собственно, совершает. Между принятием решения и получением сигнала проходит до нескольких секунд, и за это время агент принимает ещё несколько десятков решений.


Если бы этого эффекта вдруг не было, и агент за каждое своё действие мгновенно получал бы всю награду $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$, то решением задачи было бы банальное $\boldsymbol{\pi}(\boldsymbol{s})=\operatorname{argmax} \boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ - жадная оптимизация функции награды. Очевидно, это никогда не решение: «часть» награды сидит в $s^{\prime}$, в сэмпле следующего состояния, которого мы добились своим выбором действия, и в описании этого следующего состояния в силу свойства марковости обязана хранится информация о том, сколько времени осталось до получения награды за уже совершённые действия.

Смежная проблема - какое именно из многих совершённых агентом действий привело к сигналу награды? Credit assignment problem - даже для уже собранного опыта может быть тяжело оценить, какие действия были правильными, а какие нет.

Пример 16: Вы поймали скунса, сварили яичницу, завезли банку горчицы в ближайший магазин штор, написали научную статью про широты, накормили скунса яичницей, сыграли в боулинг глобусом, вернулись домой после тяжёлого дня и получили +1 . Вопрос: чему вы научились?

Поскольку функция награды может быть произвольная, довольно типично, когда сигнал от среды - неконстантная награда за шаг - приходит очень редко. Это проблема разреженной награды (sparse reward).

Пример 17 - Mountain Car: Визуализация задачи в OpenAI Gym: тележка хочет забраться на горку, но для этого необходимо поехать в противоположном направлении, чтобы набрать разгона. Состояния описываются двумя числами (х-координата тележки и её скорость); действий три (придать ускорения вправо, влево, или ничего не делать). Функция награды равна -1 всюду: задачей агента является как можно скорее завершить эпизод. Однако, терминальное состояние - это вершина горки, и для того, чтобы достичь его, нужно «уже уметь» задачу решать.

Наконец, проблема, обсуждению которой мы посвятим довольно много времени - дилемма исследованияисполъзования (exploration-exploitation trade-off). Пока ограничимся лишь примером для иллюстрации.

Пример 18: Вы решили пойти сегодня в ресторан. Следует ли отправится в ваш любимый ресторан, или попробовать новый, в котором вы ещё ни разу не были?

Практическая проблема, отчасти связанная с тем, что алгоритму необходимо постоянно «пробовать новое» - проблема «безопасного обучения» (Safe RL). Грубо говоря, некоторые взаимодействия агента со средой крайне нежелательны даже во время обучения, в том числе когда агент ещё только учится. Эта проблема возникает в первую очередь для реальных роботов.

Пример 19: Вы хотите научить реального робота мыть посуду. В начале обучения робот ничего не умеет и рандомно размахивает конечностями, бьёт всю посуду, переворачивает стол, сносит все лампочки и «в исследовательских целях» самовыкидывается из окна. В результате, начать второй обучающий эпизод становится довольно проблематично.

Здесь надо помнить, что безопасный RL не о том, как «не сбивать пешеходов» при обучении автономных автомобилей; он о том, как сбивать меньше пешеходов. RL по определению обучается на собственном опыте и в том числе собственных ошибках, и эти ошибки ему либо нужно совершить, либо получить в форме некоторой априорной информации (экспертных данных), хотя последнее всё равно не защищает от дальнейших исследований любых областей пространства состояний. Это означает, что на практике единственная полноценная защита от нежелательного поведения робота может быть проведена исключительно на этапе построения среды. То есть среда должна быть устроена так, что робот в принципе не может совершить нежелательных действий: опасные ситуации должны детектироваться средой (то есть - внешне, внешним отдельным алгоритмом), а взаимодействие - прерываться (с выдачей, например, отрицательной награды для агента).

Одна из причин распространения видеовгр для тестирования RL - отсутствие проблемы Safe RL: не нужно беспокоиться о том, что робот «что-то сломает» в процессе сбора опыта, если среда уже задана программной симуляцией.

---

# 1.2.8. Дизайн функции награды 

И есть ещё одна, вероятно, главная проблема ${ }^{7}$. Откуда берётся награда? Алгоритмы RL предполагают, что награда, как и среда, заданы, «поданы на вход», и эту проблему наши алгоритмы обучения, в отличие от предыдущих, решать идеологически не должны. Но понятно, что если для практического применения обучения с учителем боттликом часто является необходимость размечивать данные - «предоставлять обучающий сигнал» - то в RL необходимо аккуратно описать задачу при помощи функции награды.

Определение 18: «Reward hypothesis»: любую интеллектуальную задачу можно задать (определить) при помощи функции награды.

В обучении с подкреплением принято полагать эту гипотезу истинной. Но так ли это? RL будет оптимизировать ту награду, которую ему предоставят, и дизайн функции награды в ряде практических задач оказывается проблемой.

Пример 20 - «Взлом» функции награды: Классический пример того, как RL оптимизирует не то, что мы ожидаем, а ту награду, которую ему подсунули. Причина зацикливания агента видна в нижнем левом углу, где отображается счёт игры.

Пример 21: Попробуйте сформулировать функцию награды для следующих интеллектуальных задач:

- очистка мебели от пыли;
- соблюдение правил дорожного движения автономным автомобилем;
- захват мира;

Общее практическое правило звучит так: хорошая функция награды поощряет агента за достигнутые результаты, а не за то, каким способом агент этих результатов добивается. Это логично: если вдруг дизайнеру награды кажется, что он знает, как решать задачу, то вероятно, RL не особо и нужен. K сожалению, такая «хорошая» функция награды обычно разреженная.

Пример 22: Если вы хотите научиться доезжать до дерева, то хорошая функция награды - выдать агенту +1 в момент, когда он доехал до дерева. Если попытаться поощрять агента каждый шаг в раз-


мере «минус расстояние до дерева», то может возникнуть непредвиденная ситуация: агент поедет прямо в забор, который стоит возле дерева, и это может оказаться с точки зрения такой награды выгоднее, чем делать длинный крюк по району вдали от дерева с целью этот самый забор объехать.


Есть, однако, один способ, как можно функцию награды модифицировать, не изменяя решаемую задачу, то есть эквивалентным образом. K сигналу «из среды» можно что-то добавлять при условии, что на следующем шаге это что-то обязательно будет вычтено. Другими словами, можно применять трюк, который называется телескопирующая сумма (telescoping sums): для любой последовательности $\boldsymbol{a}_{\boldsymbol{t}}$, т. ч. $\lim _{t \rightarrow \infty} \boldsymbol{a}_{\boldsymbol{t}}=\mathbf{0}$, верно

$$
\sum_{t \geq 0}^{\infty}\left(a_{t+1}-a_{t}\right)=-a_{0}
$$

Определение 19: Пусть дана некоторая функция $\boldsymbol{\Phi}(\boldsymbol{s}): \mathcal{S} \rightarrow \mathbb{R}$, которую назовём потенциалом, и которая удовлетворяет двум требованиям: она ограничена и равна нулю в терминальных состояниях. Будем говорить, что мы проводим reward shaping при помощи потенциала $\boldsymbol{\Phi}(\boldsymbol{s})$, если мы заменяем функцию награды по следующей формуле:

$$
r^{\text {new }}\left(s, a, s^{\prime}\right) \vDash r(s, a)+\gamma \Phi\left(s^{\prime}\right)-\Phi(s)
$$

Teорема 3 - Reward Shaping: Проведение любого reward shaping по формуле (1.7) не меняет задачи.
Доказательство. Посчитаем суммарную награду для произвольной траектории. До reward shaping сум-

[^0]
[^0]:    ${ }^{7}$ а точнее, в принципе главная проблема всей нашей жизни (what is your reward function?)

---

марная награда равнялась $\sum_{t \geq 0} \gamma^{t} r_{t}$. Теперь же суммарная награда равна

$$
\sum_{t \geq 0} \gamma^{t} r_{t}+\sum_{t \geq 0}\left[\gamma^{t+1} \Phi\left(s_{t+1}\right)-\gamma^{t} \Phi\left(s_{t}\right)\right]
$$

В силу свойства телескопирующей суммы (1.6), все слагаемые во второй сумме посокращаются, кроме первого слагаемого $-\boldsymbol{\Phi}\left(s_{0}\right)$, который не зависит от стратегии взаимодействия и поэтому не влияет на итоговую задачу оптимизации, «последнего слагаемого», которое есть иоль: действительно, $\lim _{t \rightarrow \infty} \gamma^{t} \Phi\left(s_{t}\right)=0$ в силу ограниченности функции потенциала. Если же в игре конечное число шагов $\boldsymbol{T}$, не сокращающееся слагаемое $\boldsymbol{\Phi}\left(s_{T}\right)$ есть значение потенциала в терминальном состоянии $s_{T}$, которая по условию также равно нулю.

Reward shaping позволяет поменять награду, «не ломая» задачу. Это может быть способом борьбы с разреженной функцией награды, если удаётся придумать какой-нибудь хороший потенциал. Конечно, для этого нужно что-то знать о самой задаче, и на практике reward shaping - это инструмент внесения каких-то априорных знаний, дизайна функции награды. Тем не менее, в будущем в главе 3.2 мы рассмотрим один хороший универсальный потенциал, который будет работать всегда.

# 1.2.9. Бенчмарки 

Для тестирования RL алгоритмов есть несколько распространившихся бенчмарков. Неистощаемым источником тестов для обучения с подкреплением с дискретным пространством действий являются видеоигры, где, помимо прочего, уже задана функция награды - счёт игры.

Пример 23 - Игры Atari: Atari - набор из 57 игр с дискретным пространством действий. Наблюдением является экран видео-игры (изображение), у агента имеется до 18 действий (в некоторых играх действия, соответствующие бездействующим кнопкам джойстика, по умолчанию убраны). Награда - счёт в игре. Визуализация игр из OpenAI Gym.

При запуске алгоритмов на Atari обычно используется препроцессинг. В общем случае MDP, заданный исходной игрой, не является полностью наблюдаемым: например, в одной из самых простых игр Pong текущего экрана недостаточно, чтобы понять, в какую сторону летит шарик. Для борьбы с этим состоянием считают последние 4 кадра игры (frame stack), что для большинства игр достаточно.

Чтобы агент не имел возможности менять действие сильно чаще человека, применяют frame skip - агент выбирает следующее действие не каждый кадр, а, скажем, раз в 4 кадра. В течение этих 4 кадров агент нажимает одну и ту же комбинацию кнопок. Если агент «видит» из-за этого только кратные кадры, могут встретиться неожиданные последствия: например, в Space Invaders каждые 4 кадра «исчезают» все выстрелы на экране, и агент может перестать их видеть.

Обычно добавляется и препроцессинг самого входного изображения - в оригинале оно сжимается до размера $84 \times 84$ и переводится в чёрно-белое.

В играх часто встречается понятие «жизней». Полезно считать, что потеря жизни означает конец эпизода, и выдавать в этот момент алгоритму флаг done.

Функция переходов в Atari - детерминированная; рандомизировано только начальное состояние. Есть опасения, что это может приводить к «запоминанию» хороших траекторий, поэтому распространено использование sticky actions - текущее выбранное действие повторяется для $\boldsymbol{k}$ кадров, где $\boldsymbol{k}$ определяется случайно (например, действие повторяется только 2 раза, затем для очередного кадра подбрасывается монетка, и с вероятностью 0.5 агент повторяет действие снова; монетка подбрасывается снова, и так далее, пока не выпадет останов).

Для оценки алгоритмов используется Human normalized score: пусть agentScore - полученная оценка $J(\pi)$ для обучившегося агента, randomScore - случайной стратегии, humanScore - средний результат человека, тогда Human normalized score равен

$$
\frac{\text { agentScore }- \text { randomScore }}{\text { humanScore }- \text { randomScore }}
$$

Эта величина усредняется по 57 играм для получения качества алгоритма. При этом по условию бенчмарка алгоритм должен быть запущен на всех 57 играх с одними и теми же настройками и гиперпараметрами.

---

Пример 24 - Atari RAM: Игры Atari представлены в ещё одной версии - «RAM-версии». Состоянием считается не изображение экрана, а 128 байт памяти Atari-консоли, в которой содержится вся информация, необходимая для расчёта игры (координаты игрока и иные параметры). По определению, такое состояние «полностью наблюдаемое», и также может использоваться для тестирования алгоритмов.

Для задач непрерывного управления за тестовыми средами обращаются к физическим движкам и прочим симуляторам. Здесь нужно оговориться, что схожие задачи в разных физических движках могут оказаться довольно разными, в том числе по сложности для RL алгоритмов.

Пример 25 - Locomotion: Задача научить ходить какое-нибудь существо весьма разнообразна. Под «существом» понимается набор сочленений, каждое из которых оно способно «напрягать» или «расслаблять» (для каждого выдаётся вещественное число в диапазоне $[-1,1]$ ). Состояние обычно описано положением и скоростью всех сочленений, то есть является небольшим компактным векторочком.

Типичная задача заключается в том, чтобы в рамках представленной симуляции физики научиться добираться как можно дальше в двумерном или трёхмерном пространстве. Функция награды, описывающая такую задачу, не так проста: обычно она состоит из ряда слагаемых, дающих бонусы за продолжение движения, награду за скоррелированность вектора скорости центра масс с желаемым направлением движения и штрафы за трату энергии.

Такая награда, хоть и является довольно плотной, обычно «плохо отнормирована»: суммарное значение за эпизод может быть довольно высоким. Нормировать награду «автоматически» могут в ходе самого обучения, считая, например, средний разброс встречаемых наград за шаг и деля награду на посчитанное стандартное отклонение. Распространено считать разброс не наград за шаг, а суммарных наград с начала эпизода, и делить награды за шаг на их посчитанное стандартное отклонение.

Несмотря на малую размерность пространства состояний и действий (случаи, когда нужно выдавать в качестве действия векторы размерности порядка 20, уже считаются достаточно тяжёлыми), а также информативную функцию награды,


дающую постоянную обратную связь агенту, подобные задачи непрерывного управления обычно являются довольно сложными.

Пример 26: В робототехнике и симуляциях робот может получать информацию об окружающем мире как с камеры, наблюдая картинку перед собой, так и узнавать о располагающихся вокруг объектах с помощью разного рода сенсоров, например, при помощи ray cast-ов - расстояния до препятствия вдоль некоторого направления, возможно, с указанием типа объекта. Преимущество последнего представления перед видеокамерой в компактности входного описания (роботу не нужно учиться обрабатывать входное изображение). В любом случае, входная информация редко когда полностью описывает состояние всего окружающего мира, и в подобных реальных задачах требуется формализм частично наблюдаемых сред.

---

# Мета-эвристики 

В данной главе мы рассмотрим первый подход к решению задачи, часто называемый «эволюционным». В этом подходе мы никак не будем использовать формализацию процесса принятия решений (а следовательно, не будем использовать какие-либо результаты, связанные с изучением MDP) и будем относиться к задаче как к black-box оптимизации: мы можем отправить в среду поиграть какую-то стратегию и узнать, сколько примерно она набирает, и задача алгоритма оптимизации состоит в том, чтобы на основе лишь этой информации предлагать, какие стратегии следует попробовать следующими.

## §2.1. Бэйзлайны

### 2.1.1. Задача безградиентной оптимизации

Определение 20: Мета-эвристикой (metaheuristic) называется метод black-box оптимизации

$$
J(\boldsymbol{\theta}) \rightarrow \max _{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
$$

со стохастическим оракулом нулевого порядка (stochastic zeroth-order oracle), то есть возможностью для каждой точки $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ получить несмещённую оценку $\hat{J} \approx J(\boldsymbol{\theta})$.

Такие методы также называются безградиентными (gradient-free), поскольку не используют градиент функции и в принципе не предполагают её дифференцируемости. Понятно, что такие методы - «универсальный» инструмент (читать, «инструмент последней надежды»), который можно использовать для любой задачи оптимизации. В первую очередь, этот инструмент полезен, если пространство аргументов $\Theta$ нетривиально (например, графы) или если оптимизируемая функция принципиально недифференцируема, состоит из седловых точек («inadequate landscape») или есть другие препятствия для градиентной оптимизации.

Заметим, что если $\Theta$ - конечное множество (о градиентной оптимизации тогда речи идти не может), задача сводится к следующей: надо найти тот аргумент $\boldsymbol{\theta} \in \boldsymbol{\Theta}$, для которого настоящее значение $\boldsymbol{J}(\boldsymbol{\theta})$ максимально, при этом используя как можно меньше стохастичных оценок $\hat{J}$ оракула. Это задача многорукого бандита, которую мы обсудим отдельно в секции 7.1. В теории мета-эвристик опция «вызвать оракул в одной и той же точке несколько раз» в ходе алгоритма обычно не рассматривается; предполагается достаточно богатое пространство $\Theta$, для которого более прагматичной альтернативой кажется запросить значение условно в «соседней» точке вместо уточнения значения оракула для одной и той же.

В контексте обучения с подкреплением, чтобы свести задачу к black-box оптимизации, достаточно представить стратегию $\pi_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ в параметрическом семействе с параметрами $\boldsymbol{\theta} \in \boldsymbol{\Theta}$. В качестве $\boldsymbol{J}$, конечно, выступает наш оптимизируемый функционал (1.5)

$$
J(\boldsymbol{\theta}):=\mathbb{E}_{\mathcal{T} \sim \pi_{\boldsymbol{\theta}}} R(\mathcal{T}) \rightarrow \max _{\boldsymbol{\theta}}
$$

а в качестве $\hat{\boldsymbol{J}}-$ его Монте-Карло оценка:

$$
\hat{J}(\boldsymbol{\theta}):=\frac{1}{B} \sum_{i=1}^{B} R\left(\mathcal{T}_{i}\right), \quad \mathcal{T}_{i} \sim \pi_{\boldsymbol{\theta}}, i \in\{1 \ldots B\}
$$

Если дисперсия оценки достаточно высока (число сэмплов $\boldsymbol{B}$ недостаточно велико), почти все далее рассматриваемые алгоритмы сломаются (будут выживать «везучие», а не «сильнейшие»). Поэтому может оказаться крайне существенным использовать $\boldsymbol{B}>\mathbf{1}$, даже если $\boldsymbol{\pi}_{\boldsymbol{\theta}}$ - семейство детерминированных стратегий.

---

Будем проникаться местной терминологией:
Определение 21: Точку $\boldsymbol{\theta}$, в которой алгоритм оптимизации запрашивает значение оракула, будем называть особъю (individuals, particles), а само значение $\overline{\boldsymbol{J}}(\boldsymbol{\theta})$ для данной особи - её оченкой или приспособленностью (fitness).

В силу гигантского разнообразия мета-эвристик (от метода светлячков до колоний императорских пингвинов) на полноту дальнейшее изложение, конечно же, не претендует, и стоит воспринимать рассуждение как попытку структурировать мотивации некоторых из основных идей. В частности, нас в первую очередь будут интересовать алгоритмы, в той или иной степени успешно применявшиеся в RL.

# 2.1.2. Случайный поиск 

Случайный поиск (random search) - метод оптимизации и самый простой пример мета-эвристики.
| Определение 22: Распределение $\boldsymbol{q}(\boldsymbol{\theta})$ в пространстве $\boldsymbol{\Theta}$ будем называть стратегией перебора.
Случайный поиск сводится к сэмплированию из стратегии перебора особей $\boldsymbol{\theta}_{\boldsymbol{k}} \sim \boldsymbol{q}(\boldsymbol{\theta})(\boldsymbol{k} \in\{0,1,2 \ldots\})$, после чего в качестве результата выдаётся особь с наилучшей оценкой.


Забавно, что случайный поиск - метод глобальной оптимизации: если $\forall \boldsymbol{\theta} \in \boldsymbol{\Theta}: \boldsymbol{q}(\boldsymbol{\theta})>\mathbf{0}$, после достаточного числа итераций метод найдёт сколь угодно близкое к глобальному оптимуму решение ${ }^{1}$. Есть ещё один парадокс грубого перебора: если в наличии есть неограниченное число серверов, то возможно запустить на каждом вычисление приспособленности одной особи, и за время одного вычисления провести «глобальную» оптимизацию.

Идея случайного поиска, на самом деле, вводит основные понятия мета-эвристик. Нам придётся так или иначе запросить у оракула приспособленности некоторого набора особей и так или иначе в итоге отобрать лучший. Для имитации умности происходящего введём весёлую нотацию.
| Определение 23: Набор особей $\mathscr{P}:=\left(\boldsymbol{\theta}_{\boldsymbol{i}} \mid \boldsymbol{i} \in\{1,2 \ldots N\}\right)$ называется популяиией (population) размера $\boldsymbol{N}$.

Определение 24: Запрос оракула для всех особей популяции называется очениванием (evaluation) популяции:

$$
\overline{\boldsymbol{J}}(\mathscr{P}):=\left(\overline{\boldsymbol{J}}\left(\boldsymbol{\theta}_{\boldsymbol{i}}\right) \mid \boldsymbol{i} \in\{1,2 \ldots N\}\right)
$$

| Определение 25: Процедурой отбора (selection) называется выбор (возможно, случайный, возможно, с повторами) $\boldsymbol{M}$ особей из популяции. Формально, это распределение $\boldsymbol{\operatorname { e e l e c t }}\left(\mathscr{P}^{+} \mid \mathscr{P}, \overline{\boldsymbol{J}}(\mathscr{P})\right)$, такое что $\forall \boldsymbol{\theta} \in \mathscr{P}^{+}: \boldsymbol{\theta} \in \mathscr{P}$ с вероятностью 1.

Определение 26: Жадный (greedy) отбор select ${ }_{M}^{t o p}$ - выбор топ- $\boldsymbol{M}$ самых приспособленных особей.

[^0]$$
J(\theta)=\left\{\begin{array}{lll}
0 & \theta \neq \theta^{\star} \\
1 & \theta=\theta^{\star}
\end{array}\right.
$$

никакой метод оптимизации в $\Theta \equiv \mathbb{R}$ не прооптимизирует.


[^0]:    ${ }^{1}$ с оговоркой, что метод сможет понять, что нашёл оптимум, для чего придётся предположить некоторые условия регулярности для $\boldsymbol{J}(\boldsymbol{\theta})$; понятно, что функцию «иголка в стоге сена» (needle in a haystack)

---

Жадный отбор плох тем, что у нас нет гарантий, что мы на самом деле выбираем наилучшую точку из рассмотренных - наши оценки $\hat{\boldsymbol{J}}$ могут быть неточны, и наилучшей на самом деле может оказаться особь с не самой высокой приспособленностью. В частности поэтому могут понадобиться альтернативы жадного отбора.

Пример 28 - Пропорциональный отбор: Зададимся некоторым распределением на особях популяции, которые сэмплирует особь тем чаще, тем выше её приспособленность. Например:

$$
p(\boldsymbol{\theta}) \propto \exp \hat{J}(\boldsymbol{\theta})
$$

Для отбора $\boldsymbol{M}$ особей засэмплируем (обычно с возвращением) из этого распределения $\boldsymbol{M}$ раз.
Пример 29 - Турнирный отбор: Для отбора $\boldsymbol{M}$ особей $\boldsymbol{M}$ раз повторяется следующая процедура: случайно выбираются $\boldsymbol{K}$ особей популяции (из равномерного распределения) и отбирается та из них, чья приспособленность выше. Число $\boldsymbol{K}$ называется размером турнира и регулирует вероятность плохо приспособленной особи быть отобранной: чем больше $\boldsymbol{K}$, тем меньше у слабенькой особи шансов выжить.

Одна и та же особь в ходе отбора может быть выбрана несколько раз: это можно читать как «особи повезло оставить больше потомства» ${ }^{2}$.

Итак, случайный поиск можно переформулировать на языке мета-энристик так: сгенерировать из данной стратегии перебора популяцию заданного размера $\boldsymbol{N}$ и отобрать из неё 1 особь жадно.

# 2.1.3. Hill Climbing 

Процедура отбора позволяет только «сокращать» разнообразие популяции. Хочется как-то обусловить процесс генерации новых кандидатов на уже имеющуюся информацию (которая состоит только из особей и их приспособленностей). Мотивация введения мутаций в том, что даже в сложных пространствах $\boldsymbol{\Theta}$ зачастую можно что-то «поделать» с точкой $\boldsymbol{\theta}$ так, чтобы она превратилась в другую точку $\hat{\boldsymbol{\theta}}$.

Определение 27: Myтацией (mutation) называется распределение $\mathbf{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})$, где $\boldsymbol{\theta}$ называется родителем (parent), $\hat{\boldsymbol{\theta}}$ - потомком (child).

Пример 30: Пусть $\boldsymbol{\Theta}$ - множество путей обхода вершин некоторого графа. Такое пространство аргументов возникает во многих комбинаторных задачах (таких как задача коммивояжёра). Пусть $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ - некоторый путь обхода, то есть упорядоченное множество вершин графа. Мутацией может выступать выбор случайных двух вершин и смена их местами в порядке обхода. Например, в исходном обходе мы проходили пять вершин графа в порядке $(4,3,1,5,2)$, а после мутации - в порядке $(4,2,1,5,3)$.

Рассмотрим простейший способ использования мутации. На $\boldsymbol{k}$-ом шаге алгоритма будем генерировать $\boldsymbol{N}$ потомков особи $\boldsymbol{\theta}_{\boldsymbol{k}}$ при помощи мутации и отбирать из них жадно особь $\boldsymbol{\theta}_{\boldsymbol{k}+1}$. Очень похоже на градиентный подъём: мы сэмплим несколько точек вокруг себя и идём туда, где значение функции максимально. Поэтому отчасти можно считать, что Hill Climbing с большим $\boldsymbol{N}$ - локальная оптимизация: мы не можем взять наилучшее направление изменения $\boldsymbol{\theta}$ из, например, градиентов, но можем поискать хорошее направление, условно, случайным перебором.

Пример 31 - Hill Climbing:


[^0]
[^0]:    ${ }^{2} \mathrm{C}$ точки зрения эволюционной теории, критерием оптимизации для особи является исключительно преумиожение количества своих генов за счёт размножения. Отбор - не столько про «выживание сильнейших», сколько про количество успешных передач генов потомкам.

---

Что получается: если мутация такова, что $\forall \boldsymbol{\theta}, \hat{\boldsymbol{\theta}}: \mathfrak{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})>\mathbf{0}$, остаются гарантии оказаться в любой точке пространства, и алгоритм остаётся методом глобальной оптимизации. При этом, мутация может быть устроена так, что вероятность оказаться «неподалёку» от родителя выше, чем в остальной области пространства.

Пример 32: В примере 30 мутация не удовлетворяла свойству $\forall \boldsymbol{\theta}, \hat{\boldsymbol{\theta}}: \mathfrak{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})>\mathbf{0}$ : из текущего пути обхода мы могли получить только очень похожий. Применение мета-эвристик с такой мутацией чревато скорым застреванием в локальном оптимуме: мы можем попасть в точку, применение мутации к которой с вероятностью один даёт ещё менее приспособленные особи. Чтобы полечить это, создадим другую мутацию: засэмплируем натуральное $\boldsymbol{n}$ из распределения Пуассона или из $\boldsymbol{p}(\boldsymbol{n}):=\frac{1}{2^{n}}$ и применим такое количество мутаций вида «сменить две вершины местами». Так мы гарантируем, что с небольшой вероятностью мы сможем мутировать в произвольную точку пространства аргументов.

Понятно, что если мутация генерирует очень непохожие на родителя особи, алгоритм схлопывается примерно в случайный поиск. И понятно, что если мутация, наоборот, с огромной вероятностью генерирует очень близкие к родителю особи, алгоритм, помимо того, что будет сходиться медленно, будет сильно надолго застревать в локальных оптимумах. Возникает trade-off между использованием (exploitation) и исследованийм (exploration): баланс между выбором уже известных хороших точек и поиском новых «вдали»; изучением окрестностей найденных локальных оптимумов и поиском новых.

Пример 33: Если $\boldsymbol{\Theta} \equiv \mathbb{R}^{\boldsymbol{h}}$, то типичным выбором мутации является $\boldsymbol{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta}):=\boldsymbol{\mathcal { N }}\left(\boldsymbol{\theta}, \boldsymbol{\sigma}^{2} \boldsymbol{I}_{h \times h}\right)$, где $\boldsymbol{\sigma}>\mathbf{0}$ гиперпараметр, и, чем ближе $\boldsymbol{\sigma}$ к нулю, тем «ближе» к родителю потомки.

Возникает вопрос: а как подбирать гиперпараметры мета-эвристик, тоже мета-эвристикой? Любопытный ответ - самовдаптирующиеся параметры (self-adaptive mutations). Параметры мутации кодируются в пространстве аргументов $\boldsymbol{\Theta}$; то есть одна из координат каждого $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ и отвечает за, например, дисперсию $\boldsymbol{\sigma}$ в добавляемом шуме из нормального распределения. Появляется надежда, что мета-эвристика «сама подберёт себе хорошие гиперпараметры».

# 2.1.4. Имитация отжига 

Имитация отжсига (simulated annealing) решает «проблему исследования» при помощи более умной процедуры отбора: вероятность выбрать потомка $\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime} \sim \boldsymbol{m}\left(\theta_{\boldsymbol{k}+1}^{\prime} \mid \boldsymbol{\theta}_{\boldsymbol{k}}\right)$, а не остаться в родительской точке $\boldsymbol{\theta}_{\boldsymbol{k}}$, вводится так:

$$
\text { select }\left(\boldsymbol{\theta}_{\boldsymbol{k}+1}=\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime}\right):=\min \left(1, \exp \frac{\hat{J}\left(\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime}\right)-\hat{J}\left(\boldsymbol{\theta}_{\boldsymbol{k}}\right)}{\boldsymbol{\tau}_{\boldsymbol{k}}}\right)
$$

где $\boldsymbol{\tau}_{\boldsymbol{k}}>\mathbf{0}$ - температура, гиперпараметр, зависящий от номера итерации. Иными словами, если новая точка более приспособлена, то мы «принимаем» новую точку $\boldsymbol{\theta}_{\boldsymbol{k + 1}}^{\prime}$ с вероятностью 1 ; если же новая точка менее приспособлена, мы не выкидываем её, а переходим в неё с некоторой вероятностью. Эта вероятность тем ближе к единице, чем «похожее» значения оракула, и температура регулирует понятие похожести между скалярами относительно масштаба оптимизируемой функции $\boldsymbol{J}$.

Наша цепочка $\boldsymbol{\theta}_{0}, \boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2} \ldots$ задаёт марковскую цепь: мы генерируем каждую следующую особь на основе только предыдущей, используя некоторое стохастичное правило перехода. Теория марковских цепей говорит, что может существовать стационарное распределение (stationary distribution): распределение, из которого приходят $\boldsymbol{\theta}_{\boldsymbol{k}}$, при стремлении $\boldsymbol{k} \rightarrow \infty$ всё ближе к некоторому $\boldsymbol{p}(\boldsymbol{\theta})$, которое определяется лишь нашей функцией переходов и не зависит от инициализации $\boldsymbol{\theta}_{0}$.

Теорема 4 - Алгоритм Метрополиса-Гастингса: Пусть в пространстве $\boldsymbol{\Theta}$ задано распределение $\boldsymbol{p}(\boldsymbol{\theta})$ и распределение $\boldsymbol{q}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})$, удовлетворяющее $\boldsymbol{\forall} \hat{\boldsymbol{\theta}}, \boldsymbol{\theta}: \boldsymbol{q}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})>\mathbf{0}$. Пусть строится цепочка $\boldsymbol{\theta}_{0}, \boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2} \ldots$ по следующему правилу: генерируется $\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime} \sim \boldsymbol{q}\left(\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime} \mid \boldsymbol{\theta}_{\boldsymbol{k}}\right)$, после чего с вероятностью

$$
\min \left(1, \frac{\boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime}\right)}{\boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{k}}\right)} \frac{\boldsymbol{q}\left(\boldsymbol{\theta}_{\boldsymbol{k}} \mid \boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime}\right)}{\boldsymbol{q}\left(\boldsymbol{\theta}_{\boldsymbol{k}+1}^{\prime} \mid \boldsymbol{\theta}_{\boldsymbol{k}}\right)}\right)
$$

$\boldsymbol{\theta}_{\boldsymbol{k + 1}}$ полагается равным $\boldsymbol{\theta}_{\boldsymbol{k + 1}}^{\prime}$, а иначе $\boldsymbol{\theta}_{\boldsymbol{k + 1}}:=\boldsymbol{\theta}_{\boldsymbol{k}}$. Тогда для любого $\boldsymbol{\theta}_{\boldsymbol{0}}$ :

$$
\lim _{\boldsymbol{k} \rightarrow \infty} p\left(\boldsymbol{\theta}_{\boldsymbol{k}}\right)=p(\boldsymbol{\theta})
$$

Бел доказательства.

---

Утверждение 3: Пусть оракул точный, то есть $\tilde{\boldsymbol{J}}(\boldsymbol{\theta}) \equiv \boldsymbol{J}(\boldsymbol{\theta})$, а мутация удовлетворяет

$$
\forall \boldsymbol{\theta}, \tilde{\boldsymbol{\theta}}: \mathrm{m}(\tilde{\boldsymbol{\theta}} \mid \boldsymbol{\theta})=\mathrm{m}(\boldsymbol{\theta} \mid \tilde{\boldsymbol{\theta}})>\mathbf{0}
$$

Тогда, если температура $\boldsymbol{\tau}$ не зависит от итерации, для любой инициализации $\boldsymbol{\theta}_{\boldsymbol{0}}$ алгоритм имитации отжига строит марковскую цепь со следующим стационарным распределением:

$$
\lim _{k \rightarrow \infty} p\left(\theta_{k}\right) \propto \exp \frac{J\left(\theta_{k}\right)}{\tau}
$$

Алгоритм Метрополиса даёт, вообще говоря, «гарантии сходимости» для имитации отжига: то, что через достаточно большое количество итераций мы получим сэмпл из распределения $\exp \frac{J\left(\theta_{k}\right)}{\tau}$, нас, в общем-то, устраивает. Если температура достаточно маленькая, это распределение очень похоже на вырожденное в точке максимального значения оптимизируемой функции. Одновременно, правда, маленькая температура означает малую долю исследований в алгоритме, и тогда процедура вырождается в наивный поиск при помощи мутации. Поэтому на практике температуру снижают постепенно; подобный отжсиг (annealing) часто применяется для увеличения доли исследований в начале работы алгоритма и уменьшения случайных блужданий в конце.

# Пример 34 - Имитация отжига: 



Если в операторе мутации есть параметр, отвечающий за «силу мутаций», связанный с балансом исследования-иснользования (например, дисперсия $\boldsymbol{\sigma}$ гауссовского шума в примере 33), его аналогично можно подвергнуть отжигу: в начале алгоритма его значение выставляется достаточно большим, после чего с ходом алгоритма оно постепенно по некоторому расписанию уменьшается.

### 2.1.5. Эволюционные алгоритмы

Hill Climbing - эвристика с «одним состоянием»: есть какая-то одна текущая основная особь-кандидат, на основе которой и только которой составляется следующая особь-кандидат. Нам, вообще говоря, на очередном шаге доступна вся история проверенных точек. Первый вариант - построить суррогат-приближение $\tilde{\boldsymbol{J}}(\boldsymbol{\theta})$, которую легко можно прооптимизировать и найти так следующего кандидата (к нему относятся, например, алгоритмы на основе гауссовских процессов), но этот вариант не сработает в сложных пространствах $\boldsymbol{\Theta}$. Второй вариант - перейти к «эвристикам с $\boldsymbol{N}$ состояниями», то есть использовать последние $\boldsymbol{N}$ проверенных особей для порождения новых кандидатов.

[^0]Практически все мета-эвристики сводятся к эволюционным алгоритмам. При этом заметим, что, пока единственным инструментом «генерации» новых особей выступает мутация, алгоритмы различаются только процедурой отбора. Таким образом, в алгоритме всегда поддерживается текущая популяция из $\boldsymbol{N}$ особей (первая популяция генерируется из некоторой стратегии перебора $\boldsymbol{q}(\boldsymbol{\theta})$ ), из них отбирается $\boldsymbol{N}$ особей (отбор может выбирать одну особь несколько раз, поэтому этот шаг нетривиален), и дальше каждая отобранная особь мутируется. Эвристика элитизма (elitism) предлагает некоторые из отобранных особей не мутировать, и оставить для следующей популяции; это позволяет уменьшить шансы популяции «потерять» найденную область хороших значений функции, но увеличивает шансы застревания в локальном оптимуме. При элитизме для каждой особи хранится её возраст (age) - число популяций, через которые особь прошла без мутаций; далее этот возраст влияет на отбор, например, выкидывая все особи старше определённого возраста. Далее будем рассматривать алгоритмы без элитизма.


[^0]:    Определение 28: Алгоритм называется эволюционным (evolutionary), если он строит последовательность популяций $\mathscr{P}_{1}, \mathscr{P}_{2} \ldots$, на $\boldsymbol{k}$-ом шаге строя очередное поколение (generation) $\mathscr{P}_{\boldsymbol{k}}$ на основе предыдущего.

---

Придумаем простейший эволюционный алгоритм. Любой алгоритм локальной оптимизации (градиентный спуск или Hill Climbing), результат работы которого зависит от начального приближения $\boldsymbol{\theta}_{0} \sim \boldsymbol{q}(\boldsymbol{\theta})$, можно «заменить» на метод глобальной оптимизации, запустив на каждом условном сервере по «nотоку» (thread) со своим начальным $\boldsymbol{\theta}_{0}$. Время работы алгоритма не изменится (считая, конечно, что сервера работают параллельно), а обнаружение неограниченного числа локальных оптимумов с ненулевым шансом найти любой гарантирует нахождение глобального. Набор из текущих состояний всех потоков можно считать текущей популяцией.

При этом, любая процедура отбора позволит потокам «обмениваться информацией между собой». Для примера рассмотрим распространённую схему ( $\boldsymbol{M}, \boldsymbol{K}$ )-эволюционной стратегии, в которой процедура отбора заключается в том, чтобы топ- $\boldsymbol{M}$ особей отобрать по $\boldsymbol{K}$ раз каждую (дать каждой особи из топа породить $\boldsymbol{K}$ детей). Иными словами, мы параллельно ведём $\boldsymbol{M}$ Hill Climbing-ов, в каждом из которых для одного шага генерируется $\boldsymbol{K}$ потомков; если число потоков $\boldsymbol{M}=\mathbf{1}$, алгоритм вырождается в обычный Hill Climbing. Порождённые $\boldsymbol{N}=\boldsymbol{M} \boldsymbol{K}$ особей образуют текущую популяцию алгоритма. Среди них отбирается $\boldsymbol{M}$ лучших особей, которые могут как угодно распределиться по потокам. Получится, что некоторые потоки, которые не нашли хороших областей $\boldsymbol{\Theta}$, будут прерваны, а хорошие получат возможность сгенерировать больше потомков и как бы «размножаться» на несколько процессов.

# Алгоритм 1: $\left(M, K\right)$-эволюционная стратегия 

Вход: оракул $\hat{\boldsymbol{J}}(\boldsymbol{\theta})$
Гиперпараметры: $\mathbf{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})-$ мутация, $\boldsymbol{q}(\boldsymbol{\theta})-$ стратегия перебора, $\boldsymbol{M}-$ число потоков, $\boldsymbol{K}-$ число сэмплов на поток

Инициализируем $\mathscr{P}_{0}:=\left(\boldsymbol{\theta}_{i} \sim \boldsymbol{q}(\boldsymbol{\theta}) \mid i \in\{1,2, \ldots, M K\}\right)$
На $\boldsymbol{k}$-ом шаге:

1. проводим жадный отбор: $\mathscr{P}_{k}^{+}:=\operatorname{select}_{M}^{\text {top }}\left(\mathscr{P}_{k}, \hat{\boldsymbol{J}}\left(\mathscr{P}_{k}\right)\right)$
2. размножаем: $\mathscr{P}_{k+1}:=\left(\hat{\theta}_{i} \sim \mathrm{~m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta}) \mid \boldsymbol{\theta} \in \mathscr{P}_{k}^{+}, \boldsymbol{i} \in\{1,2, \ldots, \boldsymbol{K}\}\right)$

Пример 35 - $\{4,5\}$-эволюционная стратегия:


### 2.1.6. Weight Agnostic Neural Networks (WANN)

Поскольку мета-эвристики - универсальный метод оптимизации, они могут применяться к нейронным сетям. Такой подход даёт такие преимущества, как возможность использовать дискретные веса или недопустимые для градиентной оптимизации функции активации вроде пороговой функции Хевисайда $(\boldsymbol{f}(\boldsymbol{x}):=\mathbb{I}[\boldsymbol{x} \geq \mathbf{0}])$.

Особенностью нейроэволюции (neuroevolution) является возможность искать топологию сети вместе со значениями самих весов: для этого достаточно предложить некоторый оператор мутации. Рассмотрим распространённый пример: пусть дана некоторая нейросеть с произвольной топологией ${ }^{3}$ и некоторыми весами. Для весов процедуру мутирования можно взять стандартную (добавление шума с заданной дисперсией; дополнительно можно для каждого веса сэмплировать бинарную величину, будет ли данный вес мутировать). Далее

[^0]
[^0]:    ${ }^{3}$ на заре нейросетевого подхода разумность использования полносвязных слоёв была под вопросом. Считаем, что нейрон принимает несколько входов, домножает каждый вход на некоторый вес, складывает и применяет функцию активации, выдавая скалярную величину. Выход нейрона отправляется некоторым из других нейронов на вход; понятие «слоёв» обычно не вводится, а единственное ограничение на вычислительный граф - ацикличность.

---

случайно выбирается, будет ли проходить мутация архитектуры (топологии) сети, и если да, то какого типа (обычно рассматривается несколько типов мутации).

Пример 36 - Топологические мутации нейросети: Распространён набор из трёх видов топологических мутаций: добавление связи, добавление нейрона и смена функции активации.

Добавление связи означает, что случайно выбираются два ранее не соединённых связью нейрона, и выход одного добавляется ко входу в другой (вес инициализируется, например, случайно). Какой из двух нейронов является входом, а какой - выходом, однозначно определяется требованием ацикличности к вычислительному графу.

Под добавлением нейрона понимается именно разбиение уже имеющейся связи: имевшаяся связь А-В, где А, В - нейроны, выключается, и появляются связи А-С и С-В, где С - новый нейрон. Новые нейроны необходимо добавлять именно так, чтобы они сразу же участвовали в вычислительном процессе.

Смена функции активации меняет функцию активацию в случайном нейроне на произвольную из предопределённого набора.

Заметим, что в таком операторе мутации отсутствуют шансы на уменьшение числа связей. Эволюционный процесс с таким оператором мутации будет рассматривать пространство $\Theta$ нейросетей с различными топологиями от «более простых» архитектур к «более сложным». В частности поэтому рекомендуется изначально алгоритм инициализировать «пустой» топологией: между входами и выходами связей нет, а появляться они будут в ходе постепенных мутаций. Похоже, что к такой эвристике привёл эмпирический опыт, и введение мутаций, удаляющих часть архитектуры, не приводило к особым успехам; результатом работы нейроэволюционного алгоритма типично является крайне минимальная по современным меркам нейросеть, с довольно малым количеством связей.

Weight Agnostic сети зашли ещё дальше и, по сути, отказались от настройки весов нейросети в принципе, ограничившись только поиском топологии. Алгоритм WANN основан на $(\boldsymbol{M}, \boldsymbol{K})$-эволюционной стратегии с оператором мутации из рассмотренного примера 36. Единственным изменением является процедура отбора. Для данной топологии оракул запускается шесть раз. В каждом запуске всем весам сети присваивается одно и то же значение (авторы использовали значения $[-2,-1,-0.5,0.5,1,2]$ ). Рассматривается три критерия приспособленности особи: среднее из шести значение $\boldsymbol{J}$, максимальное из шести значение $\boldsymbol{J}$ и число связей. Эти три критерия не смешиваются со скалярными гиперпараметрами, вместо этого проводится турнирный отбор (пример 29): из популяции выбираются два кандидата, выбирается случайный критерий, и выживает сильнейший по данному критерию. Так отбирается $\boldsymbol{M}$ особей, которые и порождают $\boldsymbol{K}$ потомков каждый.

# 2.1.7. Видовая специализация 

Возникает простой вопрос к $(\boldsymbol{M}, \boldsymbol{K})$-эволюционной стратегии: а не случится ли такого, что он схлопнется к Hill Climbing-у, если в очередной популяции среди топ- $\boldsymbol{M}$ останутся только дети одного и того же родителя? Получится, что, хоть мы и исходили из идеи исследовать параллельно много локальных оптимумов, жадный отбор может убить все потоки, кроме одного, и «область пространства аргументов», покрываемая текущей популяцией, «схлопнется».

Для борьбы с этим эффектом в мета-эвристиках рассматривают методы защиты инноваций (innovation protection). Если для получения новых хороших свойств необходимо сделать «несколько» шагов эволюции, то мы каким-то образом помогаем выживать особям, оказавшихся в не исследуемых местах пространства $\Theta$. Самый простой способ - использование более «мягких» процедур отбора, когда у неприспособленной особи есть небольшой шанс выжить. Более интеллектуально было бы как-то оценить, насколько «новой» является область пространства аргументов, в которой оказалась особь, и дать ей больше шансов выжить, если алгоритм эту область ещё не рассматривал: ведь проблема мягких процедур отбора, очевидно, в том, что слабые особи выживают в том числе там, где функция уже в достаточной степени исследована.

Рассмотрим общую идею видов (species). Допустим, мы сможем в $\Theta$ придумать метрику (или хоть сколькото функцию близости) $\rho\left(\theta_{1}, \theta_{2}\right)$ и на её основе разбить все особи популяции $\mathscr{P}$ на непересекающиеся множества - «виды». Процесс разбиения, что важно, не обязан удовлетворять каким-то особым свойствам и может быть стохастичным, в том числе чтобы быть вычислительно дешёвым.

Пример 37 - Процедура разделения на виды: Изначально множество видов пусто. На очередном шаге берём очередную особь из $\mathscr{P}$ и в случайном порядке перебираем имеющиеся виды. Для каждого вида сэмплируем одну из ранее отнесённых к нему особей и сравниваем расстояние $\rho$ с порогом-гиперпараметром процесса: меньше порога - относим рассматриваемую особь к этому виду и переходим к следующей особи, больше порога переходим к следующему виду. Если ни для одного вида проверка не прошла, особь относится к новому виду. Пересчёт видов проводится для каждой популяции заново.

---

Разделение на виды позволяет делать, например, explicit fitness sharing: особи соревнуются только внутри своих видов. Пусть $\mathscr{P} \subseteq \mathscr{P}$ - вид, а $\tilde{\boldsymbol{J}}_{\text {mean }}(\widetilde{\mathscr{P}})$ - среднее значение приспособленности в данном виде. Тогда на основе этих средних значений между видами проводится (в некоторой «мягкой» форме - слабые виды должны выживать с достаточно высокой вероятностью) некоторый мягкий отбор; например, виду $\widetilde{\mathscr{P}}$ позволяется сгенерировать потомков пропорционально $\exp \tilde{\boldsymbol{J}}_{\text {mean }}(\widetilde{\mathscr{P}})$ с учётом того, что в сумме все виды должны породить заданное гиперпараметром число особей. Генерация необходимого числа потомков внутри каждого вида происходит уже, например, стандартным, «агрессивным» образом: отбирается некоторая доля топ-особей, к которым и применяется по несколько раз мутация.

Виды защищены мягким отбором, и поэтому заставнивнишеся вдали особи, образующие новый вид, будут умирать реже; при этом в скоплениях слабых особей в одном месте пройдёт жёсткий внутривидовой отбор, а сам вид получит не так много «слотов потомства», и число точек сократится.

Пример 38 - Эволюция с видовой специализацией: В данном примере текущая популяция из 20 особей разделяется на виды согласно процедуре из примера 37 с метрикой $\rho\left(\theta_{1}, \theta_{2}\right)=\left|\theta_{1}-\theta_{2}\right|$ и порогом 1. Для видов считается $\tilde{\boldsymbol{J}}_{\text {mean }}(\widetilde{\mathscr{P}})$ (указан как fit в легенде), после чего при помощи сэмплирования из распределения $\propto \exp \tilde{\boldsymbol{J}}_{\text {mean }}(\widetilde{\mathscr{P}}) 20$ раз разыгрываются между видами «слоты потомства». Внутри каждого вида жадно отбирается 1 особь, она и порождает разыгранное число потомков. Разбиение на виды проводится для новой полученной популяции заново.


# 2.1.8. Генетические алгоритмы 

До сих пор мы умели создавать новые особи только при помощи мутации. В генетических алгоритмах дополнительно вводится этап рекомбинации (recombination), когда новые особи можно строить на основе сразу нескольких особей, как-то «совмещая» свойства тех и других в надежде получить «лучшее от двух миров»; найти хороший оптимум между двумя локальными оптимумами. В ванильной версии генетических алгоритмов у детей по два родителя, хотя можно рассматривать и скрещивание большего числа особей:

Определение 29: Кроссинговером (crossover) называется распределение $\boldsymbol{\epsilon}\left(\tilde{\boldsymbol{\theta}} \mid \boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}\right)$, где $\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}$ называются родителями (parent), $\tilde{\boldsymbol{\theta}}$ - потомком (child).

Пример 39: Для $\Theta \equiv \mathbb{R}^{d}$ или $\Theta \equiv\{0,1\}^{d}$ (или их смеси) можно придумать много разных кроссинговеров; генетика подсказывает, что если элементы векторов это «гены», то нужно, например, некоторые гены взять от одного родителя, а другие от другого. Некоторые гены можно сцеплять (сцепленные гены должны быть взяты из одного родителя), или вводить порядок на генах (брать от одного родителя «правую» часть, от другого «левую»).

Чтобы сохранить свойство «глобальности» оптимизации, желательно было бы, опять же, чтобы мы могли при помощи такого инструмента порождения оказаться в любой точке пространства, т.е. $\forall \tilde{\boldsymbol{\theta}}, \boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2} \in \Theta: \boldsymbol{\epsilon}(\tilde{\boldsymbol{\theta}} \mid$ $\left|\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}\right\rangle>0$. Однако, для практически любых примеров кроссинговера это не так. Поэтому считается, что это требование НЕ выполняется: процедура рекомбинации, возможно, стохастична, но всегда приводит к точке «между» $\boldsymbol{\theta}_{1}$ и $\boldsymbol{\theta}_{2}$. Это означает, что, используя только кроссинговер, область, покрываемая потомством, будет уже области, покрываемой родителями: теряется исследование. Чтобы полечить это, в генетических алгоритмах всё равно остаётся этап применения мутации.

---

# Алгоритм 2: Генетический поиск 

## Дано: оракул $\hat{\boldsymbol{J}}(\boldsymbol{\theta})$

Гиперпараметры: $\boldsymbol{\varepsilon}\left(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta}_{\mathbf{1}}, \boldsymbol{\theta}_{\mathbf{2}}\right)$ - кроссинговер, $\mathbf{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta})-$ мутация, $\operatorname{select}\left(\mathscr{P}^{+} \mid \mathscr{P}, \hat{\boldsymbol{J}}(\mathscr{P})\right)-$ процедура отбора, $\boldsymbol{q}(\boldsymbol{\theta})$ - стратегия перебора, $\boldsymbol{N}$ - размер популяции

Инициализируем $\mathscr{P}_{0}:=\left(\boldsymbol{\theta}_{\boldsymbol{i}} \sim \boldsymbol{q}(\boldsymbol{\theta}) \mid \boldsymbol{i} \in\{1,2, \ldots, \boldsymbol{N}\}\right)$
На $\boldsymbol{k}$-ом шаге:

1. проводим отбор: $\mathscr{P}_{k}^{+} \sim \operatorname{select}\left(\mathscr{P}_{k}^{+} \mid \mathscr{P}_{k}, \hat{\boldsymbol{J}}\left(\mathscr{P}_{k}\right)\right)$
2. проводим размножение: $\mathscr{P}_{k+1}:=\left(\boldsymbol{\theta}_{\boldsymbol{i}} \sim \boldsymbol{\varepsilon}\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}_{\mathbf{1}}, \boldsymbol{\theta}_{r}\right) \mid \boldsymbol{\theta}_{\mathbf{1}}, \boldsymbol{\theta}_{r} \in \mathscr{P}_{k}^{+}\right)$
3. проводим мутирование: $\mathscr{P}_{k+1} \leftarrow\left(\hat{\boldsymbol{\theta}} \sim \mathbf{m}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{\theta}) \mid \boldsymbol{\theta} \in \mathscr{P}_{k+1}\right)$

При эволюционном обучении нейросетей, в отличие от ряда других задач, кроссинговер во многом неудобен. Нельзя взять «половинку» одной хорошей нейросети и присоединить к «половинке» другой хорошей нейросети - каждый нейрон рассчитывает на тот набор входов, для которого он был обучен (неважно, эволюционно или градиентно). Поэтому генетические алгоритмы для нас не представляют особого интереса, по крайней мере на момент написания данного текста.

Пример 40 - Neuroevolution of Augmented Topologies (NEAT): Рассмотрим в качестве примера набор эвристик нейроэволюционного алгоритма NEAT, который всё-таки был основан на генетическом поиске (алг. 2), то есть дополнительно вводил оператор кроссинговера для нейросетей (двух произвольных топологий).

В NEAT все связи всех особей, помимо веса, имеют статус «включена-выключена» и уникальный идентификационный номер (id), или исторический маркер (historical marker). Связи могут появляться только в ходе мутаций (используется оператор мутации из примера 36); в момент создания связи ей присваивается уникальный (в рамках всего алгоритма) id и статус «включена». Наличие у двух особей связи с одним id будет означать наличие общего предка. У изначальной «пустой» топологии связей нет вообще. Связь может


попасть в статус «выключена» только во время мутации вида «добавление нейрона», когда имевшаяся связь А-В «исчезает»: то есть, соответствующий «ген» не удаляется из генома особи, а переходит в статус «выключена». Выключенная связь означает, что у особи был предок, у которого связь была включена.

Как введение статусов и id-шников связей позволяет устраивать между разными топологиями кроссинговер? Если связь с данным id имеется у обоих скрещиваемых особей, связь сохраняется и у потомка, с весом и статусом случайного родителя. Связи, имеющиеся только у одного из родителей, будем называть непарными; они копируются из того родителя, чья приспособленность выше (или из обоих сразу, если приспособленности одинаковые).


NEAT также использует видовую специализацию (как описано в разделе 2.1.7) для процесса отбора, для чего на таких генотипах необходимо задать метрику $\boldsymbol{\rho}$. Пусть $\boldsymbol{\theta}_{\mathbf{1}}, \boldsymbol{\theta}_{\mathbf{2}}$ - две особи, $\boldsymbol{G}_{\mathbf{1}}, \boldsymbol{G}_{\mathbf{2}}$ - количество связей у этих особей, $\boldsymbol{D}$ - число непарных связей, $\boldsymbol{w}_{\mathbf{1}}, \boldsymbol{w}_{\mathbf{2}}$ - веса особей в парных связях. Понятно, что веса мы можем сравнить только для парных связях, и понятно, что чем больше непарных связей, тем больше должно

---

быть расстояние. В NEAT предлагается просто объединить эти два критерия:

$$
\rho\left(\theta_{1}, \theta_{2}\right):=\alpha_{1} \frac{D}{\max \left(G_{1}, G_{2}\right)}+\alpha_{2}\left\|w_{1}-w_{2}\right\|_{1}
$$

где $\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}$ - гиперпараметры. Внутри самих видов отбор жадный.
NEAT - исторически один из первых алгоритмов, которые можно с каким-то результатом применить для RL задач без подготовленного удобного признакового описания состояний, например, изображений. Можно найти много интересных примеров применения алгоритма к разным задачам, с визуализацией получающейся сети (например, Mario).

# §2.2. Эволюционные стратегии 

### 2.2.1. Идея эволюционных стратегий

Когда мы пытаемся генерировать $\boldsymbol{k}+\mathbf{1}$-ое поколение, используя особей $\boldsymbol{k}$-го поколения в качестве исходного материала, единственная зависимость от всей истории заложена в составе $\boldsymbol{k}$-ой популяции. Мы можем рассмотреть распределение, из которого появляются особи очередной популяции: весь смысл нашей процедуры отбора в том, чтобы это распределение для очередной итерации поменялось так, что вероятность появления более хороших особей стала выше. Давайте обобщим эту идею: будем в явном виде хранить распределение для порождения особей новой популяции и по информации со всей популяции аккумулировать всю информацию внутри его параметров - «скрещивать все особи».

Определение 30: Распределение $\boldsymbol{q}\left(\boldsymbol{\theta} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right)$, из которого генерируются особи $\boldsymbol{k}$-ой популяции, называется эволюционной стратегией (evolutionary strategy, ES):

$$
\mathscr{P}_{k}:=\left\{\theta_{i} \sim q\left(\theta \mid \lambda_{k}\right) \mid i \in\{1,2 \ldots N\}\right\}
$$

где $\boldsymbol{\lambda}_{\boldsymbol{k}}$ - параметры эволюционной стратегии.
Из каких соображений подбирать $\boldsymbol{\lambda}_{\boldsymbol{k}}$ на очередном шаге? В принципе, мы хотели бы найти такой генератор особей, что их оценки как можно больше, то есть нащупать область $\Theta$ с высоким значением $\boldsymbol{J}(\boldsymbol{\theta})$. Эти соображения можно формализовать довольно по-разному и таким образом оправдывать разные метаэвристики. В частности, мы можем сказать, что $\boldsymbol{\lambda}$ есть особь или несколько особей (что приведёт нас к примерно ранее рассматривавшимся алгоритмам ${ }^{4}$ ), но мы можем отойти от пространства $\Theta$ и учить модель-генератор особей с какой-то хорошей параметризацией $\boldsymbol{\lambda}$. Мы дальше рассмотрим две основные идеи, как это можно делать.


### 2.2.2. Оценка вероятности редкого события

Первую идею возьмём немного сбоку. Допустим, стоит задача оценки вероятности редкого события:

$$
\boldsymbol{l}=\mathrm{P}(\boldsymbol{f}(\boldsymbol{x}) \geq \gamma)=\mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{p}(\boldsymbol{x})} \mathbb{I}[\boldsymbol{f}(\boldsymbol{x}) \geq \gamma]-\text { ? }
$$

где $\boldsymbol{p}(\boldsymbol{x})$ - некоторое распределение, $\boldsymbol{f}: \boldsymbol{X} \rightarrow \mathbb{R}$ - функционал, $\boldsymbol{\gamma}$ - некоторый порог.
Под словами «редкое событие» подразумевается, что условие внутри индикатора $\boldsymbol{f}(\boldsymbol{x}) \geq \gamma$ выполняется с вероятностью, крайне близкой к нулю. Это означает, что Монте-Карло оценка с разумным на практике числом сэмплов $\boldsymbol{N}$ выдаст или ноль или $\frac{1}{N}$, если один раз повезёт; короче, лобовой подход не годится.

Хочется сэмплировать $\boldsymbol{x}$ не из того распределения, которое нам дали $-\boldsymbol{p}(\boldsymbol{x})$, - а из чего-нибудь получше. Для этого применим importance sampling с некоторым распределением $\boldsymbol{q}(\boldsymbol{x})$, которое мы будем выбирать сами:

$$
\boldsymbol{l}=\mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{q}(\boldsymbol{x})} \frac{\boldsymbol{p}(\boldsymbol{x})}{\boldsymbol{q}(\boldsymbol{x})} \mathbb{I}[\boldsymbol{f}(\boldsymbol{x}) \geq \gamma]
$$

Нам хочется выбрать такое $\boldsymbol{q}(\boldsymbol{x})$, чтобы дисперсия Монте-Карло оценки такого интеграла была как можно меньше. Желание может быть исполнено:

[^0]
[^0]:    ${ }^{4}$ понятие эволюционных стратегий довольно общее и размытое - любой эволюционный алгоритм «неявно» определяет распределение для порождения особей очередной популяции и формально подпадает под эволюционные стратегии. Можно считать, что здесь ключевая идея заключается в том, что мы явно ищем это распределение в некотором параметрическом семействе.

---

Утверждение 4: Дисперсия Монте-Карло оценки (2.2) минимальна при

$$
q(x) \propto p(x) \mathbb{I}[f(x) \geq \gamma]
$$

Доказательство. Искомое значение $\boldsymbol{l}$ (2.1) является нормировочной константой такого распределения. Подставим данное $\boldsymbol{q}(\boldsymbol{x})$ в подынтегральную функцию:

$$
\frac{p(x)}{q(x)} \mathbb{I}[f(x) \geq \gamma]=\boldsymbol{l} \frac{p(x) \mathbb{I}[f(x) \geq \gamma]}{p(x) \mathbb{I}[f(x) \geq \gamma]}=\boldsymbol{l}
$$

Поскольку всё сократилось, для любых сэмплов $\boldsymbol{x} \sim \boldsymbol{q}(\boldsymbol{x})$ значение Монте-Карло оценки будет равно $\boldsymbol{l}$; то есть, дисперсия равна нулю.

Посчитать такое $\boldsymbol{q}(\boldsymbol{x})$ мы не можем, однако можем пытаться приблизить в параметрическом семействе $\boldsymbol{q}(\boldsymbol{x} \mid$ $|\lambda)$, минимизируя, например, такую KL-дивергенцию ${ }^{5}$ :

$$
\operatorname{KL}(q(x) \| q(x \mid \lambda))=\operatorname{const}(\lambda)-\mathbb{E}_{q(x)} \log q(x \mid \lambda) \rightarrow \min _{\lambda}
$$

Единственное зависящее от параметров $\boldsymbol{\lambda}$ слагаемое называется кросс-энтропией (cross entropy) и даёт название методу.

Пока что мы променяли шило на мыло, поскольку для такой оптимизации всё равно нужно уметь сэмплировать из $\boldsymbol{q}(\boldsymbol{x})$. Однако от задачи оценки числа (которую мы кроме как через Монте-Карло особо решать не умеем) мы перешли к поиску распределения. Поскольку это распределение мы строили так, чтобы оно помогало сэмплировать нам точки из редкого события, можно воспользоваться им же с прошлой итерации, чтобы помочь самим себе решать ту же задачу лучше. А то есть: строим последовательность $\boldsymbol{q}\left(\boldsymbol{x} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right), \boldsymbol{\lambda}_{\boldsymbol{0}}$ - любое, на очередной итерации:

$$
\begin{aligned}
\lambda_{k+1} & =\underset{\lambda}{\operatorname{argmin}}-\mathbb{E}_{q(x)} \log q(x \mid \lambda)= \\
\{\text { подставляем вид оптимального } \boldsymbol{q}(\boldsymbol{x}) \text { из }(2.3)\} & =\underset{\lambda}{\operatorname{argmin}}-\mathbb{E}_{p(x)} \mathbb{I}[f(x) \geq \gamma] \log q(x \mid \lambda)= \\
\left\{\text { importance sampling через } \boldsymbol{q}\left(\boldsymbol{x} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right)\right\} & =\underset{\lambda}{\operatorname{argmin}}-\mathbb{E}_{q\left(x \mid \lambda_{k}\right)} \frac{p(x)}{q\left(x \mid \lambda_{k}\right)} \mathbb{I}[f(x) \geq \gamma] \log q(x \mid \lambda)
\end{aligned}
$$

Каждая задача нахождения $\boldsymbol{\lambda}_{\boldsymbol{k}}$ всё ещё тяжела в связи с тем, что подынтегральное выражение всё ещё почти всегда ноль. Ключевая идея: поскольку мы теперь строим целую последовательность, мы можем поначалу решать сильно более простую задачу, разогревая $\gamma$. Будем на $\boldsymbol{k}$-ом шаге брать $\gamma$ не из условия задачи, а поменьше, так, чтобы с итерациями $\gamma$ увеличивалась (и мы решали бы задачу, всё более похожую на ту, что требовалось решить исходно), и одновременно достаточное число сэмплов значения подынтегральной функции были отличны от нуля.

Важно, что мы можем не задавать заранее последовательность $\gamma_{\boldsymbol{k}}$, а определять очередное значение прямо на ходу, например, исходя из сэмплов $\boldsymbol{x}_{\mathbf{1}} \ldots \boldsymbol{x}_{\boldsymbol{N}} \sim \boldsymbol{q}\left(\boldsymbol{x} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right)$ и значений $\boldsymbol{f}(\boldsymbol{x})$ в них.

# Алгоритм 2: Метод Кросс-Энтропии для оценки вероятности редкого события 

Вход: распределение $\boldsymbol{p}(\boldsymbol{x})$, функция $\boldsymbol{f}(\boldsymbol{x})$, порог $\gamma$
Гиперпараметры: $\boldsymbol{q}(\boldsymbol{x} \mid \boldsymbol{\lambda})$ - параметрическое семейство, $\boldsymbol{N}$ - число сэмплов, $\boldsymbol{M}$ - порог отбора
Инициализируем $\boldsymbol{\lambda}_{0}$ произвольно.
На $\boldsymbol{k}$-ом шаге:

1. сэмплируем $x_{1} \ldots x_{N} \sim q\left(x \mid \lambda_{k}\right)$
2. сортируем значения $\boldsymbol{f}\left(\boldsymbol{x}_{i}\right): \boldsymbol{f}_{(1)} \leq \boldsymbol{f}_{(2)} \leq \cdots \leq \boldsymbol{f}_{(N)}$
3. полагаем $\gamma_{k}:=\min \left(\gamma, f_{(M)}\right)$
4. решаем задачу оптимизации:

$$
\lambda_{k+1} \leftarrow \underset{\lambda}{\operatorname{argmax}} \frac{1}{N} \sum_{j=1}^{N} \mathbb{I}\left[f\left(x_{j}\right) \geq \gamma_{k}\right] \frac{p\left(x_{j}\right)}{q\left(x_{j} \mid \lambda_{k}\right)} \log q\left(x_{j} \mid \lambda\right)
$$

[^0]
[^0]:    ${ }^{5}$ здесь и всюду далее под обозначением const( $\boldsymbol{\lambda}$ ) мы будем подразумевать функции, константные относительно $\boldsymbol{\lambda}$; такие слагаемые не влияют на оптимизацию по $\boldsymbol{\lambda}$ и могут быть опущены.

---

5. критерий останова: $\gamma_{k}=\gamma$

Получение итоговой оценки:

1. сэмплируем $x_{1} \ldots x_{N} \sim q\left(x \mid \lambda_{k}\right)$
2. возвращаем

$$
l \approx \frac{1}{N} \sum_{j=1}^{N} \mathbb{I}\left[f\left(x_{j}\right) \geq \gamma_{k}\right] \frac{p\left(x_{j}\right)}{q\left(x_{j} \mid \lambda_{k}\right)}
$$

# 2.2.3. Метод Кросс-Энтропии для стохастической оптимизации 

Ну, в рассуждении было видно, что мы практически учим $q(x \mid \lambda)$ нащупывать область с высоким значением заданной функции без использования какой-либо информации о ней. Поэтому мы можем адаптировать метод, чтобы он стал мета-эвристикой. Для этого вернёмся к нашей задаче безградиентной оптимизации:

$$
J(\theta) \rightarrow \max _{\theta}
$$

и перепишем алгоритм 3 в условиях, когда порог $\gamma$ «не ограничен», ну или что тоже самое, $\gamma:=\max _{\theta} J(\theta)$. Формально мы также можем выбирать любое $\boldsymbol{p}(\boldsymbol{x})$; положим $\boldsymbol{p}(\boldsymbol{x}):=\boldsymbol{q}\left(\boldsymbol{x} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right)$, просто чтобы в задаче (2.4) сократилась importance sampling коррекция. Мы получим очень простой на вид алгоритм, в котором фактически на очередном шаге минимизируется такое расстояние:

$$
\mathbf{K L}\left(\mathbb{I}\left[\boldsymbol{J}(\boldsymbol{x}) \geq \gamma_{k}\right] \boldsymbol{q}\left(\boldsymbol{x} \mid \lambda_{k-1}\right) \| \boldsymbol{q}\left(\boldsymbol{x} \mid \lambda_{k}\right)\right) \rightarrow \min _{\lambda_{k}}
$$

где первое распределение задано с точностью до нормировочной константы.

## Алгоритм 4: Метод Кросс-Энтропии для оптимизации с оракулом нулевого порядка

Вход: оракул $\hat{J}(\theta)$
Гиперпараметры: $q(\theta \mid \lambda)$ - параметрическое семейство, $N$ - число сэмплов, $M$ - порог отбора
Инициализируем $\boldsymbol{\lambda}_{0}$ произвольно.
На $\boldsymbol{k}$-ом шаге:

1. сэмплируем $\mathscr{P}_{k}:=\left(\theta_{i} \sim q\left(\theta \mid \lambda_{k}\right) \mid i \in\{1,2, \ldots, N\}\right)$
2. проводим отбор $\mathscr{P}_{k}^{+}:=\operatorname{select}_{M}^{t o p}\left(\mathscr{P}_{k}\right)$
3. решаем задачу оптимизации:

$$
\lambda_{k+1} \leftarrow \underset{\lambda}{\operatorname{argmax}} \sum_{\theta \in \mathscr{P}_{k}^{+}} \log q(\theta \mid \lambda)
$$

Видно, что мы по сути действуем эволюционно: хотим генерировать при помощи распределения $q$ точки из области, где значение функции велико; берём и сэмплируем несколько точек из текущего приближения; из сгенерированных отбираем те, где значение функции было наибольшим и учим методом максимального правдоподобия повторять эти точки. Поскольку некоторая доля плохих точек была выкинута из выборки, распределение, которое учит очередное $\boldsymbol{q}\left(\boldsymbol{x} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right)$, лучше предыдущего. Это первый способ обучения эволюционных стратегий.

Пример 41 - Кросс-энтропийный метод для black-box оптимизации:

---



# 2.2.4. Метод Кросс-Энтропии для обучения с подкреплением (CEM) 

В обучении с подкреплением в кросс-энтропийном методе можно сделать ещё один очень интересный шаг. В отличие от всех остальных рассматриваемых в этой главе мета-эвристик, мы можем проводить эволюционный отбор не в пространстве возможных стратегий (в пространстве $\Theta$ ), а в пространстве траекторий. У нас будет одна текущая стратегия, из которой мы сгенерируем несколько траекторий, и в силу стохастичности некоторые из этих траекторий выдадут лучший результат, чем другие. Мы отберём лучшие и будем методом максимального правдоподобия (по сути, имитационным обучением) учиться повторять действия из лучших траекторий.

## Алгоритм 5: Cross-Entropy Method

Гиперпараметры: $\pi(a \mid s, \theta)$ - стратегия с параметрами $\boldsymbol{\theta}, \boldsymbol{N}$ - число сэмплов, $\boldsymbol{M}$ - порог отбора
Инициализируем $\boldsymbol{\theta}_{0}$ произвольно.
На $\boldsymbol{k}$-ом шаге:

1. сэмплируем $\boldsymbol{N}$ траекторий $\mathcal{T}_{1} \ldots \mathcal{T}_{\boldsymbol{N}}$ игр при помощи стратегии $\boldsymbol{\pi}\left(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{\theta}_{\boldsymbol{k}}\right)$
2. считаем кумулятивные награды $\boldsymbol{R}\left(\mathcal{T}_{i}\right)$
3. сортируем значения: $\boldsymbol{R}_{(1)} \leq \boldsymbol{R}_{(2)} \leq \cdots \leq \boldsymbol{R}_{(N)}$
4. полагаем $\gamma_{k}:=\boldsymbol{R}_{(M)}$
5. решаем задачу оптимизации:

$$
\theta_{k+1} \leftarrow \underset{\theta}{\arg \max } \frac{1}{N} \sum_{j=1}^{N} \mathbb{I}\left[\boldsymbol{R}\left(\mathcal{T}_{j}\right) \geq \gamma_{k}\right] \sum_{s, a \in \mathcal{T}_{j}} \log \pi(a \mid s, \theta)
$$

### 2.2.5. Натуральные эволюционные стратегии (NES)

Рассмотрим альтернативный вариант ${ }^{6}$ подбора параметров эволюционной стратегии $\boldsymbol{q}(\boldsymbol{\theta} \mid \boldsymbol{\lambda})$. Будем подбирать $\boldsymbol{\lambda}$, исходя из следующего функционала:

$$
g(\lambda):=\mathbb{E}_{\boldsymbol{\theta} \sim q(\theta \mid \lambda)} J(\boldsymbol{\theta}) \rightarrow \max _{\lambda}
$$

Будем оптимизировать этот функционал градиентно по $\boldsymbol{\lambda}$. Давайте подробно разберём, как дифференцировать функции подобного вида, поскольку в дальнейшем мы будем активно пользоваться этой техникой.

Теорема 5:

$$
\nabla_{\lambda} g(\lambda)=\mathbb{E}_{\boldsymbol{\theta} \sim q(\boldsymbol{\theta} \mid \lambda)} \nabla_{\lambda} \log q(\boldsymbol{\theta} \mid \lambda) J(\boldsymbol{\theta})
$$

Доказательство.

$$
\nabla_{\lambda} g(\lambda)=\nabla_{\lambda} \mathbb{E}_{\boldsymbol{\theta} \sim q(\boldsymbol{\theta} \mid \lambda)} J(\boldsymbol{\theta})=
$$

[^0]
[^0]:    ${ }^{6}$ смысл названия «натуральные эволюционные стратегии» (natural evolution strategies) будет объяснён позже.

---

$$
\begin{gathered}
=\{\text { мат.ожидание - это интеграл }\}=\nabla_{\lambda} \int_{\Theta} q(\theta \mid \lambda) J(\theta) \mathrm{d} \theta= \\
=\{\text { проносим градиент внутрь интеграла }\}=\int_{\Theta} \nabla_{\lambda} q(\theta \mid \lambda) J(\theta) \mathrm{d} \theta=(*)
\end{gathered}
$$

Теперь мы применим стандартный технический трюк, называемый log-derivative trick: мы хотим преобразовать данное выражение к виду мат.ожидания по $\boldsymbol{q}(\boldsymbol{\theta} \mid \boldsymbol{\lambda})$ от чего-то. Как мы сейчас увидим, это «что-то» - градиент логарифма правдодобия. Мы воспользуемся следующим тождеством:

$$
\nabla_{\lambda} \log q(\theta \mid \lambda)=\frac{\nabla_{\lambda} q(\theta \mid \lambda)}{q(\theta \mid \lambda)}
$$

Домножим и поделим наше выражение на $\boldsymbol{q}(\boldsymbol{\theta} \mid \boldsymbol{\lambda})$, чтобы получить внутри интеграла мат.ожидание:

$$
\begin{aligned}
(*) & =\int_{\Theta} q(\theta \mid \lambda) \frac{\nabla_{\lambda} q(\theta \mid \lambda)}{q(\theta \mid \lambda)} J(\theta) \mathrm{d} \theta= \\
=\{\text { замечаем градиент логарифма }(2.7)\} & =\int_{\Theta} q(\theta \mid \lambda) \nabla_{\lambda} \log q(\theta \mid \lambda) J(\theta) \mathrm{d} \theta= \\
& =\{\text { выделяем мат.ожидание }\}=\mathbb{E}_{\theta \sim q(\theta \mid \lambda)} \nabla_{\lambda} \log q(\theta \mid \lambda) J(\theta)
\end{aligned}
$$

Итак, есть следующая идея: сгенерируем популяцию $\mathscr{P}_{k}$ при помощи $\boldsymbol{q}\left(\boldsymbol{\theta} \mid \boldsymbol{\lambda}_{\boldsymbol{k}}\right)$, после чего воспользуемся особями как сэмплами для несмещённой оценки градиента функционала (2.5), чтобы улучшить параметры $\boldsymbol{\lambda}$ и впоследствии сгенерировать следующее поколение $\mathscr{P}_{k+1}$ из более хорошего распределения.

# 2.2.6. OpenAI-ES 

Рассмотрим подход на примере OpenAI-ES, где рассматривается обучение нейросети (с фиксированной топологией) с вещественными весами $\Theta \equiv \mathbb{R}^{h}$, и полагается

$$
q(\theta \mid \lambda):=\mathcal{N}\left(\lambda, \sigma^{2} I_{h \times h}\right)
$$

где $\boldsymbol{\sigma}$ - гиперпараметр, $\boldsymbol{\lambda} \in \mathbb{R}^{\boldsymbol{h}}$ - по сути, кодирует одну особь, $\boldsymbol{I}_{\boldsymbol{h} \times \boldsymbol{h}}$ - диагональная единичная матрица размера $\boldsymbol{h} \times \boldsymbol{h}$.

Теорема 6: Для эволюционной стратегии (2.8) оценка градиента (2.5) равна

$$
\nabla_{\lambda} g(\lambda)=\frac{1}{N \sigma^{2}} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta)(\theta-\lambda)
$$

Доказательство.

$$
\nabla_{\lambda} \log q(\theta \mid \lambda)=-\nabla_{\lambda} \frac{(\theta-\lambda)^{T}(\theta-\lambda)}{2 \sigma^{2}}=\frac{\theta-\lambda}{\sigma^{2}}
$$

Достаточно подставить выражение в общую формулу (2.6).
Полученная формула легко интерпретируема: мы находимся в некотором «центре» $\boldsymbol{\lambda}$, который является нашим «текущим найденным решением». Дальше мы сэмплируем несколько векторов $\boldsymbol{\theta}-\boldsymbol{\lambda}$ из стандартного нормального распределения и складываем эти вектора («скрещиваем всех детей») с весами, пропорциональными приспособленности.

Подход работает при большом количестве серверов и ещё одном трюке, позволяющем не обмениваться векторами $\boldsymbol{\theta}$ (имеющими размерность, например, по числу параметров нейросети) между процессами. Для этого достаточно зафиксировать random seed на всех процессорах, в каждом процессе генерировать всё поколение, оценивать только особь с соответствующим процессу номеру и обмениваться с другими процессами исключительно оценками $\hat{\boldsymbol{J}}$ (скалирами!). Цель такой процедуры - избежать обмена весами нейросетей (пусть даже не очень больших) между серверами. За счёт параллелизации удаётся так «обучать» сетки играть в одну игру Атари за 10 минут. Если у вас есть 1440 процессоров. Естественно, ни про какой sample efficiency речь не идёт.

Пример 42 - OpenAI-ES:

---



Рассмотрим альтернативный взгляд на этот алгоритм. Допустим, мы находимся в точке $\boldsymbol{\theta}$ и хотим сдвинуться как бы по градиенту $\boldsymbol{J}(\boldsymbol{\theta})$, который вычислить мы не можем. Но мы можем приблизить градиент вдоль любого направления $\boldsymbol{\nu}$, например, так:

$$
\left.\nabla\right|_{\nu} J(\theta) \approx \frac{\hat{J}(\theta+\sigma \nu)-\hat{J}(\theta)}{\sigma}
$$

для некоторого небольшого скаляра $\sigma$.
Лобовая идея ${ }^{7}$ : давайте возьмём $\boldsymbol{N}$ случайных направлений $\boldsymbol{\nu}_{\mathbf{1}} \ldots \boldsymbol{\nu}_{\boldsymbol{N}} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})$ и сделаем шаг по всем этим направлениям:

$$
\theta_{k+1}:=\theta_{k}+\alpha \underbrace{\sum_{i=0}^{N} \frac{\hat{J}\left(\theta+\sigma \nu_{i}\right)-\hat{J}(\theta)}{\sigma}}_{\text {приближение градиента }} \nu_{i}
$$

где $\boldsymbol{\alpha}$ - learning rate.
Утверждение 5: Формулы (2.10) и (2.9) эквивалентны.
Доказательство. Поскольку $\boldsymbol{\nu}$ сэмплируется из стандартной гауссианы, то в среднем:

$$
\mathbb{E}_{\nu \sim \mathcal{N}(0, I)} \frac{\hat{J}(\theta)}{\sigma} \nu=\frac{\hat{J}(\theta)}{\sigma} \mathbb{E}_{\nu \sim \mathcal{N}(0, I)} \nu=\mathbf{0}
$$

Следовательно, (2.10) оценивает тот же градиент, что и формула

$$
\theta_{k+1}:=\theta_{k}+\alpha \sum_{i=0}^{N} \frac{\hat{J}\left(\theta+\sigma \nu_{i}\right)}{\sigma} \nu_{i}
$$

что совпадает с формулой OpenAI-ES с точностью до замены обозначений: достаточно заметить, что $\boldsymbol{\theta}+\boldsymbol{\sigma} \boldsymbol{\nu}$ есть сэмпл из $\mathcal{N}\left(\boldsymbol{\theta}, \boldsymbol{\sigma}^{2} \boldsymbol{I}\right)$.

# 2.2.7. Адаптация матрицы ковариации (CMA-ES) 

Более глубокомысленно было бы для $\boldsymbol{\Theta} \equiv \mathbb{R}^{\boldsymbol{h}}$ адаптировать не только среднее, но и матрицу ковариации, которая имеет смысл «разброса» очередной популяции. Covariance Matrix Adaptation Evolution Strategy (CMAES) - алгоритм, включающий довольной большой набор эвристик, в основе которого лежит эволюционная стратегия, адаптирующая не только среднее, но и матрицу ковариации:

$$
q(\theta \mid \lambda):=\mathcal{N}(\mu, \Sigma)
$$

где $\boldsymbol{\lambda}:=(\boldsymbol{\mu}, \boldsymbol{\Sigma})$.
Нам придётся хранить матрицу ковариации размера $\mathbb{R}^{h \times h}$, где $\boldsymbol{h}$ - количество параметров нашей стратегии $\left(\boldsymbol{\Theta} \equiv \mathbb{R}^{h}\right.$ ). Если стратегия задана нейронной сетью, то, чтобы такое было возможно, сеть должна быть по современным меркам минимальнейшей. Однако, во многих задачах непрерывного управления это довольно

[^0]
[^0]:    ${ }^{7}$ в алгоритме ARS (Advanced Random Search, хотя такой подход не совсем «случайный поиск») делается ровно это, за тем небольшим исключением, что используется приближение градиента по направлению за два вызова оракула:

    $$
    \nabla \cdot \int_{\nu} J(\theta) \approx \frac{\hat{J}(\theta+\sigma \nu)-\hat{J}(\theta-\sigma \nu)}{2 \sigma}
    $$

---

типичная ситуация, когда на вход в качестве состояния подаётся небольшой (размера 100-200) вектор, а на выходе также ожидается вектор размера порядка 10-30. Тогда стратегия может быть задана полноспязной нейросетью всего в несколько слоёв, и хранить $\boldsymbol{\Sigma}$ теоретически становится возможно.

Мы рассмотрим только основную часть формул алгоритма, касающихся формулы обновления $\boldsymbol{\Sigma}$. Посмотрим на формулу для обновления среднего (2.9) (с точностью до learning rate):

$$
\mu_{k+1}=\mu_{k}+\alpha \sum_{\theta \in \mathscr{P}_{k}} \underbrace{\boldsymbol{J}(\theta)}_{\text {нес }} \underbrace{\left(\theta-\mu_{k}\right)}_{\text {«предлагаемое» }} \underbrace{\left(\theta-\mu_{k}\right)}_{\text {особ́ью изменение }}
$$

Для обновления матрицы ковариации будем рассуждать также: каждая особь популяции $\boldsymbol{\theta}$ «указывает» на некоторую ковариацию $\left(\boldsymbol{\theta}-\mu_{k}\right)\left(\boldsymbol{\theta}-\mu_{k}\right)^{\boldsymbol{T}}$ и таким образом «предлагает» следующее изменение:

$$
\left(\theta-\mu_{k}\right)\left(\theta-\mu_{k}\right)^{T}-\Sigma_{k}
$$

где $\boldsymbol{\Sigma}_{\boldsymbol{k}}$ - матрица ковариации на текущей итерации. Усредним эти «предложения изменения» по имеющейся популяции, взвесив их на $\boldsymbol{J}(\boldsymbol{\theta})$, и получим «градиент» для обновления матрицы:

$$
\boldsymbol{\Sigma}_{k+1}:=\boldsymbol{\Sigma}_{k}+\alpha \sum_{\theta \in \mathscr{P}_{k}} \hat{J}(\theta)\left(\left(\theta-\mu_{k}\right)\left(\theta-\mu_{k}\right)^{T}-\Sigma_{k}\right)
$$

Здесь нужно оговориться, что мы используем оценку ковариации как бы «методом максимального правдоподобия при условии известного среднего $\boldsymbol{\mu}_{\boldsymbol{k}}$ » (мы знаем, что именно с таким средним генерировались особи прошлой популяции) ${ }^{8}$. Исторически к этим формулам пришли эвристически, но позже у формулы появилось теоретическое обоснование.

Теорема 7: Формулы (2.12) и (2.13) для обновления $\boldsymbol{\lambda}=(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ являются формулами натурального градиентного спуска для (2.5).

Доказательство требует обсуждения такой большой темы, как натуральный градиентный спуск (см. приложение А.1), а вывод формулы потребует небольшого введения в Кронекерову алгебру, поэтому доказательство вынесено в приложение А.2.

Именно поэтому данный вид алгоритмов для обучения эволюционных стратегий называется «натуральны$\boldsymbol{\mu} \boldsymbol{u}$ » (natural): считается, что функционал (2.5) корректнее оптимизировать именно при помощи натурального градиентного спуска, инвариантного к параметризации $\boldsymbol{q}(\boldsymbol{\theta} \mid \boldsymbol{\lambda})$, а не обычного.


Пример 44: Эволюционные стратегии и особенно CMA-ES весьма успешно применимы в задачах непрерывного управления вроде Locomotion (пример 25), в которых нужно научить разных существ ходить. Много интересных примеров можно найти в этом видео.

Если мы попробуем проделать с данным подходом (оптимизацией (2.5)) тот же трюк, что и с кроссэнтропийным методом, и попытаемся считать градиент «в пространстве траекторий», а не в пространстве стратегий, то получим методы оптимизации $\boldsymbol{J}(\boldsymbol{\theta})$ уже первого порядка - «policy gradient» методы. K ним мы перейдём в главе 5 .

[^0]
[^0]:    ${ }^{8}$ мы бы пришли к немного другим формулам, если бы использовали метод максимального правдоподобия при условии неизвестного среднего $\left(\boldsymbol{\mu}_{\boldsymbol{k}}\right.$ заменилось бы на $\boldsymbol{\mu}_{\boldsymbol{k + 1}}$ ):

    $$
    \left(\boldsymbol{\theta}-\boldsymbol{\mu}_{\boldsymbol{k}+\mathbf{1}}\right)\left(\boldsymbol{\theta}-\boldsymbol{\mu}_{\boldsymbol{k}+\mathbf{1}}\right)^{\boldsymbol{T}}-\boldsymbol{\Sigma}_{\boldsymbol{k}}
    $$

---

# Классическая теория 

В данной главе будут доказаны основные теоретические результаты о MDP и получены важные «табличные» алгоритмы, работающие в случае конечных пространств состояний и действий; они лягут в основу всех дальнейших алгоритмов.

## §3.1. Оценочные функции

### 3.1.1. Свойства траекторий

Как это всегда бывает, чем более общую задачу мы пытаемся решать, тем менее эффективный алгоритм мы можем придумать. В RL мы сильно замахиваемся: хотим построить алгоритм, способный обучаться решению «произвольной» задачи, заданной средой с описанной функцией награды. Однако в формализме MDP в постановке мы на самом деле внесли некоторые ограничения: марковость и стационарность. Эти предположения практически не ограничивают общность нашей задачи с точки зрения здравого смысла с одной стороны и при этом вносят в нашу задачу некоторую «структуру»; мы сможем придумать более эффективные алгоритмы решения за счёт эксплуатации этой структуры.

Что значит «структуру»? Представим, что мы решаем некоторую абстрактную задачу последовательного принятия решения, максимизируя некоторую кумулятивную награду. Вот мы находимся в некотором состоянии и должны выбрать некоторое действие. Интуитивно ясно, что на прошлое - ту награду, которую мы уже успели собрать - мы уже повлиять не можем, и нужно максимизировать награду в будущем. Более того, мы можем отбросить всю нашу предыдущую историю и задуматься лишь над тем, как максимизировать награду с учётом сложившейся ситуации - «текущего состояния».

Пример 45 - Парадокс обжоры: Обжора пришёл в ресторан и заказал кучу-кучу еды. В середине трапезы выяснилось, что оставшиеся десять блюд явно лишние и в него уже не помещаются. Обидно: они будут в счёте, да и не пропадать же еде, поэтому надо бы всё равно всё съесть. Однако, с точки зрения функции награды нужно делать противоположный вывод: блюда будут в счёте в любом случае, вне зависимости от того, будут ли они съедены - это награда за уже совершённое действие, «прошлое», - а вот за переедание может прилететь ещё отрицательной награды. Обжора понимает, что в прошлом совершил неоптимальное действие, и пытается «прооптимизировать» неизбежную награду за прошлое, в результате проигрывая ещё.

Давайте сформулируем эту интуицию формальнее. Как и в обычных Марковских цепях, в средах благодаря марковости действует закон «независимости прошлого и будущего при известном настоящем». Формулируется он так:

Утверждение 6 - Независимость прошлого и будущего при известном настоящем: Пусть
$\mathcal{T}_{\text {it }}:=\left\{s_{0}, a_{0} \ldots s_{t-1}, a_{t-1}\right\}-$ «прошлое», $s_{t}-$ «настоящее», $\mathcal{T}_{t}:=\left\{a_{t}, s_{t+1}, a_{t+1} \ldots\right\}-$ «будущее». Тогда:

$$
p\left(\mathcal{T}_{i t}, \mathcal{T}_{t} \mid s_{t}\right)=p\left(\mathcal{T}_{i t} \mid s_{t}\right) p\left(\mathcal{T}_{t} \mid s_{t}\right)
$$

Доказательство. По правилу произведения:

$$
p\left(\mathcal{T}_{i t}, \mathcal{T}_{t} \mid s_{t}\right)=p\left(\mathcal{T}_{i t} \mid s_{t}\right) p\left(\mathcal{T}_{t} \mid s_{t}, \mathcal{T}_{i t}\right)
$$

Однако в силу марковости будущее зависит от настоящего и прошлого только через настоящее:

$$
p\left(\mathcal{T}_{t} \mid s_{t}, \mathcal{T}_{i t}\right)=p\left(\mathcal{T}_{t} \mid s_{t}\right)
$$

---

Для нас утверждение означает следующее: если мы сидим в момент времени $\boldsymbol{t}$ в состоянии $\boldsymbol{s}$ и хотим посчитать награду, которую получим в будущем (то есть величину, зависящую только от $\mathcal{T}_{t_{1}}$ ), то нам совершенно не важна история попадания в $\boldsymbol{s}$. Это следует из свойства мат. ожиданий по независимым переменным:

$$
\mathbb{E}_{\mathcal{T} \mid s_{t}=s} R\left(\mathcal{T}_{t_{1}}\right)=\{\text { утв. } 6\}=\mathbb{E}_{\mathcal{T}_{t_{1} \mid s_{t}=s}} \underbrace{\mathbb{E}_{\mathcal{T}_{t_{1}} \mid s_{t}=s} R\left(\mathcal{T}_{t_{1}}\right)}_{\text {не зависит от } \mathcal{T}_{t_{1}}}=\mathbb{E}_{\mathcal{T}_{t_{1} \mid s_{t}=s}} R\left(\mathcal{T}_{t_{1}}\right)
$$

Определение 31: Для траектории $\mathcal{T}$ величина

$$
R_{t}:=R\left(\mathcal{T}_{t_{1}}\right)=\sum_{i \geq t} \gamma^{i-t} r_{i}
$$

называется reward-to-go с момента времени $\boldsymbol{t}$.
Благодаря второму сделанному предположению, о стационарности (в том числе стационарности стратегии агента), получается, что будущее также не зависит от текущего момента времени $\boldsymbol{t}$ : всё определяется исключительно текущим состоянием. Иначе говоря, агенту неважно не только, как он добрался до текущего состояния и сколько награды встретил до настоящего момента, но и сколько шагов в траектории уже прошло. Формально это означает следующее: распределение будущих траекторий имеет в точности тот же вид, что и распределение всей траектории при условии заданного начала.

Утверждение 7: Будущее определено текущим состоянием:

$$
p\left(\mathcal{T}_{t_{1}} \mid s_{t}=s\right) \equiv p\left(\mathcal{T} \mid s_{0}=s\right)
$$

Доказательство. По определению:

$$
p\left(\mathcal{T}_{t_{1}} \mid s_{t}=s\right)=\prod_{i \geq t} p\left(s_{i+1} \mid s_{i}, a_{i}\right) \pi\left(a_{i} \mid s_{i}\right)=(*)
$$

Воспользуемся однородностью MDP и однородностью стратегии, а именно:

$$
\begin{gathered}
\pi\left(a_{i} \mid s_{i}=s\right)=\pi\left(a_{0} \mid s_{0}=s\right) \\
p\left(s_{i+1} \mid s_{i}=s, a_{i}\right)=p\left(s_{1} \mid s_{0}=s, a_{0}\right) \\
\pi\left(a_{i+1} \mid s_{i+1}\right)=\pi\left(a_{1} \mid s_{1}\right) \\
p\left(s_{i+2} \mid s_{i+1}, a_{i+1}\right)=p\left(s_{2} \mid s_{1}, a_{1}\right)
\end{gathered}
$$

и так далее, получим:

$$
(*)=\prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right) \pi\left(a_{t} \mid s_{t}\right)=p\left(\mathcal{T} \mid s_{0}=s\right)
$$

Утверждение 8: Для любого $\boldsymbol{t}$ и любой функции $\boldsymbol{f}$ от траекторий:

$$
\mathbb{E}_{\mathcal{T} \mid s_{0}=s} f(\mathcal{T})=\mathbb{E}_{\mathcal{T} \mid s_{t}=s} f\left(\mathcal{T}_{t_{1}}\right)
$$

Доказательство.

$$
\mathbb{E}_{\mathcal{T} \mid s_{0}=s} f(\mathcal{T})=\{\text { утв. } 7\}=\mathbb{E}_{\mathcal{T}_{t_{1}} \mid s_{t}=s} f\left(\mathcal{T}_{t_{1}}\right)=\{\text { утв. } 6\}=\mathbb{E}_{\mathcal{T} \mid s_{t}=s} f\left(\mathcal{T}_{t_{1}}\right)
$$

Мы показали, что все свойства reward-to-go определяются исключительно стартовым состоянием.

---

# 3.1.2. V-функция 

Итак, наша интуиция заключается в том, что, когда агент приходит в состояние $\boldsymbol{s}$, прошлое не имеет значения, и оптимальный агент должен максимизировать в том числе и награду, которую он получит, стартуя из состояния $\boldsymbol{s}$. Поэтому давайте «обобщим» наш оптимизируемый функционал, варьируя стартовое состояние:

Определение 32: Для данного MDP V-функиией (value function) или оценочной функцией состояний (state value function) для данной стратегии $\boldsymbol{\pi}$ называется величина

$$
V^{\pi}(s):=\mathbb{E}_{\boldsymbol{T} \sim \pi \mid s_{0}=s} R(\mathcal{T})
$$

По определению функция ценности состояния, или V-функция - это сколько набирает в среднем агент из состояния $\boldsymbol{s}$. Причём в силу марковости и стационарности неважно, случился ли старт на нулевом шаге эпизода или на произвольном $t$-ом:

Утверждение 9: Для любого $t$ верно:

$$
V^{\pi}(s)=\mathbb{E}_{\boldsymbol{T} \sim \pi \mid s_{t}=s} R_{t}
$$

Пояснение. Применить утверждение 8 для $\boldsymbol{R}(\mathcal{T})$.

## Утверждение 10: $\boldsymbol{V}^{\boldsymbol{\pi}}(s)$ ограничено.

Утверждение 11: Для терминальных состояний $V^{\pi}(s)=0$.
Заметим, что любая политика $\boldsymbol{\pi}$ индуцирует $\boldsymbol{V}^{\boldsymbol{\pi}}$. То есть для данного MDP и данной стратегии $\boldsymbol{\pi}$ функция $\boldsymbol{V}^{\boldsymbol{\pi}}$ однозначно задана своим определением; совсем другой вопрос, можем ли мы вычислить эту функцию.

Пример 46: Посчитаем V-функцию для MDP и стратегии $\boldsymbol{\pi}$ с рисунка, $\boldsymbol{\gamma}=\mathbf{0 . 8}$. Её часто удобно считать «с конца», начиная с состояний, близких к терминальным, и замечая связи между значениями функции для разных состояний.

Начнём с состояния C: там агент всегда выбирает действие получает -1 , и эпизод заканчивается: $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{C})=-\mathbf{1}$.

Для состояния В с вероятностью 0.5 агент выбирает действие и получает +4 . Иначе он получает +2 и возвращается снова в состояние В. Вся дальнейшая награда будет дисконтирована на $\gamma=0.8$ и тоже равна $V^{\pi}(s=B)$ по определению. Итого:

$$
V^{\pi}(s=B)=\underbrace{0.5 \cdot 4}_{B}+\underbrace{0.5 \cdot\left(2+\gamma V^{\pi}(s=B)\right)}_{B}
$$



Решая это уравнение относительно $V^{\pi}(s=B)$, получаем ответ 5 .

Для состояния А достаточно аналогично рассмотреть все дальнейшие события:

$$
V^{\pi}(s=A)=\underbrace{0.25}_{B} \cdot\left(\underbrace{0.75 \cdot 0}_{t e r m i n a l}+\underbrace{0.25 \gamma V^{\pi}(s=B)}_{B}\right)+\underbrace{0.75}_{B} \underbrace{\gamma V^{\pi}(s=C)}_{C}
$$

Подставляя значения, получаем ответ $V^{\pi}(s=A)=-0.35$.

---

# 3.1.3. Уравнения Беллмана 

Если $\boldsymbol{s}_{\mathbf{0}}$ - стартовое состояние, то $\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}_{\mathbf{0}}\right)$ по определению и есть функционал (1.5), который мы хотим оптимизировать. Формально, это единственная величина, которая нас действительно волнует, так как она нам явно задана в самой постановке задачи, но мы понимаем, что для максимизации $\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}_{\mathbf{0}}\right)$ нам нужно промаксимизировать и $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ (строго мы это пока не показали). Другими словами, у нас в задаче есть подзадачи эквивалентной структуры: возможно, они, например, проще, и мы можем сначала их решить, а дальше как-то воспользоваться этими решениями для решения более сложной. Вот если граф MDP есть дерево, например, то очевидно, как считать $\boldsymbol{V}^{\boldsymbol{\pi}}$ : посчитать значение в листьях (листья соответствуют терминальным состояниям - там ноль), затем в узлах перед листьями, ну и так далее индуктивно добраться до корня.

Мы заметили, что в примере 46 на значения V-функции начали появляться рекурсивные соотношения. В этом и есть смысл введения понятия оценочных функций - «дополнительных переменных»: в том, что эти значения связаны между собой уравнениями Беллмана (Bellman equations).

Теорема 8 - Уравнение Беллмана (Bellman expectation equation) для $V^{\boldsymbol{\pi}}$ :

$$
V^{\pi}(s)=\mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi}\left(s^{\prime}\right)\right]
$$

Доказательство. Интуиция: награда за игру равна награде за следующий шаг плюс награда за оставшуюся игру; награда за хвост равна следующей награде плюс награда за хвост. Действительно, для всех траекторий $\mathcal{T}$ и для любых $\boldsymbol{t}$ верно:

$$
R_{t}=r_{t}+\gamma R_{t+1}
$$

Соответственно, для формального доказательства раскладываем сумму по времени как первое слагаемое плюс сумма по времени и пользуемся утверждением 9 о независимости Vфункции от времени:

$$
\begin{aligned}
V^{\pi}(s)=\mathbb{E}_{\mathcal{T} \mid s_{t}=s} R_{t} & =\mathbb{E}_{a_{t}}\left[r_{t}+\gamma \mathbb{E}_{s_{t+1}} \mathbb{E}_{\mathcal{T} \sim \pi \mid s_{t+1}} R_{t+1}\right]= \\
& =\mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi}\left(s^{\prime}\right)\right]
\end{aligned}
$$



Пример 47: Выпишем уравнения Беллмана для MDP и стратегии $\boldsymbol{\pi}$ из примера 46. Число уравнений совпадает с числом состояний. Разберём подробно уравнение для состояния $\boldsymbol{A}$ :

$$
V^{\pi}(A)=\underbrace{0.25\left(0+\gamma 0.25 V^{\pi}(B)\right)+0.75\left(0+\gamma V^{\pi}(C)\right)}_{\mathbf{A}}
$$

С вероятностью 0.25 будет выбрано действие после чего случится дисконтирование на $\gamma$; с вероятностью 0.75 эпизод закончится и будет выдана нулевая награда, с вероятностью 0.25 агент перейдёт в состояние В. Второе слагаемое уравнения будет отвечать выбору действия агент тогда перейдёт в состояние С и, начиная со следующего шага, получит в будущем $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{C})$. Аналогично расписываются два оставшихся уравнения.

$$
\begin{aligned}
& V^{\pi}(A)=\frac{1}{16} \gamma V^{\pi}(B)+\frac{3}{4} \gamma V^{\pi}(C) \\
& V^{\pi}(B)=0.5\left(2+\gamma V^{\pi}(B)\right)+0.5 \cdot 4 \\
& V^{\pi}(C)=-1
\end{aligned}
$$

Заметим, что мы получили систему из трёх линейных уравнений с тремя неизвестными.


Позже мы покажем, что $\boldsymbol{V}^{\boldsymbol{\pi}}$ является единственной функцией $\mathcal{S} \rightarrow \mathbb{R}$, удовлетворяющей уравнениям Беллмана для данного MDP и данной стратегии $\boldsymbol{\pi}$, и таким образом однозначно ими задаётся.

---

# 3.1.4. Оптимальная стратегия 

У нас есть конкретный функционал $\boldsymbol{J}(\boldsymbol{\pi})=\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}_{\mathbf{0}}\right)$, который мы хотим оптимизировать. Казалось бы, понятие оптимальной политики очевидно как вводить:
| Определение 33: Политика $\pi^{*}$ оптимальна, если $\forall \boldsymbol{\pi}: \boldsymbol{V}^{\boldsymbol{\pi}^{*}}\left(\boldsymbol{s}_{\mathbf{0}}\right) \geq \boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}_{\mathbf{0}}\right)$.
Введём альтернативное определение:
| Определение 34: Политика $\boldsymbol{\pi}^{*}$ оптимальна, если $\forall \boldsymbol{\pi}, \boldsymbol{s}: \boldsymbol{V}^{\boldsymbol{\pi}^{*}}(\boldsymbol{s}) \geq \boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$.
Теорема 9: Определения не эквивалентны.
Доказательство. Из первого не следует второе (из второго первое, конечно, следует). Контрпример приведён на рисунке. С точки зрения нашего функционала, оптимальной будет стратегия сразу выбрать и закончить игру. Поскольку оптимальный агент выберет $\square$ с вероятностью 0 , ему неважно, какое решение он будет принимать в состоянии $\boldsymbol{B}$, в котором он никогда не окажется. Согласно первому определению, оптимальная политика может действовать в $\boldsymbol{B}$ как угодно. Однако, чтобы быть оптимальной согласно второму определению и в том числе максимизировать $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{B})$, стратегия обязана выбирать в $\boldsymbol{B}$ только действие $\square$.


Интуиция подсказывает, что различие между определениями проявляется только в состояниях, которые оптимальный агент будет избегать с вероятностью 1 (позже мы увидим, что так и есть). Задавая оптимальность вторым определением, мы чуть-чуть усложняем задачу, но упрощаем теоретический анализ: если бы мы оставили первое определение, у оптимальных политик могли бы быть разные V-функции (см. пример из последнего доказательства); согласно второму определению, V-функция всех оптимальных политик совпадает.
| Определение 35: Оптимальные стратегии будем обозначать $\boldsymbol{\pi}^{*}$, а соответствующую им оптимальную $\boldsymbol{V}$ функиию $-\boldsymbol{V}^{*}$ :

$$
V^{*}(s)=\max _{\pi} V^{\pi}(s)
$$

Пока нет никаких обоснований, что найдётся стратегия, которая максимизирует $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ сразу для всех состояний. Вдруг в одних $\boldsymbol{s}$ максимум (3.4) достигается на одной стратегии, а в другом - на другой? Тогда оптимальных стратегий в сильном смысле вообще не существует, хотя формальная величина (3.4) существует. Пока заметим лишь, что для ситуации, когда MDP - дерево, существование оптимальной стратегии в смысле второго определения можно опять показать «от листьев к корню».

Пример 48: Рассмотрим MDP из примера $9 ; \gamma=\frac{10}{11}$, множество стратегий параметризуется единственным числом $\theta:=\pi(a=\square \mid s=A)$.

По определению оптимальная V-функция для состояния A равна

$$
V^{*}(s=A)=\max _{\theta \in[0,1]} J(\pi)=\max _{\theta \in[0,1]}[3+5 \theta]=8
$$

Оценочные функции для состояния В для всех стратегий совпадают и равны $\boldsymbol{V}^{*}(\boldsymbol{s}=\boldsymbol{B})=\mathbf{1}+\gamma+\gamma^{2}+\ldots=11$. Для терминальных состояний $\boldsymbol{V}^{*}(\boldsymbol{s})=\mathbf{0}$.


### 3.1.5. Q-функция

V-функции нам не хватит. Если бы мы знали оптимальную value-функцию $\boldsymbol{V}^{*}(\boldsymbol{s})$, мы не смогли бы восстановить хоть какую-то оптимальную политику из-за отсутствия в общем случае информации о динамике среды. Допустим, агент находится в некотором состоянии и знает его ценность $\boldsymbol{V}^{*}(\boldsymbol{s})$, а также знает ценности всех других состояний; это не даёт понимания того, какие действия в какие состояния приведут - мы никак не дифференцируем действия между собой. Поэтому мы увеличим количество переменных: введём схожее определение для ценности не состояний, но пар состояние-действие.
| Определение 36: Для данного MDP Q-функиией (state-action value function, action quality function) для данной стратегии $\boldsymbol{\pi}$ называется

$$
Q^{\pi}(s, a):=\mathbb{E}_{\boldsymbol{T} \sim \pi \mid s_{0}=s, a_{0}=a} \sum_{t \geq 0} \gamma^{t} r_{t}
$$

Теорема 10 - Связь оценочных функций: V-функции и Q-функции взаимозависимы, а именно:

$$
Q^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi}\left(s^{\prime}\right)
$$

---

$$
V^{\pi}(s)=\mathbb{E}_{a \sim \pi(a \mid s)} Q^{\pi}(s, a)
$$

Доказательство. Следует напрямую из определений.
Итак, если V-функция - это сколько получит агент из некоторого состояния, то Q-функция - это сколько получит агент после выполнения данного действия из данного состояния. Как и V-функция, Q-функция не зависит от времени, ограничена по модулю при рассматриваемых требованиях к MDP, и, аналогично, для неё существует уравнение Беллмана:

Теорема 11 - Уравнение Беллмана (Bellman expectation equation) для Q-функции:

$$
Q^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime}} Q^{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

Доказательство. Можно воспользоваться (3.5) + (3.6), можно расписать как награду на следующем шаге плюс хвостик.

Пример 49: Q-функция получает на вход пару состояние-действие и ничего не говорится о том, что это действие должно быть как-то связано с оцениваемой стратегией $\boldsymbol{\pi}$.

Давайте в MDP с рисунка рассмотрим стратегию $\boldsymbol{\pi}$, которая всегда детерминировано выбирает действие $\square$ Мы тем не менее можем посчитать $Q^{\pi}(s, \square)$ для любых состояний (например, для терминальных это значение формально равно нулю). Сделаем это при помощи QV уравнения:

$$
\begin{aligned}
& Q^{\pi}(s=A, \square)=0.25 \gamma V^{\pi}(s=B) \\
& Q^{\pi}(s=B, \square)=2+\gamma V^{\pi}(s=B) \\
& Q^{\pi}(s=C, \square)=0.8 \gamma V^{\pi}(s=B)+0.2 \gamma V^{\pi}(s=C)
\end{aligned}
$$



Внутри $V^{\pi}$ сидит дальнейшее поведение при помощи стратегии $\boldsymbol{\pi}$, то есть выбор исключительно действий соответственно, $V^{\pi}(s=B)=4, V^{\pi}(s=C)=-1$.

Мы получили все уравнения Беллмана для оценочных функций (с условными названиями VV, VQ, QV и, конечно же, QQ). Как видно, они следуют напрямую из определений; теперь посмотрим, что можно сказать об оценочных функциях оптимальных стратегий.

# 3.1.6. Принцип оптимальности Беллмана 

Определение 37: Для данного MDP оптимальной Q-функиией (optimal Q-function) называется

$$
Q^{*}(s, a):=\max _{\pi} Q^{\pi}(s, a)
$$

Формально очень хочется сказать, что $\boldsymbol{Q}^{*}$ - оценочная функция для оптимальных стратегий, но мы пока никак не связали введённую величину с $\boldsymbol{V}^{*}$ и показать это пока не можем. Нам доступно только такое неравенство пока что:

## Утверждение 12:

$$
Q^{*}(s, a) \leq r+\gamma \mathbb{E}_{s^{\prime}} V^{*}\left(s^{\prime}\right)
$$

Доказательство.

$$
\begin{gathered}
Q^{*}(s, a)=\left\{\text { определение } Q^{*}(3.8)\right\}=\max _{\pi} Q^{\pi}(s, a)= \\
=\left\{\text { связь QV (3.5) }\right\}=\max _{\pi}\left[r+\gamma \mathbb{E}_{s^{\prime}} V^{\pi}\left(s^{\prime}\right)\right] \leq \\
\leq\left\{\text { максимум среднего не превосходит среднее максимума }\right\} \leq r+\gamma \mathbb{E}_{s^{\prime}} \max _{\pi} V^{\pi}\left(s^{\prime}\right)= \\
=\left\{\text { определение } V^{*}(3.4)\right\}=r+\gamma \mathbb{E}_{s^{\prime}} V^{*}\left(s^{\prime}\right)
\end{gathered}
$$

Равенство в месте с неравенством случилось бы, если бы мы доказали следующий факт: что вообще существует такая стратегия $\boldsymbol{\pi}$, которая максимизирует $\boldsymbol{V}^{\boldsymbol{\pi}}$ сразу для всех состояний $\boldsymbol{s}$ одновременно (и которую мы определили как оптимальную). Другими словами, нужно показать, что максимизация $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ для одного состояния «помогает» максимизировать награду для других состояний. Для V-функции мы можем построить аналогичную оценку сверху:

---

# Утверждение 13: 

$$
V^{*}(s) \leq \max _{a} Q^{*}(s, a)
$$

Доказательство.

$$
\begin{aligned}
& V^{*}(s)=\left\{\text { определение } V^{*}(3.4)\right\}=\max _{\pi} V^{\pi}(s)= \\
& \quad=\{\text { связь } \mathrm{VQ}(3.6)\}=\max _{\pi} \mathbb{E}_{a \sim \pi(a \mid s)} Q^{\pi}(s, a) \leq \\
& \quad \leq\left\{\text { по определению } Q^{*}(3.8)\right\} \leq \max _{\pi} \mathbb{E}_{a \sim \pi(a \mid s)} Q^{*}(s, a) \leq \\
& \leq\left\{\text { свойство } \mathbb{E}_{x} f(x) \leq \max _{x} f(x)\right\} \leq \max _{a} Q^{*}(s, a)
\end{aligned}
$$

Можно ли получить $\max _{a} Q^{*}(s, a)$, то есть достигнуть этой верхней оценки? Проведём нестрогое следующее рассуждение: представим, что мы сидим в состоянии $s$ и знаем величины $Q^{*}(s, a)$, определённые как (3.8). Это значит, что если мы сейчас выберем действие $\boldsymbol{a}$, то в дальнейшем сможем при помощи какой-то стратегии $\boldsymbol{\pi}$, на которой достигается максимум ${ }^{1}$ для конкретно данной пары $\boldsymbol{s}, \boldsymbol{a}$, получить $Q^{*}(s, a)$. Следовательно, мы, выбрав сейчас то действие $a$, на которых достигается максимум, в предположении «дальнейшей оптимальности своего


поведения», надеемся получить из текущего состояния $\max _{a} Q^{*}(s, a)$.

Определение 38: Для данного приближения Q-функции стратегия $\pi(s):=\underset{a}{\operatorname{argmax}} Q(s, a)$ называется $\mathscr{\text { acaд- }}$ ной (greedy with respect to Q-function).

Определение 39: Приниип оптимальности Беллмана: жадный выбор действия в предположении оптимальности дальнейшего поведения оптимален.

Догадку несложно доказать для случая, когда MDP является деревом: принятие решения в текущем состоянии $s$ никак не связано с выбором действий в «поддеревьях». Если в поддереве, соответствующему одному действию, можно получить больше, чем в другом поддереве, то понятно, что выбирать нужно его. В общем случае, однако, нужно показать, что жадный выбор в $s$ «позволит» в будущем набрать то $Q^{*}(s, a)$, которое мы выбрали - вдруг для того, чтобы получить в будущем $Q^{*}(s, a)$, нужно будет при попадании в то же состояние $s$ выбирать действие как-то по-другому? Если бы это было так, было бы оптимально искать стратегию в классе нестационарных стратегий.

### 3.1.7. Отказ от однородности

Утверждение, позволяющее, во-первых, получить вид оптимальной стратегии, а как следствие связать оптимальные оценочные функции, будет доказано двумя способами. В этой секции докажем через отказ от однородности («классическим» способом), а затем в секции 3.2 про Policy Improvement мы поймём, что все желаемые утверждения можно получить и через него.

Отказ от однородности заключается в том, что мы в доказательстве будем искать максимум $\max _{\pi} V^{\pi}(s)$ не только среди стационарных, но и нестационарных стратегий. Заодно мы убедимся, что достаточно искать стратегию в классе стационарных стратегий. Ранее стационарность означала, что вне зависимости от момента времени наша стратегия зависит только от текущего состояния. Теперь же, для каждого момента времени $\boldsymbol{t}=\mathbf{0}, \mathbf{1} \ldots$ мы запасёмся своей собственной стратегией $\boldsymbol{\pi}_{\boldsymbol{t}}(\boldsymbol{a} \mid \boldsymbol{s})$. Естественно, что теорема 9 о независимости оценочной функции от времени тут перестаёт быть истинной, и, вообще говоря, оценочные функции теперь зависимы от текущего момента времени $t$.

Определение 40: Для данного MDP и нестационарной стратегии $\boldsymbol{\pi}=\left\{\boldsymbol{\pi}_{t}(a \mid s) \mid t \geq 0\right\}$ обозначим её оценочные функиии как

$$
\begin{gathered}
V_{t}^{\pi}(s):=\mathbb{E}_{\pi_{t}\left(a_{t} \mid s_{t}=s\right)} \mathbb{E}_{p\left(s_{t+1} \mid s_{t}=s, a_{t}\right)} \mathbb{E}_{\pi_{t+1}\left(a_{t+1} \mid s_{t+1}\right)} \cdots R_{t} \\
Q_{t}^{\pi}(s, a):=\mathbb{E}_{p\left(s_{t+1} \mid s_{t}=s, a_{t}=a\right)} \mathbb{E}_{\pi_{t+1}\left(a_{t+1} \mid s_{t+1}\right)} \cdots R_{t}
\end{gathered}
$$

Пример 50: Действительно, мы можем в состоянии $s$ смотреть на часы, если $t=0$ - кушать тортики, а если $t=7$ - бросаться в лаву, т.е. $V_{t=0}^{\pi}(s) \neq V_{t=7}^{\pi}(s)$ для неоднородных $\boldsymbol{\pi}$.

[^0]
[^0]:    ${ }^{1}$ мы знаем, что Q-функция ограничена, и поэтому точно существует супремум. Для полной корректности рассуждений надо говорить об $\varepsilon$-оптимальности, но для простоты мы это опустим.

---

Утверждение 14: Для нестационарных оценочных функций остаются справедливыми уравнения Беллмана:

$$
\begin{gathered}
V_{t}^{\pi}(s)=\mathbb{E}_{\pi_{t}(a \mid s)} Q_{t}^{\pi}(s, a) \\
Q_{t}^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{t+1}^{\pi}\left(s^{\prime}\right)
\end{gathered}
$$

Доказательство. Всё ещё следует из определений.
Определение 41: Для данного MDP оптимальными оценочными функииями среди нестационарных стратегий назовём

$$
\begin{aligned}
V_{t}^{*}(s) & :=\max _{\substack{\pi_{t+1} \\
\boldsymbol{\pi}_{t+1}}} V_{t}^{\pi}(s) \\
Q_{t}^{*}(s, a) & :=\max _{\substack{\pi_{t+1} \\
\boldsymbol{\pi}_{t+2}}} Q_{t}^{\pi}(s, a)
\end{aligned}
$$

Заметим, что в определении Q-функции максимум берётся по стратегиям, начиная с $\pi_{t+1}$, поскольку по определению Q-функция не зависит от $\pi_{t}$ (действие в момент времени $t$ уже «дано» в качестве входа).

Утверждение 15: В стационарных MDP (а мы рассматриваем только их) оптимальные оценочные функции не зависят от времени, т.е. $\forall s, a, t_{1}, t_{2}$ верно:

$$
V_{t_{1}}^{*}(s)=V_{t_{2}}^{*}(s) \quad Q_{t_{1}}^{*}(s, a)=Q_{t_{2}}^{*}(s, a)
$$

Доказательство. Вообще говоря, по построению, так как зависимость от времени заложена исключительно в стратегиях, по которым мы берём максимум (а его мы берём по одним и тем же симплексам вне зависимости от времени):

$$
V_{t}^{*}(s)=\max _{\substack{\pi_{t} \\ \pi_{t+1}}} \mathbb{E}_{a_{t}, s_{t+1} \cdots \mid s_{t}=s} R_{t}=\max _{\substack{\pi_{t} \\ \pi_{1}}} \mathbb{E}_{a_{0}, s_{1} \cdots \mid s_{0}=s} R_{0}
$$

Последнее наблюдение само по себе нам ничего не даёт. Вдруг нам в условном MDP с одним состоянием выгодно по очереди выбирать каждое из трёх действий?

# 3.1.8. Вид оптимальной стратегии (доказательство через отказ от однородности) 

Мотивация в отказе от однородности заключается в том, что наше MDP теперь стало деревом: эквивалентно было бы сказать, что мы добавили в описание состояний время $t$. Теперь мы не оказываемся в одном состоянии несколько раз за эпизод; максимизация $Q_{t}^{*}(s, a)$ требует оптимальных выборов «в поддереве», то есть настройки $\pi_{t+1}, \pi_{t+2}$ и так далее, а для $\pi_{t}(a \mid s)$ будет выгодно выбрать действие жадно. Покажем это формально.

Теорема 12: Стратегия $\pi_{t}(s):=\underset{a}{\operatorname{argmax}} Q_{t}^{*}(s, a)$ оптимальна, то есть для всех состояний $s$ верно $V_{t}^{\pi}(s)=$ $=V_{t}^{*}(s)$, и при этом справедливо:

$$
V_{t}^{*}(s)=\max _{a} Q_{t}^{*}(s, a)
$$

Доказательство. В силу VQ уравнения (3.9), максимизация $V_{t}^{\pi}(s)$ эквивалентна максимизации

$$
V_{t}^{*}(s)=\max _{\substack{\pi_{t+1} \\ \pi_{t+1}}} V_{t}^{\pi}(s)=\max _{\substack{\pi_{t} \\ \pi_{t+1}}} \mathbb{E}_{\pi_{t}(a \mid s)} Q_{t}^{\pi}(s, a)
$$

Мы уже замечали, что $Q_{t}^{\pi}$ по определению зависит только от $\pi_{t+1}(a \mid s), \pi_{t+2}(a \mid s) \ldots$. Максимум $Q_{t}^{\pi}(s, a)$ по ним по определению (3.12) есть $Q_{t}^{*}(s, a)$. Значит,

$$
V_{t}^{*}(s) \leq \max _{\pi_{t}} \mathbb{E}_{\pi_{t}(a \mid s)} \max _{\substack{\pi_{t+1} \\ \pi_{t+2}}} Q_{t}^{\pi}(s, a)=\max _{\pi_{t}} \mathbb{E}_{\pi_{t}(a \mid s)} Q_{t}^{*}(s, a)
$$

Покажем, что эта верхняя оценка достигается. Сначала найдём $\pi_{t}$ такую, что:

$$
\left\{\begin{array}{l}
\mathbb{E}_{\pi_{t}(a \mid s)} Q_{t}^{*}(s, a) \rightarrow \max _{\pi_{t}} \\
\int \pi_{t}(a \mid s) \mathrm{d} a=1 ; \quad \forall a \in \mathcal{A}: \pi_{t}(a \mid s) \geq 0
\end{array}\right.
$$

---

Решением такой задачи, в частности*, будет детерминированная стратегия

$$
\pi_{t}^{*}(s):=\underset{a}{\operatorname{argmax}} Q_{t}^{*}(s, a)
$$

а сам максимум, соответственно, будет равняться $\max _{a} Q_{t}^{*}(s, a)$. Соответственно, $\max _{\pi_{t+2}^{*}} V_{t}^{\pi}(s)$ достигает верхней оценки при этой $\pi_{t}^{*}$ и том наборе $\pi_{t+1}^{*}, \pi_{t+2}^{*} \ldots$, на котором достигается значение $Q_{t}^{*}\left(s, \pi_{t}^{*}(s)\right)$.

[^0]Утверждение 16: Для нестационарных оценочных функций верно:

$$
Q_{t}^{*}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{t+1}^{*}\left(s^{\prime}\right)
$$

Доказательство. Получим аналогично оценку сверху на $Q_{t}^{*}(s, a)$ :

$$
\begin{gathered}
Q_{t}^{*}(s, a)=\max _{\pi_{t+2}^{*}} Q_{t}^{\pi}(s, a)=\{\text { связь } \mathrm{QV}(3.10)\}=\max _{\pi_{t+1}} \frac{\pi_{t}^{*}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{t+1}^{\pi}\left(s^{\prime}\right)\right] \leq}{\pi_{t+2}} \\
\leq\{\text { определение } V^{*} \text { (3.11) }\} \leq r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{t+1}^{*}\left(s^{\prime}\right)
\end{gathered}
$$

Эта верхняя оценка достигается на стратегии $\pi_{t}(s)=\underset{a}{\operatorname{argmax}} Q_{t}^{*}(s, a)$, на которой, как мы доказали в предыдущей теореме 12 , достигается максимум $V_{t}^{\pi}(s)=V^{*}(s)$ сразу для всех $s$ одновременно.

Таким образом мы показали, что в нестационарном случае наши $\boldsymbol{Q}^{*}$ и $\boldsymbol{V}^{*}$ являются оценочными функциями оптимальных стратегий, максимизирующих награду из всех состояний. Осталось вернуться к стационарному случаю, то есть показать, что для стационарных стратегий выполняется то же утверждение.

Утверждение 17: Оптимальные оценочные функции для стационарных и нестационарных случаев совпадают, то есть, например, для V-функции:

$$
\max _{\pi} V^{\pi}(s)=\max _{\pi_{t+2}^{*}} V_{t}^{\pi}(s)
$$

где в левой части максимум берётся по стационарным стратегиям, а в правой - по нестационарным.
Доказательство. По теореме 12 максимум справа достигается на детерминированной $\pi_{t}(s)=$ $=\underset{a}{\operatorname{argmax}} Q_{t}^{*}(a, s)$. В силу утверждения 15 , для всех моментов времени $Q_{t}^{*}$ совпадают, следовательно такая $\pi_{t}$ тоже совпадает для всех моментов времени и является стационарной стратегией.

Интуитивно: мы показали, что об MDP «с циклами в графе» можно думать как о дереве. Итак, в полученных результатах можно смело заменять все нестационарные оптимальные оценочные функции на стационарные.

# 3.1.9. Уравнения оптимальности Беллмана 

## Теорема 13 - Связь оптимальных оценочных функций:

$$
\begin{gathered}
V^{*}(s)=\max _{a} Q^{*}(s, a) \\
Q^{*}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{*}\left(s^{\prime}\right)
\end{gathered}
$$

Теперь $\boldsymbol{V}^{*}$ выражено через $\boldsymbol{Q}^{*}$ и наоборот. Значит, можно получить выражение для $\boldsymbol{V}^{*}$ через $\boldsymbol{V}^{*}$ и $\boldsymbol{Q}^{*}$ через $Q^{*}$ :

Теорема 14 - Уравнения оптимальности Беллмана (Bellman optimality equation):

$$
\begin{aligned}
Q^{*}(s, a) & =r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \\
V^{*}(s) & =\max _{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{*}\left(s^{\prime}\right)\right]
\end{aligned}
$$

Доказательство. Подставили (3.15) в (3.16) и наоборот.


[^0]:    * здесь записана просто задача линейного программирования на симплексе ( $\pi_{t}$ обязано быть распределением); общим решением задачи, соответственно, будет любое распределение, которое размазывает вероятности между элементами множества $\underset{a}{\operatorname{argmax}} Q_{t}^{*}(s, a)$

---

Хотя для строгого доказательства нам и пришлось поднапрячься и выписать относительно громоздкое рассуждение, уравнения оптимальности Беллмана крайне интуитивны. Для $\boldsymbol{Q}^{*}$, например, можно рассудить так: что даст оптимальное поведение из состояния $\boldsymbol{s}$ после совершения действия $\boldsymbol{a}$ ? Что с нами случится дальше: мы получим награду за этот выбор $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$, на что уже повлиять не можем. Остальная награда будет дисконтирована. Затем среда переведёт нас в какое-то следующее состояние $\boldsymbol{s}^{\prime}$ - нужно проматожидать по функции переходов. После этого мы, пользуяся принципом Беллмана, просто выберем то действие, которое позволит в будущем набрать наибольшую награду, и тогда сможем получить $\max _{\boldsymbol{a}^{\prime}} \boldsymbol{Q}^{*}\left(s^{\prime}, \boldsymbol{a}^{\prime}\right)$.

Пример 51: Сформулируем для MDP с рисунка уравнения оптимальности Беллмана для $\boldsymbol{V}^{*}$. Мы получим систему из трёх уравнений с трёмя неизвестными.

$$
\begin{aligned}
& V^{*}(s=A)=\max (\underbrace{0.25 \gamma V^{*}(s=B)}_{\mathbf{A}}, \underbrace{\gamma V^{*}(s=C)}_{\mathbf{B}}) \\
& V^{*}(s=B)=\max (\underbrace{2+\gamma V^{*}(s=B)}_{\mathbf{C}}, \underbrace{4}_{\mathbf{A}}) \\
& V^{*}(s=C)=\max \underbrace{(0.8 \gamma V^{*}(s=B)+0.2 \gamma V^{*}(s=C)}_{\mathbf{D}}, \underbrace{-1}_{\mathbf{C}})
\end{aligned}
$$



Заметим, что в полученных уравнениях не присутствует мат.ожиданий по самим оптимальным стратегиям - предположение дальнейшей оптимальности поведения по сути «заменяет» их на взятие максимума по действиям. Более того, мы позже покажем, что оптимальные оценочные функции - единственные решения систем уравнений Беллмана. А значит, вместо поиска оптимальной стратегии можно искать оптимальные оценочные функции! Таким образом, мы свели задачу оптимизации нашего функционала к решению системы нелинейных уравнений особого вида. Беллман назвал данный подход «динамическое программирование» (dynamic programming).

# 3.1.10. Критерий оптимальности Беллмана 

Давайте сформулируем критерий оптимальности стратегий в общей форме, описывающей вид всего множества оптимальных стратегий. Для доказательства нам понадобится факт, который мы технически докажем в рамках повествования чуть позже: для данного MDP $\boldsymbol{Q}^{*}$ - единственная функция $\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, удовлетворяющая уравнениям оптимальности Беллмана.

Теорема 15 - Критерий оптимальности Беллмана: $\boldsymbol{\pi}$ оптимальна тогда и только тогда, когда $\forall s, a: \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})>0$ верно:

$$
a \in \underset{a}{\operatorname{Argmax}} Q^{\pi}(s, a)
$$

Необходимость. Пусть $\boldsymbol{\pi}$ - оптимальна. Тогда её оценочные функции совпадают с $\boldsymbol{V}^{*}, \boldsymbol{Q}^{*}$, для которых выполнено уравнение (3.15):

$$
V^{\pi}(s)=V^{*}(s)=\max _{a} Q^{*}(s, a)=\max _{a} Q^{\pi}(s, a)
$$

С другой стороны из связи VQ (3.6) верно $V^{\pi}(s)=\mathbb{E}_{\pi(a \mid s)} Q^{\pi}(s, a)$; получаем

$$
\mathbb{E}_{\pi(a \mid s)} Q^{\pi}(s, a)=\max _{a} Q^{\pi}(s, a)
$$

из чего вытекает доказываемое.
Достаточность. Пусть условие выполнено. Тогда для любой пары $s, \boldsymbol{a}$ :

$$
Q^{\pi}(s, a)=\{\text { связь } \mathrm{QQ}(3.7)\}=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \mathbb{E}_{\pi\left(a^{\prime} \mid s^{\prime}\right)} Q^{\pi}\left(s^{\prime}, a^{\prime}\right)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{a^{\prime}} Q^{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

Из единственности решения этого уравнения следует $Q^{\pi}(s, a)=Q^{*}(s, a)$, и, следовательно, $\pi$ оптимальна.

Иначе говоря: теорема говорит, что оптимальны ровно те стратегии, которые пользуются принципом оптимальности Беллмана. Если в одном состоянии два действия позволят в будущем набрать максимальную награду, то между ними можно любым способом размазать вероятности выбора. Давайте при помощи этого критерия

---

окончательно ответим на вопросы о том, существует ли оптимальная стратегия и сколько их вообще может быть.

Утверждение 18: Если $|\mathcal{A}|<+\infty$, всегда существует оптимальная стратегия.
Доказательство. $\operatorname{Argmax} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ для конечных множеств $\mathcal{A}$ всегда непуст, следовательно существует детерминированная оптимальная стратегия $\boldsymbol{\pi}(\boldsymbol{s}):=\underset{a}{\operatorname{argmax}} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$.

Утверждение 19: Оптимальной стратегии может не существовать.
Контрпример. Одно состояние, $\mathcal{A}=[-1,1]$, после первого выбора эпизод заканчивается; в качестве награды $\boldsymbol{r}(\boldsymbol{a})$ можем рассмотреть любую не достигающую своего максимума функцию. Просто придумали ситуацию, когда $\operatorname{Argmax}_{\boldsymbol{a}} \boldsymbol{Q}^{*}(\boldsymbol{a})$ пуст.

Утверждение 20: Если существует хотя бы две различные оптимальные стратегии, то существует континуум оптимальных стратегий.

Доказательство. Существование двух различных оптимальных стратегий означает, что в каком-то состоянии в множество $\operatorname{Argmax} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ содержит по крайней мере два элемента. Между ними можно размазать вероятности выбора любым способом и в любом случае получить максимальную награду.

Утверждение 21: Если существует хотя бы одна оптимальная стратегия, то существует детерминированная оптимальная стратегия.

Доказательство. Пусть $\boldsymbol{\pi}^{*}$ - оптимальна. Значит, $\operatorname{Argmax}_{\boldsymbol{a}} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ не пуст для всех $\boldsymbol{s}$, и существует детерминированная оптимальная стратегия $\boldsymbol{\pi}(\boldsymbol{s}):=\underset{a}{\operatorname{argmax}} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$.

Пример 52: Найдём все оптимальные стратегии в MDP из примера 51 для $\gamma=\mathbf{0 . 5}$.
Мы могли бы составить уравнения оптимальности Беллмана для $\boldsymbol{Q}^{*}$ и решать их, но сделаем чуть умнее и воспользуемся критерием оптимальности Беллмана (теорема 15). Например, в состоянии В оптимально или выбирать какое-то одно из двух действий с вероятностью 1 , или действия эквивалентны, и тогда оптимально любое поведение. Допустим, мы будем выбирать всегда $\square$, тогда мы получим $\frac{2}{1-\gamma}=4$; если же будем выбирать $\square$, то получим +4 . Значит, действия эквивалентны, оптимально любое поведение, и $\boldsymbol{V}^{*}(\boldsymbol{s}=\boldsymbol{B})=$ $=4$.

Проведём аналогичное рассуждение для состояния С. Если опти-


мально действие $\square$, то

$$
\boldsymbol{Q}^{*}(\boldsymbol{s}=\boldsymbol{C}, \square)=0.2 \gamma \boldsymbol{Q}^{*}(\boldsymbol{s}=\boldsymbol{C}, \square)+0.8 \gamma \boldsymbol{V}^{*}(\boldsymbol{s}=\boldsymbol{B})
$$

Решая это уравнение относительно неизвестного $\boldsymbol{Q}^{*}(\boldsymbol{s}=\boldsymbol{C}, \square)$, получаем $\frac{16}{9}>\boldsymbol{Q}^{*}(\boldsymbol{s}=\boldsymbol{C}, \square)=-1$. Значит, в С оптимальная стратегия обязана выбирать $\square$, и $\boldsymbol{V}^{*}(\boldsymbol{C})=\frac{16}{9}$.

Для состояния А достаточно сравнить $\boldsymbol{Q}^{*}(\boldsymbol{s}=\boldsymbol{A}, \square)=0.25 \gamma \boldsymbol{V}^{*}(\boldsymbol{s}=\boldsymbol{B})=\frac{1}{4}$ и $\boldsymbol{Q}^{*}(\boldsymbol{s}=\boldsymbol{A}, \square)=$ $=\gamma \boldsymbol{V}^{*}(\boldsymbol{s}=\boldsymbol{C})=\frac{8}{9}$, определив, что оптимальная стратегия должна выбирать $\square$

# §3.2. Улучшение политики 

### 3.2.1. Advantage-функция

Допустим, мы находились в некотором состоянии $\boldsymbol{s}$, и засэмплировали $\boldsymbol{a} \sim \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$ такое, что $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})>$ $>\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$. Что можно сказать о таком действии? Мы знаем, что вообще в среднем политика $\boldsymbol{\pi}$ набирает из данного состояния $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, но какой-то выбор действий даст в итоге награду больше $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, а какой-то меньше. Если $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})>\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, то после того, как мы выбрали действие $\boldsymbol{a}$, «приняли решение», наша средняя будущая награда вдруг увеличилась.

Мы ранее обсуждали в разделе 1.2.7 такую особую проблему обучения с подкреплением, как credit assignment, которая звучит примерно так: допустим, мы засэмплировали траекторию $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}, \ldots$ до конца эпизода, и в конце в финальном состоянии через $\boldsymbol{T}$ шагов получили сигнал (награду) +1 . Мы приняли $\boldsymbol{T}$ решений, но какое

---

из всех этих действий повлекло получение этого +1 ? «За что нас наградили?» Повлияло ли на получение +1 именно то действие $\boldsymbol{a}$, которое мы засэмплировали в стартовом $\boldsymbol{s}$ ? Вопрос нетривиальный, потому что в RL есть отложенный сигнал: возможно, именно действие $\boldsymbol{a}$ в состоянии $\boldsymbol{s}$ запустило какую-нибудь цепочку действий, которая дальше при любом выборе $\boldsymbol{a}^{\prime}, \boldsymbol{a}^{\prime \prime}, \cdots$ приводит к награде +1 . Возможно, конечно, что первое действие и не имело никакого отношения к этой награде, и это поощрение именно за последний выбор. А ещё может быть такое, что имело место везение, и просто среда в какой-то момент перекинула нас в удачное состояние.

Но мы понимаем, что если какое-то действие «затригтерило» получение награды через сто шагов, в промежуточных состояниях будет информация о том, сколько времени осталось до получения этой отложенной награды. Например, если мы выстрелили во вражеский инонланетный корабль, и через 100 шагов выстрел попадает во врага, давая агенту +1 , мы будем видеть в состояниях расстояние от летящего выстрела до цели, и знать, что через такое-то время нас ждёт +1 . Другими словами, вся необходимая информация лежит в идеальных оценочных функциях $\boldsymbol{Q}^{\boldsymbol{\pi}}$ и $\boldsymbol{V}^{\boldsymbol{\pi}}$.

Так, если в некотором состоянии $\boldsymbol{s}$ засэмплировалось такое $\boldsymbol{a}$, что $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, то мы можем заключить, что выбор действия на этом шаге не привёл ни к какой «неожиданной» награде. Если же $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})>$ $>\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ - то мы приняли удачное решение, $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})<\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ - менее удачное, чем обычно. Если, например, $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})+\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}^{\prime}\right)>\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, то мы можем заключить, что имело место везение: среда засэмплировала такое $\boldsymbol{s}^{\prime}$, что теперь мы получим больше награды, чем ожидали после выбора $\boldsymbol{a}$ в состоянии $\boldsymbol{s}$. И так далее: мы сможем отследить, в какой конкретно момент случилось то событие (сэмплирование действия или ответ среды), за счёт которого получена награда.

Таким образом, идеальный «кредит» влияния действия $\boldsymbol{a}$, выбранного в состоянии $\boldsymbol{s}$, на будущую награду равен

$$
Q^{\pi}(s, a)-V^{\pi}(s)
$$

и именно эта величина на самом деле будет для нас ключевой. Поэтому из соображений удобства вводится ещё одно обозначение:

Определение 42: Для данного MDP Advantage- $\boldsymbol{\text { фуикиией }}$ политики $\boldsymbol{\pi}$ называется

$$
A^{\pi}(s, a):=Q^{\pi}(s, a)-V^{\pi}(s)
$$

Утверждение 22: Для любой политики $\boldsymbol{\pi}$ и любого состояния $\boldsymbol{s}$ :

$$
\mathbb{E}_{\pi(a \mid s)} A^{\pi}(s, a)=0
$$

Доказательство.

$$
\begin{aligned}
\mathbb{E}_{\pi(a \mid s)} A^{\pi}(s, a) & =\mathbb{E}_{\pi(a \mid s)} Q^{\pi}(s, a)-\mathbb{E}_{\pi(a \mid s)} V^{\pi}(s)= \\
\left\{V^{\pi} \text { не зависит от } a\right\} & =\mathbb{E}_{\pi(a \mid s)} Q^{\pi}(s, a)-V^{\pi}(s)= \\
\{\text { связь } V \text { через } Q(3.6)\} & =V^{\pi}(s)-V^{\pi}(s)=0
\end{aligned}
$$

Утверждение 23: Для любой политики $\boldsymbol{\pi}$ и любого состояния $\boldsymbol{s}$ :

$$
\max _{a} A^{\pi}(s, a) \geq 0
$$

Advantage - это, если угодно, «центрированная» Q-функция. Если $\boldsymbol{A}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})>\mathbf{0}$ - действие $\boldsymbol{a}$ «лучше среднего» для нашей текущей политики в состоянии $\boldsymbol{s}$, меньше нуля - хуже. И интуиция, что процесс обучения нужно строить на той простой идеи, что первые действия надо выбирать чаще, а вторые - реже, нас не обманывает.

Естественно, подвох в том, что на практике мы не будем знать точное значение оценочных функций, а значит, и истинное значение Advantage. Решая вопрос оценки значения Advantage для данной пары $\boldsymbol{s}, \boldsymbol{a}$, мы фактически будем проводить credit assignment - это одна и та же задача.

# 3.2.2. Relative Performance Identity (RPI) 

Мы сейчас докажем одну очень интересную лемму, которая не так часто нам будет нужна в будущем, но которая прям открывает глаза на мир. Для этого вспомним формулу reward shaping-a (1.7) и заметим, что мы можем выбрать в качестве потенциала V-функцию произвольной стратегии $\boldsymbol{\pi}_{2}$ :

$$
\Phi(s):=V^{\pi_{2}}(s)
$$

Действительно, требований к потенциалу два: ограниченность (для V-функций это выполняется в силу наших ограничений на рассматриваемые MDP) и равенство нулю в терминальных состояниях (для V-функций это

---

верно по определению). Подставив такой потенциал, мы получим связь между performance-ом $\boldsymbol{J}(\boldsymbol{\pi})=\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}_{\mathbf{0}}\right)$ двух разных стратегий. В общем виде лемма сравнивает V-функции двух стратегий в одном состоянии:

Теорема 16 - Relative Performance Identity: Для любых двух политик $\pi_{1}, \pi_{2}$ :

$$
V^{\pi_{2}}(s)-V^{\pi_{1}}(s)=\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} A^{\pi_{1}}\left(s_{t}, a_{t}\right)
$$

Доказательство.

$$
\begin{aligned}
V^{\pi_{2}}(s)-V^{\pi_{1}}(s) & =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} r_{t}-V^{\pi_{1}}(s)= \\
& =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s}\left[\sum_{t \geq 0} \gamma^{t} r_{t}-V^{\pi_{1}}\left(s_{0}\right)\right]= \\
\{\text { телескопирующая сумма }(1.6)\} & =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s}\left[\sum_{t \geq 0} \gamma^{t} r_{t}+\sum_{t \geq 0}\left[\gamma^{t+1} V^{\pi_{1}}\left(s_{t+1}\right)-\gamma^{t} V^{\pi_{1}}\left(s_{t}\right)\right]\right]= \\
\{\text { перегруппируем слагаемые }\} & =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t}\left(r_{t}+\gamma V^{\pi_{1}}\left(s_{t+1}\right)-V^{\pi_{1}}\left(s_{t}\right)\right)= \\
\left\{\text { фокус } \mathbb{E}_{x} \boldsymbol{f}(\boldsymbol{x})=\mathbb{E}_{x} \mathbb{E}_{x} \boldsymbol{f}(\boldsymbol{x})\right\} & =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t}\left(r_{t}+\gamma \mathbb{E}_{s_{t+1}} V^{\pi_{1}}\left(s_{t+1}\right)-V^{\pi_{1}}\left(s_{t}\right)\right)= \\
\{\text { выделяем Q-функцию (3.5) }\} & =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t}\left(Q^{\pi_{1}}\left(s_{t}, a_{t}\right)-V^{\pi_{1}}\left(s_{t}\right)\right) \\
\{\text { по определению (3.19) }\} & =\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} A^{\pi_{1}}\left(s_{t}, a_{t}\right)
\end{aligned}
$$

Мы смогли записать наш функционал как мат.ожидание по траекториям, сгенерированным одной политикой, по оценочной функции другой стратегии. Фактически, мы можем награду заменить Advantage-функцией произвольной другой стратегии, и это сдвинет оптимизируемый функционал на константу! Прикольно.

Конечно, это теоретическое утверждение, поскольку на практике узнать точно оценочную функцию какой-то другой стратегии достаточно сложно (хотя ничто не мешает в качестве потенциала использовать произвольную функцию, приближающую $V^{\pi_{1}}(s)$ ). Однако в этой «новой» награде замешаны сигналы из будущего, награды, которые будут получены через много шагов, и эта «новая» награда априори информативнее исходной $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$.

Представим, что мы оптимизировали исходный функционал

$$
\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} r\left(s_{t}, a_{t}\right) \rightarrow \max _{\pi_{2}}
$$

и сказали: слушайте, мы не знаем, как управлять марковской цепью, не очень понимаем, как выбор тех или иных действий в состоянии влияет на структуру траектории $\boldsymbol{p}\left(\mathcal{T} \mid \pi_{2}\right)$. А давайте мы притворимся, что у нас нет в задаче отложенного сигнала (что очень существенное упрощение), и будем просто во всех состояниях $\boldsymbol{s}$ оптимизировать $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ : выбирать «хорошие» действия $\boldsymbol{a}$, где функция награды высокая. То есть будем просто выбирать $\pi_{2}(s)=\operatorname{argmax} r(s, a)$. Смысла в этом будет мало.

Теперь же мы преобразовали функционал, сменив функцию награды:

$$
\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} A^{\pi_{1}}\left(s_{t}, a_{t}\right) \rightarrow \max _{\pi_{2}}
$$

Что, если мы поступим также с новой наградой? Мы, например, знаем, что Advantage - не произвольная функция, и она обязана в среднем равняться нулю (утв. 22). Значит, если мы выберем

$$
\pi_{2}(s)=\underset{a}{\operatorname{argmax}} A^{\pi_{1}}(s, a)
$$

то все встречаемые пары $(\boldsymbol{s}, \boldsymbol{a})$ в траекториях из $\boldsymbol{\pi}_{\mathbf{2}}$ будут обязательно с неотрицательными наградами за шаг $A^{\pi_{1}}(s, a) \geq 0$. Значит и вся сумма наград будет положительна для любого стартового состояния:

$$
\mathbb{E}_{\mathcal{T} \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} \frac{A^{\pi_{1}}\left(s_{t}, a_{t}\right)}{\geq 0} \geq 0
$$

---

И тогда из теоремы 16 об RPI мы можем заключить, что для любого $s$ :

$$
V^{\pi_{2}}(s)-V^{\pi_{1}}(s) \geq 0
$$

Это наблюдение - ключ к оптимизации стратегии при известной оценочной функции другой стратегии.

# 3.2.3. Policy Improvement 

Определение 43: Будем говорить, что стратегия $\pi_{2}$ «не хуже» $\pi_{1}$ (запись: $\pi_{2} \succeq \pi_{1}$ ), если $\forall s$ :

$$
V^{\pi_{2}}(s) \geq V^{\pi_{1}}(s)
$$

и лучше (запись: $\pi_{2} \succ \pi_{1}$ ), если также найдётся $s$, для которого неравенство выполнено строго:

$$
V^{\pi_{2}}(s)>V^{\pi_{1}}(s)
$$

Мы ввели частичный порядок на множестве стратегий (понятно, что можно придумать две стратегии, которые будут «не сравнимы»: когда в одном состоянии одна будет набирать больше второй, в другом состоянии вторая будет набирать больше первой).

Зададимся следующим вопросом. Пусть для стратегии $\pi_{1}$ мы знаем оценочную функцию $Q^{\pi_{1}}$; тогда мы знаем и $V^{\pi_{1}}$ из VQ уравнения (3.6) и $\boldsymbol{A}^{\pi_{1}}$ по определению (3.19). Давайте попробуем построить $\pi_{2} \succ \pi_{1}$. Для этого покажем более «классическим» способом, что стратегии $\pi_{2}$ достаточно лишь в среднем выбирать действия, дающие неотрицательный Advantage стратегии $\pi_{1}$, чтобы быть не хуже.

Теорема 17 - Policy Improvement: Пусть стратегии $\pi_{1}$ и $\pi_{2}$ таковы, что для всех состояний $s$ выполняется:

$$
\mathbb{E}_{\pi_{2}(a \mid s)} Q^{\pi_{1}}(s, a) \geq V^{\pi_{1}}(s)
$$

или, в эквивалентной форме:

$$
\mathbb{E}_{\pi_{2}(a \mid s)} A^{\pi_{1}}(s, a) \geq 0
$$

Тогда $\pi_{2} \succeq \pi_{1}$; если хотя бы для одного $s$ неравенство выполнено строго, то $\pi_{2} \succ \pi_{1}$.
Доказательство. Покажем, что $V^{\pi_{2}}(s) \geq V^{\pi_{1}}(s)$ для любого $s$ :

$$
\begin{aligned}
V^{\pi_{1}}(s) & =\{\text { связь VQ }(3.6)\}=\mathbb{E}_{\pi_{1}(a \mid s)} Q^{\pi_{1}}(s, a) \leq \\
& =\{\text { по построению } \pi_{2}\}=\mathbb{E}_{\pi_{2}(a \mid s)} Q^{\pi_{1}}(s, a)= \\
& =\{\text { связь QV }(3.5)\}=\mathbb{E}_{\pi_{2}(a \mid s)}\left[r+\gamma \mathbb{E}_{s^{\prime}} V^{\pi_{1}}\left(s^{\prime}\right)\right] \leq \\
\leq & \{\text { применяем это же неравенство рекурсивно }\}=\mathbb{E}_{\pi_{2}(a \mid s)}\left[r+\mathbb{E}_{s^{\prime}} \mathbb{E}_{\pi_{2}\left(a^{\prime} \mid s^{\prime}\right)}\left[\gamma r^{\prime}+\gamma^{2} \mathbb{E}_{s^{\prime \prime}} V^{\pi_{1}}\left(s^{\prime \prime}\right)\right]\right] \leq \\
\leq & \{\text { раскручиваем цепочку далее }\} \leq \cdots \leq \mathbb{E}_{T \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} r_{t}= \\
& =\{\text { по определению }(3.2)\}=V^{\pi_{2}}(s)
\end{aligned}
$$

Если для какого-то $s$ неравенство из условия теоремы было выполнено строго, то для него первое неравенство в этой цепочке рассуждений выполняется строго, и, значит, $V^{\pi_{2}}(s)>V^{\pi_{1}}(s)$.

Что означает эта теорема? Знание оценочной функции позволяет улучшить стратегию. Улучшать стратегию можно прямо в отдельных состояниях, например, выбрав некоторое состояние $s$ и сказав: неважно, как это повлияет на частоты посещения состояний, но будем конкретно в этом состоянии $s$ выбирать действия так, что значение

$$
\mathbb{E}_{\pi_{2}(a \mid s)} Q^{\pi_{1}}(s, a)
$$

как можно больше. Тогда, если в $s$ действие выбирается «новой» стратегией $\pi_{2}$, а в будущем агент будет вести себя не хуже, чем $\pi_{1}$, то и наберёт он в будущем не меньше $Q^{\pi_{1}}(s, a)$. Доказательство теоремы 17 показывает, что выражение (3.21) является нижней оценкой на награду, которую соберёт «новый» агент со стратегией $\pi_{2}$.

Если эта нижняя оценка поднята выше $V^{\pi_{1}}(s)$, то стратегию удалось улучшить: и тогда какой бы ни была $\pi_{1}$, мы точно имеем гарантии $\pi_{2} \succeq \pi_{1}$. Важно, что такой policy improvement работает всегда: и для «тупых» стратегий, близких к случайному поведению, и для уже умеющих что-то разумное делать.

В частности, мы можем попробовать нижнюю оценку (3.21) максимально поднять, то есть провести жсадный (greedy) policy improvement. Для этого мы формально решаем такую задачу оптимизации:

$$
\mathbb{E}_{\pi_{2}(a \mid s)} Q^{\pi_{1}}(s, a) \rightarrow \max _{\pi_{2}}
$$

и понятно, что решение находится в детерминированной $\pi_{2}$ :

$$
\pi_{2}(s)=\underset{a}{\operatorname{argmax}} Q^{\pi_{1}}(s, a)=\underset{a}{\operatorname{argmax}} A^{\pi_{1}}(s, a)
$$

---

Конечно, мы так не получим «за один ход» сразу оптимальную стратегию, поскольку выбор $\pi_{2}(\boldsymbol{a} \mid \boldsymbol{s})$ сколь угодно хитро может изменить распределение траекторий, но тем не менее.

Пример 53: Попробуем улучшить стратегию $\boldsymbol{\pi}$ из примера 46, $\boldsymbol{\gamma}=\mathbf{0 . 8}$. Например, в состоянии C она выбирает $\square$ с вероятностью 1 и получает -1 ; попробуем посчитать $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{C}, \square)$ :

$$
Q^{\pi}(s=C, \square)=0.2 \gamma V^{\pi}(C)+0.8 \gamma V^{\pi}(B)
$$

Подставляя ранее подсчитанные $V^{\pi}(C)=-1, V^{\pi}(B)=$ $=\mathbf{5}$, видим, что действие $\square$ принесло бы нашей стратегии $\pi$ куда больше -1 , а именно $\boldsymbol{Q}^{\pi}(\boldsymbol{s}=\boldsymbol{C}, \square)=\mathbf{3 . 0 4}$. Давайте построим $\pi_{2}$, скопировав $\pi$ в А и В, а в С будем с вероятностью 1 выбирать $\square$

Что говорит нам теория? Важно, что она не даёт нам значение $\boldsymbol{V}^{\pi_{2}}(\boldsymbol{C})$; в частности, нельзя утверждать, что $\boldsymbol{Q}^{\pi_{2}}(\boldsymbol{s}=\boldsymbol{C}, \square)=3.04$, и повторение вычислений подтвердит, что это не так. Однако у нас есть гарантии, что, во-первых, $\boldsymbol{Q}^{\pi_{2}}(\boldsymbol{s}=\boldsymbol{C}, \square) \geq \mathbf{3 . 0 4}$, и, что важнее, из состояния С мы начали набирать больше награды: $\boldsymbol{V}^{\pi_{2}}(\boldsymbol{C})>\boldsymbol{V}^{\pi_{1}}(\boldsymbol{C})$ строго. Во-вторых, есть гарантии, что мы не «сломали» стратегию в других состояниях: во всех остальных состояниях гарантированно $\boldsymbol{V}^{\pi_{2}}(\boldsymbol{s}) \geq \boldsymbol{V}^{\pi_{1}}(\boldsymbol{s})$. Для Q-функции, как можно показать, выполняются аналогичные неравенства.


# 3.2.4. Вид оптимальной стратегии (доказательство через РI) 

Что, если для некоторой $\pi_{1}$ мы «не можем» провести Policy Improvement? Под этим будем понимать, что мы не можем выбрать $\pi_{2}$ так, что $\mathbb{E}_{\pi_{2}(a \mid s)} Q^{\pi_{1}}(s, a)>\boldsymbol{V}^{\pi_{1}}(\boldsymbol{s})$ строго хотя бы для одного состояния $\boldsymbol{s}$ (ну, равенства в любом состоянии $\boldsymbol{s}$ мы добыёмся всегда, скопировав $\pi_{1}(\cdot \mid \boldsymbol{s})$ ). Такое может случиться, если и только если $\pi_{1}$ удовлетворяет следующему свойству:

$$
\max _{a} Q^{\pi_{1}}(s, a)=V^{\pi_{1}}(s) \quad \Leftrightarrow \quad \max _{a} A^{\pi_{1}}(s, a)=0
$$

Но это в точности критерий оптимальности Беллмана, теорема 15! Причём мы можем, воспользовавшись теоремами RPI 16 и о Policy Improvement 17, теперь доказать этот критерий альтернативным способом, не прибегая к формализму оптимальных оценочных функций ${ }^{2}$ и не требуя рассуждения про отказ от стационарности и обоснования единственности решения уравнений оптимальности Беллмана.

Теорема 18 - Критерий оптимальности (альт. доказательство): $\boldsymbol{\pi}$ оптимальна тогда и только тогда, когда $\forall s: \max _{a} A^{\pi}(s, a)=0$.

Достаточность. Допустим, это не так, и существует $\pi_{2}, s: V^{\pi_{2}}(s)>V^{\pi}(s)$. Тогда по RPI (3.20)

$$
\mathbb{E}_{\boldsymbol{T} \sim \pi_{2} \mid s_{0}=\boldsymbol{s}} \sum_{t \geq 0} \gamma^{t} \boldsymbol{A}^{\pi}\left(s_{t}, a_{t}\right)>0
$$

однако все слагаемые в сумме неположительны. Противоречие.
Необходимость. Допустим, что $\boldsymbol{\pi}$ оптимальна, но для некоторого $\hat{\boldsymbol{s}}$ условие не выполнено, и $\max _{a} \boldsymbol{A}^{\boldsymbol{\pi}}(\hat{s}, \boldsymbol{a})>\mathbf{0}$ (меньше нуля он, ещё раз, быть не может в силу утв. 23). Рассмотрим детерминированную $\boldsymbol{\pi}_{2}$, которая в состоянии $\hat{\boldsymbol{s}}$ выбирает какое-нибудь. $\hat{\boldsymbol{a}}$, такое что $\boldsymbol{A}^{\boldsymbol{\pi}}(\hat{s}, \hat{\boldsymbol{a}})>\mathbf{0}$ (это можно сделать по условию утверждения - сам максимум может вдруг оказаться недостижим для сложных пространств действий, но какое-то действие с положительным advantage-ем мы найдём), а в остальных состояниях выбирает какое-нибудь действие, т.ч. advantage-функция неотрицательна. Тогда

$$
V^{\pi_{2}}(\hat{s})-V^{\pi}(\hat{s})=\mathbb{E}_{\boldsymbol{T} \sim \pi_{2} \mid s_{0}=\hat{s}} \sum_{t \geq 0} \gamma^{t} \boldsymbol{A}^{\pi}\left(s_{t}, a_{t}\right)>0
$$

поскольку все слагаемые неотрицательны, и во всех траекториях с вероятностью* 1 верно $s_{0}=\hat{s}, a_{0}=\hat{a}$, то есть первое слагаемое равно $\boldsymbol{A}(\hat{s}, \hat{a})>\mathbf{0}$.

[^0]
[^0]:    ${ }^{2}$ в доказательствах RPI и Policy Improvement мы не использовали понятия $\boldsymbol{Q}^{*}$ и $\boldsymbol{V}^{*}$ и их свойства; тем не менее, из этих теорем все свойства оптимальных оценочных функций следуют: например, пусть $\boldsymbol{A}^{*}, \boldsymbol{Q}^{*}, \boldsymbol{V}^{*}$ - оценочные функции оптимальных стратегий, тогда в силу выводимого из RPI критерия оптимальности (теорема 18) $\forall s: \max _{a} \boldsymbol{A}^{*}(s, a)=0$, или, что тоже самое, $\max _{a}\left[Q^{*}(s, a)-V^{*}(s)\right]=0$; отсюда $\boldsymbol{V}^{*}(s)=\max _{a} \boldsymbol{Q}^{*}(s, a)$. Аналогично достаточно просто можно получить все остальные утверждения об оптимальных оценочных функциях, не прибегая к рассуждению с отказом от стационарности.

---

*мы специально стартовали из $\boldsymbol{\delta}$, чтобы пары $\boldsymbol{\delta}, \boldsymbol{\delta}$ «встретились» в траекториях, иначе могло бы быть такое, что агент в это состояние $\delta$ «никогда не попадает», и отделиться от нуля не получилось бы.

Итак, мораль полученных результатов такая: зная $\boldsymbol{Q}^{\boldsymbol{\pi}}$, мы можем придумать стратегию лучше. Не можем - значит, наша текущая стратегия $\boldsymbol{\pi}$ уже оптимальная.

# §3.3. Динамическое программирование 

### 3.3.1. Метод простой итерации

Мы увидели, что знание оценочных функций открывает путь к улучшению стратегии. Напрямую по определению считать их затруднительно; попробуем научиться решать уравнения Беллмана. И хотя уравнения оптимальности Беллмана нелинейные, они, тем не менее, имеют весьма определённый вид и, как мы сейчас увидим, обладают очень приятными свойствами. Нам понадобится несколько понятий внезапно из функана о том, как решать системы


нелинейных уравнений вида $\boldsymbol{x}=\boldsymbol{f}(\boldsymbol{x})$.

Определение 44: Оператор $\boldsymbol{f}: \boldsymbol{X} \rightarrow \boldsymbol{X}$ называется сжимающим (contraction) с коэффициентом сжатия $\gamma<1$ по некоторой метрике $\rho$, если $\forall x_{1}, x_{2}$ :

$$
\rho\left(f\left(x_{1}\right), f\left(x_{2}\right)\right)<\gamma \rho\left(x_{1}, x_{2}\right)
$$

Определение 45: Точка $\boldsymbol{x} \in \boldsymbol{X}$ для оператора $\boldsymbol{f}: \boldsymbol{X} \rightarrow \boldsymbol{X}$ называется неподвижной (fixed point), если

$$
x=f(x)
$$

Определение 46: Построение последовательности $\boldsymbol{x}_{\boldsymbol{k}+1}=\boldsymbol{f}\left(\boldsymbol{x}_{\boldsymbol{k}}\right)$ для начального приближения $\boldsymbol{x}_{0} \in \boldsymbol{X}$ называется методом простой итерации (point iteration) решения уравнения $\boldsymbol{x}=\boldsymbol{f}(\boldsymbol{x})$.

Теорема 19 - Теорема Банаха о неподвижной точке: В полном* метрическом пространстве $\boldsymbol{X}$ у сжимающего оператора $\boldsymbol{f}: \boldsymbol{X} \rightarrow \boldsymbol{X}$ существует и обязательно единственна неподвижная точка $\boldsymbol{x}^{*}$, причём метод простой итерации сходится к ней из любого начального приближения.

Сходимость метода простой итерации. Пусть $\boldsymbol{x}_{0}$ - произвольное, $\boldsymbol{x}_{\boldsymbol{k}+1}=\boldsymbol{f}\left(\boldsymbol{x}_{\boldsymbol{k}}\right)$. Тогда для любого $k>0$ :

$$
\begin{aligned}
\rho\left(x_{k}, x_{k+1}\right)=\{ & \left.\text { определение } x_{k}\right\}=\rho\left(f\left(x_{k-1}\right), f\left(x_{k}\right)\right) \leq \\
& \leq\{\text { свойство сжатия }\} \leq \gamma \rho\left(x_{k-1}, x_{k}\right) \leq \cdots \leq \\
& \leq\{\text { аналогичным образом }\} \leq \cdots \leq \gamma^{k} \rho\left(x_{0}, x_{1}\right)
\end{aligned}
$$

Теперь посмотрим, что произойдёт после применения оператора $f n$ раз:

$$
\begin{aligned}
\rho\left(x_{k}, x_{k+n}\right) & \leq \\
\{\text { неравенство треугольника }\} & \leq \rho\left(x_{k}, x_{k+1}\right)+\rho\left(x_{k+1}, x_{k+2}\right)+\cdots+\rho\left(x_{k+n-1}, x_{k+n}\right) \leq \\
& \leq\{(3.22)\} \leq\left(\gamma^{k}+\gamma^{k+1}+\ldots \gamma^{k+n-1}\right) \rho\left(x_{0}, x_{1}\right) \leq \\
\leq\{\text { геом. прогрессия }\} & \leq \frac{\gamma^{k}}{1-\gamma} \rho\left(x_{0}, x_{1}\right) \xrightarrow{k \rightarrow \infty} 0
\end{aligned}
$$

Итак, последовательность $\boldsymbol{x}_{\boldsymbol{k}}$ - фундаментальная, и мы специально попросили такое метрическое пространство («полное»), в котором обязательно найдётся предел $\boldsymbol{x}^{*}:=\lim _{k \rightarrow \infty} \boldsymbol{x}_{\boldsymbol{k}}$.

Существование неподвижной точки. Покажем, что $\boldsymbol{x}^{*}$ и есть неподвижная точка $\boldsymbol{f}$, то есть покажем, что наш метод простой итерации конструктивно её построил. Заметим, что для любого $\boldsymbol{k}>\mathbf{0}$ :

$$
\begin{aligned}
\rho\left(x^{*}, f\left(x^{*}\right)\right) \leq \\
\leq\{\text { неравенство треугольника }\} & \leq \rho\left(x^{*}, x_{k}\right)+\rho\left(x_{k}, f\left(x^{*}\right)\right)= \\
=\{\text { определение } \boldsymbol{x}_{\boldsymbol{k}}\} & =\rho\left(\boldsymbol{x}^{*}, \boldsymbol{x}_{\boldsymbol{k}}\right)+\rho\left(f\left(\boldsymbol{x}_{\boldsymbol{k}-1}\right), \boldsymbol{f}\left(\boldsymbol{x}^{*}\right)\right) \leq \\
\leq\{\text { свойство сжатия }\} & \leq \rho\left(\boldsymbol{x}^{*}, \boldsymbol{x}_{\boldsymbol{k}}\right)+\gamma \rho\left(\boldsymbol{x}_{\boldsymbol{k}-1}, \boldsymbol{x}^{*}\right)
\end{aligned}
$$

Устремим $\boldsymbol{k} \rightarrow \infty$; слева стоит константа, не зависящая от $\boldsymbol{k}$. Тогда расстояние между $\boldsymbol{x}_{\boldsymbol{k}}$ и $\boldsymbol{x}^{*}$ устремится к нулю, ровно как и между $\boldsymbol{x}_{\boldsymbol{k}-1}, \boldsymbol{x}^{*}$ поскольку $\boldsymbol{x}^{*}$ - предел $\boldsymbol{x}_{\boldsymbol{k}}$. Значит, константа равна нулю, $\rho\left(\boldsymbol{x}^{*}, \boldsymbol{f}\left(\boldsymbol{x}^{*}\right)\right)=$ 0 , следовательно, $\boldsymbol{x}^{*}=f\left(\boldsymbol{x}^{*}\right)$.

---

Единственность. Пусть $\boldsymbol{x}_{1}, \boldsymbol{x}_{2}$ - две неподвижные точки оператора $\boldsymbol{f}$. Ну тогда:

$$
\rho\left(x_{1}, x_{2}\right)=\rho\left(f\left(x_{1}\right), f\left(x_{2}\right)\right) \leq \gamma \rho\left(x_{1}, x_{2}\right)
$$

Получаем, что такое возможно только при $\rho\left(x_{1}, x_{2}\right)=0$, то есть только если $x_{1}$ и $x_{2}$ совпадают.

* любая фундаментальная последовательность имеет предел


# 3.3.2. Policy Evaluation 

Вернёмся к RL. Известно, что $\boldsymbol{V}^{\boldsymbol{\pi}}$ для данного MDP и фиксированной политики $\boldsymbol{\pi}$ удовлетворяет уравнению Беллмана (3.3). Для нас это система уравнений относительно значений $\boldsymbol{V}^{\boldsymbol{\pi}}(s) . \boldsymbol{V}^{\boldsymbol{\pi}}(s)$ - объект (точка) в функциональном пространстве $\mathcal{S} \rightarrow \mathbb{R}$.

Будем решать её методом простой итерации ${ }^{3}$. Для этого определим оператор $\mathfrak{B}$, то есть преобразование из одной функции $\mathcal{S} \rightarrow \mathbb{R}$ в другую. На вход этот оператор принимает функцию $\boldsymbol{V}: \mathcal{S} \subseteq \mathbb{R}^{n}$ и выдаёт некоторую другую функцию от состояний $\mathfrak{B} \boldsymbol{V}$. Чтобы задать выход оператора, нужно задать значение выходной функции в каждом $s \in \mathcal{S}$; это значение мы будем обозначать $[\mathfrak{B} \boldsymbol{V}](\boldsymbol{s})$ (квадратные скобки позволяют не путать применение оператора с вызовом самой функции) и определим его как правую часть решаемого уравнения (3.3). Итак:
| Определение 47: Введём оператор Беллмана (Bellman operator) для заданного MDP и стратегии $\boldsymbol{\pi}$ как

$$
[\mathfrak{B} V](\boldsymbol{s}):=\mathbb{E}_{a \sim \pi(a \mid s)}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right)\right]
$$

Также нам нужна метрика на множестве функций $\mathcal{S} \rightarrow \mathbb{R}$; возьмём

$$
d_{\infty}\left(V_{1}, V_{2}\right):=\max _{s}\left|V_{1}(s)-V_{2}(s)\right|
$$

Теорема 20: Если $\gamma<1$, оператор $\mathfrak{B}$ - сжимающий с коэффициентом сжатия $\gamma$.
Доказательство.

$$
\begin{aligned}
& d_{\infty}\left(\mathfrak{B} V_{1}, \mathfrak{B} V_{2}\right)=\max _{s}\left|\left[\mathfrak{B} V_{1}\right](s)-\left[\mathfrak{B} V_{2}\right](s)\right|= \\
& =\{\text { подставляем значение операторов, т.е. правые части решаемого уравнения }\}= \\
& =\max _{s}\left|\mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{1}\left(s^{\prime}\right)\right]-\mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{2}\left(s^{\prime}\right)\right]\right|= \\
& =\{\text { слагаемые } r(s, a) \text { сокращаются }\}= \\
& =\gamma \max _{s}\left|\mathbb{E}_{a} \mathbb{E}_{s^{\prime}}\left[V_{1}\left(s^{\prime}\right)-V_{2}\left(s^{\prime}\right)\right]\right| \leq \\
& \leq\{\text { используем свойство } \mathbb{E}_{x} f(x) \leq \max _{x} f(x)\} \leq \\
& \leq \gamma \max _{s} \max _{s^{\prime}}\left|V_{1}\left(s^{\prime}\right)-V_{2}\left(s^{\prime}\right)\right|=\gamma d_{\infty}\left(V_{1}, V_{2}\right)
\end{aligned}
$$

Итак, мы попали в теорему Банаха, и значит, метод простой итерации

$$
V_{k+1}:=\mathfrak{B} V_{k}
$$

гарантированно сойдётся к единственной неподвижной точке при любой стартовой инициализации $\boldsymbol{V}_{0}$. По построению мы знаем, что $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ такова, что $\mathfrak{B} \boldsymbol{V}^{\boldsymbol{\pi}}=\boldsymbol{V}^{\boldsymbol{\pi}}$ (это и есть уравнение Беллмана), поэтому к ней и придём.

Важно помнить, что на каждой итерации такой процедуры текущее приближение не совпадает с истинной оценочной функцией: $\boldsymbol{V}_{\boldsymbol{k}}(\boldsymbol{s}) \approx \boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, но точного равенства поставить нельзя. Распространено (но, к сожалению, не везде применяется) соглашение обозначать аппроксимации оценочных функций без верхнего индекса: просто $\boldsymbol{V}$ или $\boldsymbol{Q}$. Однако, иногда, чтобы подчеркнуть, что алгоритм учит именно $\boldsymbol{V}^{\boldsymbol{\pi}}$, верхний индекс оставляют, что может приводить к путанице.

Обсудим, что случится в ситуации, когда $\gamma=1$; напомним, что в таких ситуациях мы требовали эпизодичность сред, с гарантиями завершения всех эпизодов за $\boldsymbol{T}^{\text {max }}$ шагов. Оператор Беллмана формально сжатием являться уже не будет, и мы не подпадаем под теорему, поэтому этот случай придётся разобрать отдельно.

Теорема 21: В эпизодичных средах метод простой итерации сойдётся к единственному решению уравнений Беллмана не более чем за $\boldsymbol{T}^{\text {max }}$ шагов даже при $\gamma=1$.

[^0]
[^0]:    ${ }^{3}$ вообще говоря, это система линейных уравнений относительно значений $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, которую в случае табличных MDP можно решать любым методом решения СЛАУ. Однако, дальнейшие рассуждения через метод простой итерации обобщаются, например, на случай непрерывных пространств состояний $\mathcal{S} \subseteq \mathbb{R}^{n}$.

---

Доказательство. Мы уже доказывали теорему 2, что граф таких сред является деревом. Будем говорить, что состояние $\boldsymbol{s}$ находится на ярусе $\boldsymbol{T}$, если при старте из $\boldsymbol{s}$ у любой стратегии есть гарантии завершения за $\boldsymbol{T}$ шагов. Понятно, что для состояния $\boldsymbol{s}$ на ярусе $\boldsymbol{T}$ верно, что $\forall \boldsymbol{s}^{\prime}, \boldsymbol{a}$, для которых $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)>\mathbf{0}$, ярус $\boldsymbol{s}^{\prime}$ не превосходит $\boldsymbol{T}-\mathbf{1}$.

Осталось увидеть, что на $\boldsymbol{k}$-ой итерации метода простой итерации вычисляет точные значения $\boldsymbol{V}_{\boldsymbol{k}}(\boldsymbol{s})=$ $=\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ для всех состояний на ярусах до $\boldsymbol{k}$ : действительно, покажем по индукции. Считаем, что терминальные состояния имеют нулевой ярус; а на $\boldsymbol{k}$-ом шаге при обновлении $\boldsymbol{V}_{\boldsymbol{k}+1}(\boldsymbol{s}):=\left[\boldsymbol{\mathfrak { B }} \boldsymbol{V}_{\boldsymbol{k}}\right](\boldsymbol{s})$ для $\boldsymbol{s}$ на $\boldsymbol{k}$-ом ярусе в правой части уравнения Беллмана будет стоять мат. ожидание по $\boldsymbol{s}^{\prime}$ с ярусов до $\boldsymbol{k}-\mathbf{1}$-го, для которых значение по предположению индукции уже посчитано точно.

Соответственно, за $\boldsymbol{T}^{\text {max }}$ шагов точные значения распространятся на все состояния, и конструктивно значения определены однозначно.

Если $\gamma=1$, а среда неэпизодична (такие MDP мы не допускали к рассмотрению), метод простой итерации может не сойтись, а уравнения Беллмана могут в том числе иметь бесконечно много решений. Пример подобного безобразия. Пусть в MDP без терминальных состояний с нулевой функцией награды (где, очевидно, $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})=\mathbf{0}$ для всех $\boldsymbol{\pi}, \boldsymbol{s}$ ) мы проинициализировали $\boldsymbol{V}_{0}(\boldsymbol{s})=\mathbf{1 0 0}$ во всех состояниях $\boldsymbol{s}$. Тогда при обновлении наша аппроксимация не будет меняться: мы уже в неподвижной точке уравнений Беллмана. В частности поэтому на практике практически никогда не имеет смысл выставлять $\gamma=1$, особенно в сложных средах, где, может быть, даже и есть эпизодичность, но, тем не менее, есть «позожие состояния»: они начнут работать «как петли», когда мы перейдём к приближённым методам динамического программирования в дальнейшем.

Утверждение 24: Если некоторая функция $\tilde{\boldsymbol{V}}: \mathcal{S} \rightarrow \mathbb{R}$ удовлетворяет уравнению Беллмана (3.3), то $\tilde{\boldsymbol{V}} \equiv \boldsymbol{V}^{\boldsymbol{\pi}}$.
Мы научились решать задачу оценивания стратегии (Policy Evaluation): вычислять значения оценочной функции по данной стратегии $\boldsymbol{\pi}$ в ситуации, когда мы знаем динамику среды. На практике мы можем воспользоваться этим результатом только в «табличном» случае (tabular RL), когда пространство состояний и пространство действий конечны и достаточно малы, чтобы все пары состояние-действие было возможно хранить в памяти компьютера и перебирать за разумное время. В такой ситуации $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ - конечный векторочек, и мы умеем считать оператор Беллмана и делать обновления $\boldsymbol{V}_{\boldsymbol{k}+1}=\boldsymbol{\mathfrak { B }} \boldsymbol{V}_{\boldsymbol{k}}$.

# Алгоритм 6: Policy Evaluation 

Вход: $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})-$ стратегия
Гиперпараметры: $\varepsilon-$ критерий останова
Инициализируем $\boldsymbol{V}_{0}(\boldsymbol{s})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}$
На $\boldsymbol{k}$-ом шаге:

1. $\forall s: V_{k+1}(s):=\mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{k}\left(s^{\prime}\right)\right]$
2. критерий останова: $\max _{s}\left|V_{k}(s)-V_{k+1}(s)\right|<\varepsilon$

Выход: $V_{k}(s)$

Пример 54: Проведём оценивание стратегии, случайно выбирающей, в какую сторону ей пойти, с $\gamma=\mathbf{0 . 9}$. Угловые клетки с ненулевой наградой терминальны; агент остаётся в той же клетке, если упирается в стенку. На каждой итерации отображается значение текущего приближения $\boldsymbol{V}_{\boldsymbol{k}}(\boldsymbol{s}) \approx \boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$.


Итак, мы научились считать $\boldsymbol{V}^{\boldsymbol{\pi}}$ в предположении известной динамики среды. Полностью аналогичное рассуждение верно и для уравнений QQ (3.7); то есть, расширив набор переменных, в табличных MDP можно

---

методом простой итерации находить $\boldsymbol{Q}^{\boldsymbol{\pi}}$ и «напрямую». Пока модель динамики среды считается известной, это не принципиально: мы можем посчитать и Q-функцию через V-функцию по формуле QV (3.5).

# 3.3.3. Value Iteration 

Теорема Банаха позволяет аналогично Policy Evaluation (алг. 6) решать уравнения оптимальности Беллмана (3.17) через метод простой итерации. Действительно, проведём аналогичные рассуждения (мы сделаем это для $\boldsymbol{Q}^{*}$, но совершенно аналогично можно было бы сделать это и для $\boldsymbol{V}^{*}$ ):

Определение 48: Определим оператор оптимальности Беллмана (Bellman optimality operator, Bellman control operator) $\mathfrak{B}^{*}$ :

$$
\left[\mathfrak{B}^{*} Q\right](s, a):=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)
$$

В качестве метрики на множестве функций $\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ аналогично возьмём

$$
d_{\infty}\left(Q_{1}, Q_{2}\right):=\max _{s, a}\left|Q_{1}(s, a)-Q_{2}(s, a)\right|
$$

Нам понадобится следующий факт:
Утверждение 25:

$$
\left|\max _{x} f(x)-\max _{x} g(x)\right| \leq \max _{x}|f(x)-g(x)|
$$

Доказательство. Рассмотрим случай $\max _{x} f(x)>\max _{x} g(x)$. Пусть $x^{*}$ - точка максимума $f(x)$. Тогда:

$$
\max _{x} f(x)-\max _{x} g(x)=f\left(x^{*}\right)-\max _{x} g(x) \leq f\left(x^{*}\right)-g\left(x^{*}\right) \leq \max _{x}|f(x)-g(x)|
$$

Второй случай рассматривается симметрично.
Теорема 22: Если $\gamma<1$, оператор $\mathfrak{B}^{*}$ - сжимающий.
Доказательство.

$$
\begin{aligned}
& d_{\infty}\left(\mathfrak{B}^{*} Q_{1}, \mathfrak{B}^{*} Q_{2}\right)=\max _{s, a}\left|\left[\mathfrak{B}^{*} Q_{1}\right](s, a)-\left[\mathfrak{B}^{*} Q_{2}\right](s, a)\right|= \\
& =\{\text { подставляем значения операторов, т.е. правые части решаемой системы уравнений }\}= \\
& =\max _{s, a}\left|\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{a^{\prime}} Q_{1}\left(s^{\prime}, a^{\prime}\right)\right]-\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{a^{\prime}} Q_{2}\left(s^{\prime}, a^{\prime}\right)\right]\right|= \\
& =\{\text { слагаемые } r(s, a) \text { сокращаются }\}= \\
& =\gamma \max _{s, a}\left|\mathbb{E}_{s^{\prime}}\left[\max _{a^{\prime}} Q_{1}\left(s^{\prime}, a^{\prime}\right)-\max _{a^{\prime}} Q_{2}\left(s^{\prime}, a^{\prime}\right)\right]\right| \leq \\
& \leq\{\text { используем свойство } \mathbb{E}_{x} f(x) \leq \max _{x} f(x)\} \leq \\
& \leq \gamma \max _{s, a} \max _{s^{\prime}}\left|\max _{a^{\prime}} Q_{1}\left(s^{\prime}, a^{\prime}\right)-\max _{a^{\prime}} Q_{2}\left(s^{\prime}, a^{\prime}\right)\right|= \\
& \leq\{\text { используем свойство максимумов }(3.24)\} \leq \\
& \leq \gamma \max _{s, a} \max _{s^{\prime}} \max _{a^{\prime}}\left|Q_{1}\left(s^{\prime}, a^{\prime}\right)-Q_{2}\left(s^{\prime}, a^{\prime}\right)\right| \leq \\
& =\{\text { внутри стоит определение } \boldsymbol{d}_{\infty}\left(\boldsymbol{Q}_{1}, \boldsymbol{Q}_{2}\right) \text {, а от внешнего максимума ничего не зависит }\}= \\
& =\gamma d_{\infty}\left(Q_{1}, Q_{2}\right)
\end{aligned}
$$

Теорема 23: В эпизодичных средах метод простой итерации сойдётся к единственному решению уравнений оптимальности Беллмана не более чем за $T^{\max }$ шагов даже при $\gamma=1$.

Доказательство. Полностью аналогично доказательству теоремы 21.

Утверждение 26: Если некоторая функция $\tilde{\boldsymbol{Q}}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ удовлетворяет уравнению оптимальности Беллмана (3.17), то $\tilde{\boldsymbol{Q}} \equiv \boldsymbol{Q}^{*}$.

Утверждение 27: Метод простой итерации сходится к $\boldsymbol{Q}^{*}$ из любого начального приближения.
Вообще, если известна динамика среды, то нам достаточно решить уравнения оптимальности для $\boldsymbol{V}^{*}$ это потребует меньше переменных. Итак, в табличном случае мы можем напрямую методом простой итерации

---

решать уравнения оптимальности Беллмана и в пределе сойдёмся к оптимальной оценочной функции, которая тут же даёт нам оптимальную стратегию.

# Алгоритм 7: Value Iteration 

Вход: $\varepsilon$ - критерий останова
Инициализируем $\boldsymbol{V}_{0}(\boldsymbol{s})$ произвольно для всех $\boldsymbol{s} \in \boldsymbol{\mathcal { S }}$
На $\boldsymbol{k}$-ом шаге:

1. для всех $s: V_{k+1}(s):=\max _{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{k}\left(s^{\prime}\right)\right]$
2. критерий останова: $\max _{s}\left|V_{k+1}(s)-V_{k}(s)\right|<\varepsilon$

Выход: $\pi(s):=\underset{a}{\operatorname{argmax}}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right)\right]$

Итак, мы придумали наш первый табличный алгоритм планирования - алгоритм, решающий задачу RL в условиях известной модели среды. На каждом шаге мы обновляем («бэканим») нашу текущую аппроксимацию V-функции на её одношаговое приближение (one-step approximation): смотрим на один шаг в будущее ( $\boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$ ) и приближаем всё остальное будущее текущей же аппроксимацией. Такой «бэкап динамического программирования» (dynamic programming backup, DP-backup) - обновление «бесконечной ширины»: мы должны перебрать все возможные варианты следующего одного шага, рассмотреть все свои действия (по ним мы возьмём максимум) и перебрать


всевозможные ответы среды - $s^{\prime}$ (по ним мы должны рассчитать мат.ожидание). Поэтому этот алгоритм в чистом виде напоминает то, что обычно и понимается под словами «динамическое программирование»: мы «раскрываем дерево игры» полностью на один шаг вперёд.

Пример 55: Решим задачу из примера $54, \gamma=\mathbf{0 . 9}$; на каждой итерации отображается значение текущего приближения $\boldsymbol{V}_{\boldsymbol{k}}(\boldsymbol{s}) \approx \boldsymbol{V}^{*}(\boldsymbol{s})$. В конце концов в силу детерминированности среды станет понятно, что можно избежать попадания в терминальное -1 и кратчайшим путём добираться до терминального +1 .


### 3.3.4. Policy Iteration

Мы сейчас в некотором смысле «обобщим» Value Iteration и придумаем более общую схему алгоритма планирования для табличного случая.

Для очередной стратегии $\pi_{k}$ посчитаем её оценочную функцию $Q^{\pi_{k}}$, а затем воспользуемся теоремой Policy Improvement 17 и построим стратегию лучше; например, жадно:

$$
\pi_{k+1}(s):=\underset{a}{\operatorname{argmax}} Q^{\pi_{k}}(s, a)
$$

Тогда у нас есть второй алгоритм планирования, который, причём, перебирает детерминированные стратегии, обладающие свойством монотонного возрастания качества: каждая следующая стратегия не хуже предыдущей. Он работает сразу в классе детерминированных стратегий, и состоит из двух этапов:

- Policy Evaluation: вычисление $Q^{\pi}$ для текущей стратегии $\boldsymbol{\pi}$;
- Policy Improvement: улучшение стратегии $\boldsymbol{\pi}(\boldsymbol{s}) \leftarrow \underset{a}{\operatorname{argmax}} Q^{\pi}(s, a)$;

---

При этом у нас есть гарантии, что когда алгоритм «останавливается» (не может провести Policy Improvement), то он находит оптимальную стратегию. Будем считать ${ }^{4}$, что в такой момент остановки после проведения Policy Improvement наша стратегия не меняется: $\pi_{k+1} \equiv \pi_{k}$.

Теорема 24: В табличном сеттинге Policy Iteration завершает работу за конечное число итераций.
Доказательство. Алгоритм перебирает детерминированные стратегии, и, если остановка не происходит, каждая следующая лучше предыдущей:

$$
\pi_{k} \succ \pi_{k-1} \succ \cdots \succ \pi_{0}
$$

Это означает, что все стратегии в этом ряду различны. Поскольку в табличном сеттинге число состояний и число действий конечны, детерминированных стратегий конечное число; значит, процесс должен закончится.

# Алгоритм 8: Policy Iteration 

Гиперпараметры: $\varepsilon$ - критерий останова для процедуры PolicyEvaluation
Инициализируем $\pi_{0}(s)$ произвольно для всех $s \in \mathcal{S}$
На $\boldsymbol{k}$-ом шаге:

1. $V^{\pi_{k}}:=$ PolicyEvaluation $\left(\pi_{k}, \varepsilon\right)$
2. $Q^{\pi_{k}}(s, a):=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi_{k}}\left(s^{\prime}\right)$
3. $\pi_{k+1}(s):=\underset{a}{\operatorname{argmax}} Q^{\pi_{k}}(s, a)$
4. критерий останова: $\pi_{k} \equiv \pi_{k+1}$


### 3.3.5. Generalized Policy Iteration

Policy Iteration - идеализированный алгоритм: на этапе оценивания в табличном сеттинге можно попробовать решить систему уравнений Беллмана $V^{\pi}$ с достаточно высокой точностью за счёт линейности этой системы уравнений, или можно считать, что проводится достаточно большое количество итераций метода простой итерации. Тогда, вообще говоря, процедура предполагает бесконечное число шагов, и на практике нам нужно когда-то остановиться; теоретически мы считаем, что доводим вычисления до некоторого критерия останова, когда значения вектора не меняются более чем на некоторую погрешность

$\varepsilon>0$.

Но рассмотрим такую, пока что, эвристику: давайте останавливать Policy Evaluation после ровно $\boldsymbol{N}$ шагов, а после обновления стратегии не начинать оценивать $\pi_{k+1}$ с нуля, а использовать последнее $V(s) \approx V^{\pi_{k}}$ в качестве инициализации. Тогда наш алгоритм примет следующий вид:

[^0]
[^0]:    ${ }^{4}$ считаем, что аргмакс берётся однозначно для любой Q-функции: в случае, если в Argmax содержится более одного элемента, множество действий как-то фиксированно упорядочено, и берётся действие с наибольшим приоритетом.

---

# Алгоритм 9: Generalized Policy Iteration 

Гиперпараметры: $\boldsymbol{N}$ - количество шагов
Инициализируем $\boldsymbol{\pi}(\boldsymbol{s})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}$
Инициализируем $\boldsymbol{V}(\boldsymbol{s})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}$
На $\boldsymbol{k}$-ом шаге:

1. Повторить $N$ раз:

$$
\begin{aligned}
& \bullet \forall s: V(s) \leftarrow \mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right)\right] \\
& \text { 2. } Q(s, a) \leftarrow r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right) \\
& \text { 3. } \pi(s) \leftarrow \underset{a}{\operatorname{argmax}} Q(s, a)
\end{aligned}
$$

Мы формально теряем гарантии улучшения стратегии на этапе Policy Improvement, поэтому останавливать алгоритм после того, как стратегия не изменилась, уже нельзя: возможно, после следующих $\boldsymbol{N}$ шагов обновления оценочной функции, аргмакс поменяется, и стратегия всё-таки сменится. Но такая схема в некотором смысле является наиболее общей, и вот почему:

Утверждение 28: Generalized Policy Iteration (алг. 9) совпадает с Value Iteration (алг. 7) при $\boldsymbol{N}=\mathbf{1}$ и с Policy Iteration (алг. 8) при $\boldsymbol{N}=\infty$.

Доказательство. Второе очевидно; увидим первое. При $\boldsymbol{N}=\mathbf{1}$ наше обновление V-функции имеет следующий вид:

$$
V(s) \leftarrow \mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right)\right]
$$

Вспомним, по какому распределению берётся мат. ожидание $\mathbb{E}_{a}$ : по $\boldsymbol{\pi}$, которая имеет вид

$$
\pi(s)=\underset{a}{\operatorname{argmax}} Q(s, a)=\underset{a}{\operatorname{argmax}}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right)\right]
$$

Внутри аргмакса как раз стоит содержимое нашего мат.ожидания в обновлении V , поэтому это обновление выродится в

$$
V(s) \leftarrow \max _{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V\left(s^{\prime}\right)\right]
$$

Это в точности обновление из алгоритма Value Iteration.
Итак, Generalized Policy Iteration при $\boldsymbol{N}=\mathbf{1}$ и при $\boldsymbol{N}=\infty$ - это ранее разобранные алгоритмы, физический смысл которых нам ясен. В частности, теперь понятно, что в Value Iteration очередное приближение $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a}) \approx$ $\approx \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ можно также рассматривать как приближение $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ для $\boldsymbol{\pi}(\boldsymbol{s}):=\underset{a}{\operatorname{argmax}} \boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$; то есть в алгоритме хоть и не потребовалось в явном виде хранить «текущую» стратегию, она всё равно неявно в нём присутствует.

Давайте попробуем понять, что происходит в Generalized Policy Iteration при промежуточных $\boldsymbol{N}$. Заметим, что повторение $\boldsymbol{N}$ раз шага метода простой итерации для решения уравнения $\boldsymbol{\mathfrak { B }} \boldsymbol{V}^{\boldsymbol{\pi}}=\boldsymbol{V}^{\boldsymbol{\pi}}$ эквивалентно одной итерации метода простой итерации для решения уравнения $\boldsymbol{\mathfrak { B }}{ }^{\boldsymbol{N}} \boldsymbol{V}^{\boldsymbol{\pi}}=\boldsymbol{V}^{\boldsymbol{\pi}}$ (где запись $\boldsymbol{\mathfrak { B }}{ }^{\boldsymbol{N}}$ означает повторное применение оператора $\boldsymbol{\mathfrak { B }} \boldsymbol{N}$ раз), для которого, очевидно, искомая $\boldsymbol{V}^{\boldsymbol{\pi}}$ также будет неподвижной точкой. Что это за оператор $\mathfrak{B}^{\boldsymbol{N}}$ ?

В уравнениях Беллмана мы «раскручивали» наше будущее на один шаг вперёд и дальше заменяли оставшийся «хвост» на определение V-функции. Понятно, что мы могли бы раскрутить не на один шаг, а на $\boldsymbol{N}$ шагов вперёд.

Теорема $25-N$-шаговое уравнение Беллмана: Для любого состояния $s_{0}$ :

$$
V^{\boldsymbol{\pi}}\left(s_{0}\right)=\mathbb{E}_{\mathcal{T}, N \sim \pi \mid s_{0}}\left[\sum_{t=0}^{N-1} \gamma^{t} r_{t}+\gamma^{N} \mathbb{E}_{s_{N}} V^{\boldsymbol{\pi}}\left(s_{N}\right)\right]
$$

Доказательство по индукиии. Для получения уравнения на $\boldsymbol{N}$ шагов берём $\boldsymbol{N}$ - 1-шаговое и подставляем в правую часть раскрутку на один шаг из уравнения (3.3). Это в точности соответствует применению оператора Беллмана $N$ раз.

---

Доказательство без индукиии. Для любых траекторий $\mathcal{T}$ верно, что

$$
R(\mathcal{T})=\sum_{t=0}^{N-1} \gamma^{t} r_{t}+\gamma^{N} R_{N}
$$

Возьмём мат.ожидание $\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}}$ слева и справа:

$$
\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}} R(\mathcal{T})=\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}}\left[\sum_{t=0}^{N-1} \gamma^{t} r_{t}+\gamma^{N} R_{N}\right]
$$

Слева видно определение V-функции. Справа достаточно разделить мат.ожидание на мат.ожидание по первым $N$ шагам и хвост:

$$
V^{\pi}\left(s_{0}\right)=\mathbb{E}_{\mathcal{T}, N \sim \pi \mid s_{0}}\left[\sum_{t=0}^{N-1} \gamma^{t} r_{t}+\gamma^{N} \mathbb{E}_{s_{N}} \mathbb{E}_{\mathcal{T}_{N}, \sim \pi \mid s_{N}} R_{N}\right]
$$

Осталось выделить справа во втором слагаемом определение V-функции.

Утверждение 29: $\mathfrak{B}^{N}$ - оператор с коэффициентом сжатия $\gamma^{N}$.
Доказательство.

$$
\rho\left(\mathfrak{B}^{N} V_{1}, \mathfrak{B}^{N} V_{2}\right) \leq \gamma \rho\left(\mathfrak{B}^{N-1} V_{1}, \mathfrak{B}^{N-1} V_{2}\right) \leq \cdots \leq \gamma^{N} \rho\left(V_{1}, V_{2}\right)
$$

Означает ли это, что метод простой итерации решения $\boldsymbol{N}$-шаговых уравнений сойдётся быстрее? Мы по сути просто «за один шаг» делаем $\boldsymbol{N}$ итераций метода простой итерации для решения обычного одношагового уравнения; в этом смысле, мы ничего не выигрываем. В частности, если мы устремим $\boldsymbol{N}$ к бесконечности, то мы получим просто определение V-функции; формально, в правой части будет стоять выражение, вообще не зависящее от поданной на вход оператору $\boldsymbol{V}(s)$, коэффициент сжатия будет поль, и метод простой итерации как бы сходится тут же за один шаг. Но для проведения этого шага нужно выинтегрировать все траектории «раскрыть дерево полностью».

Но теперь у нас есть другой взгляд на Generalized Policy Iteration: мы чередуем одну итерацию решения $\boldsymbol{N}$-шагового уравнения Беллмана с Policy Improvement-ом.

Теорема 26: Алгоритм Generalized Policy Iteration 9 при любом $\boldsymbol{N}$ сходится к оптимальной стратегии и оптимальной оценочной функции.

Без доказательства.
Интуитивно, такой алгоритм «стабилизируется», если оценочная функция будет удовлетворять уравнению Беллмана для текущей $\boldsymbol{\pi}$ (иначе оператор $\mathfrak{B}^{N}$ изменит значение функции), и если $\boldsymbol{\pi}$ выбирает $\operatorname{argmax} \boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ из неё; а если аппроксимация V-функции удовлетворяет уравнению Беллмана, то она совпадает с $\boldsymbol{V}^{\boldsymbol{\pi}}$, и значит $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})=$ $=\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. То есть, при сходимости стратегия $\boldsymbol{\pi}$ будет выбирать действие жадно по отношению к своей же $\boldsymbol{Q}^{\boldsymbol{\pi}}$, а мы помним, что это в


точности критерий оптимальности.

Все алгоритмы, которые мы будем обсуждать далее, так или иначе подпадают под обобщённую парадигму «оценивание-улучшение». У нас будет два процесса оптимизации: обучение вктёра (actor), политики $\boldsymbol{\pi}$, и критика (critic), оценочной функции (Q или V). Критик обучается оценивать текущую стратегию, текущего актёра: сдвигаться в сторону решения какого-нибудь уравнения, для которого единственной неподвижной точкой является $\boldsymbol{V}^{\boldsymbol{\pi}}$ или $\boldsymbol{Q}^{\boldsymbol{\pi}}$. Актёр же будет учиться при помощи policy improvement-a: вовсе не обязательно делать это жадно, возможно учиться выбирать те действия, где оценка критика «побольше», оптимизируя в каких-то состояниях (в каких - пока открытый вопрос) функционал (3.21):

$$
\mathbb{E}_{\pi(a \mid s)} Q(s, a) \rightarrow \max _{\pi}
$$

Причём, возможно, в этом функционале нам не понадобится аппроксимация (модель) Q-функции в явном виде, и тогда мы можем обойтись лишь какими-то оценками $\boldsymbol{Q}^{\boldsymbol{\pi}}$; в таких ситуациях нам достаточно будет на этапе оценивания политики обучать лишь модель $\boldsymbol{V}^{\boldsymbol{\pi}}$ для текущей стратегии. А, например, в эволюционных методах мы обошлись вообще без обучения критика именно потому, что смогли обойтись лишь Монте-Карло оценками будущих наград. Этот самый простой способ решать задачу RL - погенерировать несколько случайных стратегий и выбрать среди них лучшую - тоже условно подпадает под эту парадигму: мы считаем Монте-Карло оценки значения $\boldsymbol{J}(\boldsymbol{\pi})$ для нескольких разных стратегий (evaluation) и выбираем наилучшую

---

стратегию (improvement). Поэтому Policy Improvement, как мы увидим, тоже может выступать в разных формах: например, возможно, как в Value Iteration, у нас будет приближение Q-функции, и мы будем просто всегда полагать, что policy improvement проводится жадно, и текущей стратегией неявно будет $\operatorname{argmax} \boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$.

Но главное, что эти два процесса, оценивание политики (обучение критика) и улучшение (обучение актёра) можно будет проводить стохастической оптимизацией. Достаточно, чтобы лишь в среднем модель оценочной функции сдвигалась в сторону $\boldsymbol{V}^{\boldsymbol{\pi}}$ или $\boldsymbol{Q}^{\boldsymbol{\pi}}$, а актёр лишь в среднем двигался в сторону $\operatorname{argmax}_{\boldsymbol{a}} \boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$. И такой «рецепт» алгоритма всегда будет работать: пока оба этих процесса проводятся корректно, итоговый алгоритм запустится на практике. Это в целом фундаментальная идея всего RL. В зависимости от выбора того, как конкретно проводить эти процессы, получатся разные по свойствам алгоритмы, и, в частности, отдельно интересными будут алгоритмы, «схлопывающие» схему Generalized Policy Iteration в её предельную форму, в Value Iteration.

Мы далее начнём строить model-free алгоритмы, взяв наши алгоритмы планирования - Policy Iteration и Value Iteration, - и попробовав превратить их в табличные алгоритмы решения задачи.

# §3.4. Табличные алгоритмы 

### 3.4.1. Монте-Карло алгоритм

Value iteration и Policy iteration имели два ограничения: 1) необходимо уметь хранить табличку размером $|\mathcal{S}|$ в памяти и перебирать все состояния и действия за разумное время 2) должна быть известна динамика среды $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Первое полечим нейронками, а сейчас будем лечить второе: в сложных средах проблема даже не столько в том, чтобы приблизить динамику среды, а в том, что интегралы $\mathbb{E}_{\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)}$ мы не возьмём в силу огромного числа состояний и сможем только оценивать по Монте-Карло. Итак, мы хотим придумать табличный model-free RL-алгоритм: мы можем отправить в среду пособирать траектории какую-то стратегию, и дальше должны проводить итерации алгоритма, используя лишь эти сэмплы траекторий. Иначе говоря, для данного $\boldsymbol{s}$ мы можем выбрать $\boldsymbol{a}$ и получить ровно один сэмпл из очередного $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, причём на следующем шаге нам придётся проводить сбор сэмплов именно из $\boldsymbol{s}^{\prime}$. Как в таких условиях «решать» уравнения Беллмана - неясно.

Рассмотрим самый простой способ превратить Policy Iteration в model-free метод. Давайте очередную стратегию $\boldsymbol{\pi}_{\boldsymbol{k}}$ отправим в среду, сыграем несколько эпизодов, и будем оценивать $\boldsymbol{Q}^{\boldsymbol{\pi}_{\boldsymbol{k}}}$ по Монте-Карло:

$$
Q^{\pi_{k}}(\boldsymbol{s}, \boldsymbol{a}) \approx \frac{1}{N} \sum_{i=0}^{N} R\left(\mathcal{T}_{i}\right), \quad \mathcal{T}_{i} \sim \pi_{k} \mid s_{0}=s, a_{0}=a
$$

Теперь, доиграв эпизод до конца, мы для каждой встретившейся пары $\boldsymbol{s}, \boldsymbol{a}$ в полученной траектории можем посчитать reward-togo и использовать этот сэмпл для обновления нашей аппроксимации Q-функции - проведения Монте-Карло бэкана (MC-backup). Такое обновление полностью противоположно по свойствам бэкапу динамического программирования: это «бэкап ширины один» бесконечной длины - мы использовали лишь один сэмпл будущего и при этом заглянули в него на бесконечное число шагов вперёд.


Формально, из-за петлей сэмплы являются скоррелированными. Если мы крутимся в петле, а потом в какой-то момент эпизода вышли и получили +1 , то сэмплы будут выглядеть примерно так: $\boldsymbol{\gamma}^{\boldsymbol{x}}, \boldsymbol{\gamma}^{\boldsymbol{4}}, \boldsymbol{\gamma}^{\boldsymbol{3}} \ldots$. Причина в том, что мы взяли по несколько сэмплов для одной и той же Монте-Карло оценки (для одной и той же пары $\boldsymbol{s}, \boldsymbol{a}$ ) из одного и того же эпизода («every-visit»); для теоретической корректности следует гарантировать независимость сэмплов, например, взяв из каждого эпизода для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ сэмпл только для первого посещения («first-visit»).


Монте-Карло алгоритм, на первый взгляд, плох примерно всем. Нужно доигрывать игры до конца, то есть алгоритм неприменим в неэпизодичных средах; Монте-Карло оценки также обладают огромной дисперсией, поскольку мы заменили на сэмплы все мат.ожидания, стоящие в мат.ожидании по траекториям; наконец, мы практически перестали использовать структуру задачи. Если мы получили сэмпл +100 в качестве очередного сэмпла для $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, то мы «забыли», какую часть из этой +100 мы получили сразу же после действия $\boldsymbol{a}$, а какая была получена в далёком будущем - не использовали разложение награды за эпизод в сумму наград за шаг. Также мы посеяли «информацию о соединениях состояниях»: пусть у нас было две траектории (см. рисунок), имевших пересечение в общем состоянии. Тогда для начал этих траекторий мы всё равно считаем, что собрали лишь один сэмпл reward-to-go, хотя в силу марковости у нас есть намного больше информации.

---

Ещё одна проблема алгоритма: если для некоторых $\boldsymbol{s}, \boldsymbol{a}: \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})=\mathbf{0}$, то мы ничего не узнали об $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. А, как было видно в алгоритмах динамического программирования, мы существенно опираемся в том числе и на значения Q-функции для тех действий, которые $\pi$ никогда не выбирает; только за счёт этого мы умеем проводить policy improvement для детерминированных стратегий.

А ещё в таком Монте-Карло алгоритме встаёт вопрос: когда заканчивать оценивание Q-функции и делать шаг Policy Improvement-a? Точное значение $\boldsymbol{Q}^{\boldsymbol{\pi}_{k}}$ за конечное время мы Монте-Карло оценкой всё равно не получим, и в какой-то момент improvement проводить придётся, с потерей теоретических гарантий. Возникает вопрос: насколько разумно в таких условиях после очередного обновления стратегии начинать расчёт оценочной функции $Q^{\pi_{k+1}}$ «с нуля»? Может, имеет смысл проинициализировать Q-функцию для новой стратегии $\pi_{k+1}$ как-то при помощи текущего приближения $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{\boldsymbol{\pi}_{k}}(\boldsymbol{s}, \boldsymbol{a})$ ? Да, хоть оно и считалось для «предыдущей» стратегии и формально содержит сэмплы не из того распределения, но всё-таки этих сэмплов там было аккумулировано много, да и стратегия потенциально поменялась не сильно; и всяко лучше какой-нибудь нулевой инициализации. Возникает желание усреднять сэмплы с приоритетом более свежих, приходящих из «правильной» стратегии; а «неправильные» сэмплы, из старой стратегии, всё-таки использовать, но с каким-то маленьким весом. Всё это хочется делать как-то онлайн, не храня всю историю Монте-Карло оценок.

# 3.4.2. Экспоненциальное сглаживание 

Рассмотрим такую задачу. Нам приходят сэмплы $\boldsymbol{x}_{1}, \boldsymbol{x}_{2} \ldots \boldsymbol{x}_{n} \sim \boldsymbol{p}(\boldsymbol{x})$. Хотим по ходу получения сэмплов оценивать мат.ожидание случайной величины $\boldsymbol{x}$. Давайте хранить Монте-Карло оценку, усредняя все имеющиеся сэмплы; для этого достаточно пользоваться следующим рекурсивным соотношением:

$$
m_{k}:=\frac{1}{k} \sum_{i=1}^{k} x_{i}=\frac{k-1}{k} m_{k-1}+\frac{1}{k} x_{k}
$$

Обозначим за $\boldsymbol{\alpha}_{\boldsymbol{k}}:=\frac{1}{k}$. Тогда формулу можно переписать так:

$$
m_{k}:=\left(1-\alpha_{k}\right) m_{k-1}+\alpha_{k} x_{k}
$$

Определение 49: Экспоненциальным сглаживанием (exponential smoothing) для последовательности $x_{1}, x_{2}, x_{3} \ldots$ будем называть следующую оценку:

$$
m_{k}:=\left(1-\alpha_{k}\right) m_{k-1}+\alpha_{k} x_{k}
$$

где $m_{0}$ - некоторое начальное приближение, последовательность $\alpha_{k} \in[0,1]$ - гиперпараметр, называемый learning rate.

Можно ли оценивать мат.ожидание как-то по-другому? В принципе, любая выпуклая комбинация имеющихся сэмплов будет несмещённой оценкой. В частности, если $\boldsymbol{\alpha}_{\boldsymbol{k}}>\frac{1}{\boldsymbol{k}}$, то мы «выдаём» более свежим сэмплам больший вес. Зададимся таким техническим вопросом: при каких других последовательностях $\boldsymbol{\alpha}_{\boldsymbol{k}}$ формула позволит оценивать среднее?

Определение 50: Будем говорить, что learning rate $\alpha_{k} \in[0,1]$ удовлетворяет условиям Роббинса-Монро (Robbins-Monro conditions), если:

$$
\sum_{k \geq 0}^{\infty} \alpha_{k}=+\infty, \quad \sum_{k \geq 0}^{\infty} \alpha_{k}^{2}<+\infty
$$

Теорема 27: Пусть $\boldsymbol{x}_{1}, \boldsymbol{x}_{2} \ldots$ - независимые случайные величины, $\mathbb{E} \boldsymbol{x}_{\boldsymbol{k}}=\boldsymbol{m}, \mathbb{D} \boldsymbol{x}_{\boldsymbol{k}} \leq \boldsymbol{C}<+\infty$, где $\boldsymbol{C}$ - некоторая конечная константа. Пусть $\boldsymbol{m}_{\mathbf{0}}$ - произвольно, а последовательность чисел $\boldsymbol{\alpha}_{\boldsymbol{k}} \in[0,1]$ удовлетворяет условиям Роббинса-Монро (3.26). Тогда экспоненциальное сглаживание

$$
m_{k}:=\left(1-\alpha_{k}\right) m_{k-1}+\alpha_{k} x_{k}
$$

сходится к $\boldsymbol{m}$ с вероятностью 1 .
Доказательство. Без ограничения общности будем доказывать утверждение для $\boldsymbol{m}=\mathbf{0}$, поскольку для сведения к этому случаю достаточно вычесть $\boldsymbol{m}$ из правой и левой части (3.27) и перейти к обозначениям $\hat{m}_{k}:=m_{k}-m, \hat{x}_{k}:=x_{k}-m$.

Итак, пусть $\mathbb{E} \boldsymbol{x}_{\boldsymbol{k}}=\mathbf{0}$. Будем доказывать, что $\boldsymbol{v}_{\boldsymbol{k}}:=\mathbb{E} \boldsymbol{m}_{\boldsymbol{k}}^{2} \xrightarrow{k \rightarrow \infty} \mathbf{0}$. Для начала возведём обе стороны уравнения (3.27) в квадрат:

$$
m_{k}^{2}=\left(1-\alpha_{k}\right)^{2} m_{k-1}^{2}+\alpha_{k}^{2} x_{k}^{2}+2 \alpha_{k}\left(1-\alpha_{k}\right) x_{k} m_{k-1}
$$

Возьмём справа и слева мат.ожидание:

$$
\mathbb{E} m_{k}^{2}=\left(1-\alpha_{k}\right)^{2} \mathbb{E} m_{k-1}^{2}+\alpha_{k}^{2} \mathbb{E} x_{k}^{2}+2 \alpha_{k}\left(1-\alpha_{k}\right) \mathbb{E}\left(x_{k} m_{k-1}\right)
$$

---

Последнее слагаемое зануляется, поскольку в силу независимости $\mathbb{E}\left(\boldsymbol{x}_{\boldsymbol{k}} \boldsymbol{m}_{\boldsymbol{k}-\mathbf{1}}\right)=\mathbb{E} \boldsymbol{x}_{\boldsymbol{k}} \mathbb{E} \boldsymbol{m}_{\boldsymbol{k}-\mathbf{1}}$, а $\mathbb{E} \boldsymbol{x}_{\boldsymbol{k}}$ равно нулю по условию. Используя введённое обозначение, получаем такой результат:

$$
v_{k}=\left(1-\alpha_{k}\right)^{2} v_{k-1}+\alpha_{k}^{2} \mathbb{E} x_{k}^{2}
$$

Сейчас мы уже можем доказать, что $\boldsymbol{v}_{\boldsymbol{k}} \leq \boldsymbol{C}$. Действительно, сделаем это по индукции. База: $\boldsymbol{v}_{0}=\mathbf{0} \leq \boldsymbol{C}$ по определению. Шаг: пусть $\boldsymbol{v}_{\boldsymbol{k}-1} \leq \boldsymbol{C}$, тогда

$$
v_{k}=\left(1-\alpha_{k}\right)^{2} v_{k-1}+\alpha_{k}^{2} \mathbb{E} x_{k}^{2} \leq\left(1-\alpha_{k}\right)^{2} C+\alpha_{k}^{2} C \leq C
$$

где последнее неравенство верно при любых $\boldsymbol{\alpha}_{\boldsymbol{k}} \in[0,1]$.
Особенность дальнейшего доказательства в том, что последовательность $\boldsymbol{v}_{\boldsymbol{k}}$ вовсе не обязана быть монотонной. Поэтому применим пару фокусов в стиле матана. Сначала раскроем скобки в рекурсивном выражении (3.28):

$$
v_{k}-v_{k-1}=-2 \alpha_{k} v_{k-1}+\alpha_{k}^{2}\left(v_{k-1}+\mathbb{E} x_{k}^{2}\right)
$$

Мы получили счётное число равенств, проиндексированных $\boldsymbol{k}$. Просуммируем первые $\boldsymbol{n}$ из них:

$$
v_{n}-v_{0}=-2 \sum_{k=0}^{n-1} \alpha_{k} v_{k}+\sum_{k=0}^{n-1} \alpha_{k}^{2}\left(v_{k}+\mathbb{E} x_{k}^{2}\right)
$$

Заметим, что $\boldsymbol{v}_{0}=\mathbf{0}$, а $\boldsymbol{v}_{\boldsymbol{n}} \geq \mathbf{0}$ по определению как дисперсия. Значит:

$$
2 \sum_{k=0}^{n-1} \alpha_{k} v_{k} \leq \sum_{k=0}^{n-1} \alpha_{k}^{2}\left(v_{k}+\mathbb{E} x_{k}^{2}\right)
$$

Применяем ограниченность $\boldsymbol{v}_{\boldsymbol{k}}$ и $\mathbb{E} \boldsymbol{x}_{\boldsymbol{k}}^{2}$ :

$$
2 \sum_{k=0}^{n-1} \alpha_{k} v_{k} \leq \sum_{k=0}^{n-1} \alpha_{k}^{2}(C+C)=2 C \sum_{k=0}^{n-1} \alpha_{k}^{2}
$$

Ряд справа сходится при $\boldsymbol{n} \rightarrow+\infty$. Значит, сходится и ряд слева. Коли так, имеет предел правая часть (3.29). Значит, имеет предел и левая.

Было доказано, что последовательность $\boldsymbol{v}_{\boldsymbol{k}}$ имеет предел. Понятно, что он неотрицателен. Допустим, он положителен и отделён от 0 , равен некоторому $\boldsymbol{b}>\mathbf{0}$. Возьмём какое-нибудь небольшое $\varepsilon>\mathbf{0}$, так что $\boldsymbol{b}-$ $-\varepsilon>0$. Тогда, начиная с некоторого номера $\boldsymbol{i}$ все элементы последовательности $\boldsymbol{v}_{\boldsymbol{k}}>\boldsymbol{b}-\varepsilon$ при $\boldsymbol{k} \geq \boldsymbol{i}$. Получим:

$$
\sum_{k=0}^{n-1} \alpha_{k} v_{k} \geq \sum_{k=i}^{n-1} \alpha_{k} v_{k} \geq(b-\varepsilon) \sum_{k=i}^{n-1} \alpha_{k}
$$

Правая часть неравенства расходится, поскольку мы просили расходимость ряда из $\boldsymbol{\alpha}_{\boldsymbol{k}}$; но ранее мы доказали сходимость левой части. Значит, предел равен нулю.

# 3.4.3. Стохастическая аппроксимация 

Мы научились, можно считать, решать уравнения такого вида:

$$
x=\mathbb{E}_{\varepsilon} f(\varepsilon)
$$

где справа стоит мат.ожидание по неизвестному распределению $\varepsilon \sim p(\varepsilon)$ от какой-то функции $\boldsymbol{f}$, которую для данного значения сэмпла $\varepsilon$ мы умеем считать. Это просто стандартная задача оценки среднего, для которой мы даже доказали теоретические гарантии сходимости следующего итеративного алгоритма:

$$
x_{k}:=\left(1-\alpha_{k}\right) x_{k-1}+\alpha_{k} f(\varepsilon), \quad \varepsilon \sim p(\varepsilon)
$$

Аналогично у нас есть итеративный алгоритм для решения систем нелинейных уравнений

$$
x=f(x)
$$

где справа стоит, если угодно, «хорошая» функция - сжатие. Формула обновления в методе простой итерации выглядела вот так:

$$
x_{k}:=f\left(x_{k-1}\right)
$$

---

Определение 51: Стохастическая аппроксимация (Stochastic approximation) - задача решения уравнения вида

$$
x=\mathbb{E}_{\varepsilon \sim p(\varepsilon)} f(x, \varepsilon)
$$

где справа стоит мат.ожидание по неизвестному распределению $\boldsymbol{p}(\boldsymbol{\varepsilon})$, из которого доступны лишь сэмплы, от функции, которую при данном сэмпле $\boldsymbol{\varepsilon}$ и некотором значении неизвестной переменной $\boldsymbol{x}$ мы умеем считать.

Можно ли объединить идеи метода простой итерации и экспоненциального сглаживания («перевзвешанной» Монте-Карло оценки)? Давайте запустим аналогичный итеративный алгоритм: на $\boldsymbol{k}$-ой итерации подставим текущее приближение неизвестной переменной $\boldsymbol{x}_{\boldsymbol{k}}$ в правую часть $\boldsymbol{f}\left(\boldsymbol{x}_{\boldsymbol{k}}, \boldsymbol{\varepsilon}\right)$ для $\boldsymbol{\varepsilon} \sim \boldsymbol{p}(\boldsymbol{\varepsilon})$, но не будем «жёстко» заменять $\boldsymbol{x}_{\boldsymbol{k}+\mathbf{1}}$ на полученное значение, так как оно является лишь несмещённой оценкой правой части; вместо этого сгладим старое значение $\boldsymbol{x}_{\boldsymbol{k}}$ и полученный новый «сэмпл»:

$$
x_{k}=\left(1-\alpha_{k}\right) x_{k-1}+\alpha_{k} f\left(x_{k-1}, \varepsilon\right), \quad \varepsilon \sim p(\varepsilon)
$$

Есть хорошая надежда, что, если функция $\boldsymbol{f}$ «хорошая», распределение $\boldsymbol{p}(\boldsymbol{\varepsilon})$ не сильно страшное (например, имеет конечную дисперсию, как в теореме о сходимости экспоненциального сглаживания), a learning rate $\boldsymbol{\alpha}_{\boldsymbol{k}}$ удовлетворяют условиям (3.26), то такая процедура будет в пределе сходиться.

Поймём, почему задача стохастической аппроксимации тесно связана со стохастической оптимизацией. Для этого перепишем формулу (3.31) в альтернативной очень интересной форме:

$$
x_{k}=x_{k-1}+\alpha_{k}\left(f\left(x_{k-1}, \varepsilon\right)-x_{k-1}\right), \quad \varepsilon \sim p(\varepsilon)
$$

Да это же формула стохастического градиентного спуска (stochastic gradient descent, SGD)! Действительно, в нём мы, оптимизируя некоторую функцию $\boldsymbol{f}(\boldsymbol{x})$, прибавляем к очередному приближению $\boldsymbol{x}_{\boldsymbol{k}-1}$ с некоторым learning rate $\boldsymbol{\alpha}_{\boldsymbol{k}}$ несмещённую оценку градиента $\boldsymbol{\nabla} \boldsymbol{f}\left(\boldsymbol{x}_{\boldsymbol{k}-1}\right)$, которую можно обозначить как $\boldsymbol{\nabla} \boldsymbol{f}\left(\boldsymbol{x}_{\boldsymbol{k}-1}, \boldsymbol{\varepsilon}\right)$ :

$$
x_{k}=x_{k-1}+\alpha_{k} \nabla f\left(x_{k-1}, \varepsilon\right), \quad \varepsilon \sim p(\varepsilon)
$$

О стохастической оптимизации можно думать не как об оптимизации, а как о поиске решения уравнения $\boldsymbol{\nabla} \boldsymbol{f}(\boldsymbol{x})=\mathbf{0}$. Действительно: SGD, как и любая локальная оптимизация, может идти как к локальному оптимуму, так и к седловым точкам, но в них $\mathbb{E}_{\boldsymbol{\varepsilon}} \boldsymbol{\nabla} \boldsymbol{f}(\boldsymbol{x}, \boldsymbol{\varepsilon})=\mathbf{0}$.

И, глядя на формулу (3.32), мы понимаем, что, видимо, выражение $\boldsymbol{f}\left(\boldsymbol{x}_{\boldsymbol{k}-1}, \boldsymbol{\varepsilon}\right)-\boldsymbol{x}_{\boldsymbol{k}-1}$ есть какой-то «стохастический градиент». Стохастический он потому, что это выражение случайно: мы сэмплируем $\boldsymbol{\varepsilon} \sim \boldsymbol{p}(\boldsymbol{\varepsilon})$; при этом это несмещённая оценка «градиента», поскольку она обладает следующим свойством: в точке решения, то есть в точке $\boldsymbol{x}^{*}$, являющейся решением уравнения (3.30), в среднем его значение равно нулю:

$$
\mathbb{E}_{\varepsilon}\left(f\left(x^{*}, \varepsilon\right)-x^{*}\right)=0
$$

И по аналогии можно предположить, что если стохастический градиентный спуск ищет решение $\mathbb{E}_{\varepsilon} \boldsymbol{\nabla} \boldsymbol{f}(\boldsymbol{x}, \boldsymbol{\varepsilon})=$ $=\mathbf{0}$, то процесс (3.32) ищет решение уравнения $\mathbb{E}_{\varepsilon} \boldsymbol{f}(\boldsymbol{x}, \boldsymbol{\varepsilon})-\boldsymbol{x}=\mathbf{0}$.

Это наблюдение будет иметь для нас ключевое значение. B model-free режиме как при оценивании стратегии, когда мы решаем уравнение Беллмана

$$
Q^{\boldsymbol{\pi}}(s, a)=\mathbb{E}_{\boldsymbol{s}^{\prime}}\left[r(s, a)+\gamma \mathbb{E}_{a^{\prime}} Q^{\boldsymbol{\pi}}\left(s^{\prime}, a^{\prime}\right)\right]
$$

так и когда мы пытаемся реализовать Value Iteration и напрямую решать уравнения оптимальности Беллмана

$$
Q^{*}(s, a)=\mathbb{E}_{s^{\prime}}\left[r(s, a)+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right]
$$

мы сталкиваемся в точности с задачей стохастической аппроксимации (3.30)! Справа стоит мат.ожидание по недоступному нам распределению, содержимое которого мы, тем не менее, при данном сэмпле $s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$ и текущем приближении Q-функции, умеем считать. Возникает идея проводить стохастическую аппроксимацию: заменять правые части решаемых уравнений на несмещённые оценки и сглаживать текущее приближение с получаемыми сэмплами.

Отметим интересный факт: уравнение $\mathrm{V}^{*} \mathrm{~V}^{*}$ (3.18), имеющее вид «максимум от мат.ожидания по неизвестному распределению»

$$
V^{*}(s)=\max _{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{*}\left(s^{\prime}\right)\right]
$$

под данную форму не попадает. И с такими уравнениями мы ничего сделать не сможем. К счастью, нам и не понадобится.

---

# 3.4.4. Temporal Difference 

Итак, попробуем применить идею стохастической аппроксимации для решения уравнений Беллмана. На очередном шаге после совершения действия $\boldsymbol{a}$ из состояния $\boldsymbol{s}$ мы получаем значение награды $\boldsymbol{r}:=\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$, сэмпл следующего состояния $s^{\prime}$ и генерируем $\boldsymbol{a}^{\prime} \sim \boldsymbol{\pi}$, после чего сдвигаем с некоторым learning rate наше текущее приближение $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ в сторону сэмпла

$$
y:=\boldsymbol{r}+\gamma \boldsymbol{Q}\left(s^{\prime}, a^{\prime}\right)
$$

который также будем называть таргетом ${ }^{5}$ (Bellman target). Получаем следующую формулу обновления:

$$
Q_{k+1}(s, a) \leftarrow Q_{k}(s, a)+\alpha_{k} \underbrace{\left(\begin{array}{c}
\text { mapzет } \\
\boldsymbol{r}+\gamma \boldsymbol{Q}_{k}\left(s^{\prime}, a^{\prime}\right)-\boldsymbol{Q}_{k}(s, a)
\end{array}\right)}_{\text {временная разность }}
$$

Выражение $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})+\gamma \boldsymbol{Q}_{\boldsymbol{k}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)-\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$ называется временной разностью (temporal difference): это отличие сэмпла, который нам пришёл, от текущей оценки среднего.

Мы таким образом придумали T D-backup: обновление, имеющее как ширину, так и длину один. Мы рассматриваем лишь одну версию будущего (один сэмпл) и заглядываем на один шаг вперёд, приближая всё дальнейшее будущее своей собственной текущей аппроксимацией. Этот ход позволяет нам учиться, не доигрывая эпизоды до конца: мы можем обновить одну ячейку нашей Q-функции сразу же после одного шага в среде, после сбора одного перехода $\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$.

Формула (3.33) отличается от Монте-Карло обновлений Q-функции лишь способом построения таргета: в Монте-Карло мы бы взяли в ка-


честве $\boldsymbol{y}$ reward-to-go, когда здесь же используем одношаговое приближение. Поэтому смотреть на эту формулу также можно через интуицию бутстрапирования (bootstrapping) ${ }^{6}$. Мы хотим получить сэмплы для оценки нашей текущей стратегии $\boldsymbol{\pi}$, поэтому делаем один шаг в среде и приближаем всю оставшуюся награду нашей текущей аппроксимацией среднего. Такой «псевдосэмпл» уже не будет являться корректным сэмплом для $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, но в некотором смысле является «более хорошим», чем то, что у нас есть сейчас, за счёт раскрытия дерева на один шаг и получения информации об $\boldsymbol{r}, \boldsymbol{s}^{\prime}$. Такое движение нашей аппроксимации в сторону чего-то хоть чуть-чуть более хорошего и позволяет нам чему-то учиться.

Пример 57: Вы сидите в кафе ( $s$ ) и хотите вернуться домой. Вы прикидываете, что в среднем этой займёт $\overline{\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})}=\mathbf{3 0}$ минут. Вы тратите одну минуту $(-\boldsymbol{r})$ на выход из кафе и обнаруживаете пробку $\left(\boldsymbol{s}^{\prime}\right)$. За счёт этой новой информации вы можете дать более точную оценку времени до возвращения до дома: $-\boldsymbol{Q}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)=$ 40 минут. Как можно в будущем откорректировать наши прогнозы? Можно засечь время, сколько в итоге заняла поездка - доиграть эпизод до конца и посчитать Монте-Карло оценку - а можно уже сделать вывод о том, что случилась некоторая «временная разность», ошибка за один шаг, равная $\mathbf{4 1}-\mathbf{3 0}=\mathbf{1 1}$ минут. Заменять исходное приближение расстояния от кафе до дома $-\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ на 41 минуту, конечно же, слишком грубо: но временная разность говорит, что 30 минут было заниженной оценкой и её надо чуть-чуть увеличить.

Обсудим следующий важный технический момент. Какие есть ограничения на переходы $\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$, которые мы можем использовать для обновлений по формуле (3.33)? Пока мы говорили, что мы хотим заниматься оцениванием стратегии $\boldsymbol{\pi}$, и поэтому предлагается, видимо, ею и взаимодействовать со средой, собирая переходы. С точки же зрения стохастической аппроксимации, для корректности обновлений достаточно выполнения всего лишь двух требований:

- $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$; если $\boldsymbol{s}^{\prime}$ приходит из какой-то другой функции переходов, то мы учим Q-функцию для какого-то другого MDP.
- $\boldsymbol{a}^{\prime} \sim \boldsymbol{\pi}\left(\boldsymbol{a}^{\prime} \mid \boldsymbol{s}^{\prime}\right)$; если $\boldsymbol{a}^{\prime}$ приходит из какой-то другой стратегии, то мы учим Q-функцию для вот этой другой стратегии.

Оба этих утверждения вытекают из того, что обновление (3.33) неявно ищет решение уравнения

$$
Q(s, a)=\mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime}} y
$$

[^0]
[^0]:    ${ }^{5}$ в англоязычной литературе встречается слово gиеев - «догэдка»; этот таргет имеет смысл наших собственных предположений о том, какое будущее нас ждёт.
    ${ }^{6}$ бутстрап - «ремешки на ботинках», происходит от выражения «потянуть самого себя за ремешки на ботинках и так перелезть через ограду». Русскоязычный аналог - «тащить самого себя за волосы из болота». Наши таргеты построены на принципе такого бутстрапирования, поскольку мы начинаем «из воздуха» делать себе сэмплы из уже имеющихся сэмплов.

---

как схема стохастической аппроксимации. Поэтому мы будем уделять много внимания тому, из правильных ли распределений приходят данные, которые мы «скармливаем» этой формуле обновления. Но и с другой стороны: схема не требует, например, чтобы после ячейки $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ мы обязательно на следующем шаге обновили ячейку $\boldsymbol{Q}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$, ровно как и не говорит ничего о требованиях на распределение, из которого приходят пары $\boldsymbol{s}, \boldsymbol{a}$. Как мы увидим позже, это наблюдение означает, что нам вовсе не обязательно обучаться с онлайн-опыта.

# 3.4.5. Q-learning 

Поскольку довести $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ до точного значения $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ с гарантиями мы всё равно не сможем, однажды в алгоритме нам всё равно придётся сделать policy improvement. Что, если мы будем обновлять нашу стратегию $\pi_{k}(s):=\underset{a}{\operatorname{argmax}} \boldsymbol{Q}_{k}(\boldsymbol{s}, \boldsymbol{a})$ после каждого шага в среде и каждого обновления Q-функции? Наше приближение Policy Iteration схемы, аналогично ситуации в динамическом программировании, превратится в приближение Value Iteration схемы:

$$
\begin{aligned}
y & =r+\gamma \boldsymbol{Q}_{k}\left(s^{\prime}, a^{\prime}\right)= \\
& =r+\gamma \boldsymbol{Q}_{k}\left(s^{\prime}, \pi_{k}\left(s^{\prime}\right)\right)= \\
& =r+\gamma \boldsymbol{Q}_{k}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} \boldsymbol{Q}_{k}\left(s^{\prime}, a^{\prime}\right)\right)= \\
& =r+\gamma \max _{a^{\prime}} \boldsymbol{Q}_{k}\left(s^{\prime}, a^{\prime}\right)
\end{aligned}
$$

Мы получили в точности таргет для решения методом стохастической аппроксимации уравнения оптимальности Беллмана; поэтому для такого случая будем обозначать нашу Q-функцию как аппроксимацию $\boldsymbol{Q}^{*}$, и тогда наше обновление принимает следующий вид: для перехода $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$ обновляется только одна ячейка нашего приближения Q-функции:

$$
\boldsymbol{Q}_{k+1}(\boldsymbol{s}, \boldsymbol{a}):=\boldsymbol{Q}_{k}(\boldsymbol{s}, \boldsymbol{a})+\boldsymbol{\alpha}_{k}\left(r+\gamma \max _{a^{\prime}} \boldsymbol{Q}_{k}\left(s^{\prime}, a^{\prime}\right)-\boldsymbol{Q}_{k}(\boldsymbol{s}, \boldsymbol{a})\right)
$$

Теорема 28 - Сходимость Q-learning: Пусть пространства состояний и действий конечны, $\boldsymbol{Q}_{0}(\boldsymbol{s}, \boldsymbol{a})$ - начальное приближение, на $\boldsymbol{k}$-ой итерации $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$ для всех пар $\boldsymbol{s}, \boldsymbol{a}$ строится по правилу

$$
\boldsymbol{Q}_{k+1}(\boldsymbol{s}, \boldsymbol{a}):=\boldsymbol{Q}_{k}(\boldsymbol{s}, \boldsymbol{a})+\boldsymbol{\alpha}_{k}(\boldsymbol{s}, \boldsymbol{a})\left(r(\boldsymbol{s}, \boldsymbol{a})+\gamma \max _{a^{\prime}} \boldsymbol{Q}_{k}\left(s_{k}^{\prime}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{a}^{\prime}\right)-\boldsymbol{Q}_{k}(\boldsymbol{s}, \boldsymbol{a})\right)
$$

где $s_{k}^{\prime}(\boldsymbol{s}, \boldsymbol{a}) \sim p\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, а $\boldsymbol{\alpha}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a}) \in[0,1]$ - случайные величины, с вероятностью один удовлетворяющие для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ условиям Роббинса-Мопро:

$$
\sum_{k \geq 0}^{\infty} \alpha_{k}(s, a)=+\infty \quad \sum_{k \geq 0}^{\infty} \alpha_{k}(s, a)^{2}<+\infty
$$

Тогда $\boldsymbol{Q}_{\boldsymbol{k}}$ сходится к $\boldsymbol{Q}^{*}$ с вероятностью один.
Доказательство вынесено в приложение А.3.
В частности, если агент некоторым образом взаимодействует со средой и на $\boldsymbol{k}$-ом шаге обновляет своё текущее приближение $\boldsymbol{Q} \approx \boldsymbol{Q}^{*}$ только для одной пары $\boldsymbol{s}, \boldsymbol{a}$, то можно считать, что $\boldsymbol{\alpha}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a}) \neq \mathbf{0}$ только для неё. При этом ограничения (3.35) всё ещё могут оказаться выполненными, поэтому эта интересующая нас ситуация есть просто частный случай сформулированной теоремы.

Заметим, что для выполнения условий сходимости необходимо для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ проводить бесконечное число обновлений $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ : в противном случае, в ряду $\sum_{k \geq 1} \boldsymbol{\alpha}_{k}(\boldsymbol{s}, \boldsymbol{a})$ будет конечное число ненулевых членов, и мы не попадём под теорему. Значит, наш сбор опыта должен удовлетворять условию infinite visitation все пары $\boldsymbol{s}, \boldsymbol{a}$ должны гарантированно встречаться бесконечно много раз. По сути теорема говорит, что это требование является достаточным условием на процесс порождения переходов $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$ :

Утверждение 30: Пусть сбор опыта проводится так, что пары $\boldsymbol{s}, \boldsymbol{a}$ встречаются бесконечное число раз. Пусть $\boldsymbol{n}(\boldsymbol{s}, \boldsymbol{a})$ - счётчик количества выполнений действия $\boldsymbol{a}$ в состоянии $\boldsymbol{s}$ во время взаимодействия агента со средой. Тогда можно положить $\boldsymbol{\alpha}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a}):=\frac{1}{n(\boldsymbol{s}, \boldsymbol{a})}$, чтобы гарантировать сходимость алгоритма к $\boldsymbol{Q}^{*}$.

---

# 3.4.6. Exploration-exploitation дилемма 

Так какой же стратегией играть со средой, чтобы получать траектории? Коли мы учим $\boldsymbol{Q}^{*}$, у нас на очередном шаге есть текущее приближение $\pi_{k}(s):=\underset{a}{\operatorname{argmax}} Q_{k}(s, a)$, и мы можем использовать (exploit) эти знания.

Теорема 29: Собирая траектории при помощи жадной стратегии, есть риск не сойтись к оптимальной.
Доказательство. Приведём простой пример. Есть два действия: получить приз $(+100)$ и тупить в стену $(+0)$, после выполнения любого игра заканчивается. Заинициализировали $\boldsymbol{Q}_{0}$ (приз) $=\boldsymbol{- 1}, \boldsymbol{Q}_{0}$ (стена) $=$ $=+1$. В первой игре, пользуясь текущей аппроксимацией $\pi_{0}(s):=$ $\underset{a}{\operatorname{argmax}} Q_{0}(s, a)$, выбираем тупить в стену. Получая 0 , сглаживаем 0 и текущее приближение +1 , получая новое значение $\boldsymbol{Q}_{\boldsymbol{k}=1}$ (стена) $\geq 0$. Оно превосходит $\boldsymbol{Q}_{\boldsymbol{k}=1}$ (приз) $=\boldsymbol{- 1}$. Очевидно, и в дальнейших играх агент никогда не выберет приз, и оптимальная стратегия не будет найдена.


Действительно, детерминированные стратегии не позволяют получить свойство infinite visitation: многие пары $\boldsymbol{s}, \boldsymbol{a}$ просто принципиально не встречаются в порождаемых ими траекториях. В частности, из-за неё нельзя ограничиваться рассмотрением только класса детерминированных стратегий, хоть и была доказана теорема 21 о существовании в нём оптимальной стратегии: мы показали, что для сбора данных - взаимодействия со средой - детерминированные стратегии не подходят. Какие подходят?

Теорема 30: Любая стратегия, для которой $\forall \boldsymbol{s}, \boldsymbol{a}: \boldsymbol{\mu}(\boldsymbol{a} \mid \boldsymbol{s})>0$, удовлетворяет условию infinite visitation, то есть с отличной от нуля вероятностью в траектории, порождённой $\boldsymbol{\pi}$, встретится любая пара $\boldsymbol{s}, \boldsymbol{a}$.

Доказательство. Для любого $\boldsymbol{s}$ существует набор действий $\boldsymbol{a}_{0}, \boldsymbol{a}_{1} \ldots \boldsymbol{a}_{\boldsymbol{N}}$, которые позволяют с некоторой ненулевой вероятностью добраться до него из $\boldsymbol{s}_{\mathbf{0}}: \boldsymbol{p}\left(\boldsymbol{s} \mid \boldsymbol{s}_{\mathbf{0}}, \boldsymbol{a}_{\mathbf{0}} \ldots \boldsymbol{a}_{\boldsymbol{N}}\right)>\mathbf{0}$; если это не так, то ни одна стратегия никогда не попадёт в $\boldsymbol{s}$, и мы без ограничения общности можем считать, что таких «параллельных вселенных» в нашей среде не существует. Значит, $\boldsymbol{\pi}$ с некоторой ненулевой вероятностью выберет эту цепочку действий $\boldsymbol{a}_{0} \ldots \boldsymbol{a}_{\boldsymbol{N}}$, после чего с ненулевой вероятностью окажется в $\boldsymbol{s}$ и с ненулевой вероятностью выберет заданное $\boldsymbol{a}$.

Если мы воспользуемся любой такой стохастической стратегией, мы попадём под действие теоремы о сходимости 28. Совершая случайные действия, мы рискуем творить ерунду, но занимаемся исследованием (exploration) - поиском новых возможностей в среде. Означает ли это, что нам достаточно взять полностью случайную стратегию, которая равновероятно выбирает из множества действий, отправить её в среду порождать переходики $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$, обучать на них Q-функцию по формуле (3.34), и на выходе в пределе мы получим $Q^{*}$ ? В пределе - да.

Пример 58: Представьте, что вы отправили случайную стратегию играть в Марио. Через экспоненциально много попыток она случайно пройдёт первый уровень и попадёт на второй; для состояний из второго уровня будет проведено первое обновление Q-функции. Через условно бесконечное число таких удач Q-функция для состояний из второго уровня действительно будет выучена...

Иначе говоря, исследование - корректный, но неэффективный способ генерации данных. Использование куда более эффективный метод, позволяющий быстро добираться до «трудподоступных областей» в среде такими областями обычно являются области с высокой наградой, иначе задача RL скорее всего не является особо интересной - и быстрее набирать сэмплы для обновлений пока ещё редко обновлённых ячеек $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$. Но это «некорректный» способ: детерминированная стратегия может застрять в локальном оптимуме и никогда не увидеть, что в каком-то месте другое действие даёт ещё большую награду.

Обсудим базовые варианты, как можно решать этот exploration-exploitation trade-off в контексте вычисления оптимальной Q-функции методом временных разностей. Нам нужно взять нашу стратегию использования $\pi(s):=\underset{a}{\operatorname{argmax}} Q(s, a)$ и что-нибудь с ней поделать так, чтобы она стала формально стохастичной.

Определение 52: $\varepsilon$-жадной ( $\varepsilon$-greedy) называется стратегия

$$
\mu(s):=\left\{\begin{array}{ll}
a \sim \operatorname{Uniform}(\mathcal{A}) & \text { с вероятностью } \varepsilon \\
\underset{a}{\operatorname{argmax}} Q(s, a) & \text { иначе. }
\end{array}\right.
$$

---

Определение 53: Болъимановской с температурой $\tau$ называется стратегия

$$
\mu(a \mid s) \vDash \operatorname{softmax}_{a}\left(\frac{Q(s, a)}{\tau}\right)
$$

Первый вариант никак не учитывает, насколько кажутся хорошими другие действия помимо жадного, и, если решает «исследовать», выбирает среди них случайно. Второй же вариант будет редко выбирать очень плохие действия (это может быть крайне полезным свойством), но чувствителен к масштабу приближения Qфункции, что обычно менее удобно и требует настройки температуры $\boldsymbol{\tau}$. Для Больцмановской стратегии мы увидим интересную интерпретацию в контексте обсуждения Maximum Entropy RL (раздел 6.2).

# 3.4.7. Реплей буфер 

Итак, мы теперь можем собрать классический алгоритм табличного RL под названием Q-learning. Это метод временных разностей для вычисления оптимальной Q-функции с $\varepsilon$-жадной стратегией исследования.

## Алгоритм 10: Q-learning

Гиперпараметры: $\boldsymbol{\alpha}$ - параметр экспоненциального сглаживания, $\boldsymbol{\varepsilon}$ - параметр исследований
Инициализируем $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}, \boldsymbol{a} \in \mathcal{A}$
Наблюдаем $s_{0}$
На $k$-ом шаге:

1. с вероятностью $\varepsilon$ играем $\boldsymbol{a}_{\boldsymbol{k}} \sim \operatorname{Uniform}(\mathcal{A})$, иначе $\boldsymbol{a}_{\boldsymbol{k}}=\underset{\boldsymbol{a}_{\boldsymbol{k}}}{\operatorname{argmax}} \boldsymbol{Q}\left(\boldsymbol{s}_{\boldsymbol{k}}, \boldsymbol{a}_{\boldsymbol{k}}\right)$
2. наблюдаем $\boldsymbol{r}_{\boldsymbol{k}}, \boldsymbol{s}_{\boldsymbol{k}+1}$
3. обновляем $\boldsymbol{Q}\left(\boldsymbol{s}_{\boldsymbol{k}}, \boldsymbol{a}_{\boldsymbol{k}}\right) \leftarrow \boldsymbol{Q}\left(\boldsymbol{s}_{\boldsymbol{k}}, \boldsymbol{a}_{\boldsymbol{k}}\right)+\alpha\left(\boldsymbol{r}_{\boldsymbol{k}}+\gamma \max _{\boldsymbol{a}_{\boldsymbol{k}+1}} \boldsymbol{Q}\left(\boldsymbol{s}_{\boldsymbol{k}+1}, \boldsymbol{a}_{\boldsymbol{k}+1}\right)-\boldsymbol{Q}\left(\boldsymbol{s}_{\boldsymbol{k}}, \boldsymbol{a}_{\boldsymbol{k}}\right)\right)$

Естественно, в эпизодичных средах нужно всегда следить за флагом done и учитывать, что для терминальных состояний оценочные функции равны нулю. Мнемонически можно запомнить, что этот флаг является частью дисконтирования: домножение на 1 - done происходит всюду, где мы домножаем приближение будущей награды на $\gamma$.

Q-learning является типичным представителем off-policy алгоритмов: нам нигде не требовались сэмплы взаимодействия со средой конкретной стратегии. Наша стратегия сбора данных могла быть, вообще говоря, произвольной. Это крайне существенное свойство, потому что в таких алгоритмах возможно обучение с буфера: допустим, некоторый «эксперт» $\boldsymbol{\pi}^{\text {expert }}$ провёл много-много сессий взаимодействия со средой и собрал для нас кучу траекторий. Рассмотрим их как набор переходов. Тогда мы можем, вообще не взаимодействуя больше со средой, провести обучение $\boldsymbol{Q}^{*}$ с буфера: сэмплируем равномерно переход $\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right)$ и делаем обновление ячейки $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ по формуле (3.34). Что мы тогда выучим?

Определение 54: Для данного буфера - набора переходов $\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right)$ - будем называть эмпирическим $\boldsymbol{M D P}$ (empirical MDP) MDP с тем же пространством состояний, действий и функций награды, где функция переходов задана следующим образом:

$$
\hat{\boldsymbol{p}}\left(s^{\prime} \mid s, a\right) \vDash \frac{N\left(s, a, s^{\prime}\right)}{N(s, a)}
$$

где $\boldsymbol{N}\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}\right)$ - число троек $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$, входящих в буфер, $\boldsymbol{N}(\boldsymbol{s}, \boldsymbol{a})$ - число пар $\boldsymbol{s}, \boldsymbol{a}$, входящих в буфер.

Утверждение 31: При выполнении условий на learning rate, Q-learning, запущенный с фиксированного буфера, выучит $Q^{*}$ для эмпирического MDP.

Доказательство. Именно из эмпирического распределения $\hat{\boldsymbol{p}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ приходит $\boldsymbol{s}^{\prime}$ в формуле обновления (при равномерном сэмплировании переходов из буфера). Следовательно, для такого MDP мы и выучим оптимальную Q-функцию в силу теоремы 28.

Естественно, если буфер достаточно большой, то мы выучим очень близкую $\boldsymbol{Q}^{*}$ к настоящей. Ещё более интересно, что Q-learning как off-policy алгоритм может обучаться со своего собственного опыта - со своего же собственного буфера, составленного из порождённых очень разными стратегиями переходов.

---

Определение 55: Реплей буфер (replay buffer, experience replay) - это память со всеми собранными агентом переходами ( $s, a, r, s^{\prime}$, done).

Если взаимодействие со средой продолжается, буфер расширяется, и распределение, из которого приходит $s^{\prime}$, становится всё больше похожим на настоящее $\boldsymbol{p}\left(s^{\prime} \mid s, a\right)$; на факт сходимости это не влияет.

# Алгоритм 11: Q-learning with experience replay 

Гиперпараметры: $\boldsymbol{\alpha}$ - параметр экспоненциального сглаживания, $\boldsymbol{\varepsilon}$ - параметр исследований
Инициализируем $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}, \boldsymbol{a} \in \mathcal{A}$
Наблюдаем $s_{0}$
На $k$-ом шаге:

1. с вероятностью $\varepsilon$ играем $\boldsymbol{a}_{\boldsymbol{k}} \sim \operatorname{Uniform}(\mathcal{A})$, иначе $\boldsymbol{a}_{\boldsymbol{k}}:=\underset{a_{k}}{\operatorname{argmax}} Q\left(s_{k}, a_{k}\right)$
2. наблюддем $r_{k}, s_{k+1}$
3. кладём $s_{k}, a_{k}, r_{k}, s_{k+1}$ в буфер
4. сэмплируем случайный переход $s, a, r, s^{\prime}$ из буфера
5. обновляем $Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q\left(s^{\prime}, a^{\prime}\right)\right)$

Реплей буфер - ключевое преимущество off-policy алгоритмов: мы сможем с каждого перехода потенциально обучаться бесконечное количество раз. Это развязывает нам руки в плане соотношения числа собираемых данных и количества итераций обновления нашей модели, поскольку здесь мы можем, в общем-то, сами решать, сколько переходов на один шаг обновления мы будем проводить. Проявляется это в том, что в Q-learning, как и в любом другом off-policy алгоритме, есть два независимых этапа: сбор данных (взаимодействие со средой при помощи $\varepsilon$-жадной стратегии и сохранение опыта в памяти - первые три шага), и непосредственно обучение (сэмплирование перехода из буфера и обновление ячейки - шаги 4-5). Можно проводить несколько шагов сбора данных на одно обновление, или наоборот: несколько обновлений на один шаг сбора данных, или даже проводить эти процессы независимо параллельно.

### 3.4.8. SARSA

Мы придумали off-policy алгоритм: мы умеем оценивать нашу текущую стратегию ( $\underset{a}{\operatorname{argmax}} \boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$, неявно сидящий внутри формулы обновления), используя сэмплы другой стратегии. Иными словами, у нас в алгоритме различаются понятия целевой политики (target policy) - стратегии, которую алгоритм выдаст по итогам обучения, она же оцениваемая политика, то есть та политика, для которой мы хотим посчитать оценочную функцию - и политики взаимодействия (behavior policy) - стратегии взаимодействия со средой, стратегии с подмешанным эксплорейшном. Это различие было для нас принципиально: оптимальны детерминированные стратегии, а взаимодействовать со средой мы готовы лишь стохастичными стратегиями. У этого «несовпадения» есть следующий эффект.

Пример 59 - Cliff World: Рассмотрим MDP с рисунка с детерминированной функцией переходов, действиями вверх-вниз-вправо-влево и $\gamma<1$; за попадание в лаву начисляется огромный штраф, а эпизод прерывается. За попадание в целевое состояние агент получает +1 , и эпизод также завершается; соответственно, задача агента - как можно быстрее добраться до цели, не угодяв в лаву.

Q-learning, тем не менее, постепенно сойдётся к оптимальной стратегии: кратчайшим маршрутом агент может добраться до терминального состояния с положительной наградой. Однако даже после того, как оптимальная стратегия уже выучилась, Q-learning продолжает прыгать в лаву! Почему? Проходя прямо возле лавы, агент каждый шаг подбрасывает монетку и с вероятностью $\varepsilon$ совершает случайное действие, которое при невезении может отправить его гореть! Если речь не идёт о симуляции, подобное поведение даже во время обучения может быть крайне нежелательно.


Возможны ситуации, когда небезопасное поведение во время обучения не является проблемой, но для, например, реальных роботов сбивать пешеходов из-за случайных действий - не самая лучшая идея. Что, если мы попробуем как-то «учесть» тот факт, что мы обязаны всегда заниматься исследованиями? То есть внутри

---

оценочной функции должно закладываться, что «оптимальное» поведение в будущем невозможно, а возможно только около-оптимальное поведение с подмешиванием исследования. Для примера будем рассматривать $\varepsilon$-жадную стратегию, пока что с константным $\varepsilon$.

Рассмотрим очень похожий на Q-learning алгоритм. Будем использовать пятёрки $s, a, r, s^{\prime}, a^{\prime}$ (hence the name) прямо из траекторий нашего взаимодействия со средой и апдейтить текущее приближение Q-функции по формуле

$$
Q(s, a) \leftarrow Q(s, a)+\alpha\left(r(s, a)+\gamma Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)
$$

Какую Q-функцию такой алгоритм будет учить? Поскольку $\boldsymbol{a}^{\prime} \sim \boldsymbol{\mu}$, где $\boldsymbol{\mu}$ - стратегия взаимодействия со средой, то мы стохастически аппроксимируем $\boldsymbol{Q}^{\boldsymbol{\mu}}$ для этой самой $\boldsymbol{\mu}$, сдвигаясь в сторону решения обычного уравнения Беллмана. Формула обновления не будет эквивалентна формуле Q-learning-a (3.34), где мы сдвигались в сторону $\boldsymbol{Q}^{\boldsymbol{\pi}}$, где $\boldsymbol{\pi}$ было жадной стратегией по отношению к этой же Q-функции: да, в большинстве случаев (с вероятностью $1-\varepsilon$ ) $a^{\prime}$ будет аргмаксимумом, и обновление будет совпадать с Q-learning, но иногда $a^{\prime}$ будет оказываться тем самым «случайным» действием, случившимся из-за исследований, и наша Q-функция будет сдвигаться в сторону ценности случайных действий. Так в оценочную функцию будет попадать знание о том, что в будущем мы на каждом шаге с вероятностью $\varepsilon$ будем обязаны выбрать случайное действие, и подобное «дёрганье» будет мешать нам проходить по краю вдоль обрыва с лавой.

# Алгоритм 12: SAISSA 

Гиперпараметры: $\boldsymbol{\alpha}$ - параметр экспоненциального сглаживания, $\boldsymbol{\varepsilon}$ - параметр исследований
Инициализируем $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}, \boldsymbol{a} \in \mathcal{A}$
Наблюдаем $s_{0}$, сэмплируем $a_{0} \sim \operatorname{Uniform}(\mathcal{A})$
На $k$-ом шаге:

1. наблюдаем $r_{k}, s_{k+1}$
2. с вероятностью $\varepsilon$ играем $a_{k+1} \sim \operatorname{Uniform}(\mathcal{A})$, иначе $a_{k+1}=\underset{a_{k+1}}{\operatorname{argmax}} Q\left(s_{k+1}, a_{k+1}\right)$
3. обновляем $Q\left(s_{k}, a_{k}\right) \leftarrow Q\left(s_{k}, a_{k}\right)+\alpha\left(r_{k}+\gamma Q\left(s_{k+1}, a_{k+1}\right)-Q\left(s_{k}, a_{k}\right)\right)$

Попробуем понять формальнее, что происходит в таком алгоритме. Можно считать, что он чередует два шага: обновление (3.38), которое учит $Q^{\boldsymbol{\pi}}$ для текущей $\boldsymbol{\pi}$, и, неявно, некий аналог policy improvement-a: замены $\boldsymbol{\pi}$ на $\varepsilon$-greedy $(\boldsymbol{Q})$. Именно при помощи обновлённой стратегии мы будем взаимодействовать со средой на следующем шаге, то есть полагаем $\boldsymbol{\mu} \equiv \boldsymbol{\pi}$ («переходим в on-policy режим»). Является ли такое обновление policy improvement-ом (допустим, для идеально посчитанной $Q^{\boldsymbol{\pi}}$ )? Вообще говоря, нет, но наши стратегии $\boldsymbol{\pi}$, которые мы рассматриваем - не произвольные. Они все $\varepsilon$-жадные. Введём на минутку такое определение.
| Определение 56: Будем говорить, что стратегия $\boldsymbol{\pi}-\boldsymbol{\varepsilon}$-мягкая ( $\boldsymbol{\varepsilon}$-soft), если $\forall \boldsymbol{s}, \boldsymbol{a}: \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}) \geq \frac{\varepsilon}{|\mathcal{A}|}$.

Утверждение 32: Если $\pi_{1}-\varepsilon$-мягкая, то $\pi_{2} \equiv \varepsilon$-greedy $\left(Q^{\pi_{1}}\right)$ не хуже, чем $\pi_{1}$.
Доказательство. Проверим выполнение теоремы 17 ; выглядит немного страшновато, но суть этих выкладок довольно лобовая: если мы переложим всю вероятностную массу в самое «хорошее» с точки зрения $Q^{\pi_{1}}(s, a)$, оставив у остальных, не самых лучших, действий ровно $\frac{\varepsilon}{|\mathcal{A}|}$, то среднее значение увеличится.

$$
\begin{aligned}
V^{\pi_{1}}(s) & =\{\text { уравнение } \mathrm{VQ}(3.6)\}=\sum_{a} \pi_{1}(a \mid s) Q^{\pi_{1}}(s, a)= \\
& =\sum_{a}\left(\pi_{1}(a \mid s)-\frac{\varepsilon}{|\mathcal{A}|}\right) Q^{\pi_{1}}(s, a)+\frac{\varepsilon}{|\mathcal{A}|} \sum_{a} Q^{\pi_{1}}(s, a)= \\
& =(1-\varepsilon) \sum_{a} \frac{\pi_{1}(a \mid s)-\frac{\varepsilon}{|\mathcal{A}|}}{1-\varepsilon} Q^{\pi_{1}}(s, a)+\frac{\varepsilon}{|\mathcal{A}|} \sum_{a} Q^{\pi_{1}}(s, a) \leq \\
& \leq(1-\varepsilon) \max _{a} Q^{\pi_{1}}(s, a) \sum_{a} \frac{\pi_{1}(a \mid s)-\frac{\varepsilon}{|\mathcal{A}|}}{1-\varepsilon}+\frac{\varepsilon}{|\mathcal{A}|} \sum_{a} Q^{\pi_{1}}(s, a)
\end{aligned}
$$

Заметим, что последний переход был возможен только потому, что $\pi_{1}(a \mid s)-\frac{\varepsilon}{|\mathcal{A}|}>0$ по условию, так как

---

$\pi_{1}-\varepsilon$-мягкая по условию. Осталось заметить, что

$$
\sum_{a} \frac{\pi_{1}(a \mid s)-\frac{\varepsilon}{|\mathcal{A}|}}{1-\varepsilon}=\frac{\sum_{a} \pi_{1}(a \mid s)-\varepsilon}{1-\varepsilon}=1
$$

и мы показали, что $V^{\pi_{1}}(s) \leq \mathbb{E}_{\pi_{2}(a \mid s)} Q^{\pi_{1}}(s, a)$.
Давайте изменим постановку задачи: скажем, что мы запрещаем к рассмотрению стратегии, «похожие на детерминированные». Наложим ограничение в нашу задачу оптимизации: скажем, что стратегия обязательно должна быть $\varepsilon$-мягкой. В такой задаче будут свои оптимальные оценочные функции.

Определение 57: Для данного MDP оптимальными $\varepsilon$-мягкими оценочными функциями назовём:

$$
\begin{aligned}
V_{\varepsilon-\text { soft }}^{*}(s) & :=\max _{\pi \in \varepsilon-\text { soft }} V^{\pi}(s) \\
Q_{\varepsilon-\text { soft }}^{*}(s, a) & :=\max _{\pi \in \varepsilon-\text { soft }} Q^{\pi}(s, a)
\end{aligned}
$$

Как тогда будет выглядеть принцип оптимальности? Раньше нужно было выбирать самое хорошее действие, но теперь так делать нельзя. Проводя аналогичные рассуждения, можно показать, что теперь оптимально выбирать самое хорошее действие с вероятностью $1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}|}$, а всем остальным действиям выдавать минимально разрешённую вероятность $\frac{\varepsilon}{|\mathcal{A}|}$. Как это понять? Мы уже поняли, что «взятие $\varepsilon$-жадной» стратегии есть местный Policy Improvement. Если мы не можем его про-


вести ни в одном состоянии, а то есть $\boldsymbol{\pi} \equiv \varepsilon$-greedy $\left(Q^{\pi}\right)$, то, видимо, придумать стратегию лучше в принципе невозможно:

Утверждение 33: Стратегия $\boldsymbol{\pi}$ оптимальна в классе $\varepsilon$-мягких стратегий тогда и только тогда, когда $\forall \boldsymbol{s}, \boldsymbol{a}$ таких, что $\boldsymbol{a} \notin \operatorname{Argmax} \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ верно $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})=\frac{\varepsilon}{|\mathcal{A}|}$.

Скетч доказательства. Притворимся, что в будущем мы сможем выбирать действия как угодно $\varepsilon$ мягко, то есть для данного состояния $\boldsymbol{s}$ для каждого действия $\boldsymbol{a}$ сможем в будущем набирать $Q_{\varepsilon-\text { soft }}^{*}(s, a)$. Как нужно выбрать действия сейчас? Нужно решить такую задачу оптимизации:

$$
\left\{\begin{array}{l}
\mathbb{E}_{\pi(a \mid s)} Q_{\varepsilon-\text { soft }}^{*}(s, a) \rightarrow \max \\
\int_{\mathcal{A}} \pi(a \mid s) \mathrm{d} a=1 ; \quad \forall \bar{a} \in \mathcal{A}: \pi(a \mid s) \geq \frac{\varepsilon}{|\mathcal{A}|}
\end{array}\right.
$$

Формально решая эту задачу условной оптимизации, получаем доказываемое.

Утверждение 34: Уравнения оптимальности, соответственно, теперь выглядят так:

$$
Q_{\varepsilon-\text { soft }}^{*}(s, a)=r+\gamma \mathbb{E}_{\boldsymbol{s}^{\prime}}\left[(1-\varepsilon) \max _{\boldsymbol{a}^{\prime}} Q_{\varepsilon-\text { soft }}^{*}\left(s^{\prime}, a^{\prime}\right)+\frac{\varepsilon}{|\mathcal{A}|} \sum_{\boldsymbol{a}^{\prime}} Q_{\varepsilon-\text { soft }}^{*}\left(s^{\prime}, a^{\prime}\right)\right]
$$

Доказательство. Их можно получить, например, взяв обычное уравнение QQ (3.7) и подставив вид оптимальной стратегии.

Мы теперь понимаем, что наша формула обновления (3.38) - просто метод решения такого уравнения оптимальности: сэмплируя $\boldsymbol{a}^{\prime}$ из $\varepsilon$-жадной стратегии, мы просто стохастически аппроксимируем по мат.ожиданию из $\varepsilon$-жадной стратегии. Мы могли бы, вообще говоря, взять это мат.ожидание полностью явно, сбив таким образом дисперсию:

$$
Q(s, a) \leftarrow Q(s, a)+\alpha\left(r(s, a)+\gamma \mathbb{E}_{a^{\prime} \sim \pi} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)
$$

где мат.ожидание по $\boldsymbol{a}^{\prime}$ по определению $\boldsymbol{\pi}$ равно

$$
\mathbb{E}_{\boldsymbol{a}^{\prime} \sim \pi} Q\left(s^{\prime}, a^{\prime}\right)=(1-\varepsilon) \max _{\boldsymbol{a}^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)+\frac{\varepsilon}{|\mathcal{A}|} \sum_{\boldsymbol{a}^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)
$$

Такая схема называется Expected SARSA - «SARSA, в которой взяли мат.ожидание». Мы всё ещё учим Qфункцию текущей политики, но не используем сэмпл из текущей траектории, а вместо этого берём мат.ожидание по действиям из уравнения Беллмана (3.7) честно. Вообще говоря, в такой схеме мы работаем не с пятёрками $s, a, r, s^{\prime}, a^{\prime}$, а с четвёрками $s, a, r, s^{\prime}$ (несмотря на название).

---

Гиперпараметры: $\boldsymbol{\alpha}$ - параметр экспоненциального сглаживания, $\boldsymbol{\varepsilon}$ - параметр исследований
Инициализируем $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ произвольно для всех $\boldsymbol{s} \in \mathcal{S}, \boldsymbol{a} \in \mathcal{A}$
Инициализируем $\boldsymbol{\pi}_{0}$ произвольно
Наблюдаем $s_{0}$
На $k$-ом шаге:

1. играем $a_{k} \sim \pi_{k}\left(a_{k} \mid s_{k}\right)$
2. наблюдаем $r_{k}, s_{k+1}$
3. $\pi_{k+1}$ есть с вероятностью $\varepsilon$ выбрать $a_{k+1} \sim \operatorname{Uniform}(\mathcal{A})$, иначе $a_{k+1}:=\underset{a_{k+1}}{\operatorname{argmax}} Q\left(s_{k+1}, a_{k+1}\right)$
4. обновляем $Q\left(s_{k}, a_{k}\right) \leftarrow Q\left(s_{k}, a_{k}\right)+\alpha\left(r_{k}+\gamma \mathbb{E}_{a_{k+1} \sim \pi_{k+1}} Q\left(s_{k+1}, a_{k+1}\right)-Q\left(s_{k}, a_{k}\right)\right)$

Соответственно, и SARSA, и Expected SARSA будут сходиться уже не к обычной оптимальной оценочной функции, а к $Q_{\varepsilon \text {-soff }}^{*}(s, a)$ - оптимальной оценочной функции в семействе $\varepsilon$-мягких стратегий.

Пример 60 - Cliff World: Попробуем запустить в MDP из примера 59 SARSA. Мы сойдёмся вовсе не к оптимальной стратегии - а «к безопасной» оптимальной стратегии. Внутри нашей оценочной функции сидит вероятность сорваться в лаву при движении вдоль неё, и поэтому кратчайший маршрут перестанет давать нам наибольшую награду просто потому, что стратегии, для которых такой маршрут был оптимален, мы перестали допускать к рассмотрению.


Принципиальное отличие схемы SARSA от Q-learning в том, что мы теперь учимся ровно на тех же сэмплах действий $\boldsymbol{a}^{\prime}$, которые отправляем в среду. Наши behavior и target policy теперь совпадают: для очередного шага алгоритма нужно сделать шаг в среде при помощи текущей $\boldsymbol{\pi}$, и поэтому мы должны учиться онлайн, «в on-policy режиме».

Чтобы понять, к чему это приводит, рассмотрим, что случится, если взять буфер некоторого эксперта $\boldsymbol{\pi}_{\text {expert }}$, генерировать пятёрки $s, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}$ из него и проводить обновления (3.38) по ним. Что мы выучим? Применяем наш стандартный ход рассуждений: $\boldsymbol{a}^{\prime} \sim \boldsymbol{\pi}_{\text {expert }}\left(\boldsymbol{a}^{\prime} \mid \boldsymbol{s}^{\prime}\right)$, и, значит, мы учим $\boldsymbol{Q}^{\boldsymbol{\pi}_{\text {expert }}}$ ! Если же мы попробуем запустить SARSA с experience replay, то есть обучаться на собственной же истории, то мы вообще творим полную ахинею: на каждом шаге мы движемся в сторону Q-функции для той стратегии $\boldsymbol{\pi}$, которая породила засэмплированный переход (например, если переход был засэмплирован откуда-то из начала обучения - скорее всего случайной или хуже). Такой алгоритм не просто будет расходиться, но и не будет иметь никакого смысла. Поэтому SARSA нельзя (в таком виде, по крайней мере) запустить с реплей буфера.

# §3.5. Bias-Variance Trade-Off 

### 3.5.1. Дилемма смещения-разброса

Мы обсудили два вида бэкапов, доступных в model-free обучении: Монте-Карло бэкап и Temporal-Difference бэкап. На самом деле, они очень похожи, поскольку делают обновление вида

$$
Q(s, a) \leftarrow Q(s, a)+\alpha\left(y_{Q}-Q(s, a)\right)
$$

и отличаются лишь выбором $\boldsymbol{y}_{\boldsymbol{Q}}$ : Монте-Карло берёт reward-to-go, a TD-backup - одношаговую бутстрапированную оценку с использованием уже имеющейся аппроксимации Q-функции:

$$
y_{Q}:=r(s, a)+\gamma Q\left(s^{\prime}, a^{\prime}\right)
$$

Какой из этих двух вариантов лучше? Мы уже обсуждали недостатки Монте-Карло оценок: высокая дисперсия, необходимость играть до конца эпизодов, игнорирование структуры получаемой награды и потеря информации о соединениях состояний. Но не то, чтобы одношаговые оценки сильно лучше: на самом деле, они обладают полностью противоположными свойствами и проблемами.

Да, одношаговые оценки аппроксимируют решение одношаговых уравнений Беллмана и приближают алгоритм динамического программирования: поэтому они не теряют информации о том, на каком шаге какой сигнал от среды был получен, и сохраняют информацию о сэмплах $s^{\prime}$ из функции переходов; в том числе, как

---

мы видели, одношаговые алгоритмы могут использовать реплей буфер, по сути и хранящий собранную выборку таких сэмплов. Взамен в одношаговых алгоритмах возникает проблема распространения сигнала.

Пример 61: Представьте, что за 100 шагов вы можете добраться до сыра (+1). Пока вы не добьётесь успеха, сигнала нет, и ваша аппроксимация Q-функции остаётся всюду нулём. Допустим, вы учитесь с одношаговых оценок с онлайн опыта. После первого успеха +1 распространится лишь в пару $\boldsymbol{s}, \boldsymbol{a}$, непосредственно предшествующей получению сыра; после второго успеха +1 распространится из пары $\boldsymbol{s}, \boldsymbol{a}$ в предыдущую и так далее. Итого, чтобы распространить сигнал на 100 шагов, понадобится сделать 100 обновлений. С другой стороны, если бы использовалась Монте-Карло оценка, после первого же успеха +1 распространился бы во все пары $s, a$ из успешной траектории.

Вместо высокой дисперсии Монте-Карло оценок в одношаговых оценках нас ждёт большое смещение (bias): если $\boldsymbol{y}_{\boldsymbol{Q}}$ оценено через нашу же текущую аппроксимацию через бутстрапирование, то оно не является несмещённой оценкой искомой $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ и может быть сколь угодно «неправильным». Как мы увидели, гарантии сходимости остаются, но естественно, что методы стохастической аппроксимации из-за смещения будут сходиться сильно дольше экспоненциального сглаживания, которому на вход поступают несмещённые оценки искомой величины. Но дисперсия $\boldsymbol{y}_{\boldsymbol{Q}}$ в temporal difference обновлениях, например, в алгоритме Q-learning 10, конечно, сильно меньше дисперсии Монте-Карло оценок: внутри нашей аппроксимации Q-функции уже усреднены все будущие награды, то есть «взяты» все интегралы, относящиеся к будущим после первого шага наградам. Итого одношаговая оценка $\boldsymbol{y}_{\boldsymbol{Q}}$ - случайная величина только от $\boldsymbol{s}^{\prime}$, а не от всего хвоста траектории $\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}, \boldsymbol{s}^{\prime \prime} \ldots$. Выбор между оценками с высокой дисперсией и отсутствием смещения (Монте-Карло) и оценками с низкой дисперсией и большим смещением (одношаговые оценки) - особая задача в обучении с подкреплением, называемая bias-variance trade-off.

# 3.5.2. N-step Temporal Difference 

Какие есть промежуточные варианты между одношаговыми оценками и Монте-Карло оценками? Давайте заглядывать в будущее не на один шаг и не до самого конца, а на $\boldsymbol{N}$ шагов. Итак, пусть у нас есть целый фрагмент траектории:

Определение 58: Фрагмент траектории $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}, \boldsymbol{r}^{\prime}, \boldsymbol{s}^{\prime \prime}, \boldsymbol{a}^{\prime \prime}, \boldsymbol{r}^{\prime \prime}, \ldots \boldsymbol{s}^{(\boldsymbol{N})}, \boldsymbol{a}^{(\boldsymbol{N})}$, где $\boldsymbol{s}^{(\boldsymbol{N})}, \boldsymbol{a}^{(\boldsymbol{N})}$ - состояние и действие, которое агент встретил через $\boldsymbol{N}$ шагов, будем называть роллаутом (rollout) длины $\boldsymbol{N}$.

Определение 59: $\boldsymbol{N}$-шаговой оценкой (N-step estimation) для $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ назовём следующий таргет:

$$
y_{Q}:=r+\gamma r^{\prime}+\gamma^{2} r^{\prime \prime}+\cdots+\gamma^{N-1} r^{(N-1)}+\gamma^{N} Q\left(s^{(N)}, a^{(N)}\right)
$$

Такой таргет является стохастической аппроксимацией правой части $\boldsymbol{N}$-шагового уравнения Беллмана (3.25), и формула (3.40) с таким таргетом позволяет эту систему уравнений решать в model-free режиме. По каким переменным мы заменили интегралы на Монте-Карло приближения в такой оценке? По переменным $s^{\prime}, a^{\prime}, s^{\prime \prime}, a^{\prime \prime} \ldots s^{(N)}, a^{(N)}$, которые, пусть и не все присутствуют явно в формуле, но неявно задают то уравнение, которое мы решаем. Соответственно, чтобы выучить $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, нужно, чтобы состояния приходили из функции переходов, а действия - из оцениваемой стратегии $\boldsymbol{\pi}$. Другими словами, роллаут, использованный для построения таргета, должен быть порождён оцениваемой стратегией.

Почему гиперпараметр $\boldsymbol{N}$ отвечает за bias-variance trade-off? Понятно, что при $\boldsymbol{N} \rightarrow \infty$ оценка переходят в Монте-Карло оценку. С увеличением $\boldsymbol{N}$ всё больше интегралов заменяется на Монте-Карло оценки, и растёт дисперсия; наше же смещённое приближение будущих наград $\boldsymbol{Q}^{\boldsymbol{\pi}}\left(\boldsymbol{s}^{(\boldsymbol{N})}, \boldsymbol{a}^{(\boldsymbol{N})}\right)$, которое может быть сколь угодно неверным, домножается на $\gamma^{N}$, и во столько же раз сбивается потенциальное смещение; с ростом $\boldsymbol{N}$ оно уходит в ноль. Замешивая в оценку слагаемое $\boldsymbol{r}+\gamma \boldsymbol{r}^{\prime}+\gamma^{2} \boldsymbol{r}^{\prime \prime}+\cdots+\gamma^{N-1} \boldsymbol{r}^{(N-1)}$ мы теряем информацию о том, что из этого в какой момент было получено, но и начинаем распространять сигнал в $\boldsymbol{N}$ раз «быстрее» одношаговой оценки.

Сразу заметим, что при $\boldsymbol{N}>\mathbf{1}$ нам необходимо иметь в $\boldsymbol{N}$-шаговой оценке сэмплы $\boldsymbol{a}^{\prime} \sim \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}), \boldsymbol{s}^{\prime \prime} \sim$ $\sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime \prime} \mid \boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$. Это означает, что мы не можем получить такую оценку с буфера: действительно, в буфере для данной четвёрки $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$ не лежит сэмпл $\boldsymbol{a}^{\prime} \sim \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$, ведь в произвольном буфере $\boldsymbol{a}^{\prime}$ генерируется стратегией сбора данных (старой версией стратегией или «экспертом»). Само по себе это, вообще говоря, не беда: мы могли бы взять из буфера четвёрку, прогнать стратегию $\boldsymbol{\pi}$, которую хотим оценить, на $\boldsymbol{s}^{\prime}$ и сгенерировать себе сэмпл $\boldsymbol{a}^{\prime}$; но для него мы не сможем получить сэмпл $\boldsymbol{s}^{\prime \prime}$ из функции переходов! Поэтому обучаться на многошаговые оценки с буфера не выйдет; по крайней мере, без какой-либо коррекции.

Также отметим, что все рассуждения одинаковы применимы как для обучения $\boldsymbol{Q}^{\boldsymbol{\pi}}$, так и $\boldsymbol{V}^{\boldsymbol{\pi}}$. Для V -функции общая формула обновления выглядит так:

$$
V(s) \leftarrow V(s)+\alpha\left(y_{V}-V(s)\right)
$$

где $\boldsymbol{y}_{\boldsymbol{V}}$ - reward-to-go при использовании Монте-Карло оценки, и $\boldsymbol{y}_{\boldsymbol{V}}:=\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})+\gamma \boldsymbol{V}\left(\boldsymbol{s}^{\prime}\right)$ для одношагового метода временных разностей, где $\boldsymbol{a} \sim \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$ (сэмплирован из текущей оцениваемой стратегии), $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid\right.$

---

$\mid \boldsymbol{s}, \boldsymbol{a})$. Соответственно, для V-функции обучаться с буфера (без каких-либо коррекций) невозможно даже при $\boldsymbol{N}=\mathbf{1}$, поскольку лежащий в буфере $\boldsymbol{a}$ сэмплирован из стратегии сбора данных, а не оцениваемой $\boldsymbol{\pi}$.

Для простоты и наглядности будем обсуждать обучение $\boldsymbol{V}^{\boldsymbol{\pi}}$. Также введём следующие обозначения:
Определение 60: Введём такое обозначение $\boldsymbol{N}$-шаговой временной разности (N-step temporal difference) для пары $\boldsymbol{s}, \boldsymbol{a}$ :

$$
\Psi_{(N)}(s, a) \equiv \sum_{t=0}^{N-1} \gamma^{t} r^{(t)}+\gamma^{N} V\left(s^{(N)}\right)-V(s)
$$

где $\boldsymbol{V}$ - текущая аппроксимация V-функции, $\boldsymbol{s}, \boldsymbol{a}, \ldots \boldsymbol{s}^{(\boldsymbol{N})}$ - роллаут, порождённый $\boldsymbol{\pi}$.
Ранее мы обсуждали temporal difference, в котором мы сдвигали нашу аппроксимацию на $\boldsymbol{\Psi}_{(1)}(s, a)$ :

$$
\begin{aligned}
V(s) & \leftarrow V(s)+\alpha\left(r+\gamma V\left(s^{\prime}\right)-V(s)\right)= \\
& =V(s)+\alpha \Psi_{(1)}(s, a)
\end{aligned}
$$

Теперь же мы можем обобщить наш метод, заменив оценку V-функции на многошаговую оценку:

$$
V(s) \leftarrow V(s)+\alpha \Psi_{(N)}(s, a)
$$

Но какое $\boldsymbol{N}$ выбрать?

# 3.5.3. Интерпретация через Credit Assingment 

Вопрос, обучаться ли со смещённых оценок, или с тех, которые имеют большую дисперсию, имеет прямое отношение к одной из центральных проблем RL - credit assignment. На самом деле, это ровно та же самая проблема.

Рассмотрим проблему credit assignment-a: за какие будущие награды «в ответе» то действие, которое было выполнено в некотором состоянии $\boldsymbol{s}$ ? Как мы обсуждали в разделе 3.2.1, «идеальное» решение задачи - значение $\boldsymbol{A}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, но на практике у нас нет точных значений оценочных функций, а есть лишь аппроксимация, допустим, $\boldsymbol{V} \approx \boldsymbol{V}^{\boldsymbol{\pi}}$. Положим, мы знаем сэмпл траектории $\boldsymbol{a}, \boldsymbol{s}^{\prime}, \boldsymbol{r}, \boldsymbol{s}^{\prime \prime}, \boldsymbol{a}^{\prime \prime}, \ldots$ до конца эпизода.

С одной стороны, мы можем выдавать кредит, полностью опираясь на аппроксимацию:

$$
\begin{gathered}
Q^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi}\left(s^{\prime}\right) \approx r(s, a)+\gamma V\left(s^{\prime}\right) \\
A^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s) \approx r(s, a)+\gamma V\left(s^{\prime}\right)-V(s)
\end{gathered}
$$

Проблема в том, что наша аппроксимация может быть сколь угодно невериа, и выдавать полную ерунду. Более «безопасный» с этой точки зрения способ - в приближении Q-функции не опираться на аппроксимацию и использовать reward-to-go:

$$
A^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s) \approx \sum_{t \geq 0} \gamma^{t} r^{(t)}-V(s)
$$

У этого второго способа выдавать кредит есть важное свойство, которого нет у первого: в среднем такой кредит будет больше у тех действий, которые действительно приводят к более высокой награде. То есть, если $\boldsymbol{a}_{1}, \boldsymbol{a}_{2}$ таковы, что $\boldsymbol{Q}^{\boldsymbol{\pi}}\left(\boldsymbol{s}, \boldsymbol{a}_{\mathbf{1}}\right)>\boldsymbol{Q}^{\boldsymbol{\pi}}\left(\boldsymbol{s}, \boldsymbol{a}_{\mathbf{2}}\right)$, то при любой аппроксимации $\boldsymbol{V}(\boldsymbol{s})$ среднее значение кредита для $\boldsymbol{s}, \boldsymbol{a}_{\mathbf{1}}$ будет больше $\boldsymbol{s}, \boldsymbol{a}_{\mathbf{2}}$. Это и есть свойство несмещённости Монте-Карло оценок в контексте проблемы выдачи кредита.

Беда Монте-Карло в том, что в этот кредит закладывается награда не только за выбор $\boldsymbol{a}$ в $\boldsymbol{s}$, но и за будущие решения. То есть: представьте, что через десять шагов агент выбрал действие, которое привело к +100 . Эта награда +100 попадёт и в кредит всех предыдущих действий, хотя те не имели к нему отношения.

Пример 62: Допустим, мы ведём машину, и в состоянии $\boldsymbol{s}$, где аппроксимация V-функции прогнозирует будущую награду ноль, решили выехать на встречку. Среда не сообщает нам никакого сигнала (пока не произошло никакой аварии), но аппроксимация V-функции резко упала; если мы проводим credit assignment одношаговым способом (3.42), то мы получаем сильно отрицательный кредит, который сообщает, что это действие было явно плохим.

Дальше агент следующими действиями исправил ситуацию, вернулся на правильную полосу и после решил поехать в магазин тортиков, где оказался юбилейным покупателем и получил бесплатный тортик +10 . Если бы мы проводили credit assignment методом Монте-Карло (3.43), кредит получился бы сильно положительным: $\boldsymbol{+ 1 0}-\mathbf{0}=\mathbf{+ 1 0}$ : мы положили, что выезд на встречку привёл к тортику.

Видно, что эти два крайних способа выдачи кредита есть в точности «градиент» $\boldsymbol{y}_{\boldsymbol{V}}-\boldsymbol{V}(\boldsymbol{s})$, по которому учится V-функция в формуле (3.40). Фактически, занимаясь обучением V-функции и используя формулу

$$
V(s) \leftarrow V(s)+\alpha \Psi(s, a)
$$

---

мы выбором функции $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$ по-разному проводим credit assignment.
Таким образом видна новая интерпретации bias-variance trade-off-а: и «дисперсия» с этой точки зрения имеет смысл возложения на действие ответственности за те будущие награды, к которым это действие отношения на самом деле не имеет. Одношаговая оценка $\boldsymbol{\Psi}_{(1)}$ говорит: действие влияет только на награду, которую агент получит тут же, и весь остальной сигнал будет учтён в аппроксимации V-функции. Монте-Карло оценка $\boldsymbol{\Psi}_{(\infty)}$ говорит, что действие влияет на все будущие награды. А $\boldsymbol{N}$-шаговая оценка $\boldsymbol{\Psi}_{(N)}$ говорит странную вещь: действие влияет на события, который происходят с агентом в течение следующих $\boldsymbol{N}$ шагов.

Как и в любом trade-off, истина лежит где-то по середине. Однако подбирать на практике «хорошее» $\boldsymbol{N}$, чтобы получить оценки с промежуточным смещением и дисперсией, затруднительно. Но что ещё важнее, все $\boldsymbol{N}$-шаговые оценки на самом деле неудачные. Во-первых, они плохи тем, что не используют всю доступную информацию: если мы знаем будущее на $\boldsymbol{M}$ шагов вперёд, то странно не использовать награды за шаг за все эти $\boldsymbol{M}$ шагов, и странно не учесть прогноз нашей аппроксимации оценочной функции $\boldsymbol{V}\left(\boldsymbol{s}^{\boldsymbol{t} \boldsymbol{t}}\right)$ для всех доступных $\boldsymbol{t}$. Во-вторых, странно, что в стационарной задаче, где всё инвариантно относительно времени, у нас появляется гиперпараметр, имеющий смысл количества шагов - времени. Поэтому нас будет интересовать далее альтернативный способ «интерполировать» между Монте-Карло и одношаговыми оценками. Мы всё равно оттолкнёмся именно от $\boldsymbol{N}$-шаговых оценок, поскольку понятно, что эти оценки «корректны»: они направляют нас к решению уравнений Беллмана, для которых искомая $\boldsymbol{V}^{\boldsymbol{\pi}}$ является единственной неподвижной точкой.

Мы придумали эти $\boldsymbol{N}$-шаговые оценки, посмотрев на задачу под следующим углом: мы знаем сэмпл будущего (хвост траектории) и хотим выдать кредит «настоящему»: самому первому действию. Такой взгляд на оценки называется «forward view»: мы после выполнения $\boldsymbol{a}$ из $\boldsymbol{s}$ знаем «вперёд» своё будущее и можем обновить оценочную функцию для этой пары.


# 3.5.4. Backward View 

Оказывается, мы можем посмотреть на задачу немного по-другому: можно, используя настоящее, обновлять кредит для прошлого. Рассмотрим эту идею, развив пример с кафе 57.

Пример 63: Ещё раз сядем в кафе ( $\boldsymbol{s}$ ) и захотим вернуться домой. Текущая аппроксимация даёт $-\boldsymbol{V}(\boldsymbol{s})=\mathbf{3 0}$ минут. Делаем один шаг в среде: тратим одну минуту $(-\boldsymbol{r})$ и обнаруживаем пробку $\left(s^{\prime}\right)$. Новая оценка времени возвращения даёт: $-\boldsymbol{V}\left(\boldsymbol{s}^{\prime}\right)=40$ минут, соответственно с нами случилась одношаговая временная разность $\boldsymbol{\Psi}_{(1)}(s, a)=41-30=11$ минут, которая позволяет нам корректировать $\boldsymbol{V}(s)$.

Давайте сделаем ещё один шаг в среде: тратим одну минуту $-\boldsymbol{r}^{\prime}$, видим пожар $\boldsymbol{s}^{\prime \prime}$ и получаем новую оценку $-\boldsymbol{V}\left(\boldsymbol{s}^{\prime \prime}\right)=\mathbf{6 0}$ минут. Тогда мы можем посчитать как одношаговую временную разность для пары $\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}$, равную $\boldsymbol{\Psi}_{(1)}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)=\mathbf{6 1}-\mathbf{4 0}=\mathbf{2 1}$ минуте, и уточнить свою аппроксимацию V-функции для состояния с пробкой; а можем посчитать и двухшаговую временную разность для кафе: мы потратили две минуты $\boldsymbol{r}+\boldsymbol{r}^{\prime}$ на два шага и наша нынешнее приближение равно 60 минутам. Итого двухшаговая временная разность равна $\boldsymbol{\Psi}_{(2)}(s, a)=62-30=32$ минуты. Forward view говорит следующее: если мы хотим учиться на двухшаговые оценки вместо


одношаговых, то нам не следовало на первом шаге использовать 11 минут для обновления $\boldsymbol{V}(\boldsymbol{s})$, а нужно было дождаться второго шага, узнать двухшаговую ошибку в 32 минуты и воспользоваться ей.

Но понятно, что двухшаговая ошибка это сумма двух одношаговых ошибок! Наши 32 минуты ошибки это 11 минут ошибки после выхода из кафе в пробку плюс 21 минута ошибки от выхода из пробки в пожар. Давайте после первого шага используем уже известную часть ошибки в 11 минут, а на втором шаге, если мы готовы обучаться с двухшаговых ошибок, возьмём и добавим недостающие 21 минуту.

Формализуем эту идею. Пусть мы взаимодействуем в среде при помощи стратегии $\boldsymbol{\pi}$, которую хотим оценить; также будем считать learning rate $\boldsymbol{\alpha}$ константным. После совершения первого шага «из кафе» мы можем, зная $s, a, r, s^{\prime}$ сразу же обновить нашу V-функцию:

$$
V(s) \leftarrow V(s)+\alpha \Psi_{(1)}(s, a)
$$

Затем мы делаем ещё один шаг в среде, узнавая $\boldsymbol{a}^{\prime}, \boldsymbol{r}^{\prime}, \boldsymbol{s}^{\prime \prime}$ и временную разность за этот случившийся шаг $\boldsymbol{\Psi}_{(1)}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$. Тогда мы можем просто ещё в нашу оценку V-функции добавить слагаемое:

$$
V(s) \leftarrow V(s)+\alpha \gamma \Psi_{(1)}\left(s^{\prime}, a^{\prime}\right)
$$

Непосредственной проверкой легко убедиться, что суммарное обновление V-функции получится эквивалентным двухшаговому обновлению:

---

Утверждение 35: Последовательное применение обновлений (3.44) и (3.45) эквивалентно двухшаговому обновлению

$$
V(s) \leftarrow V(s)+\alpha \Psi_{(2)}(s, a)
$$

Доказательство.

$$
\begin{aligned}
V(s) & \leftarrow V(s)+\alpha\left(\Psi_{(1)}(s, a)+\gamma \Psi_{(1)}\left(s^{\prime}, a^{\prime}\right)\right)= \\
& =V(s)+\alpha\left(r+\gamma V\left(s^{\prime}\right)-V(s)+\gamma r^{\prime}+\gamma^{2} V\left(s^{\prime \prime}\right)-\gamma V\left(s^{\prime}\right)\right)= \\
& =V(s)+\alpha\left(r+\gamma r^{\prime}+\gamma^{2} V\left(s^{\prime \prime}\right)-V(s)\right)= \\
& =V(s)+\alpha \Psi_{(2)}(s, a)
\end{aligned}
$$

Поиятно, что можно обобщить эту идею с двухшаговых ошибок на $\boldsymbol{N}$-шаговые: действительно, ошибка за $N$ шагов равна сумме одношаговых ошибок за эти шаги.

# Теорема 31: 

$$
\Psi_{(N)}(s, a)=\sum_{t=0}^{N-1} \gamma^{t} \Psi_{(1)}\left(s^{(t)}, a^{(t)}\right)
$$

Доказательство. Докажем по индукции. Для $N=1$ справа стоит только одно слагаемое, $\boldsymbol{\Psi}_{(1)}(s, a)$, то есть одношаговая оценка.

Пусть утверждение верно для $\boldsymbol{N}$, докажем для $\boldsymbol{N}+\mathbf{1}$. В правой части при увеличении $\boldsymbol{N}$ на единицу появляется одно слагаемое, то есть для доказательства достаточно показать, что

$$
\Psi_{(N+1)}(s, a)=\Psi_{(N)}(s, a)+\gamma^{N} \Psi_{(1)}\left(s^{(N)}, a^{(N)}\right)
$$

Убедимся в этом, подставив определения:


$$
\begin{aligned}
\Psi_{(N+1)}(s, a) & =\sum_{t=0}^{N} \gamma^{t} r^{(t)}+\gamma^{N+1} V\left(s^{(N+1)}\right)-V(s)= \\
& =\sum_{t=0}^{N-1} \gamma^{t} r^{(t)}+\gamma^{N} V\left(s^{(N)}\right)-V(s)+\gamma^{N}\left(r^{(N)}+\gamma V\left(s^{(N+1)}\right)-V\left(s^{(N)}\right)\right)= \\
& =\Psi_{(N)}(s, a)+\gamma^{N} \Psi_{(1)}\left(s^{(N)}, a^{(N)}\right)
\end{aligned}
$$

Это наблюдение открывает, что все наши формулы обновления выражаются через одношаговые ошибки $\boldsymbol{\Psi}_{(1)}(s, a)$. Это интересный факт, поскольку одношаговая временная разность

$$
\Psi_{(1)}(s, a)=r+\gamma V\left(s^{\prime}\right)-V(s)
$$

очень похожа на reward shaping (1.7), где в качестве потенциала выбрана наша текущая аппроксимация $\boldsymbol{V}(\boldsymbol{s})$. Поэтому эти $\boldsymbol{\operatorname { g e л ь т ы }}$, как их ещё иногда называют, можно интерпретировать как некие «новые награды», центрированные - которые в среднем должны быть равны нулю, если аппроксимация V-функции точная.

Итого, оказывается, мы можем на каждом шаге добавлять к оцен-


кам V-функции ранее встречавшихся в эпизоде пар $s, a$ только что случившуюся одношаговую ошибку $\boldsymbol{\Psi}_{(1)}(s, a)$ и таким образом получать $\boldsymbol{N}$-шаговые обновления: достаточно пару $\boldsymbol{s}, \boldsymbol{a}$, посещённую $\boldsymbol{K}$ шагов назад, обновить с весом $\boldsymbol{\gamma}^{\boldsymbol{N}}$. То есть мы начинаем действовать по-другому: зная, что было в прошлом, мы правильным образом обновляем оценочную функцию посещённых состояний из прошлого, используя временную разность за один последний шаг («настоящее»). Такой подход к обновлению оценочной функции называется, соответственно, «backward view», и он позволяет взглянуть на обучение оценочных функций под другим углом.

---

# 3.5.5. Eligibility Trace 

Рассмотрим случай $\boldsymbol{N}=+\infty$, то есть допустим, что мы готовы обновлять V-функцию с reward-to-go. Наше рассуждение, можно сказать, позволяет теперь это делать, не доигрывая эпизоды до конца: мы сразу же в ходе эпизода можем уже «начинать» сдвигать V-функцию в правильном направлении. Это позволяет запустить «как бы Монте-Карло» алгоритм даже в неэпизодичных средах. Однако, на каждом шаге нам нужно перебирать все встретившиеся ранее в эпизоде состояния, чтобы обновить их, и, если эпизоды длинные (или среда неэпизодична), это хранение истории на самом деле становится избыточным.

Допустим, мы сделали шаг в среде и получили на этом одном шаге какую-то одношаговую ошибку $\boldsymbol{\Psi}_{(\mathbf{1})}$. Рассмотрим какое-нибудь состояние $\boldsymbol{s}$. С каким весом, помимо learning rate, нужно добавить эту ошибку к нашей текущей аппроксимации? Это состояние в течение прошлого эпизода было, возможно, посещено несколько раз, и за каждое посещение вес увеличивается на $\gamma^{\boldsymbol{K}}$, где $\boldsymbol{K}$ - число шагов с момента посещения. Такой счётчик можно сохранить в памяти: заведём вектор $\boldsymbol{e}(\boldsymbol{s})$ размером с число состояний, проинициализируем его нулём и далее на $\boldsymbol{t}$-ом шаге будем обновлять следующим образом:

$$
e_{t}(s):= \begin{cases}\gamma e_{t-1}(s)+1 & \text { если } s=s_{t} \\ \gamma e_{t-1}(s) & \text { иначе }\end{cases}
$$

После этого на каждом шаге мы будем добавлять текущую одношаговую ошибку, временную разность $\boldsymbol{\Psi}_{(1)}\left(s_{t}, a_{t}\right)=r_{t}+\gamma \boldsymbol{V}^{\boldsymbol{t}} s_{t+1}\left(-V\left(s_{t}\right)\right.$, ко всем состояниям с коэффициентом $\left.e_{t}(s)$.

Определение 61: Будем называть $\boldsymbol{e}_{\boldsymbol{t}}(\boldsymbol{s})$ следом (eligibility trace) для состояния $\boldsymbol{s}$ в момент времени $\boldsymbol{t}$ эпизода коэффициент, с которым алгоритм обновляет оценочную функцию $\boldsymbol{V}(\boldsymbol{s})$ на $\boldsymbol{t}$-ом шаге при помощи текущей одношаговой ошибки $\boldsymbol{\Psi}_{(1)}\left(s_{t}, a_{t}\right)$ :

$$
V(s) \leftarrow V(s)+\alpha e_{t}(s) \Psi_{(1)}\left(s_{t}, a_{t}\right)
$$

Допустим, эпизод доигран до конца, и мы в алгоритме используем формулу (3.49) со следом (3.48). Одношаговое обновление будет превращено в двухшаговое, двухшаговое - в трёхшаговое и так далее до $\boldsymbol{N}$-шагового, где $\boldsymbol{N}$ - количество шагов до конца эпизода. Таким образом (если в ходе таких обновлений learning rate и оцениваемая стратегия не меняется) наши обновления в точности соответствуют Монте-Карло.

Важно, что eligibility trace имеет физический смысл «кредита», который выдан решениям, принятым в состоянии $s$ : это та степень ответственности, с которой выбранное в этом состоянии действия влияют на события настоя-


щего, на получаемые сейчас награды (временные разности). Действительно, обновление (3.49) говорит следующее: прогноз будущей награды в состоянии $s$ нужно увеличить с некоторым learning rate-ом на получаемую награду ( $\left.\left\langle\boldsymbol{\Psi}_{(1)}\left(s_{t}, a_{t}\right)\right\rangle\right)$, домноженную на степень ответственности решений в $\boldsymbol{s}$ за события настоящего ( $\left.\boldsymbol{e}_{\boldsymbol{t}}(\boldsymbol{s})\right\rangle$ ). И пока мы используем Монте-Карло обновления, кредит ведёт себя так: как только мы принимаем в $\boldsymbol{s}$ решение, он увеличивается на единичку и дальше не затухает. Домножение на $\boldsymbol{\gamma}$ можно не интерпретировать как затухание, поскольку это вызвано дисконтированием награды в нашей задаче, «затуханием» самой награды со временем. То есть мы рисуем мелом на стене «я здесь был», и выдаём постоянный кредит этому состоянию.

А что, если мы не хотим использовать бесконечношаговые (Монте-Карло) обновления? Мы помним, что это одна крайность, в которой обновления имеют большую дисперсию. Можно броситься в другую крайность: если мы хотим ограничиться лишь, допустим, одношаговыми, то мы можем использовать просто «другое» определение следа:

$$
e_{t}(s):= \begin{cases}1 & \text { если } s=s_{t} \\ 0 & \text { иначе }\end{cases}
$$

То есть для одношаговых оценок нам нужно домножать след не на $\boldsymbol{\gamma}$, а на ноль. Тогда вектор $\boldsymbol{e}(\boldsymbol{s})$ на каждой итерации будет нулевым за исключением одного лишь только что встреченного состояния $\boldsymbol{s}_{\boldsymbol{t}}$, для которого он будет равен единице, и обновление 3.49 будет эквивалентно обычному одношаговому temporal difference. В таком кредите решение, принятое в $\boldsymbol{s}$, влияет только на то, что произойдёт на непосредственно том же шаге; «мел на стене испаряется мгновенно».

### 3.5.6. $\mathrm{TD}(\lambda)$

Как можно интерполировать между Монте-Карло обновлениями и одношаговыми с точки зрения backward view? Раз eligibility trace может затухать в $\gamma$ раз, а может в 0 , то, вероятно, можно тушить его и любым другим промежуточным способом. Так мы теперь можем придумать другой вид «промежуточных оценок» между Монте-Карло и одношаговыми. Пусть на очередном моменте времени след для состояния $\boldsymbol{s}$ затухает сильнее, чем в $\gamma$ раз, но больше, чем в ноль; что тогда произойдёт?

---

После момента посещения состояния $s$ след для него вырастет на единичку и оценочная функция обновится следующим образом:

$$
V(s) \leftarrow V(s)+\alpha \Psi_{(1)}(s, a)
$$

Допустим, на следующем шаге мы выбрали $\boldsymbol{\lambda}_{1} \in[0,1]$ - «коэффициент затухания» - и потушили след не с коэффициентом $\boldsymbol{\gamma}$, а с коэффициентом $\gamma \lambda_{1}$. Тогда дальше мы добавим новую текущую временную разность $\boldsymbol{\Psi}_{(1)}\left(s^{\prime}, a^{\prime}\right)$ с коэффициентом $\boldsymbol{\lambda} \boldsymbol{\gamma}$, получая суммарно следующее обновление:

$$
V(s) \leftarrow V(s)+\alpha\left(\Psi_{(1)}(s, a)+\lambda_{1} \gamma \Psi_{(1)}\left(s^{\prime}, a^{\prime}\right)\right)
$$

которое в силу (3.46) для $N=2$ преобразуется в:

$$
V(s) \leftarrow V(s)+\alpha\left(\left(1-\lambda_{1}\right) \Psi_{(1)}(s, a)+\lambda_{1} \Psi_{(2)}(s, a)\right)
$$

Таким образом, мы заансамблировали одношаговое и двухшаговое обновление. Из этого видно, что такая процедура корректна: $V^{\boldsymbol{\pi}}$ является неподвижной точкой как одношагового, так и двухшагового уравнения Беллмана, и значит неподвижной точкой любой их выпуклой комбинации.

С точки зрения кредита, мы сказали, что решение, принятое в $s$, влияет на то, что случится через 2 шага, но не так сильно, как на то, что случится через 1 шаг: степень ответственности за один шаг затухла в $\boldsymbol{\lambda}_{1}$ раз. Мы пользуемся здесь следующим прайором из реальной жизни: решения более вероятно влияют на ближайшее будущее, нежели чем на далёкое.

Для понятности проведём ещё один шаг рассуждений. Допустим, мы сделали ещё один шаг в среде и увидели $\boldsymbol{\Psi}_{(1)}\left(s^{\prime \prime}, a^{\prime \prime}\right)$; потушили eligibility trace $\boldsymbol{e}(s)$, на этот раз, в $\gamma \boldsymbol{\lambda}_{2}$ раз, где $\boldsymbol{\lambda}_{2} \in[0,1]$. Тогда след стал равен $\boldsymbol{e}(s)=$ $=\gamma^{2} \lambda_{1} \lambda_{2}$, и мы получим следующее обновление:

$$
V(s) \leftarrow V(s)+\alpha\left(\left(1-\lambda_{1}\right) \Psi_{(1)}(s, a)+\lambda_{1} \Psi_{(2)}(s, a)+\gamma^{2} \lambda_{1} \lambda_{2} \Psi_{(1)}\left(s^{\prime \prime}, a^{\prime \prime}\right)\right)
$$

Вспоминая формулу (3.47), последние два слагаемых преобразуются:

$$
\Psi_{(2)}(s, a)+\gamma^{2} \lambda_{2} \Psi_{(1)}\left(s^{\prime \prime}, a^{\prime \prime}\right)=\left(1-\lambda_{2}\right) \Psi_{(2)}(s, a)+\lambda_{2} \Psi_{(3)}(s, a)
$$

То есть долю $\boldsymbol{\lambda}_{\mathbf{2}}$ двухшаговой ошибки мы превращаем в трёхшаговое, а долю $1-\boldsymbol{\lambda}_{\mathbf{2}}$ - нет. Суммарное обновление становится таким ансамблем:

$$
V(s) \leftarrow V(s)+\alpha\left(\left(1-\lambda_{1}\right) \Psi_{(1)}(s, a)+\lambda_{1}\left(1-\lambda_{2}\right) \Psi_{(2)}(s, a)+\lambda_{1} \lambda_{2} \Psi_{(3)}(s, a)\right)
$$

И так далее. Интерпретировать это можно так. Если мы тушим след в $\gamma$ раз, то на очередном шаге обновления мы «превращаем» $N$-шаговое обновление в $N+1$-шаговое. Если мы тушим след полностью, зануляя его, то мы «отказываемся превращать» $N$-шаговое обновление в $N+1$-шаговое. Оба варианта корректны, и поэтому мы, выбирая $\boldsymbol{\lambda}_{i} \in[0,1]$, решаем заансамблировать их: мы возьмём и долю $\boldsymbol{\lambda}_{i} \boldsymbol{N}$-шагового обновления «превратим» в $N+1$-шаговое, а долю $1-\boldsymbol{\lambda}_{t}$ трогать не будем и оставим без изменения. Поэтому $\boldsymbol{\lambda}_{t}$ ещё называют «коэффициентом смешивания».

Мы уже обсуждали, что странно проводить credit assignment нестационарно, то есть чтобы в процедуре была какая-то зависимость от времени, прошедшего с момента посещения состояния, поэтому коэффициент затухания $\boldsymbol{\lambda} \in[0,1]$ обычно полагают не зависящем от момента времени, каким-то константным гиперпараметром, отвечающим за bias-variance trade-off. Естественно, $\boldsymbol{\lambda}=\mathbf{1}$ соответствует Монте-Карло обновлениям, $\boldsymbol{\lambda}=\mathbf{0}-$ одношаговым.
Определение 62: Будем называть temporal difference обновлением с параметром $\boldsymbol{\lambda} \in[0,1]$ («обновление $\mathrm{TD}(\boldsymbol{\lambda})$ ) обновление (3.49) со следом $\boldsymbol{e}_{t}(s)$, проинициализированным нулём и далее определённым следующим образом:

$$
e_{t}(s):= \begin{cases}\gamma \lambda e_{t-1}(s)+1 & \text { если } s=s_{t} \\ \gamma \lambda e_{t-1}(s) & \text { иначе }\end{cases}
$$

Формула следа задаёт алгоритм в парадигме backward view. Естественно, что любая оценка, придуманная в терминах backward view, то есть записанная в терминах следа (в явном виде хранящего «кредит» ответственности решений для каждого состояния), неределывается в парадигме forward view (как и наоборот), когда мы, используя сэмплы будущего, строим некоторую оценку Advantage $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{A}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ и просто сдвигаем по нему значение V-функции:

$$
V(s) \leftarrow V(s)+\alpha \Psi(s, a)
$$

Какому обновлению в парадигме forward view соответствует обновление $\operatorname{TD}(\boldsymbol{\lambda})$ ?

---

Теорема 32 - Эквивалентные формы $\operatorname{TD}(\lambda)$ : Обновление $\operatorname{TD}(\boldsymbol{\lambda})$ эквивалентно (3.50) с оценкой

$$
\boldsymbol{\Psi}(s, a):=\sum_{t \geq 0} \gamma^{t} \boldsymbol{\lambda}^{t} \boldsymbol{\Psi}_{(1)}\left(s^{(t)}, a^{(t)}\right)=(1-\lambda) \sum_{t>0} \lambda^{t-1} \boldsymbol{\Psi}_{(t)}(s, a)
$$

Доказательство. Составим такую табличку: какое суммарное обновление у нас получается в $\operatorname{TD}(\boldsymbol{\lambda})$ после $\boldsymbol{t}$ шагов в среде. Справа запишем, с какими весами входят разные $\boldsymbol{N}$-шаговые оценки в получающийся ансамбль.

| Step | Update | $\boldsymbol{\Psi}_{(1)}(s, a)$ | $\boldsymbol{\Psi}_{(2)}(s, a)$ | $\boldsymbol{\Psi}_{(3)}(s, a)$ | $\ldots$ | $\boldsymbol{\Psi}_{(N)}(s, a)$ |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| 1 | $\boldsymbol{\Psi}_{(1)}(s, a)$ | 1 | 0 | 0 |  | 0 |
| 2 | $\boldsymbol{\Psi}_{(1)}(s, a)+\gamma \lambda \boldsymbol{\Psi}_{(1)}\left(s^{\prime}, a^{\prime}\right)$ | $1-\lambda$ | $\lambda$ | 0 |  |  |
| 3 | $\begin{gathered} \boldsymbol{\Psi}_{(1)}(s, a)+\gamma \lambda \boldsymbol{\Psi}_{(1)}\left(s^{\prime}, a^{\prime}\right)+ \\ +\left(\gamma \lambda\right)^{2} \boldsymbol{\Psi}_{(1)}\left(s^{\prime \prime}, a^{\prime \prime}\right) \end{gathered}$ | $1-\lambda$ | $(1-\lambda) \lambda$ | $\lambda^{2}$ |  | 0 |
| $\vdots$ |  |  |  |  | $\ddots$ |  |
| N | $\sum_{t \geq 0}^{N}(\gamma \lambda)^{t} \boldsymbol{\Psi}_{(1)}\left(s^{(t)}, a^{(t)}\right)$ | $1-\lambda$ | $(1-\lambda) \lambda$ | $(1-\lambda) \lambda^{2}$ |  | $\lambda^{N}$ |

Продолжая строить такую табличку, можно по индукции увидеть, что после окончания эпизода суммарное обновление V-функции получится следующим:

$$
V(s) \leftarrow V(s)+\alpha \sum_{t \geq 0}(\gamma \lambda)^{t} \Psi_{(1)}\left(s^{(t)}, a^{(t)}\right)
$$

Это и есть суммарное обновление V-функции по завершении эпизода.
Итак, полученная формула обновления имеет две интерпретации. Получается, что подобный ансамбль многошаговых оценок эквивалентен дисконтированной сумме будущих одношаговых ошибок («модифицированных наград» с потенциалом $\boldsymbol{V}(\boldsymbol{s})$ ), где коэффициент дисконтирования равен $\gamma \boldsymbol{\lambda}$; исходный коэффициент $\gamma$ отвечает за затухание ценности наград со временем из исходной постановки задачи, а $\boldsymbol{\lambda}$ соответствует затуханию кредита ответственности действия за полученные в будущем награды.

А с другой стороны можно интерпретировать $\operatorname{TD}(\boldsymbol{\lambda})$ как ансамбль многошаговых оценок разной длины. Мы взяли одношаговую оценку с весом $\boldsymbol{\lambda}$, двухшаговую с весом $\boldsymbol{\lambda}^{2}$, $\boldsymbol{N}$-шаговую с весом $\boldsymbol{\lambda}^{N}$ и так далее. Сумма весов в ансамбле, как водится, должна равняться единице, отсюда в формуле домножение на $1-\lambda$.

Если через $\boldsymbol{N}$ шагов после оцениваемого состояния $\boldsymbol{s}$ эпизод закончился, то все многошаговые оценки длины больше $\boldsymbol{N}$, понятно, совпадают с $\boldsymbol{N}$-шаговой и равны reward-to-go. Следовательно, в такой ситуации reward-to-go по определению замешивается в оценку с весом $(1-\lambda)\left(\lambda^{N-1}+\lambda^{N}+\ldots\right)=\lambda^{N-1}$.

Мы привыкли, что любая формула обновления для нас стохастическая аппроксимация решения какого-то уравнения. $\operatorname{TD}(\boldsymbol{\lambda})$ не исключение. Если $\boldsymbol{N}$-шаговая оценка направляет нас в сторону решения $\boldsymbol{N}$-шагового уравнения Беллмана $V^{\boldsymbol{\pi}}=$ $=\boldsymbol{\mathfrak { B }}{ }^{N} V^{\boldsymbol{\pi}}$, то ансамбль из оценок направляет нас в сторону решения ансамбля $\boldsymbol{N}$-шаговых уравнений Беллмана:

$$
V^{\pi}=(1-\lambda)\left(\mathfrak{B} V^{\pi}+\lambda \mathfrak{B}^{2} V^{\pi}+\lambda^{2} \mathfrak{B}^{3} V^{\pi}+\ldots\right)=(1-\lambda) \sum_{N>0} \lambda^{N-1} \mathfrak{B}^{N} V^{\pi}
$$

Поскольку $\boldsymbol{V}^{\boldsymbol{\pi}}$ является неподвижной точкой для операторов $\mathfrak{B}^{\boldsymbol{N}}$ для всех $\boldsymbol{N}$, то и для их выпуклой комбинации, «ансамбля», он тоже будет неподвижной точкой. Итого $\operatorname{TD}(\boldsymbol{\lambda})$ дало нам прикольную идею: мы не могли выбрать одну из многошаговых оценок, и поэтому взяли их все сразу.

Итак, мы получили алгоритм $\operatorname{TD}(\boldsymbol{\lambda})$ оценивания стратегии, или temporal difference с параметром $\boldsymbol{\lambda}$, который при $\boldsymbol{\lambda}=\mathbf{1}$ эквивалентен Монте-Карло алгоритму (с постоянным обновлением V-функции «по ходу эпизода»), а при $\boldsymbol{\lambda}=\mathbf{0}$ эквивалентен одношаговому temporal difference методу, который мы, в частности, применяли в Q-learning и SARSA для оценивания стратегии.

---

# Алгоритм 14: TD( $\boldsymbol{\lambda}$ ) 

Вход: $\boldsymbol{\pi}$ - стратегия
Гиперпараметры: $\boldsymbol{\alpha}$ - параметр экспоненциального сглаживания, $\boldsymbol{\lambda} \in[0,1]$ - степень затухания следа
Инициализируем $\boldsymbol{V}(\boldsymbol{s})$ произвольно для всех $\boldsymbol{s} \in \boldsymbol{S}$
Инициализируем $\boldsymbol{e}(\boldsymbol{s})$ пулём для всех $\boldsymbol{s} \in \boldsymbol{S}$
Наблюдаем $s_{0}$
На $k$-ом шаге:

1. выбираем $a_{k} \sim \pi\left(a_{k} \mid s_{k}\right)$
2. играем $a_{k}$ и наблюдаем $r_{k}, s_{k+1}$
3. обновляем след $e\left(s_{k}\right) \leftarrow e\left(s_{k}\right)+1$
4. считаем одношаговую ошибку $\boldsymbol{\Psi}_{(1)}:=r_{k}+\gamma \boldsymbol{V}\left(s_{k+1}\right)-\boldsymbol{V}\left(s_{k}\right)$
5. для всех $s$ обновляем $\boldsymbol{V}(\boldsymbol{s}) \leftarrow \boldsymbol{V}(\boldsymbol{s})+\boldsymbol{\alpha} \boldsymbol{e}(\boldsymbol{s}) \boldsymbol{\Psi}_{(1)}$
6. для всех $s$ обновляем $\boldsymbol{e}(\boldsymbol{s}) \leftarrow \gamma \boldsymbol{\lambda} \boldsymbol{e}(\boldsymbol{s})$

## Выход: $V(s)$

Мы обсуждали и выписали этот алгоритм для V-функции для задачи именно оценивания стратегии; естественно, мы могли бы сделать это для Q-функции или добавить policy improvement после, например, каждого шага в среде, получив табличный алгоритм обучения стратегии. Позже в разделе 3.5.7 мы рассмотрим формулировку теоремы о сходимости таких алгоритмов для ещё более общей ситуации.

Очевидно, $\operatorname{TD}(\boldsymbol{\lambda})$ обновление не эквивалентно никаким $\boldsymbol{N}$-шаговым temporal difference формулам: в нём замешана как Монте-Карло оценка, то есть замешана вся дальнейшая награда (весь будущий сигнал), так и приближения V-функции во всех промежуточных состояний (при любом $\boldsymbol{\lambda} \in(0,1)$ ). Гиперпараметр $\boldsymbol{\lambda}$ также не имеет смысла времени, и поэтому на практике его легче подбирать.

Полезность $\operatorname{TD}(\boldsymbol{\lambda})$ в том, что $\boldsymbol{\lambda}$ непрерывно и позволяет более гладкую настройку «длины следа». На практике алгоритмы будут чувстительны к выбору $\boldsymbol{\lambda}$ в намного меньшей степени, чем к выбору $\boldsymbol{N}$. При этом даже если $\boldsymbol{\lambda}<\mathbf{1}$, в оценку «поступает» информация о далёкой награде, и использование $\operatorname{TD}(\boldsymbol{\lambda})$ позволит бороться с проблемой распространения сигнала.

Всюду далее в ситуациях, когда нам понадобится разрешать bias-variance trade-off, мы будем обращаться к формуле forward view $\mathrm{TD}(\boldsymbol{\lambda})$ обновления (3.51). Однако как отмечалось ранее, для работы с многошаговыми оценками, а следовательно и с обновлением (3.51) $\operatorname{TD}(\boldsymbol{\lambda})$, необходимо работать в on-policy режиме, то есть иметь сэмплы взаимодействия со средой именно оцениваемой стратегии $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$, и поэтому возможность разрешать bias-variance trade-off у нас будет только в on-policy алгоритмах.

### 3.5.7. $\operatorname{Retrace}(\lambda)$

Что же тогда делать в off-policy режиме? Итак, пусть дан роллаут $s, a, r, s^{\prime}, a^{\prime}, r^{\prime}, s^{\prime \prime}, \ldots$, где действия сэмплированы из стратегии $\boldsymbol{\mu}$. Мы хотим при этом провести credit assignment для стратегии $\boldsymbol{\pi}$, то есть понять, как обучать $V \approx V^{\pi}$ или $Q \approx Q^{\pi}$.

Проблема в том, что на самом деле это не всегда даже в принципе возможно. Представим, что $\boldsymbol{a}$ таково, что $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})=\mathbf{0}$. Тогда в роллауте хранится информация о событиях, которые произойдут с агентом, использующем стратегию $\boldsymbol{\pi}$, с вероятностью 0 . Понятно, что никакой полезной информации о том, как менять аппроксимацию V-функции, мы из таких данных не получим. Поэтому для детерминированных стратегий задача обучения с многошаговых оценок в off-policy режиме может запросто оказаться бессмысленной.

Пример 64: Допустим, стратегия $\boldsymbol{\mu}$ с вероятностью один в первом же состоянии прыгает в лаву. Мы же хотим посчитать $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$, будущую награду, для стратегии $\boldsymbol{\pi}$, которая с вероятностью один не прыгает в лаву, а кушает тортики. Поскольку в задаче RL функция переходов $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ для разных $\boldsymbol{a}$ может быть произвольно разной, мы ничего не можем сказать о ценности одного действия по информации о другом действии.

Возможность в принципе обучаться off-policy в Q-learning обеспечивалась тем, что, когда мы учим Qфункцию с одношаговых оценок, нам для любых $\boldsymbol{s}, \boldsymbol{a}$, которые можно взять из буфера, достаточно лишь сэмпла $s^{\prime}$. При этом сэмпл $a^{\prime} \sim \pi\left(a^{\prime} \mid s^{\prime}\right)$ мы всегда сможем сгенерировать «онлайн», прогнав на взятом из буфера $s^{\prime}$ оцениваемую стратегию. Обычно в off-policy режиме учат именно Q-функцию, что важно в том числе тем, что по крайней мере одношаговая оценка будет доступна всегда. Если же $\boldsymbol{a}^{\prime}$ из буфера такого, что $\boldsymbol{\pi}\left(\boldsymbol{a}^{\prime} \mid \boldsymbol{s}^{\prime}\right)=\mathbf{0}$,

---

то любые наши коррекции схлопуутся в одношаговую оценку (или же будут теоретически некорректны), но по крайней мере хоть что-то мы сможем сделать.

Итак, попробуем разрешить bias-variance trade-off-а для Q-функции. Для этого снова вернёмся к идеи следа. Допустим, для взятых из буфера $s, a, r, s^{\prime}$ мы составили одношаговое обновление:

$$
Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma Q\left(s^{\prime}, a_{\pi}^{\prime}\right)-Q(s, a)\right)
$$

где $a_{\pi}^{\prime} \sim \pi\left(\cdot \mid s^{\prime}\right)$, положив неявно значение следа $\boldsymbol{e}(\boldsymbol{s}, \boldsymbol{a})=\mathbf{1}$ (след при обучении Q-функции зависит от пары $\boldsymbol{s}, \boldsymbol{a})$. Также лучше, если есть такая возможность, использовать не сэмпл $\boldsymbol{a}_{\pi}^{\prime}$, а усреднить по нему (аналогично тому, как было проделано в формуле (3.39) Expected SARSA):

$$
Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime}, a_{\pi}^{\prime}\right)-Q(s, a)\right)
$$

Затем возьмём из буфера $a^{\prime}, r^{\prime}, s^{\prime \prime}$. Какое значение следа мы можем выбрать? Можно $\boldsymbol{e}(\boldsymbol{s}, \boldsymbol{a})=\mathbf{0}$, оставив одношаговое обновление и «не превращая» его в двухшаговое, это одна крайность. А как превратить обновление в двухшаговое целиком? Хотелось бы прибавить

$$
\gamma\left(r^{\prime}+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime \prime}, a_{\pi}^{\prime \prime}\right)-\mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime}, a_{\pi}^{\prime}\right)\right)
$$

где $s^{\prime \prime} \sim p\left(s^{\prime \prime} \mid s^{\prime}, a_{\pi}^{\prime}\right)$. Однако в буфере у нас нет такого сэмпла, а вместо него есть сэмпл $s^{\prime \prime} \sim p\left(s^{\prime \prime} \mid s^{\prime}, a^{\prime}\right)$. Технически, ошибка за второй шаг является случайной величиной от $a^{\prime}$, который должен приходить из $\pi$, когда у нас есть сэмпл лишь из $\mu$. Поэтому здесь необходимо применить importance sampling коррекцию, после которой ошибка за второй шаг принимает следующий вид ${ }^{7}$ :

$$
Q(s, a) \leftarrow Q(s, a)+\alpha \gamma \frac{\pi\left(a^{\prime} \mid s^{\prime}\right)}{\mu\left(a^{\prime} \mid s^{\prime}\right)}\left(r^{\prime}+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime \prime}, a_{\pi}^{\prime \prime}\right)-Q\left(s^{\prime}, a^{\prime}\right)\right)
$$

где $a^{\prime}, r^{\prime}, s^{\prime \prime}$ - взяты из буфера.
Утверждение 36: Последовательное применение обновлений (3.53) и (3.54) является корректным двухшаговым обновлением, то есть эквивалентно

$$
Q(s, a) \leftarrow Q(s, a)+\alpha(y(Q)-Q(s, a))
$$

где среднее значение $y(Q)$ по всей заложенной в ней стохастике является правой частью двухшагового уравнения Беллмана для Q-функции:

$$
\mathbb{E} y(Q)=\mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime} \sim \pi} \mathbb{E}_{s^{\prime \prime}} \mathbb{E}_{a^{\prime \prime} \sim \pi}\left[r(s, a)+\gamma r\left(s^{\prime}, a^{\prime}\right)+\gamma^{2} Q\left(s^{\prime \prime}, a^{\prime \prime}\right)\right]
$$

Доказательство. После двух рассматриваемых обновлений получается, что «целевая переменная», таргет, равен:

$$
y(Q)=r+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime}, a_{\pi}^{\prime}\right)+\gamma \frac{\pi\left(a^{\prime} \mid s^{\prime}\right)}{\mu\left(a^{\prime} \mid s^{\prime}\right)}\left(r^{\prime}+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime \prime}, a_{\pi}^{\prime \prime}\right)-Q\left(s^{\prime}, a^{\prime}\right)\right)
$$

где случайными величинами являются $s^{\prime}, a^{\prime}, s^{\prime \prime}$, и $a^{\prime} \sim \mu\left(a^{\prime} \mid s^{\prime}\right)$. Возьмём среднее по этим величинам:

$$
\mathbb{E} y(Q)=\mathbb{E}_{s^{\prime}}\left[r+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime}, a_{\pi}^{\prime}\right)+\gamma \mathbb{E}_{a^{\prime} \sim \mu} \frac{\pi\left(a^{\prime} \mid s^{\prime}\right)}{\mu\left(a^{\prime} \mid s^{\prime}\right)}\left(r^{\prime}+\gamma \mathbb{E}_{s^{\prime \prime}} \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime \prime}, a_{\pi}^{\prime \prime}\right)-Q\left(s^{\prime}, a^{\prime}\right)\right)\right]
$$

Раскроем importance sampling коррекцию, воспользовавшись

$$
\mathbb{E}_{a^{\prime} \sim \mu} \frac{\pi\left(a^{\prime} \mid s^{\prime}\right)}{\mu\left(a^{\prime} \mid s^{\prime}\right)} f\left(a^{\prime}\right)=\mathbb{E}_{a^{\prime} \sim \pi} f\left(a^{\prime}\right)
$$

[^0]или даже не корректировать последнее слагаемое importance sampling коррекцией, так как в нём не требуется брать $a^{\prime}$ обязательно из буфера:

$$
\gamma \frac{\pi\left(a^{\prime} \mid s^{\prime}\right)}{\mu\left(a^{\prime} \mid s^{\prime}\right)}\left(r^{\prime}+\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime \prime}, a_{\pi}^{\prime \prime}\right)\right)-\gamma \mathbb{E}_{a_{\pi}^{\prime} \sim \pi} Q\left(s^{\prime}, a_{\pi}^{\prime}\right)
$$

Далее в формулах предполагается вариант, рассматриваемый в статье про Retrace(A).


[^0]:    ${ }^{7}$ видно, что в отношении последнего слагаемого возможны вариации, например, ошибку за второй шаг можно оценить как

---

$$
\mathbb{E} y(Q)=\mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime} \sim \pi} \mathbb{E}_{s^{\prime \prime}} \mathbb{E}_{a_{w}^{\prime \prime} \sim \pi}\left[r+\gamma Q\left(s^{\prime}, a^{\prime}\right)+\gamma\left(r^{\prime}+\gamma Q\left(s^{\prime \prime}, a_{w}^{\prime \prime}\right)-Q\left(s^{\prime}, a^{\prime}\right)\right)\right]
$$

Осталось заметить, что слагаемое $\gamma Q\left(s^{\prime}, a^{\prime}\right)$ по аналогии с on-policy режимом сокращается.
Таким образом, одношаговую ошибку за второй шаг для получения полного превращения одношагового обновления в двухшаговое необходимо добавить не с весом $\gamma$, как в on-policy режиме, а с весом $\gamma \frac{\pi\left(a^{\prime} \mid s^{\prime}\right)}{\mu\left(a^{\prime} \mid s^{\prime}\right)}$.

Продолжая рассуждение дальше, можно получить, что одношаговая ошибка через $t$ шагов после выбора оцениваемого действия $\boldsymbol{a}$ в состоянии $\boldsymbol{s}$ в off-policy режиме зависит от случайных величин $\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}, \boldsymbol{s}^{\prime \prime}, \ldots \boldsymbol{s}^{(\boldsymbol{t}+1)}$, и поэтому importance sampling коррекция для неё будет равна:

$$
\prod_{i=1}^{i=t} \frac{\pi\left(a^{(i)} \mid s^{(i)}\right) p\left(s^{(i+1)} \mid s^{(i)}, a^{(i)}\right)}{\mu\left(a^{(i)} \mid s^{(i)}\right) p\left(s^{(i+1)} \mid s^{(i)}, a^{(i)}\right)}=\prod_{i=1}^{i=t} \frac{\pi\left(a^{(i)} \mid s^{(i)}\right)}{\mu\left(a^{(i)} \mid s^{(i)}\right)}
$$

Здесь и далее считается, что при $t=0$ подобные произведения равны единице.
Получается, что для того, чтобы строить оценку максимальной длины, нужно на $t$-ом шаге домножать след на $\gamma \frac{\pi\left(a^{(t)} \mid s^{(t)}\right)}{\mu\left(a^{(t)}\right) s^{(t)}}$ Интоговую формулу обновления часто записывают в следующем виде:

$$
Q(s, a) \leftarrow Q(s, a)+\alpha \sum_{t \geq 0} \gamma^{t}\left(\prod_{i=1}^{i=t} \frac{\pi\left(a^{(i)} \mid s^{(i)}\right)}{\mu\left(a^{(i)} \mid s^{(i)}\right)}\right) \Psi_{(1)}\left(s^{(t)}, a^{(t)}\right)
$$

где

$$
\Psi_{(1)}\left(s^{(t)}, a^{(t)}\right)=r^{(t)}+\gamma \mathbb{E}_{a_{w}^{(t+1)} \sim \pi} Q\left(s^{(t+1)}, a_{w}^{(t+1)}\right)-Q\left(s^{(t)}, a^{(t)}\right)
$$

Мы получили «off-policy» Монте-Карло оценку в терминах следа. Теперь по аналогии с $\operatorname{TD}(\boldsymbol{\lambda})$ проинтерволируем между одношаговыми (где след занулятся после первого шага) и бесконечношаговыми обновлениями (где след есть importance sampling дробь): оказывается, оценка будет корректна при любом промежуточном значении следа. Чтобы записать это формально, перепишем формулу (3.55) в следующем виде:

$$
Q(s, a) \leftarrow Q(s, a)+\alpha \sum_{t \geq 0} \gamma^{t}\left(\prod_{i=1}^{i=t} c_{i}\right) \Psi_{(1)}\left(s^{(t)}, a^{(t)}\right)
$$

где $c_{i}$ - коэффициенты затухания следа: в on-policy они могли быть в диапазоне $[0,1]$ (и мы выбирали его равным гиперпараметру $\boldsymbol{\lambda}$ ), а здесь, в off-policy режиме, он может быть в диапазоне

$$
c_{i} \in\left[0, \frac{\pi\left(a^{(i)} \mid s^{(i)}\right)}{\mu\left(a^{(i)} \mid s^{(i)}\right)}\right]
$$

То, что при любом выборе способа затухания следа табличные алгоритмы, использующие оценку (3.57), будут сходиться - один из ключевых и самых общих результатов табличного RL. Приведём несколько нестрогую формулировку этой теоремы:

Теорема 33 - Retrace: Пусть число состояний и действий конечно, таблица $Q_{0}(s, a)$ проинициализирована произвольно. Пусть на $\boldsymbol{k}$-ом шаге алгоритма для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ ячейка таблицы обновляется по формуле

$$
\begin{gathered}
Q_{k+1}(s, a) \leftarrow Q_{k}(s, a)+\alpha_{k}(s, a) \sum_{t \geq 0} \gamma^{t}\left(\prod_{i=1}^{i=t} c_{i}\right) \Psi_{(1)}\left(s^{(t)}, a^{(t)}\right) \\
\Psi_{(1)}\left(s^{(t)}, a^{(t)}\right)=r^{(t)}+\gamma \mathbb{E}_{a_{w}^{(t+1)}} \sim \pi_{k} Q_{k}\left(s^{(t+1)}, a_{\pi}^{(t+1)}\right)-Q\left(s^{(t)}, a^{(t)}\right)
\end{gathered}
$$

где $\mathcal{T} \sim \mu_{k} \mid s_{0}=s, a_{0}=a$ сгенерирована произвольной стратегией сбора данных $\mu_{k}$ (причём не обязательно стационарной), learning rate $\alpha_{k}(s, a)$ - случайно, $\pi_{k}$ произвольно, и коэффициенты следа - любые в диапазоне

$$
c_{i} \in\left[0, \frac{\pi_{k}\left(a^{(i)} \mid s^{(i)}\right)}{\mu_{k}\left(a^{(i)} \mid s^{(i)}\right)}\right]
$$

Тогда, если с вероятностью 1 learning rate удовлетворяет условиям Роббинса-Монро (3.26), а стратегия $\pi_{k}$ с вероятностью 1 становится жадной по отношению к $Q_{k}$ в пределе $k \rightarrow \infty$, то при некоторых технических ограничениях с вероятностью $1 Q_{k}$ сходится к оптимальной Q-функции $Q^{*}$, а $\pi_{k}$, соответственно, к оптимальной стратегии.

Без доказательства; интересующиеся могут обратиться к оригинальной статье по Retrace.

---

Пользуясь этой теоремой, мы можем в полной аналогии с $\operatorname{TD}(\boldsymbol{\lambda})$ выбрать гиперпараметр $\boldsymbol{\lambda} \in[0,1]$, и считать след по формуле

$$
c_{i}=\lambda \frac{\pi\left(a^{(t)} \mid s^{(t)}\right)}{\mu\left(a^{(t)} \mid s^{(t)}\right)}
$$

В частности, в on-policy режиме $\boldsymbol{\pi} \equiv \boldsymbol{\mu}$ мы получим коэффициент затухания следа, равный просто $\boldsymbol{\lambda}$. Это очень удобно, но на практике неприменимо.

Такая оценка страдает сразу от двух проблем. Первая проблема - типичная: затухающий след (vanishing trace), когда $\boldsymbol{\mu}\left(\boldsymbol{a}^{(t)} \mid \boldsymbol{s}^{(t)}\right) \gg \boldsymbol{\pi}\left(\boldsymbol{a}^{(t)} \mid \boldsymbol{s}^{(t)}\right)$ для какого-то $\boldsymbol{t}$, и соответствующий множитель близок к нулю. Такое случится, если, например, $\boldsymbol{\mu}$ выбирает какие-то действия, которые $\boldsymbol{\pi}$ выбирает редко, что типично. В предельном случае для детерминированных стратегий может быть такое, что числитель коэффициента равен нулю ( $\boldsymbol{\pi}$ не выбирает такого действия никогда), и коррекция скажет, что вся информация, начиная с этого шага, полностью неактуальна. Эта проблема неизбежна.

Вторая проблема - взрывающийся след (exploding trace): $\boldsymbol{\mu}\left(\boldsymbol{a}^{(t)} \mid \boldsymbol{s}^{(t)}\right) \ll \boldsymbol{\pi}\left(\boldsymbol{a}^{(t)} \mid \boldsymbol{s}^{(t)}\right)$. Такое может случиться, если в роллауте попалось редкое для $\boldsymbol{\mu}$ действие, которое тем не менее часто выполняется оцениваемой стратегией $\boldsymbol{\pi}$. С одной стороны, кажется, что как раз такие роллауты наиболее ценны для обучения $\boldsymbol{Q}^{\boldsymbol{\pi}}$, ведь в них описывается шаг взаимодействия со средой, который для $\boldsymbol{\pi}$ как раз достаточно вероятен. Но на практике взрывающийся importance sampling коэффициент - источник большой дисперсии рассматриваемой оценки.

| Название | Коффициенты $\boldsymbol{c}_{\boldsymbol{i}}$ | Проблема |
| :--: | :--: | :--: |
| $\operatorname{TD}(\boldsymbol{\lambda})$ | $\boldsymbol{\lambda}$ | только on-policy режим |
| Одношаговые | 0 | сильное смещение |
| Importance Sampling | $\boldsymbol{\lambda} \frac{\pi\left(a^{(t)} \mid s^{(t)}\right)}{\mu\left(a^{(t)} \mid s^{(t)}\right)}$ | легко взрываются |

Идея борьбы со взрывающимся коэффициентом, предложенной в оценке $\operatorname{Retrace}(\boldsymbol{\lambda})$, заключается в следующем: если importance sampling коррекция для $\boldsymbol{t}$-го шага взорвалась, давайте воспользуемся тем, что мы теоретически обоснованно можем выбрать любой коэффициент меньше («быстрее» потушить след), и выберем единицу:

$$
c_{i}:=\lambda \min \left(1, \frac{\pi\left(a^{(i)} \mid s^{(i)}\right)}{\mu\left(a^{(i)} \mid s^{(i)}\right)}\right)
$$

Коррекция с такими коэффициентами корректна, стабильна, но, конечно, никак не помогает с затуханием следа. Важно помнить, что если $\boldsymbol{\pi}$ и $\boldsymbol{\mu}$ сильно отличаются, то велика вероятность, что коэффициенты затухания будут очень близки к нулю, и мы получим что-то, парктически всегда похожее на одношаговое обновление. С этим мы фундаментально не можем ничего сделать, хотя из-за этого итоговая формула обновления получается сильно смещённой. По этой причине смысла выбирать $\boldsymbol{\lambda}<\mathbf{1}$ в off-policy режиме обычно мало, и его почти всегда полагают равным единицей.

Также обсудим, что говорит формула Retrace в ситуации, когда оцениваемая политика $\boldsymbol{\pi}$ детерминирована. Если на шаге $\boldsymbol{t}$ политика сбора данных $\boldsymbol{\mu}$ выбрала ровно то действие, которое выбирает политика $\boldsymbol{\pi}$, то коэффициент $\boldsymbol{c}_{\boldsymbol{i}}=\boldsymbol{\lambda}$, то есть ситуация совпадает с on-policy режимом (действительно, в дискретных пространствах действий в числителе единица, а в знаменателе что-то меньшее единицы, когда в непрерывных пространствах действий числитель технически равен бесконечности, поэтому мы обрежем дробь до единицы). В противном же случае дробь $\frac{\pi\left(a^{(i)} \mid s^{(i)}\right)}{\mu\left(a^{(i)} \mid s^{(i)}\right)}$ имеет ноль в числителе, и след занулится. Таким образом, пока в буфере в цепочке $\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}, \boldsymbol{s}^{\prime \prime}, \boldsymbol{a}^{\prime \prime}, \cdots$ записанные действия совпадают с теми, которые выбирает детерминированная $\boldsymbol{\pi}$, мы «пользуемся $\operatorname{TD}(\boldsymbol{\lambda})$ », но вынуждены оборвать след, как только очередное действие разойдётся. Есть некоторая польза в том, чтобы в алгоритме $\boldsymbol{\pi}$ и $\boldsymbol{\mu}$ были стохастичны: тогда след по крайней мере полностью никогда не затухнет.

Мы дальше в off-policy будем обсуждать в основном одношаговые оценки, в том числе для простоты. Из одношаговости будут вытекать все ключевые недостатки таких алгоритмов, связанные со смещённостью подобных оценок и невозможностью полноценно разрешать bias-variance trade-off. Во всех этих алгоритмах с этой проблемой можно будет частично побороться при помощи идей $\operatorname{Retrace}(\boldsymbol{\lambda})$.

---

# ГЛАВА 4 

## Value-based подход

В данной главе будет рассмотрен второй, Value-based подход к решению задачи, в котором алгоритм ищет не саму стратегию, а оптимальную Q-функцию. Для этого табличный алгоритм Value Iteration будет обобщён на более сложные пространства состояний; требование конечности пространства действий $|\mathcal{A}|$ останется ограничением подхода.

## §4.1. Deep Q-learning

### 4.1.1. Q-сетка

В сложных средах пространство состояний может быть непрерывно или конечно, но велико (например, пространство всех экранов видеоигры). В таких средах моделировать функции от состояний, будь то стратегии или оценочные функции, мы можем только приближённо при помощи параметрических семейств.

Попробуем промоделировать в сложных средах алгоритм 10 Q learning. Для этого будем приближать оптимальную Q-функцию $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ при помощи нейронной сети $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$ с параметрами $\boldsymbol{\theta}$. Заметим, что для дискретных пространств действий сетка может как принимать действия на входе, так и принимать на вход только состояние $s$, а выдавать $|\mathcal{A}|$ чисел $\boldsymbol{Q}_{\boldsymbol{\theta}}\left(\boldsymbol{s}, \boldsymbol{a}_{1}\right) \ldots \boldsymbol{Q}_{\boldsymbol{\theta}}\left(\boldsymbol{s}, \boldsymbol{a}_{|\mathcal{A}|}\right)$. В последнем случае мы можем за константу находить жадное действие $\boldsymbol{\pi}(\boldsymbol{s})=$

$=\operatorname{argmax} Q_{\theta}(s, a)$.

В случае, если пространство действий непрерывно, выдать по числу для каждого варианта уже не получится. При этом, если непрерывное действие подаётся на вход вместе с состоянием, то оптимизировать по нему для поиска максимума или аргмаксимума придётся при помощи серии прямых и обратных проходов (для дискретного пространства - за $|\mathcal{A}|$ прямых проходов), что вычислительно ни в какие ворота. Поэтому такой вариант на практике не встречается, а алгоритм пригоден в таком виде только для дискретных пространств состояний (позже мы пофиксим это при обсуждении алгоритма DDPG в главе 6.1).

Использование нейросеток позволяет обучаться для сред, в которых состояния $\boldsymbol{s}$ заданы, например, пиксельным представлением экранов видеоигр. Стандартным вариантом архитектуры является несколько (не очень много) свёрточных слоёв, обычно без использования макспулингов (важно не убить информацию о расположении распознанных объектов на экране). Использование батч-нормализаций и дроп-аутов сопряжено с вопросами о том, нужно ли их включать-выключать на этапах генерации таргетов (который, как мы увидим позже, тоже распадается на два этапа), сборе опыта с возможностью исследования и так далее. Чаще их не используют, чем используют, так как можно вариаться на неожиданные эффекты. Важно помнить, что все эти блоки были придуманы для решения немного других задач, и стоит осторожно переносить их в контекст обучения с подкреплением.

### 4.1.2. Переход к параметрической Q-функции

Как обучать параметры $\boldsymbol{\theta}$ нейронной сети так, чтобы $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ ? Вообще говоря, мы помним, что мы хотим решать уравнения оптимальности Беллмана (3.17), и можно было бы, например, оптимизировать невязку:

$$
\left(Q_{\theta}(s, a)-r(s, a)-\gamma \mathbb{E}_{s^{\prime}} \max _{a^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)\right)^{2} \rightarrow \min _{\theta}
$$

однако маг.ожидание $\mathbb{E}_{\boldsymbol{s}^{\prime}}$ в формуле берётся по неизвестному нам распределению (а даже если известному, то почти наверняка в сложных средах аналитически ничего не возьмётся) и никак не выносится (несмещённые оценки градиента нас бы устроили).

---

В Q-learning-e мы смогли с сохранением теоретических гарантий побороться с этим при помощи стохастической аппроксимации, получив формулу (3.34). Хочется сделать какой-то аналогичный трюк:

$$
Q_{\theta}(s, a) \leftarrow Q_{\theta}(s, a)+\alpha\left(r(s, a)+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)-Q_{\theta}(s, a)\right)
$$

Мы уже отмечали ключевое наблюдение о том, что формула стохастической аппроксимации очень напоминает градиентный спуск, а $\boldsymbol{\alpha}$ играет роль learning rate. Да даже условия сходимости 3.35 , собственно, те же, что в стохастическом градиентном спуске ${ }^{1}$ ! И, действительно, табличный Q-learning является градиентным спуском для решения некоторой задачи регрессии.

Поскольку это очень принципиальный момент, остановимся в этом месте подробнее. Пусть у нас есть текущая версия $Q_{\theta_{k}}(s, a)$, и мы хотим проделать шаг метода простой итерации для решения уравнения $\mathrm{Q}^{*} \mathrm{Q}^{*}$ (3.17). Зададим следующую задачу регрессии:

- входом является пара $s, a$
- искомым (!) значением на паре $s, a$ - правая часть уравнения оптимальности Беллмана (3.17), т.е.

$$
f(s, a) \equiv r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{\boldsymbol{a}^{\prime}} Q_{\theta_{k}}\left(s^{\prime}, a^{\prime}\right)
$$

- наблюдаемым («зашумлённым») значением целевой переменной или mapzетом (target)

$$
y(s, a) \equiv r(s, a)+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta_{k}}\left(s^{\prime}, a^{\prime}\right)
$$

где $s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$

- функцией потерь MSE: $\operatorname{Loss}(y, \hat{y})=\frac{1}{2}(y-\hat{y})^{2}$

Заметим, что, как и в классической постановке задачи машинного обучения, значение целевой переменной - это её «зашумлённое» значение: по входу $s, a$ генерируется $s^{\prime}$, затем от $s^{\prime}$ считается детерминированная функция, и результат $\boldsymbol{y}$ является наблюдаемым значением. Мы можем для данной пары $\boldsymbol{s}, \boldsymbol{a}$ засэмплировать себе таргет, взяв переход ( $s, a, r, s^{\prime}$ ) и воспользовавшись сэмплом $s^{\prime}$, но существенно, что такой таргет - случайная функция от входа. Принципиально: согласно уравнениям Беллмана, мы хотим выучить мат.ожидание такого таргета, а значит, нам нужна квадратичная функция потерь.

Теорема 34: Пусть $Q_{\theta_{k+1}}(s, a)$ - достаточно ёмкая модель (model with enough capacity), выборка неограниченно большая, а оптимизатор идеальный. Тогда решением вышеописанной задачи регрессии будет шаг метода простой итерации для поиска оптимальной Q-функции:

$$
Q_{\theta_{k+1}}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{\boldsymbol{a}^{\prime}} Q_{\theta_{k}}\left(s^{\prime}, a^{\prime}\right)
$$

Доказательство. Под «достаточно ёмкой моделью» подразумевается, что модель может имитировать любую функцию, например для каждой пары $s, a$ из выборки выдавать любое значение. Найдём оптимальное значение для пары $s, a$ : для неё $Q_{\theta_{k+1}}(s, a)$ выдаёт некоторое число, которое должно минимизировать MSE:

$$
\frac{1}{2} \mathbb{E}_{s^{\prime}}\left(y(s, a)-Q_{\theta_{k+1}}(s, a)\right)^{2} \rightarrow \min _{\theta_{k+1}}
$$

Оптимальным константным решением регрессии с MSE как раз является среднее, получаем

$$
Q_{\theta_{k+1}}(s, a)=\mathbb{E}_{s^{\prime}} y(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{\boldsymbol{a}^{\prime}} Q_{\theta_{k}}\left(s^{\prime}, a^{\prime}\right)
$$

Другими словами, когда мы решаем задачу регрессии с целевой переменной $\boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})$ по формуле (4.1), мы в среднем сдвигаем нашу аппроксимацию в сторону правой части уравнения оптимальности Беллмана. Полезно наглядно увидеть это в формуле самого градиента. Представим на секунду, что мы решаем задачу регрессии с «идеальной», незашумлённой целевой переменной $f(s, a) \equiv r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \max _{\boldsymbol{a}^{\prime}} Q_{\theta_{k}}\left(s^{\prime}, a^{\prime}\right)$ : для одного примера $s, a$ градиент MSE тогда будет равен:

$$
\nabla_{\theta} \frac{1}{2}\left(f(s, a)-Q_{\theta}(s, a)\right)^{2}=\overbrace{\left(f(s, a)-Q_{\theta}(s, a)\right)}^{c \text { калир }} \nabla_{\theta} Q_{\theta}(s, a)
$$

направление увеличения

[^0]
[^0]:    ${ }^{1}$ это не случайность: корни теории одни и те же.

---

И наша «зашумлённая» целевая переменная $\boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})$ есть несмещённая оценка $\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})$, поскольку специально построена так, что $\mathbb{E}_{\boldsymbol{a}^{\prime}} \boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})$. Значит, мы можем несмещённо оценить этот градиент как

$$
\left(y(s, a)-Q_{\theta}(s, a)\right) \nabla_{\theta} Q_{\theta}(s, a)=\nabla_{\theta} \frac{1}{2}\left(y(s, a)-Q_{\theta}(s, a)\right)^{2}
$$

Мы всегда оптимизируем нейронные сети стохастической градиентной оптимизации, поэтому несмещённая оценка градиента нам подойдёт.

В частности, формула Q-learning (3.34) - это частный случай решения указанной задачи регрессии стохастическим градиентным спуском для специального вида параметрических распределений:

Теорема 35: Пусть Q-функция задана «табличным параметрическим семейством», то есть табличкой размера $|\mathcal{S}|$ на $|\mathcal{A}|$, где в каждой ячейке записан параметр $\theta_{s, a}$ :

$$
Q_{\theta}(s, a)=\theta_{s, a}
$$

Тогда формула (3.34) представляет собой градиентный спуск для решения обсуждаемой задачи регрессии.
Доказательство. Посчитаем градиент функции потерь на одном объекте $\boldsymbol{s}, \boldsymbol{a}$. Предсказанием модели будет $Q_{\theta}(s, a)=\theta_{s, a}$, то есть градиент предсказания по параметрам модели равен

$$
\nabla_{\theta} Q_{\theta}(s, a)=e_{s, a}
$$

где $e_{s, a}$ - вектор из $\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ из всех нулей с единственной единичкой в позиции, соответствующей паре $\boldsymbol{s}, \boldsymbol{a}$. Теперь посчитаем градиент функции потерь:

$$
\nabla_{\theta} \frac{1}{2}\left(y-Q_{\theta}(s, a)\right)^{2}=\left(Q_{\theta}(s, a)-y(s, a)\right) \nabla_{\theta} Q_{\theta}(s, a)=\left(Q_{\theta}(s, a)-y(s, a)\right) e_{s, a}
$$

Итак, градиентный спуск делает следующие апдейты параметров:

$$
\theta_{k+1}=\theta_{k}-\alpha \nabla_{\theta} \operatorname{Loss}\left(y, Q_{\theta}(s, a)\right)=\theta_{k}+\alpha\left(y(s, a)-Q_{\theta_{k}}(s, a)\right) e_{s, a}
$$

В этой формуле на каждом шаге обновляется ровно одна ячейка таблички $\boldsymbol{\theta}_{\boldsymbol{s}, \boldsymbol{a}}$, и это в точности совпадает с (3.34).

Само собой, аналогичный приём мы в будущем будем применять не только для обучения модели $\boldsymbol{Q}^{*}$, но и любых других оценочных функций. Для этого мы будем составлять задачу регрессии, выбирая в качестве целевой переменной несмещённую оценку правой части какого-нибудь уравнения Беллмана, неподвижной точкой которого является искомая оценочная функция. Этот приём является полным аналогом методов временной разности, поэтому и свойства получаемых алгоритмов будут следовать из классической теории табличных алгоритмов.

# 4.1.3. Таргет-сеть 

Рассмотренное теоретическое объяснение перехода от табличных методов к нейросетевым, конечно, предполагает, что мы решаем задачу регрессии «полностью», обучая $\boldsymbol{\theta}$ при фиксированных $\boldsymbol{\theta}_{\boldsymbol{k}}$. «Замороженные» $\boldsymbol{\theta}_{\boldsymbol{k}}$ соответствуют фиксированию формулы целевой переменной $\boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})$ (4.1), то есть фиксированию задачи регрессии. Так мы моделируем один шаг метода простой итерации, и только после этого объявляем выученные параметры модели $\boldsymbol{\theta}_{\boldsymbol{k + 1}}:=\boldsymbol{\theta}$. В этот момент задача регрессии изменится (поменяется целевая переменная), и мы перейдём к следующему шагу метода простой итерации.

Однако в Q-learning же, как и во всех табличных методах, теория стохастической аппроксимации позволяла «сменять» задачу регрессии каждый шаг, используя свежие параметры модели при построении целевой переменной. Конечно, как только мы от «табличных параметрических функций» переходим к произвольным параметрическим семействам, все теоретические гарантии сходимости Q-learning-a 3.35 теряются (теоремы сходимости использовали «табличность» нашего представления Q-функции). Как только мы используем ограниченные параметрические семейства вроде нейросеток, неидеальные оптимизаторы вроде Адама или не доводим каждый этап метода простой итерации до сходимости, гарантий нет.

Но возникает вопрос: сколько шагов градиентного спуска тратить на решение фиксированной задачи регрессии? Возникает естественное желание по аналогии с табличными методами использовать для построения таргета свежую модель, то есть менять целевую переменную в задаче регрессии каждый шаг после каждого градиентного шага:

$$
y(s, a):=r+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)
$$

Принципиально важно, что зависимость целевой переменной $\boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})$ (4.1) от параметров текущей модели $\boldsymbol{\theta}$ игнорировалось. Если вдруг в неё протекут градиенты, мы будем не только подстраивать прошлое под будущее, но и будущее под прошлое, что не будет являться корректной процедурой.

---

Эмпирически легко убедиться, что такой подход нестабилен примерно от слова совсем. Стохастическая оптимизация чревата тем, что после очередного шага модель может стать немножко «сломанной» и некоторое время выдавать неудачные значения на ряде примеров. В обычном обучении с учителем этот эффект сглаживается большим количеством итераций: обучение на последующих мини-батчах «исправляют» предыдущие ошибки, движение идёт в среднем в правильную сторону, но нет гарантий удачности каждого конкретного шага (на то это и стохастическая оптимизация). Здесь же при неудачном шаге сломанная модель может начать портить целевую переменную, на которую она же и обучается. Это приводит к цепной реакции: плохая целевая переменная начнёт портить модель, которая начнёт портить целевую переменную... Этот эффект особенно ярко проявляется ещё и потому, что мы используем одношаговые целевые переменные, которые, как мы обсуждали в главе 3.5 , сильно смещены (слишком сильно опираются на текущую же аппроксимацию).

Для стабилизации процесса одну задачу регрессии нужно решать более одной итерации градиентного спуска; необходимо сделать хотя бы условно 100-200 шагов. Проблема в том, что если таргет строится по формуле $\boldsymbol{r}+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta}(\boldsymbol{s}, \boldsymbol{a})$, то после первого же градиентного шага $\boldsymbol{\theta}$ поменяется.

Поэтому хранится копия $\boldsymbol{Q}$-сетки, называемая mapzemсетью (target network), единственная цель которой - генерировать таргеты текущей задачи регрессии для транзишнов из засэмплированных мини-батчей. Традиционно её параметры обозначаются $\boldsymbol{\theta}^{-}$. Итак, целевая переменная в таких обозначениях генерится по формуле

$$
y(\mathbb{T}) \equiv r+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)
$$


$\boldsymbol{\theta}^{-}$

а раз в $\boldsymbol{K}$ шагов веса $\boldsymbol{\theta}^{-}$просто копируются из текущей модели с весами $\boldsymbol{\theta}$ для «задания» новой задачи регрессии.

При обновлении задачи регрессии график функции потерь типично подскакивает. Поэтому распространённой, но чуть более вычислительно дорогой альтернативой является на каждом шаге устраивать экспоненциальное сглаживание весов таргет сети. Тогда на каждом шаге:

$$
\theta^{-} \leftarrow(1-\alpha) \theta^{-}+\alpha \theta
$$

где $\boldsymbol{\alpha}$ - гиперпараметр. Такой вариант тоже увеличивает стабильность алгоритма хотя бы потому, что решаемая задача регрессии меняется «постепенно».

# 4.1.4. Декорреляция сэмплов 

Q-learning после одного шага в среде делал апдейт одного значения таблицы $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$. Сейчас нас такой вариант, очевидно, не устраивает, потому что обучать нейросетки на мини-батчах размера 1 (особенно с уровнем доверия к нашим таргетам) - это явно плохая идея, но главное, сэмплы $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{y}$ сильно скоррелированы: в сложных средах последовательности состояний обычно очень сильно похожи. Нейросетки на скоррелированных данных обучаются очень плохо (чаще - не обучаются вовсе). Поэтому сбор мини-батча подряд с одной траектории здесь не сгодится.


---

Есть два доступных на практике варианта декорреляции сэмплов (sample decorrelation). Первый - запуск параллельных агентов, то есть сбор данных сразу в нескольких параллельных средах. Этот вариант доступен всегда, по крайней мере, если среда виртуальна; иначе эта опция может быть дороговатой... Второй вариант — реплей буфер, который, как мы помним, является прерогативой исключительно off-policy алгоритмов.

При наличии реплей буфера агент может решать задачи регрессии, сэмплируя мини-батчи переходов $\mathbb{T}=$ $=\left(s, \boldsymbol{a}, \boldsymbol{r}^{\prime}, \boldsymbol{s}^{\prime}\right)$ из буфера, затем делая для каждого перехода расчёт таргета ${ }^{2} \boldsymbol{y}(\mathbb{T}) \vDash \boldsymbol{r}+\gamma \max _{\boldsymbol{a}^{\prime}} \boldsymbol{Q}_{\boldsymbol{B}-\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)}$, игнорируя зависимость таргета от параметров, и проводя шаг оптимизации по такому мини-батчу. Такой батч уже будет декоррелирован.


Почему мы можем использовать здесь реплей буфер? Мы хотим решать уравнения оптимальности Беллмана для всех пар $\boldsymbol{s}, \boldsymbol{a}$. Поэтому в поставленной задаче регрессии мы можем брать тройки $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{y}$ для обучения из условно произвольного распределения до тех пор, пока оно достаточно разнообразно и нескоррелированно. Единственное ограничение - внутри $\boldsymbol{y}$ сидит $\boldsymbol{s}^{\prime}$, который должен приходить из и только из $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Но поскольку среда однородна, любые тройки $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$ из любых траекторий, сгенерированных любой стратегией, таковы, что $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, а значит, $\boldsymbol{s}^{\prime}$ может быть использован для генерации таргета. Иными словами, в рамках текущего подхода мы, как и в табличном Q-learning-e, находимся в off-policy режиме, и наши условия на процесс порождения переходов $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$ точно такие же.

На практике картинки в какой-то момент начинают переполнять оперативку и начинаются проблемы. Простое решение заключается в том, чтобы удалять самые старые переходы, то есть оставлять самый новый опыт, однако есть альтернативные варианты (см. приоритизированный реплей, раздел 4.2.6)

# 4.1.5. DQN 

Собираем алгоритм целиком. Нам придётся оставить $\varepsilon$-жадную стратегию исследования - с проблемой исследования-использования мы ничего пока не делали, и при стартовой инициализации есть риск отправиться тупить в ближайшую стену.

Также лишний раз вспомним про то, что в терминальных состояниях обязательно нужно домножаться на ( 1 - done), поскольку шансов у приближённого динамического программирования сойтись куда-то без «отправной точки» не очень много.

## Алгоритм 15: Deep Q-learning (DQN)

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{K}$ - периодичность апдейта таргет-сети, $\boldsymbol{\varepsilon}(\boldsymbol{t})$ - стратегия исследования, $\boldsymbol{Q}$ - нейросетка с параметрами $\boldsymbol{\theta}$, SGD-оптимизатор

Инициализировать $\boldsymbol{\theta}$ произвольно

[^0]
[^0]:    ${ }^{2}$ наличие реплей буфера не избавляет от необходимости использовать таргет-сеть, поскольку вычисление таргета для всего реплей буфера, конечно же, непрактично: реплей буфер обычно огромен (порядка $10^{6}$ переходов), а одна задача регрессии будет решаться суммарно 100-200 шагов на мини-батчах размера, там, 32 (итого таргет понадобится считать всего для порядка 3000 переходов).

---

Положить $\boldsymbol{\theta}^{-}:=\boldsymbol{\theta}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. выбрать $\boldsymbol{a}_{\boldsymbol{t}}$ случайно с вероятностью $\boldsymbol{\varepsilon}(\boldsymbol{t})$, иначе $\boldsymbol{a}_{\boldsymbol{t}}:=\underset{\boldsymbol{a}_{\boldsymbol{t}}}{\operatorname{argmax}} \boldsymbol{Q}_{\boldsymbol{\theta}}\left(s_{\boldsymbol{t}}, \boldsymbol{a}_{\boldsymbol{t}}\right)$
2. пронаблюдать $\boldsymbol{r}_{\boldsymbol{t}}, \boldsymbol{s}_{\boldsymbol{t}+1}$, done $_{\boldsymbol{t}+1}$
3. добавить пятёрку $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right.$, done $\left._{t+1}\right)$ в реплей буфер
4. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера
5. для каждого перехода $\mathbb{T}=(s, a, r, s^{\prime}$, done $)$ посчитать таргет:

$$
y(\mathbb{T}):=r+\gamma(1-\text { done }) \max _{a^{\prime}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)
$$

6. посчитать лосс:

$$
\operatorname{Loss}(\boldsymbol{\theta}):=\frac{1}{B} \sum_{\mathbb{T}}\left(Q_{\theta}(s, a)-y(\mathbb{T})\right)^{2}
$$

7. сделать шаг градиентного спуска по $\boldsymbol{\theta}$, используя $\boldsymbol{\nabla}_{\boldsymbol{\theta}} \operatorname{Loss}(\boldsymbol{\theta})$
8. если $\boldsymbol{t} \bmod \boldsymbol{K}=\mathbf{0}: \boldsymbol{\theta}^{-} \leftarrow \boldsymbol{\theta}$

Все алгоритмы, которые относят к Value-based подходу в RL, будут основаны на DQN: само название «valuebased» обозначает, что мы учим только оценочную функцию, а такое возможно только если мы учим модель Q-функции и полагаем, что policy improvement проводится на каждом шаге жадно. Неявно в DQN, конечно же, присутствует текущая политика, «целевая политика», которую мы оцениваем - $\underset{\boldsymbol{a}}{\operatorname{argmax}} \boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$. Тем не менее ключевое свойство алгоритма, которое стоит помнить - это то, что он работает в off-policy режиме, и потому потенциально является достаточно sample efficient.

Есть, однако, много причин, почему алгоритм может не раскрыть этот потенциал и «плохо» заработать на той или иной среде: либо совсем не обучиться, либо обучаться очень медленно. Со многими из этих недостатков можно пытаться вполне успешно бороться, что и пытаются делать модификации этого алгоритма, которые мы обсудим далее в главе 4.2.

Выделим отдельно одну особую фундаментальную причину, почему алгоритмы на основе DQN могут не справиться с оптимизацией награды и застрять на какой-то асимптоте, с которой мало что можно сделать, и которая вытекает непосредственно из off-policy режима работы DQN. DQN, как и любые алгоритмы, основанные на одношаговых целевых переменных, страдает от проблемы накапливающейся ошибки (composed error). Условно говоря, чтобы распространить награду, полученную в некоторый момент времени, на 100 шагов в прошлое, понадобится провести 100 этапов метода простой итерации. Каждый этап мы решаем задачу регрессии в сильно неидеальных условиях, и ошибка аппроксимации накапливается. На это накладывается необходимость обучать именно Q-функцию, которая должна дифференцировать между действиями.

Пример 65: Рассмотрим типичную задачу: вы можете перемещаться в пространстве в разные стороны. Если вы отправитесь вправо, через 100 шагов вы получите $+1 . Q^{*}(s$, вправо $)=\gamma^{100}$.

Посмотрим на значение функции в других действиях: например, $\boldsymbol{Q}^{*}(\boldsymbol{s}$, влево $)=\boldsymbol{\gamma}^{102}$. Вот с такой точностью наша $\boldsymbol{Q}^{*}$ должна обучиться в этом состоянии, чтобы жадная стратегия выбирала правильные действия. Именно поэтому в подобных ситуациях DQN-подобные алгоритмы не срабатывают: из-за проблемы накапливающейся ошибки сигнал на 100 шагов просто не распространяется с такой точностью.


Если же этих проблем «с точностью» не возникает, если в среде нет сильно отложенного сигнала или награда за каждый шаг очень информативна, то value-based подход может раскрыть свой потенциал и оказаться эффективнее альтернатив за счёт использования реплей буфера.

---

# §4.2. Модификации DQN 

### 4.2.1. Overestimation Bias

Отмечалось, что без таргет-сетки (при обновлении задачи регрессии каждый шаг) можно наблюдать, как $\boldsymbol{Q}$ начинает неограниченно расти. Хотя таргет-сетка более-менее справляется с тем, чтобы стабилизровать процесс, предотвратить этот эффект полностью у неё не получается: сравнение обучающейся $\boldsymbol{Q}$ с Монте-Карло оценками и зачастую просто со здравым смыслом выдаёт присутствие в алгоритме заметного смещения в сторону переоценки (overestimation bias). Почему так происходит?

Очевидно, что источник проблемы - оператор максимума в формуле построения таргета:

$$
y(\tau)=r+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)
$$

При построении таргета есть два источника ошибок: 1) ошибка аппроксимации 2) внешняя стохастика. Максимум здесь «выбирает» то действие, для которого из-за ошибки нейросети или из-за везения в прошлых играх $Q\left(s^{\prime}, a^{\prime}\right)$ больше правильного значения.

Пример 66: Вы выиграли в лотерею +100 в силу везения. В вашем опыте нет примеров того, как вы купили билет и проиграли, и поэтому алгоритм будет учить завышенное значение $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ для действия «купить билет в лотерею» (напомним, в утверждении 31 мы отмечали, что запущенный с реплей буфера Q-learning, как и DQN, учит оптимальную Q-функцию для эмпирического MDP). Это завышение из-за везения.

Из-за того, что вы моделируете Q-сетку нейросетью, при увеличении $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ случайно увеличилось значение ценности действия «играть в казино», поскольку оно опиралось на примерно те же признаки описания состояний. Это завышение из-за ошибки аппроксимации.

А дальше начинается «цепная реакция»: при построении таргета для состояния «казино» $s^{\prime}$ в задачу регрессии поступают завышенные значения, предвещающие кучу награды. Завышенное значение начинает распространятся дальше на другие состояния: вы начинаете верить, что вы всегда можете пойти в казино и получить кучу награды.

Утверждение 37: Рассмотрим одно $s^{\prime}$. Пусть $Q^{*}\left(s^{\prime}, a^{\prime}\right)$ - истинные значения, и для каждого действия $a^{\prime}$ есть модель $\boldsymbol{Q}\left(s^{\prime}, \boldsymbol{a}^{\prime}\right) \approx \boldsymbol{Q}^{*}\left(s^{\prime}, a^{\prime}\right)$, которая оценивает это действие с некоторой погрешностью $\varepsilon\left(a^{\prime}\right)$ :

$$
Q\left(s^{\prime}, a^{\prime}\right)=Q^{*}\left(s^{\prime}, a^{\prime}\right)+\varepsilon\left(a^{\prime}\right)
$$

Пусть эти погрешности $\varepsilon\left(a^{\prime}\right)$ независимы по действиям, а завышение и занижение оценки равеновероятны:

$$
\mathbf{P}\left(\varepsilon\left(a^{\prime}\right)>0\right)=\mathbf{P}\left(\varepsilon\left(a^{\prime}\right)<0\right)=0.5
$$

Тогда оценка максимума скорее завышена, чем занижена:

$$
\mathbf{P}\left(\max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)>\max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right)>0.5
$$

Доказательство. С вероятностью 0.5 модель переоценит самое хорошее действие, на котором достигается максимум истинной $Q^{*}\left(s^{\prime}, a^{\prime}\right)$; и ещё к тому же с некоторой ненулевой вероятностью возникнет настолько завышенная оценка какого-нибудь из других действий, что $Q\left(s^{\prime}, a^{\prime}\right)>\max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)$.

Пример 67: Предположим, для $s^{\prime}$ у нас есть три действия, и на самом деле $Q^{*}\left(s^{\prime}, a^{\prime}\right)=0$ для всех трёх возможных действий. Мы оцениваем каждое действие с некоторой погрешностью. Допустим даже, эта погрешность в среднем равна нулю и распределена по Гауссу: $\boldsymbol{Q}\left(s^{\prime}, \boldsymbol{a}^{\prime}\right) \sim \mathcal{N}\left(0, \sigma^{2}\right)$. Тогда случайная величина $\max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)$ - взятие максимума из трёх сэмплов из гауссианы - очевидно такова, что

$$
\mathbb{E} \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)>0
$$

Хотя истинный максимум $\max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)=0$. Получается, что, несмотря на то, что каждый элемент оценён несмещённо, максимум по аппроксимациям будет смещён в сторону завышения.

Одно из хороших решений проблемы заключается в разделении (decoupling) двух этапов подсчёта максимума:

---

выбор действия (action selection) и оценка действия (action evaluation):

$$
\max _{\boldsymbol{a}^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)=\overbrace{\boldsymbol{Q}\left(s^{\prime}, \underbrace{\operatorname{argmax}_{\boldsymbol{a}^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)}_{\text {выбор действия }}\right)}^{\text {оценка действия }}
$$

Действительно: мы словно дважды используем одну и ту же погрешность при выборе действия и оценке действия. Из-за скоррелированности ошибки на этих двух этапах и возникает эффект завышения.

Основная идея борьбы с этой проблемой заключается в следующем: предлагается ${ }^{3}$ обучать два приближения Q-функции параллельно и использовать аппроксимацию Q-функции «независимого близнеца» для этапа оценивания действия:

$$
\begin{aligned}
& y_{1}(\bar{v}) \equiv r+\gamma Q_{\theta_{2}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} Q_{\theta_{1}}\left(s^{\prime}, a^{\prime}\right)\right) \\
& y_{2}(\bar{v}) \equiv r+\gamma Q_{\theta_{1}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} Q_{\theta_{2}}\left(s^{\prime}, a^{\prime}\right)\right)
\end{aligned}
$$

Если обе аппроксимации Q-функции идеальны, то, понятное дело, мы всё равно получим честный максимум. Однако, если оба DQN честно запущены параллельно и даже собирают каждый свой опыт (что в реальности, конечно, дороговато), их ошибки аппроксимации и везения будут в «разных местах». В итоге, Q-функция близнеца выступает в роли более пессимистичного критика действия, выбираемого текущей Q-функцией.

Если сбор уникального опыта для каждого из близнецов не организуется, Q-функции всё равно получаются скоррелированными. Как минимум, можно смиллировать для обучения сеток разные мини-батчи из реплей буфера, если он общий.

Интуиция, почему «независимая» Q-функция близнеца используется именно для оценки действия, а не наоборот для выбора: если из-за неудачного градиентного шага наша сетка $Q_{\theta_{1}}$ попла куда-то не туда, мы не хотим, чтобы плохие значения попадали в её же таргет $y_{1}$. Плохие действия в таргет пусть попадают: пессимистичная оценка здесь предпочтительнее.

На практике таргет-сети в такой модели всё равно используют, и в формулах целевой переменной всюду используются именно «замороженные» $Q_{\theta_{1}^{-}}$и $Q_{\theta_{2}^{-}}$. Это ещё больше противодействует ценным реакциям. Без них всё равно может быть такое, что сломанная $Q_{\theta_{1}}$ поломает $y_{2}$, та поломает $Q_{\theta_{2}}$, та поломает $y_{1}$, а та в свою очередь продолжить ломать $Q_{\theta_{1}}$, хотя, конечно, такая «цепь» менее вероятна, чем в обычном DQN.

Пример 68: На картинке для каждого из четырёх действий указаны значения идеальной $Q^{*}\left(s^{\prime}, a^{\prime}\right)$ и две аппроксимации $Q_{1}, Q_{2}$. Каждая аппроксимация условно выдаёт значение с погрешностью, которая в среднем равна нулю (вероятности завышенной оценки или заниженной оценки равны).


Видно, что обе аппроксимации оценивают максимум по действиям завышенно. Но если одна из аппроксимаций выберет действие (то $\boldsymbol{a}^{\prime}$, на котором достигается её максимум), а другая аппроксимация выдаст значение в выбранном индексе, то получится более близкое к адекватной оценки истинного максимума значение.

[^0]
[^0]:    ${ }^{3}$ когда эту идею предлагали в 2010-ом году в рамках классического RL (тогда нейронки ещё не вставили), то назвали её Double Q-learning, но сейчас под Double DQN подразумевается алгоритм 2015-го года (см. раздел 4.2.3), а этот трюк иногда именуется «близнецами» (Twin). Правда, в последнее время из-за алгоритма Twin Delayed DDPG (см. раздел 6.1.6) под словом Twin понимается снова не эта формула, и с названиями есть небольшая путаница...

---

# 4.2.2. Twin DQN 

Разовьём интуицию дальше. Вот мы строим таргет для $\boldsymbol{Q}_{\boldsymbol{\theta}_{1}}$. Мы готовы выбрать при помощи неё же действия, но не готовы оценить их ею же самой в силу потенциальной переоценки. Для этого мы и берём «независимого близнеца» $\boldsymbol{Q}_{\boldsymbol{\theta}_{2}}$. Но что, если он выдаёт ещё больше? Что, если его оценка выбранного действия, так случилась, потенциально ещё более завышенная? Давайте уж в таких ситуациях всё-таки брать то значение, которое поменьше! Получаем следующую интересную формулу, которую называют twin-оценкой (или ещё clipped double оценкой):

$$
\begin{aligned}
& y_{1}(\mathbb{T}) \vDash r+\gamma \min _{i=1,2} Q_{\theta_{1}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} Q_{\theta_{2}}\left(s^{\prime}, a^{\prime}\right)\right) \\
& y_{2}(\mathbb{T}) \vDash r+\gamma \min _{i=1,2} Q_{\theta_{i}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} Q_{\theta_{1}}\left(s^{\prime}, a^{\prime}\right)\right)
\end{aligned}
$$

Это очень забавная формула, поскольку она говорит бороться с проблемой «клин клином»: зная, что взятие максимума по аппроксимациям приводит к завышению, мы вводим в оценку искусственное занижение, добавляя в формулу минимум по аппроксимациям! По сути, это формула «ансамблирования» Q-функций: только вместо интуитивного среднего берём минимум, «для борьбы с максимумом».

Конечно, в отличие от обычного ансамблирования в машинном обучении, такой подход плохо масштабируется с увеличением числа обучаемых Q-функций (обычно учат всё-таки только две). В случае ансамбля имеет смысл брать не среднее, и не минимум, а, например, какой-то квантиль, более близкий к минимуму, чем к медиане - но возникает неудобный гиперпараметр, что же именно брать.

### 4.2.3. Double DQN

Запускать параллельно обучение двух сеток дороговато, а при общем реплей буфере корреляция между ними всё равно будет. Поэтому предлагается простая идея: запускать лишь один DQN, а в формуле таргета для оценивания вместо «близнеца» использовать таргет-сеть. То есть: пусть $\boldsymbol{\theta}$ - текущие веса, $\boldsymbol{\theta}^{-}$- веса таргет-сети, раз в $\boldsymbol{K}$ шагов копирующиеся из $\boldsymbol{\theta}$. Тогда таргет вычисляется по формуле:

$$
y(\mathbb{T}) \vDash r+\gamma Q_{\theta^{-}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)\right)
$$

Хотя понятно, что таргет-сеть и текущая сетка очень похожи, такое изменение формулы целевой переменной всё равно «избавляет» нас от взятия оператора максимума; эмпирически оказывается, что такая декорреляция действительно помогает стабилизации процесса. При этом, в отличие от предыдущих вариантов, такое изменение бесплатно: не требует обучения второй нейросети.

Если обучение второй Q-сети тяжеловесно, то рекомендуется использовать формулу Double DQN. Если же речь идёт об обучении маленьких полносвязных нейросеток, то вероятно, обучение «ансамбля» хотя бы из двух Q-функций по крайней мере с общего буфера не должно быть особо дорогим, и тогда имеет смысл пользоваться Twin-оценкой из раздела 4.2.2.

### 4.2.4. Dueling DQN

Рассмотрим ещё одну очевидную беду Q-обучения на примере.
Пример 69: Вы в целях исследования попробовали кинуться в яму $\boldsymbol{s}$. Сидя в яме, вы попробовали $\boldsymbol{a}$ - поднять правую руку. В яме холодно и грустно, поэтому вы получили -100 . Какой вывод делает агент? Правильно: для этого состояния $\boldsymbol{s}$ оценку этого действия $\boldsymbol{a}$ нужно понизить. Остальные не трогать; оценка самого состояния (по формуле (3.15) - максимум Q-функции по действиям) скорее всего не изменится, и надо бы вернуться в это состояние и перепроверить ещё и все остальные действия: попробовать поднять левую руку, свернуться калачиком...

В сложных MDP ситуация зачастую такова, что получение негативной награды в некоторой области пространства состояний означает в целом, что попадание в эту область нежелательно. Верно, что с точки зрения теории остальные действия должны быть «исследованы», но неудачный опыт должен учитываться внутри оценки самого состояния; иначе агент будет возвращаться в плохие состояния с целью перепробовать все действия без понимания, что удача здесь маловероятна, вместо того, чтобы исследовать те ветки, где негативного опыта «не было». Понятно, что проблема тем серьёзнее, чем больше размерность пространства действий $|\mathcal{A}|$.

---

Формализуя идею, мы хотели бы в модели учить не $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ напрямую, а получать их с учётом $\boldsymbol{V}^{*}(\boldsymbol{s})$. Иными словами, модель должна знать ценность самих состояний и с её учётом выдавать ценности действий. При этом при получении, скажем, негативной информации о ценности одного из действий, должна понижаться в том числе и оценка V-функции. Дуэльная (dueling) архитектура - это модификация вычислительного графа нашей модели с параметрами $\boldsymbol{\theta}$, в которой на выходе предлагается иметь две головы, V-функцию $\boldsymbol{V}(\boldsymbol{s}) \approx \boldsymbol{V}^{*}(\boldsymbol{s})$ и Advantage-функцию

$\boldsymbol{A}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{A}^{*}(\boldsymbol{s}, \boldsymbol{a}):$

$$
\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a}):=\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})+\boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})
$$

Это интересная идея решить проблему просто сменой архитектуры вычислительного графа: при ацдейте значения для одной пары $\boldsymbol{s}, \boldsymbol{a}$ неизбежно поменяется значение $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})$ ценности всего состояния, которое общее для всех действий. Мы таким образом вводим следующий «прайор»: ценности действий в одном состоянии всётаки связаны между собой, и если одно действие в состоянии плохое, то вероятно, что и остальные тоже плохие («само состояние плохое»).

Здесь есть подвох: если $\boldsymbol{V}^{*}(\boldsymbol{s})$ - это произвольный скаляр, то $\boldsymbol{A}^{*}(\boldsymbol{s}, \boldsymbol{a})$ - не произвольный. Действительно: мы моделируем $|\boldsymbol{\mathcal { A }}|$ чисел, выдавая почему-то $|\boldsymbol{\mathcal { A }}|+\mathbf{1}$ число: то есть почему-то вводим «лишнюю степень свободы». Вообще-то, Advantage-функция не является произвольной функцией и обязана подчиняться (22). Для оптимальных стратегий в предположении жадности нашей стратегии это утверждение вырождается в следующее свойство:

# Утверждение 38: 

$$
\forall s: \max _{\boldsymbol{a}} \boldsymbol{A}^{*}(\boldsymbol{s}, \boldsymbol{a})=0
$$

Доказательство.

$$
\max _{\boldsymbol{a}} \boldsymbol{A}^{*}(\boldsymbol{s}, \boldsymbol{a})=\max _{\boldsymbol{a}} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})-\boldsymbol{V}^{*}(\boldsymbol{s})=\left\{\text { связь } \mathrm{V}^{*} \mathrm{Q}^{*}(3.15)\right\}=0
$$

Это условие можно легко учесть, вычтя максимум в формуле (4.3):

$$
\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a}):=\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})+\boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})-\max _{\hat{\boldsymbol{a}}} \boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \hat{\boldsymbol{a}})
$$

Таким образом мы гарантируем, что максимум по действиям последних двух слагаемых равен нулю, и они корректно моделируют Advantage-функцию.

Заметим, что $\boldsymbol{V}$ и $\boldsymbol{A}$ не учатся по отдельности (для $\boldsymbol{V}^{*}$ уравнение оптимальности Беллмана не сводится к регрессии, для $\boldsymbol{A}^{*}$ уравнения Беллмана не существует вообще); вместо этого минимизируется лосс для Q функции точно так же, как и в обычном DQN.

В нейросетих формулу (4.4) реализовать очень просто при помощи двух голов: одна выдаёт скаляр, другая $|\boldsymbol{\mathcal { A }}|$ чисел, из которых вычитается максимум. Дальше к каждой компоненте второй головы добавляется скаляр, выданный первой головой, и результат считается выданным моделью $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$.

Нюанс: авторы статьи эмпирически обнаружили, что замена максимума на среднее даёт чуть лучшие результаты. Вероятно, в реплей буфере очень часто встречаются пары $\boldsymbol{s}, \boldsymbol{a}$ такие, что $\boldsymbol{a}$ - наилучшее действие в этом состоянии по мнению текущей модели (а то есть $\boldsymbol{a}=\underset{\boldsymbol{a}}{\operatorname{argmax}} \boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$ ), и поэтому градиент $\boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})-$ $-\max _{\boldsymbol{a}} \boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \hat{\boldsymbol{a}})$ часто зануляется. В результате, на текущий день под дуэльной архитектурой понимают альтернативную формулу:

$$
\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a}):=\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})+\boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})-\frac{1}{|\boldsymbol{\mathcal { A }}|} \sum_{\hat{\boldsymbol{a}}} \boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \hat{\boldsymbol{a}})
$$

### 4.2.5. Шумные сети (Noisy Nets)

По дефолту, алгоритмы на основе DQN решают дилемму исследования-использования при помощи примитивной $\varepsilon$-жадной стратегии взаимодействия со средой. Этот бэйзлайн-подход плох примерно всем, в первую очередь тем, что крайне чувствителен к гиперпараметрам: для $\varepsilon$ обязательно нужно составлять какое-нибудь расписание, чтобы в начале обучения он был побольше, а потом постепенно затухал, и откуда брать это расписание - непонятно. При этом слишком большие значения шума существенно замедляют обучение, заставляя агента вести себя случайно, а раннее затухание приведёт к застреванию алгоритма в каком-нибудь локальном оптимуме (агент будет биться головой об стенку, не пробуя её обойти).

Чтобы понять, что случилось именно это, можно посмотреть игры агента: если, например, Марио всё время доходит до середины первого уровня и прыгает в одну и ту же яму, то у него просто нет положительного опыта перепрыгивания этой ямы, и поэтому он не знает, что можно набрать больше награды.

---

Ключевая причина, почему $\varepsilon$-жадная стратегия примитивна, заключается в независимости добавляемого шума от текущего состояния. Мы выдаём оценки Q-функции и в зависимости только от времени принимаем решение, использовать ли эти знания или эксплорить. Интуитивно, правильнее было бы принимать это решение в зависимости от текущего состояния: если состояние исследовано, чаще принимать решение в пользу использования знаний, если ново - в пользу исследования. Открытие новой области пространства состояний скорее всего означает, что в ней стоит поделать разные действия, когда двигаться к ней нужно за счёт использования уже накопленных знаний.


Шумные сети (noisy nets) - добавление шума с обучаемой и, главное, зависимой от состояния (входа в модель) дисперсией. Хак чисто инженерный: давайте каждый параметр в модели заменим на

$$
\theta_{i}:=w_{i}+\sigma_{i} \varepsilon_{i}, \quad \varepsilon_{i} \sim \mathcal{N}(0,1)
$$

или, другими словами, заменим веса сети на сэмплы из $\mathcal{N}\left(w_{i}, \sigma_{i}^{2}\right)$, где $w, \sigma \in \mathbb{R}^{h}$ - параметры модели, обучаемые градиентным спуском. Очевидно, что выход сети становится случайной величиной, и, в зависимости от шума $\varepsilon$, будет меняться выбор действия $\boldsymbol{a}=\underset{\boldsymbol{n}}{\operatorname{argmax}} Q_{\theta}(s, a, \varepsilon)$. При этом влияние шума на принятое решение зависит от поданного в модель входного состояния.

Если состояния - изображения, шум в свёрточные слои обычно не добавляется (зашумлять выделение объектов из изображения кажется бессмысленным).

Формально, коли наша модель стала стохастичной, мы поменяли оптимизируемый функционал: мы хотим минимизировать функцию потерь в среднем по шуму:

$$
\mathbb{E}_{\varepsilon} \operatorname{Loss}(\theta, \varepsilon) \rightarrow \min _{\theta}
$$

Видно, что градиент такого функционала можно несмещённо оценивать по Монте-Карло:

$$
\nabla_{\theta} \mathbb{E}_{\varepsilon} \operatorname{Loss}(\theta, \varepsilon)=\mathbb{E}_{\varepsilon} \nabla_{\theta} \operatorname{Loss}(\theta, \varepsilon) \approx \nabla_{\theta} \operatorname{Loss}(\theta, \varepsilon), \quad \varepsilon \sim \mathcal{N}(0, I)
$$

Гарантий, что магнитуда шума в среднем будет падать для исследованных состояний, вообще говоря, нет. Надежда этой идеи в том, что магнитуда будет подстраиваться в зависимости от текущих в модель градиентов: если модель часто видит какое-то $\boldsymbol{s}$ и функция потерь говорит, что на этом состоянии нужно выдавать, скажем, низкое значение, модель будет учиться при любых сэмплах $\boldsymbol{\varepsilon}$ выдавать указанное низкое значение. Для этого модели будет удобно уменьшать дисперсию впрыскиваемого шума и больше опираться на те нейронные связи, которые мало зашумлены. Если же в сеть поступают противоречивые сигналы о паре $\boldsymbol{s}, \boldsymbol{a}$, или это какое-то новое $\boldsymbol{s}$, которого модель ещё не видела, выходное значение модели будет, интуитивно, сильно зашумлено, и часто аргмаксимум будет достигаться именно на нём.

Ещё одно ключевое преимущество идеи в том, что в этом подходе отсутствуют гиперпараметры.
(5) Кроме инициализации. Неудачная инициализация $\boldsymbol{\sigma}$ всё равно может замедлить процесс обучения; обычно дисперсию шума инициализируют какой-нибудь константой, и эта константа становится в некотором смысле важным гиперпараметром алгоритма.

Заметим, что таргет $\boldsymbol{y}(\mathbb{T})$, который мы генерируем для каждого перехода $\mathbb{T}$ из батча, формально теперь тоже должен вычисляться как мат.ожидание по шуму:

$$
y(\mathbb{T}):=\mathbb{E}_{\varepsilon}\left[r+\gamma \max _{a^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}, \varepsilon\right)\right]
$$

Опять же, мат.ожидание несмещённо оценивается по Монте-Карло, однако с целью декорреляции полезно использовать в качестве $\boldsymbol{\varepsilon}$ другие сэмплы, нежели используемые при вычислении лосса. Считается, что подобное «зашумление» целевой переменной в DQN может даже пойти на пользу.

Генерация сэмплов шума по числу параметров нейросети на видеокарте может сильно замедлить время прохода через сеть. Для оптимизации авторы предлагают для матриц полносвязных слоёв генерировать шум следующим образом. Пусть $\boldsymbol{n}$ - число входов, $\boldsymbol{m}$ - число выходов в слое. Сэмплируются $\varepsilon_{1} \sim$ $\mathcal{N}\left(0, I_{m \times m}\right), \varepsilon_{2} \sim \mathcal{N}\left(0, I_{n \times n}\right)$, после чего полагается шум для матрицы равным

$$
\varepsilon:=\boldsymbol{f}\left(\varepsilon_{1}\right) \boldsymbol{f}\left(\varepsilon_{2}\right)^{\boldsymbol{T}}
$$

где $\boldsymbol{f}$ - масштабирующая функция, например $\boldsymbol{f}(\boldsymbol{x})=\operatorname{sign}(\boldsymbol{x}) \sqrt{|x|}$ (чтобы каждый сэмпл в среднем всё ещё имел дисперсию 1). Процедура требует всего $\boldsymbol{m}+\boldsymbol{n}$ сэмплов вместо $\boldsymbol{m} \boldsymbol{n}$, но жертвует независимостью сэмплов внутри слоя. К сожалению, даже при таком хаке время работы алгоритма заметно увеличивается, поскольку проходов через нейросеть в DQN нужно делать очень много.

---

Альтернативно, можно зашумлять выходы слоёв (тогда сэмплов понадобится на порядок меньше) или просто добавлять шум на вход. В обоих случаях, «зашумлённость» выхода будет обучаемой, а степень влияния шума на выход сети будет зависеть от состояния.

# 4.2.6. Приоритизированный реплей (Prioritized DQN) 

Off-policy алгоритмы позволяют хранить и переиспользовать весь накопленный опыт. Однако, интуитивно ясно, что встречавшиеся переходы существенно различаются по важности. Зачастую большая часть буфера, особенно понячалу обучения, состоит из записей изучения агентом ближайшей стенки, а переходы, включавшие, например, получение ещё не выученной внутри аппроксимации Q-функции награды, встречаются в буфере сильно реже и при равномерном сэмплировании редко оказываются в мини-батчах.

Важно, что при обучении оценочных функций информация о награде распространяется от последних состояний к первым. Например, на первых итерациях довольно бессмысленно обновлять те состояний, где сигнала награды не было $(\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})=\mathbf{0})$, а Q-функция для следующего состояния примерно случайна (а именно такие переходы чаще всего и попадаются алгоритму). Такие обновления лишь схлопывают выход аппроксимации к константе (которая ещё и имеет тенденцию к росту из-за оператора максимума). Ценной информацией понячалу являются терминальные состояния, где целевая


Trivial updates

переменная по определению равна $\boldsymbol{y}(\mathbb{T})=\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ и является абсолютно точным значением $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$. Типично, что на таких переходах значения временной разницы (лосса DQN) довольно высоко. Аналогичная ситуация в принципе справедлива для любых наград, которые для агента новы и ещё не распространились в аппроксимацию через уравнение Беллмана.

Очень хочется сэмплировать переходы из буфера не равномерно, а приоритизировано. Приоритет установим, например, следующим образом:

$$
\rho(\mathbb{T}):=\left(y(\mathbb{T})-Q_{\theta}(s, a)\right)^{2}=\operatorname{Loss}\left(y(\mathbb{T}), Q_{\theta}(s, a)\right)
$$

Сэмплирование переходов из буфера происходит по следующему правилу:

$$
\mathbf{P}(\mathbb{T}) \propto \rho(\mathbb{T})^{\alpha}
$$

где гиперпараметр $\boldsymbol{\alpha}>\mathbf{0}$ контролирует масштаб приоритетов (в частности, $\boldsymbol{\alpha}=\mathbf{0}$ соответствует равномерному сэмплированию, когда $\boldsymbol{\alpha} \rightarrow+\boldsymbol{\infty}$ соответствовало бы жадному сэмплированию самых «важных» переходов).

Добиться эффективного сэмплирования с приоритетами можно благодаря структуре данных SumTree: бинарному дереву, у которого в каждом узле хранится сумма значений в двух детах. Сам массив приоритетов для буфера хранится на нижнем уровне дерева; в корне, соответственно, лежит сумма всех приоритетов, нормировочная константа. Для сэмплирования достаточно взять случайную равномерную величину и спуститься по дереву. Таким образом, процедура сэмплирования имеет сложность $\boldsymbol{O}(\log M)$, где $\boldsymbol{M}$ - размер буфера. За ту же сложность проходит обновление приоритета одного элемента, для чего достаточно обновить значения во всех его предках для поддержания структуры.

Техническая проблема идеи (4.6) заключается в том, что после каждого обновления весов сети $\boldsymbol{\theta}$ приоритеты переходов меняются для всего буфера (состоящего обычно из миллионов переходов). Пересчитывать все приоритеты, конечно же, непрактично, и необходимо ввести некоторые упрощения. Например, можно обновлять приоритеты только у переходов из текущего батча, для которых значение лосса так и так считается. Это, вообще говоря, означает, что если у перехода был низкий приоритет, и до него дошла, условно, «волна распространения» награды, алгоритм не узнает об этом, пока не засэмплирует переход с тем приоритетом, который у него был.

Новые переходы добавляются в буфер с наивысшим приоритетом $\boldsymbol{\operatorname { m p x }} \boldsymbol{\rho}(\mathbb{T})$, который можно поддерживать за константу, или вычислять текущий приоритет, дополнительно рассчитывая (4.6) для онлайн-сэмплов.

В чём у такого подхода есть фундаментальная проблема? После неконтролируемой замены равномерного сэмплирования на какое-то другое, могло случиться так, что для наших переходов $\boldsymbol{s}^{\prime} \neq \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Почему это так, проще понять на примере.

Пример 70: Пусть для данной пары $\boldsymbol{s}, \boldsymbol{a}$ с вероятностью 0.9 мы попадаем в $\boldsymbol{s}^{\prime}=\boldsymbol{A}$, а с вероятностью 0.1 мы попадаем в $\boldsymbol{s}^{\prime}=\boldsymbol{B}$. В условно бесконечном буфере для этой пары $\boldsymbol{s}, \boldsymbol{a}$ среди каждых 10 сэмплов будет 1 сэмпл с $\boldsymbol{s}^{\prime}=\boldsymbol{B}$ и 9 сэмплов с $\boldsymbol{s}^{\prime}=\boldsymbol{A}$, и равномерное сэмплирование давало бы переходы, удовлетвориющие $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$

---

Для приоритизированного реплея, веса у переходов с $\boldsymbol{s}^{\prime}=\boldsymbol{A}$ могут отличаться от весов для переходов с $\boldsymbol{s}^{\prime}=\boldsymbol{B}$. Например, если мы оцениваем $\boldsymbol{V}(\boldsymbol{A})=\mathbf{0}, \boldsymbol{V}(\boldsymbol{B})=\mathbf{1}$, и уже даже правильно выучили среднее значение $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})=\mathbf{0 . 1}$, то $\operatorname{Loss}\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}=\boldsymbol{A}\right)$ будет равен $\mathbf{0 . 1 ^ { 2 }}$, а для $\operatorname{Loss}\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}=\boldsymbol{B}\right)=\mathbf{0 . 9 ^ { 2 }}$. Значит, $\boldsymbol{s}^{\prime}=\boldsymbol{B}$ будет появляться в засэмплированных переходах чаще, чем с вероятностью 0.1 , и это выбьет $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ с её правильного значения.


Иными словами, приоритизированное сэмплирование приводит к смещению (bias). Этот эффект не так страшен поначалу обучения, когда распределение, из которого приходят состояния, всё равно скорее всего не сильно разнообразно. Более существенно нивелировать этот эффект по ходу обучения, в противном случае процесс обучения может полностью дестабилизироваться или где-нибудь застрять.

Заметим, что равномерное сэмплирование не является единственным «корректным» способом, но основным доступным. Мы не очень хотим «возвращаться» к нему постепенно с ходом обучения, но можем сделать похожую вещь: раз мы хотим подменить распределение, то можем при помощи importance sampling сохранить тот же оптимизируемый функционал:

Теорема 36: При сэмплировании с приоритетами $\mathbf{P}(\mathbb{T})$ использование весов $\boldsymbol{w}(\mathbb{T}) \equiv \frac{1}{\mathbf{P}(\mathbb{T})}$ позволит избежать эффекта смещения.

Доказательство. Пусть $\boldsymbol{M}$ - размер буфера.

$$
\begin{aligned}
\mathbb{E}_{\mathbb{T} \sim \text { Uniform }} \operatorname{Loss}(\mathbb{T}) & =\sum_{i=1}^{M} \frac{1}{M} \operatorname{Loss}\left(\mathbb{T}_{i}\right)= \\
& =\sum_{i=1}^{M} \mathbf{P}\left(\mathbb{T}_{i}\right) \frac{1}{M \mathbf{P}\left(\mathbb{T}_{i}\right)} \operatorname{Loss}\left(\mathbb{T}_{i}\right)= \\
& =\mathbb{E}_{\mathbb{T} \sim \mathbf{P}(\mathbb{T})} \frac{1}{M \mathbf{P}(\mathbb{T})} \operatorname{Loss}(\mathbb{T})
\end{aligned}
$$

что с точностью до константы $\frac{1}{M}$ и есть перевзвешивание функции потерь.
Importance sampling подразумевает, что мы берём «интересные» переходы, но делаем по ним меньшие шаги (вес меньше именно для «приоритетных» переходов). Цена за такую корректировку, конечно, в том, что полезность приоритизированного сэмплирования понижается. Раз поначалу смещение нас не так беспокоит, предлагается вводить веса постепенно: а именно, использовать веса

$$
\boldsymbol{w}(\mathbb{T}) \vDash \frac{1}{\mathbf{P}(\mathbb{T})^{\boldsymbol{\beta}(\boldsymbol{t})}}
$$

где $\boldsymbol{\beta}(\boldsymbol{t})$ - гиперпараметр, зависящий от итерации алгоритма $\boldsymbol{t}$. Изначально $\boldsymbol{\beta}(\boldsymbol{t}=\mathbf{0})=\mathbf{0}$, что делает веса равномерными (корректировки не производится), но постепенно $\boldsymbol{\beta}(\boldsymbol{t})$ растёт к 1 и полностью избавляет алгоритм от эффекта смещения.

На практике веса, посчитанные по такой формуле, могут оказаться очень маленькими или большими, и их следует нормировать. Вариации, как нормировать, различаются в реализациях: можно делить веса на $\max \boldsymbol{w}(\mathbb{T})$, где максимум берётся, например, только по текущему мини-батчу, чтобы гарантировать максимальный вес 1 .

# 4.2.7. Multi-step DQN 

Уже упоминалось, что DQN из-за одношаговых целевых переменных страдает от проблемы отложенного сигнала и сопряжённой с ней в контексте нейросетевой аппроксимации проблемы накапливающейся ошибки. Эта проблема фундаментальна для off-policy подхода: в разделе 3.5 про bias-variance trade-off упоминалось, что разрешать дилемму смещения-разброса (то есть «проводить умный credit assignment») мы можем только в on-policy режиме.

Многошагооый (multi-step) DQN - теоретически некорректная эвристика для занижения этого эффекта. Грубо говоря, нам очень хочется распространять за одну итерацию награду сразу на несколько шагов вперёд, то есть решать многошаговые уравнения Беллмана (3.25). Мы как бы и можем уравнение оптимальности многошаговое выписать...

---

$$
Q^{*}\left(s_{0}, a_{0}\right)=\mathbb{E}_{\boldsymbol{T}_{/ N} \sim \pi^{*}}\left|s_{0}, a_{0}\right|\left[\sum_{t=0}^{N-1} \gamma^{t} r_{t}+\gamma^{N} \mathbb{E}_{s_{N}} \max _{s_{N}^{*}} Q^{*}\left(s_{N}, a_{N}^{*}\right)\right]
$$

Что мы можем сделать? Мы можем прикинуться, будто решаем многошаговые уравнения Беллмана, задав целевую переменную следующим образом:

$$
y\left(s_{0}, a_{0}\right):=\sum_{t=0}^{N-1} \gamma^{t} r_{t}+\gamma^{N} \max _{s_{N}} Q_{\theta}\left(s_{N}, a_{N}\right)
$$

где $s_{1}, a_{1} \ldots a_{N-1}, s_{N}$ взяты из буфера. Для этого в буфере вместо одношаговых переходов $\mathbb{T}:=$ $:=(s, a, r, s^{\prime}$, done $)$ достаточно просто хранить другую пятёрку:

$$
\mathbb{T}:=\left(s, a, \sum_{n=0}^{N-1} \gamma^{n} r^{(n)}, s^{(N)}, \text { done }\right)
$$

где $\boldsymbol{r}^{(n)}$ - награда, полученная через $\boldsymbol{n}$ шагов после посещения рассматриваемого состояния $\boldsymbol{s}, \boldsymbol{s}^{(N)}$ - состояние, посещённое через $\boldsymbol{N}$ шагов, и, что важно, флаг done указывает на то, завершился ли эпизод в течение этого $N$-шагового роллаута ${ }^{4}$. Все остальные элементы алгоритма не изменяются, в частности, можно видеть, что случай $N=1$ соответствует обычному DQN.

Видно, что теперь награда, полученная за один шаг, распространяется на $\boldsymbol{N}$ состояний в прошлое, и мы таким образом не только ускоряем обучение оценочной функции стартовых состояний, но и нивелируем проблему накапливающейся ошибки.

Почему теоретически это некорректно? Беря $s_{1}, a_{1} \ldots a_{N-1}, s_{N}$ из буфера, мы получаем состояния из функции переходов, которая стационарна и соответствует тому мат.ожиданию, которое стоит в уравнении (4.7). Но вот действия в этом мат.ожидании должны приходить из оптимальной стратегии! А в буфере $a_{1}, a_{2} \ldots a_{N-1}$ - действия нашей стратегии произвольной давности (то есть сколь угодно неоптимальные). Вместо того, чтобы оценивать оптимальное поведение за хвост траектории (по определению $Q^{*}$ ), мы $N-1$ шагов ведём себя сколько угодно неоптимально, а затем в $s^{(N)}$ подставляем оценку оптимального поведения за хвост. Иными словами, мы недооцениваем истинное значение правой части $N$-шагового уравнения Беллмана при $N>1$. Вместо уравнения оптимальности мы решаем такое уравнение: что, если я следующие $N$ шагов веду себя как стратегия $\boldsymbol{\mu}$, когда-то породившая данный роллаут, и только потом соберусь вести себя оптимально? Причём из-за нашего желания делать так в off-policy режиме $\boldsymbol{\mu}$ для каждого перехода своё, то есть схеме Generalized Policy Iteration (алг. 9) это не соответствует: в ней мы всегда должны оценивать именно текущую стратегию $\boldsymbol{\pi}$, а текущей стратегией в DQN является $\boldsymbol{\pi}(\boldsymbol{s})=\underset{a}{\operatorname{argmax}} Q_{\theta}(s, a)$.

Пример 71: Пример, когда многошаговая оценка приводит к некорректным апдейтам. На втором шаге игры в $s_{1}$ агент может скушать тортик или прыгнуть в лаву, и в первом эпизоде обучения агент совершил ошибку и получил огромную негативную награду. В буфер при $N>1$ запишется пример со стартовым состоянием $s_{0}$ и этой большой отрицательной наградой (в качестве $s^{(N)}$ будет записана лава). Пока этот пример живёт в реплей буфере, каждый раз, когда он сэмплируется в минибатче, оценочная функция для $s_{0}$ обновляется этой отрицательной наградой, даже если агент уже научился больше не совершать эту ошибку и в $s_{1}$ наслаждается


тортиками.

Эмпирически большое значение $\boldsymbol{N}$ действительно может полностью дестабилизировать процесс, как и подсказывает теория, поэтому рекомендуется выставлять небольшие значение 2-3, от силы 5.

Большие значения могут быть работоспособны в средах, где сколь угодно неоптимальное поведение в течение $N$ шагов не приводит к существенному изменению награды по сравнению с оптимальным поведением, то есть в средах, где нет моментов с «критическими решениями» (когда $\max _{a} Q^{*}(s, a)-\min _{a} Q^{*}(s, a)$ мало, то есть неоптимальное поведение в течение одного шага не приводит к сильно меньшей награде, чем оптимальное).

Пример 72: Пример среды без «критических решений»: вы робот, который хочет добраться до соседней комнаты. Действия вверх-вниз-вправо-влево чуть-чуть сдвигают робота в пространстве. Тогда «вести себя как угодно» в течение $N-1$ шагов и потом отправиться кратчайшим маршрутом до соседней комнаты приносит

[^0]
[^0]:    ${ }^{4}$ естественно, алгоритм должен рассматривать все $N$-шаговые роллауты, включая те, которые привели к завершению эпизода за $\boldsymbol{k}<\boldsymbol{N}$ шагов. Для них, естественно, $\boldsymbol{r}^{\left(k^{\prime}\right)}=0$ для $\boldsymbol{k}^{\prime}>\boldsymbol{k}$, и $\boldsymbol{Q}^{*}\left(\boldsymbol{s}^{(N)}, \boldsymbol{a}_{N}\right) \equiv \mathbf{0}$ для всех $\boldsymbol{a}_{N}$.

---

практически столько же награды, сколько и сразу отправиться кратчайшим маршрутом до соседней комнаты. Поэтому в таких ситуациях использование относительно большого $\boldsymbol{N}(5-10)$ может помочь, хоть алгоритм и может полностью дестабилизироваться (процедура некорректна).

# 4.2.8. Retrace 

Как мы обсуждали в разделе 3.5.7, теоретически корректным способом обучаться в off-policy с многошаговых оценок является использование Retrace оценки. Конечно, она может на практике схлопываться в одношаговые обновления, но по крайней мере гарантирует, что алгоритм не ломается; и важно, что если записанные в засэмплированном из буфера роллауте действия достаточно вероятны для оцениваемой политики, то оценка получается достаточно длинной.

Конечно, сложно говорить про «достаточно вероятны», когда оцениваемая политика детерминирована. Поэтому в практическом алгоритме Retrace предлагается перейти от моделирования Q-learning к моделированию SARSA (см. раздел 3.4.8): то есть, считать целевой политикой $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$ е-жадную стратегию по отношению к текущей модели Q-функции. Преимущество в том, что это делает стратегию стохастичной, и любые действия в буфере не приведут к запулению следа и полному схлопыванию в одношаговую оценку.

В буфере также нужно сохранять вероятности выбора сохранённых действий $\boldsymbol{\mu}(\boldsymbol{a} \mid \boldsymbol{s})$ в момент сбора данных (для $\varepsilon$-жадных стратегий эти значения всё время будут или $\frac{\varepsilon}{|\mathcal{A}|}$, или $\mathbf{1}-\varepsilon+\frac{s}{|\mathcal{A}|}$, где $\varepsilon-$ параметр эксплорейшна на момент сбора перехода). Вместо отдельных переходов теперь хранятся роллауты - фрагменты траекторий некоторой длины $N$.

Если в DQN обучение проводилось на мини-батчах из, скажем, 64 переходов, то в Retrace (при том же масштабе задачи) нужно засэмплировать для одного мини-батча 4 роллаута длины $N=16$. Такой выбор позволит надеяться на получение оценок длины вплоть до 16 -шаговой (что в Retrace будет достигнуто, если политика сбора данных совпадёт с оцениваемой политикой на оцениваемом роллауте). Важно помнить про проблему декорреляции: нужно, чтобы в мини-батче должны оказаться разнообразные примеры, поэтому нельзя, например, взять только один роллаут длины 64.

Далее для каждого засэмплированного из буфера роллаута $s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, \ldots s_{N}$ мы сначала для каждой пары $s_{t}, a_{t}$ считаем, используя формулы из теории Retrace, следующие вспомогательные величины: значение коэффициента затухания следа $\boldsymbol{c}_{\boldsymbol{t}}$ по формуле (3.58) (коэффициент $\boldsymbol{\lambda}$ обычно полагают равным единице) и значение одношаговой ошибки $\boldsymbol{\Psi}_{(1)}\left(s_{t}, a_{t}\right)$ по формуле (3.56):

$$
\begin{gathered}
c_{t}:=\min \left(1, \frac{\pi\left(a_{t} \mid s_{t}\right)}{\mu\left(a_{t} \mid s_{t}\right)}\right) \\
\Psi_{(1)}\left(s_{t}, a_{t}\right)=r_{t}+\gamma \mathbb{E}_{\hat{a}_{t+1} \sim \pi} Q_{\theta^{-}}\left(s_{t+1}, \hat{a}_{t+1}\right)-Q_{\theta^{-}}\left(s_{t}, a_{t}\right)
\end{gathered}
$$

Всюду, где используется $\boldsymbol{\pi}$, используется $\boldsymbol{\varepsilon}$-жадная стратегии по отношению к таргет-сети (хотя, если использовать идею Double DQN из раздела 4.2.3, то как раз во всех местах, где используется $\boldsymbol{\pi}$ - оцениваемая стратегия - имеет смысл использовать свежую версию Q-функции). В частности, мат.ожидание по $\boldsymbol{\pi}$ можно посчитать явно:

$$
\mathbb{E}_{\hat{a} \sim \pi} Q_{\theta^{-}}(s, \hat{a})=(1-\varepsilon) \max _{\hat{a}} Q_{\theta^{-}}(s, \hat{a})+\frac{\varepsilon}{|\mathcal{A}|} \sum_{\hat{a}} Q_{\theta^{-}}(s, \hat{a})
$$

После этого в Retrace для одной пары $s_{t}, a_{t}$ все будущие одношаговые ошибки нужно просуммировать (воспользуемся индексом $\hat{\boldsymbol{t}}$ для обозначения этого перебора), но заглядывание на каждый следующий $\boldsymbol{i}$-ый шаг в будущее обязывает нас потушить след в $\boldsymbol{c}_{\boldsymbol{i}}$ раз:

$$
\boldsymbol{\Psi}^{\text {retrace }}\left(s_{t}, a_{t}\right):=\sum_{\hat{t} \geq t}^{N} \gamma^{\hat{t}-t}\left(\prod_{i=t+1}^{i=\hat{t}} c_{i}\right) \Psi_{(1)}\left(s_{\hat{t}}, a_{\hat{t}}\right)
$$

Заметим, что в этой формуле внешняя сумма по $\hat{t}$ идёт не до бесконечности, как в теории Retrace, а до $\boldsymbol{N}$, до конца роллаута. После этого считаем, что след зануляется: это корректно, хотя иногда можно и потерять возможность получить более длинную оценку.

Далее в табличном методе мы бы провели обновление по формуле

$$
Q(s, a) \leftarrow Q(s, a)+\alpha \Psi^{\text {retrace }}(s, a)
$$

то есть воспользовались бы $\boldsymbol{\Psi}^{\text {retrace }}(s, a)$ как градиентом. Другими словами, оценка указывает, нужно ли увеличивать выход модели для рассматриваемой пары $s, \boldsymbol{a}$ или уменьшать. Чтобы получить задачу регрессии, целевая переменная строится по формуле

$$
y\left(s_{t}, a_{t}\right):=\boldsymbol{\Psi}^{\text {retrace }}\left(s_{t}, a_{t}\right)+Q_{\theta}\left(s_{t}, a_{t}\right)
$$

---

и дальше оптимизируется MSE, игнорируя зависимость $\boldsymbol{y}$ от $\boldsymbol{\theta}$ :

$$
\left(y\left(s_{t}, a_{t}\right)-Q_{\theta}\left(s_{t}, a_{t}\right)\right)^{2} \rightarrow \min _{\theta}
$$

Тогда градиент функции потерь по $\boldsymbol{\theta}$ для одного примера равен:

$$
\nabla_{\theta} \frac{1}{2}\left(y(s, a)-Q_{\theta}(s, a)\right)^{2}=\left(y(s, a)-Q_{\theta}(s, a)\right) \nabla_{\theta} Q_{\theta}(s, a)=\Psi^{\text {retrace }}(s, a) \nabla_{\theta} Q_{\theta}(s, a)
$$

Это полностью аналогично градиенту обычного DQN (4.2), только там оценка $\boldsymbol{\Psi ( s , a )}=\boldsymbol{r}+$ $\gamma \max _{\boldsymbol{n}} Q_{\theta^{-}}(s, a)-Q_{\theta}(s, a)$ была одношаговой, а здесь мы заглядываем настолько максимально далеко вперёд, насколько возможно (в силу использования $\boldsymbol{\lambda}=\mathbf{1}$ ).

В большинстве последних алгоритмов на основе DQN, таких как Agent57, используются формулы Retrace. Они позволяют максимально возможным образом побороться с ключевыми фундаментальными проблемами off-policy подхода, вытекающими из одношаговых целевых переменных, когда из недостатков можно выделить, пожалуй, лишь громоздкость формул.

# §4.3. Distributional RL 

### 4.3.1. Идея Distributional подхода

Задача RL такова, что в среде содержится в том числе неподконтрольная агенту стохастика: алеаторическая неопределённость (aleatoric uncertainty) ${ }^{5}$. Агент, предсказывающий, что он получит в будущем в среднем суммарную награду 6 , на самом деле может получить, например, только -10 или 10 , просто последний исход случится с вероятностью 0.8 , а первый - 0.2 . Помимо прочего, это означает, что часто агенту приходится рисковать: например, теоретически возможна ситуация, когда агент с малой вероятностью получает гигантскую награду, и тогда оптимальный агент на практике будет постоянно получать, например, какой-то штраф, компенсирующийся редко выпадающими мегаудачами. Вся эта информация заложена в распределении награды $\boldsymbol{R}(\boldsymbol{T})(1.4)$ как случайной величины.


В Distributional-подходе предлагается учить не среднее будущей награды, а всё распределение будущей награды как случайной величины. Складывается эта неопределённость как из подконтрольной агенту стохастики - его собственных будущих выборов действий - так и неподконтрольной, переходов (и награды, если рассматривается формализм со случайной функцией награды). Среднее есть лишь одна из статистик этого распределения.


Здесь стоит заранее оговориться о противоречиях, связанных с этой идеей. Обсуждение этой темы в первую очередь мотивировано эмпирическим превосходством Distributional-подхода по сравнению с алгоритмами, учащими только среднее, однако с теоретической точки зрения ясного обоснования этого эффекта нет. Даже наоборот: мы далее встретим теоретические результаты, показывающие эквивалентность Distributional-алгоритмов с обычными в рамках табличного сеттинга.

Одно гипотетическое объяснение преимущества distributional-подхода в нейросетевом сеттинге, когда будущая награда предсказывается сложной параметрической моделью, может быть следующая: обучая модель предсказывать не только среднее награды, но и другие величины (другие статистики), сильно связанные по смыслу со средней наградой, в модель отправляются более информативные градиенты. Например, если с вероятностью 0.01 во входном изображении появляется грибочек, намекающий на приближение вкусного +100 , обновление среднего будет, абстрактно говоря, проходить с учётом малой вероятности явления; когда обучение $1 \%$-го квантила наоборот крайне интересуется именно хвостом распределения и поможет быстрее выучить, например, фильтр для детекции грибочка в первых свёрточных слоях, который, в свою очередь, ускорит и более точное вычисление среднего, для которого распознавание грибочка на самом деле было существенным. Вот ещё один пример, почему это может быть хорошо:

Пример 73: Допустим, состояние описывается картинкой. Если вы видите на картинке орешек, то получите +1 . Если тигра - то -1 . Если и тигра, и орешек, то вам может повезти, а может не повезти, и вы с вероятностью 0.5

[^0]
[^0]:    ${ }^{5}$ которую не стоит путать с эпистемической неопределённостью (epistemic uncertainty), или байесовской неопределённостью, связанной с нашим субъективным незнанием о том, как устроена, например, функция переходов и награды на самом деле, связанной с нашим осознанием неточности прогнозов.

---

получите +1 , а с вероятностью 0.5 получите -1 . В такой ситуации модель, предсказывающая среднюю будущую награду, должна выдавать 0 - тоже самое значение, что и для пустой картинки, где никаких объектов нет.


Было бы здорово в таких ситуациях в последнем слое модели увидеть что-то вроде признаков, отвечающих на вопрос «есть ли на картинке орешек?» или «есть ли на картинке тигр?». Конечно, такой разметки нам никто не предоставит, и обучать такой слой напрямую не получится. Так вот, забавное наблюдение заключается в том, что распределение награды в таких ситуациях содержит информацию о том, «какие объекты» есть на картинке: если целевая переменная для пустой картинке будет «0 с вероятностью 1 », а для картинки с тигром и орешком « $\pm \mathbf{1}$ с вероятностями 0.5 », то модель научится различать такие ситуации, и скорее всего будет быстрее обучаться распознавать тигров и орешки.

Ещё одним способом придумать для модели целевую переменную, предсказание которой сильно связано со средней наградой, является использование нескольких дискаунт-факторов $\mathbf{0}<\gamma_{1}<\gamma_{2}<\ldots<$ $<\gamma_{C}<\gamma$, где $\gamma-$ коэффициент, для которого решается задача. Иначе говоря, нейросетевая модель выдаёт для каждой пары состояние-действие не одно, а $\boldsymbol{G}+\mathbf{1}$ число, соответствующее $\boldsymbol{Q}$-функции для соответствующего коэффициента дисконтирования:

$$
Q^{\pi}(s, a, g):=\mathbb{E}_{\boldsymbol{T} \mid \boldsymbol{s}_{0}=\boldsymbol{s}, \boldsymbol{a}_{0}=\boldsymbol{a}} \sum_{\boldsymbol{t} \geq 0} \gamma_{g}^{t} \boldsymbol{r}_{\boldsymbol{t}}, \quad g \in\{1 \ldots G\}
$$

Иными словами, мы дополнительно учим, какую награду мы получим в самое ближайшее время, более близкое, чем реальный рассматриваемый горизонт. Эти дополнительные величины самой стратегией использоваться не будут (взаимодействие со средой использует только значения $\boldsymbol{Q}$ для настоящего коэффициента $\boldsymbol{\gamma}$ ), однако их обучение поможет «ускорить» обучение для настоящей $\boldsymbol{Q}$ с самым большим горизонтом. Мы вернёмся к обобщению этой идеи, когда будем обсуждать multi-task RL в разделе 8.3.

# 4.3.2. Z-функция 

Определение 63: Для данного MDP оценочной функиией в distributional форме (distributional stateaction value function) для стратегии $\boldsymbol{\pi}$ называется случайная величина, обусловленная на пару состояниедействие $\boldsymbol{s}, \boldsymbol{a}$ и определяющаяся как reward-to-go для такого старта:

$$
\boldsymbol{Z}^{\pi}(s, \boldsymbol{a}) \stackrel{\text { e.d.f. }}{=} \boldsymbol{R}(\mathcal{T}), \quad \mathcal{T} \sim \boldsymbol{\pi} \mid \boldsymbol{s}_{\mathbf{0}}=\boldsymbol{s}, \boldsymbol{a}_{\mathbf{0}}=\boldsymbol{a}
$$

Здесь читателю предлагается заварить себе кофе на том факте, что введённая так называемая Z-функция ${ }^{6}$ является для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ случайной величиной. Во-первых, это скалярная случайная величина, соответственно, она задаётся некоторым распределением на $\mathbb{R}$, во-вторых, как и для любой случайной величины, существенно, на что она обуславливается. Запись $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ предполагает, что мы сидим в состоянии $\boldsymbol{s}$ и выполнили действие $\boldsymbol{a}$, после чего «бросаем кости» для сэмплирования случайной величины; нас, вообще говоря, будет интересовать её функиия распределения (cumulative distribution function, c. d. f.):

$$
\boldsymbol{F}_{\boldsymbol{Z}^{\pi}(s, a)}(x):=\mathrm{P}\left(\boldsymbol{Z}^{\pi}(s, a) \leq x\right)
$$

Нам будет удобнее работать с ними, а не с плотностями, поскольку зачастую распределение $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ - дискретное или вообще вырожденное (принимающее с вероятностью 1 только какое-то одно значение). Таким образом, пространство всевозможных Z-функций имеет такой вид:

$$
\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a}) \in \mathcal{S} \times \mathcal{A} \rightarrow \mathbf{P}(\mathbb{R})
$$

где $\mathbf{P}(\mathbb{R})$ - пространство скалярных случайных величин.
Надпись с.d.f. над равенством здесь и далее означает, что слева и справа стоят случайные величины. Очень важно, что случайные величины справа и слева в подобных равенствах обусловлены на одну и ту же информацию: справа, как и слева, стоит случайная величина, обусловленная на $\boldsymbol{s}, \boldsymbol{a}$. Случайная величина здесь задана процессом генерации: сначала генерируется случайная траектория $\mathcal{T}$ при заданных $\boldsymbol{s}_{\mathbf{0}}=\boldsymbol{s}, \boldsymbol{a}_{\mathbf{0}}=\boldsymbol{a}$ (это по определению MDP эквивалентно последовательному сэмплированию $\boldsymbol{s}_{1}, \boldsymbol{a}_{1}, \boldsymbol{s}_{2} \ldots$ ), затем от этой случайной величины считается детерминированная функция $\boldsymbol{R}(\mathcal{T})$. Запись $\stackrel{\text { e.d.f. }}{=}$ означает, что $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ имеет в точности то же распределение, что и случайная величина, генерируемая процессом, описанном справа.

По определению:

[^0]
[^0]:    ${ }^{6}$ слово «функция» здесь, конечно, не очень удачно, однако автор данного текста не справился с нахождением альтернатив. «Величина» ещё можно было бы.

---

Утверждение 41: В терминальных состояниях для всех действий $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ есть вырожденная случайная величина, с вероятностью 1 принимающая значение ноль.

Пример 74: Допустим, мы сидим в состоянии и выполнили действие . Как будет выглядеть $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{\square})$ для MDP и стратегии $\boldsymbol{\pi}$ с рисунка, $\boldsymbol{\gamma}=0.5$ ?

Нас ждёт два источника случайности: сначала среда кинет нас или в состояние В, или в состояние С, затем мы случайно будем определять своё следующее действие. Всего нас ждёт 4 возможных исхода. Для каждого мы можем посчитать его вероятность и получаемый reward-to-go. Итого $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{\square})$ - дискретное распределение с 4 исходами:

| Исход $\boldsymbol{s}^{\prime}$ | Исход $\boldsymbol{a}^{\prime}$ | Вероятность | reward-to-go |
| :--: | :--: | :--: | :--: |
| B |  | 0.3 | $\mathbf{1 + 2 \gamma}$ |
| B |  | 0.3 | $\mathbf{1}$ |
| C |  | 0.1 | $\gamma$ |
| C |  | 0.3 | $\mathbf{0}$ |



Пример 75: Посчитаем Z-функцию для MDP и стратегии $\boldsymbol{\pi}$ с рисунка, $\boldsymbol{\gamma}=0.8$.
Начнём с состояния B: если агент выбирает действие $\square$ то он получает +4 , и эпизод заканчивается. Значит, $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{B}, \square)$ всегда принимает значение 4.

Что произойдёт, если он выберет ? Агент точно получит +2 и вернётся в состояние В. Вся дальнейшая награда будет дисконтирована на $\gamma=0.8$. После этого начинается первая стохастика: агент снова будет выбирать действие! С вероятностью 0.5 он выберет и получит итоговый reward-to-go $2+\gamma \cdot 4=5.2$ за эпизод. Вот мы нашли часть нашей Z-функции для $\boldsymbol{s}=\boldsymbol{B}, \boldsymbol{a}=\square$ с вероятностью 0.5 исход будет 5.2 . Ищем, что соответствует оставшейся вероятностной массе 0.5 : мы выберем снова $\square$, получим уже суммарно $2+\gamma \cdot 2=3.6$, снова вернёмся в В, снова случится дисконтирование и снова за нами будет выбор. Мы можем найти, что соответствует ещё 0.25 нашей вероятностной массы: $2+\gamma \cdot 2+\gamma^{2} \cdot 4=0.16$. Дальше в этом распределении будет ещё исход с вероятностью $\frac{1}{8}$, затем $\frac{1}{16}$ и так далее: $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{B}, \square)$ есть распределение со счётным множеством исходов!

Очевидно, что $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{A}, \square)$ и $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{C}, \square)$ тоже будут вырожденными: reward-to-go для таких стартов однозначно определён и равен -0.8 и -1 соответственно.
$\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{A}, \square)$ содержит компоненту «с вероятностью 0.75 мы попадём в терминальное состояние и получим 0 ». Оставшиеся 0.25 соответствуют попаданию в В и распределяются пополам между выбором следующего действия (соответствует награде 3.2) и (тогда начнётся та же цепочка исходов, домноженных на $\gamma=$ $=0.8$, что и для $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{B}, \square)$. Аналогично можно расписать $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}=\boldsymbol{C}, \square)$; как видно, такая «оценочная функция» содержит в себе намного больше информации о будущих наградах.

---

# 4.3.3. Distributional-форма уравнения Беллмана 

Заметим, что в доказательстве уравнений Беллмана, например, (3.3), мы ссылаемся на то, что для reward-to-go любых траекторий верно рекурсивное соотношение. После этого мы берём по траекториям мат.ожидание слева и справа, получая традиционное уравнение Беллмана. Ясно, что мы могли бы вместо среднего взять любую другую статистику от случайной величины (дисперсию, медианы, другие квантили...), а, вообще говоря, верно совпадение левой и правой части по распределению. Иначе говоря, можно зачеркнуть символ мат.ожидания из уравнения Беллмана для получения более общего утверждения.

Теорема 37 - Уравнение Беллмана в Distributional-форме:

$$
\boldsymbol{Z}^{\boldsymbol{\pi}}(s, a) \stackrel{\text { e.d.f. }}{=} r(s, a)+\gamma \boldsymbol{Z}^{\boldsymbol{\pi}}\left(s^{\prime}, a^{\prime}\right), \quad s^{\prime} \sim p\left(s^{\prime} \mid s, a\right), a^{\prime} \sim \pi\left(a^{\prime} \mid s^{\prime}\right)
$$

Доказательство. Следует из тождества $\boldsymbol{R}(\boldsymbol{\mathcal { T }})=\boldsymbol{r}+\gamma \boldsymbol{R}\left(\boldsymbol{\mathcal { T } _ { 1 }}\right)$ для любых траекторий $\boldsymbol{\mathcal { T }}$.

Читатель подозревается в недоумении от происходящего; остановимся на этом уравнении и обсудим, что тут написано. Во-первых, необходимо пояснить, что данное уравнение есть переформулировка (другая нотация) используемых определений. Reward-to-go $\boldsymbol{R}(\boldsymbol{\mathcal { T }})$ - детерминированная функция от заданной траектории $\boldsymbol{\mathcal { T }}$, $\boldsymbol{Z}^{\boldsymbol{\pi}}$ - по сути тоже самое, только траектория рассматривается как случайная величина (а параметры $\boldsymbol{s}, \boldsymbol{a}$ указывают на начальные условия генерации траекторий). И слева, и справа в уравнении (4.11) стоят случайные величины, зависящие от $\boldsymbol{s}, \boldsymbol{a}$; равенство означает, что они имеют одинаковые распределения. Иными словами, слева и справа записаны два процесса генерации одной и той же случайной величины. Мы можем бросить кость $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ (случайная величина слева), а можем - сначала $\boldsymbol{s}^{\prime}$, потом $\boldsymbol{a}^{\prime}$, затем $\boldsymbol{Z}^{\boldsymbol{\pi}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$ и выдать исход ${ }^{7}$ $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})+\gamma \boldsymbol{Z}^{\boldsymbol{\pi}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$ (случайная величина справа), и эти две процедуры порождения эквивалентны.

Пример 76: Уравнение Беллмана всё ещё связывает «содержимое» Z-функции через неё же саму, раскрывая дерево на один шаг. Эти уравнения теперь затруднительно выписать аналитически, поскольку теперь «компоненты» pacпределения $\boldsymbol{Z}(\boldsymbol{s}, \boldsymbol{a})$ есть перевзвешанные на вероятности переходов и выборов действий (и подправленные по значению на дискаунт фактор и смещённые на награду за шаг) $\boldsymbol{Z}\left(s^{\prime}, a^{\prime}\right)$ для всевозможных $\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}$.


Подобные уравнения называются recursive distributional equations и рассматриваются математикой в одном из разделов теории вероятности. Нам далее не понадобится какая-то особая теория оттуда, однако для вдохновения рассмотрим каноничный местный пример.

Пример 77: Пусть

$$
X_{1} \stackrel{\text { e.d.f. }}{=} \frac{X_{2}}{\sqrt{2}}+\frac{X_{3}}{\sqrt{2}}
$$

где $\boldsymbol{X}_{\mathbf{1}}, \boldsymbol{X}_{\mathbf{2}}, \boldsymbol{X}_{\mathbf{3}}$ - независимые случайные величины из одного распределения $\boldsymbol{p}(\boldsymbol{x})$. То есть заданы две процедуры порождения: мы можем взять сэмпл из распределения, а может взять два сэмпла из распределения (независимо), отмасптабировать на корень из двух и сложить. Уравнение заявляет, что эти две процедуры эквивалентны. Вопрос для математики такой: каким могло бы быть $\boldsymbol{p}(\boldsymbol{x})$ ? Несколько ответов можно угадать: например, ответом является, детерминированный ноль или $\boldsymbol{\mathcal { N }}\left(\mathbf{0}, \sigma^{2}\right)$.

### 4.3.4. Distributional Policy Evaluation

Будем строить аналог Policy Evaluation для distributional-формы оценочной функции. Иными словами, мы хотим чисто теоретический алгоритм, позволяющий для данного MDP и данной стратегии $\boldsymbol{\pi}$ посчитать распределение $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. MDP пока считаем полностью известным (распределение $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ считаем данным). Действуем в полной аналогии с обычными уравнениями: начнём с ввода оператора Беллмана.

Определение 64: Для данного MDP и стратегии $\boldsymbol{\pi}$ будем называть оператором Беллмана в distributional форме оператор $\mathfrak{B}_{\boldsymbol{D}}$, действующий из пространства Z-функций (4.9) в пространство Z-функций, задающий случайную величину для $\boldsymbol{s}, \boldsymbol{a}$ на выходе оператора как правую часть distributional уравнения Беллмана

[^0]
[^0]:    ${ }^{7}$ если в формализме функция награды также случайна, вместо сдвига на константу здесь было бы сложение двух случайных величин; для расчёта распределения тогда теоретически рассматривалась бы операция свёртки.

---

$$
\left[\mathfrak{B}_{D} \mathcal{Z}\right](s, a) \stackrel{\text { c.d.f. }}{=} r(s, a)+\gamma \mathcal{Z}\left(s^{\prime}, a^{\prime}\right), \quad s^{\prime} \sim p\left(s^{\prime} \mid s, a\right), a^{\prime} \sim \pi\left(a^{\prime} \mid s^{\prime}\right)
$$

По определению, истинное $\mathcal{Z}^{\pi}$ будет неподвижной точкой такого оператора:

$$
\mathcal{Z}^{\pi}=\mathfrak{B}_{D} \mathcal{Z}^{\pi}
$$

Нас интересует вопрос о сходимости метода простой итерации. Что это означает? Если на $\boldsymbol{k}$-ой итерации мы храним большую табличку, где для каждой пары $s, \boldsymbol{a}$ хранится целиком и в точности всё распределение $\mathcal{Z}_{k}(s, a)$, то на очередном шаге для всех пар $s, a$ происходит обновление

$$
\mathcal{Z}_{k+1}(s, a) \stackrel{\text { c.d.f. }}{=} r(s, a)+\gamma \mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)
$$

где вероятности случайных величин $s^{\prime} \sim p\left(s^{\prime} \mid s, a\right), a^{\prime} \sim \pi\left(a^{\prime} \mid s^{\prime}\right)$ мы знаем и потому можем полностью посчитать свёртку распределений $\mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)$ для всевозможных пар следующих $s^{\prime}, a^{\prime}$.

Чтобы показать сходимость такой процедуры, хочется в аналогии с традиционным случаем доказать сжимаемость оператора $\mathfrak{B}_{D}$. Однако, обсуждение сжимаемости имеет смысл только при заданной метрике, а в данном случае даже для конечных пространств состояний и пространств действий пространство Z-функций бесконечномерно, поскольку бесконечномерно $\mathbf{P}(\mathbb{R})$. Нам нужна метрика в таком пространстве, и, внезапно, от её выбора будет зависеть ответ на вопрос о сжимаемости.
Определение 65: Пусть $\mathcal{D}$ - метрика в пространстве $\mathbf{P}(\mathbb{R})$. Тогда её максимальной формой (maximal form) будем называть следующую метрику в пространстве Z-функций:

$$
\mathcal{D}^{\max }\left(\mathcal{Z}_{1}, \mathcal{Z}_{2}\right) \equiv \sup _{s \in \mathcal{S}, a \in \mathcal{A}} \mathcal{D}\left(\mathcal{Z}_{1}(s, a), \mathcal{Z}_{2}(s, a)\right)
$$

Теорема 38: Для любой метрики $\mathcal{D}$ в пространстве $\mathbf{P}(\mathbb{R})$ её максимальная форма $\mathcal{D}^{\max }$ есть метрика в пространстве Z-функций.

Доказательство. Проверим неравенство треугольника. Для любых трёх Z-функций $\mathcal{Z}_{1}, \mathcal{Z}_{2}, \mathcal{Z}_{3}$ :

$$
\begin{aligned}
\mathcal{D}^{\max }\left(\mathcal{Z}_{1}, \mathcal{Z}_{2}\right) & =\sup _{s, a} \mathcal{D}\left(\mathcal{Z}_{1}(s, a), \mathcal{Z}_{2}(s, a)\right) \leq \\
\leq\{ & \text { неравенство треугольника для } \mathcal{D}\} \leq \sup _{s, a}\left[\mathcal{D}\left(\mathcal{Z}_{1}(s, a), \mathcal{Z}_{3}(s, a)\right)+\mathcal{D}\left(\mathcal{Z}_{3}(s, a), \mathcal{Z}_{2}(s, a)\right)\right] \leq \\
& \leq\{\text { свойство максимума }\} \leq \sup _{s, a} \mathcal{D}\left(\mathcal{Z}_{1}(s, a), \mathcal{Z}_{3}(s, a)\right)+\sup _{s, a} \mathcal{D}\left(\mathcal{Z}_{3}(s, a), \mathcal{Z}_{2}(s, a)\right)= \\
& =\mathcal{D}^{\max }\left(\mathcal{Z}_{1}, \mathcal{Z}_{3}\right)+\mathcal{D}^{\max }\left(\mathcal{Z}_{3}, \mathcal{Z}_{2}\right)
\end{aligned}
$$

Симметричность, неотрицательность и равенство нулю только при совпадении аргументов проверяется непосредственно.

Соответственно, вопрос о выборе метрики в пространстве Z-функций сводится к вопросу о метрике в пространстве скалярных случайных величин. Вопрос, вообще, довольно богатый. Можно пытаться посчитать расстояние между функциями распределения (такова, например, метрика Крамера), а можно - между их обратными функциями:

Определение 66: Для скалярной случайной величины $\boldsymbol{X}$ с функцией распределения $\boldsymbol{F}_{\boldsymbol{X}}(\boldsymbol{x}): \mathbb{R} \rightarrow[0,1]$ квантильной функиией (inverse distribution function (quantile function)) называется*

$$
F_{X}^{-1}(\omega):=\inf \left\{x \in \mathbb{R} \mid F_{X}(x) \geq \omega\right\}
$$

Значение этой функции $\boldsymbol{F}_{\boldsymbol{X}}^{-1}(\boldsymbol{\omega})$ в точке $\boldsymbol{\tau} \in(\mathbf{0 , 1})$ будем называть $\boldsymbol{\tau}$ квантилем.
*инфинум берётся для однозначности определения в ситуациях, когда в $\boldsymbol{F}_{\boldsymbol{X}}(\boldsymbol{x})$ есть плато.


Определение 67: Для $1 \leq p \leq+\infty$ для двух случайных скалярных величин* $\boldsymbol{X}, \boldsymbol{Y}$ с функциями распреде-

---

ления $\boldsymbol{F}_{\boldsymbol{X}}$ and $\boldsymbol{F}_{\boldsymbol{Y}}$ соответственно расстоянием Baccepитайна (Wasserstein distance) называется

$$
\begin{aligned}
\mathcal{W}_{p}(X, Y) & :=\left(\int_{0}^{1}\left|F_{X}^{-1}(\omega)-F_{Y}^{-1}(\omega)\right|^{p} \mathrm{~d} \omega\right)^{\frac{1}{p}} \\
\mathcal{W}_{\infty}(X, Y) & :=\sup _{\omega \in[0,1]}\left|F_{X}^{-1}(\omega)-F_{Y}^{-1}(\omega)\right|
\end{aligned}
$$

* с ограниченными $\boldsymbol{p}$-ми моментами, что в нашем контексте гарантируется ограниченностью награды (1.3) и ограниченностью домена.

Расстояние Вассерштайна - это довольно глубокая тема в математике; особый интерес представляет случай $\boldsymbol{p}=\mathbf{1}$ (см., например, википедию).

Теорема 39 - Эквивалентная форма $\mathcal{W}_{1}$ :

$$
\mathcal{W}_{1}(X, Y)=\int_{\mathbb{R}}\left|F_{X}(x)-F_{Y}(x)\right| \mathrm{d} x
$$

Доказательство. При $\boldsymbol{p}=\mathbf{1}$ расстояние Вассерштайна $\boldsymbol{\mathcal { W } _ { 1 }}$ есть просто площадь между графиками c.d.f. $\boldsymbol{F}_{\boldsymbol{X}}, \boldsymbol{F}_{\boldsymbol{Y}}$; тоже самое записано и в этой форме, только интегрирование («суммирование») проводится по оси значений, а не оси квантилей, но как график не поверни, площадь остаётся той же.

Формальное обоснование даёт теорема из матана о том, что в двойном интеграле можно менять местами интегралы:

$$
\int_{\mathbb{R}}\left|F_{X}(x)-F_{Y}(x)\right| \mathrm{d} x=\int_{\mathbb{R}} \mathrm{d} x \int_{\min \left(F_{X}(x), F_{Y}(x)\right)}^{\max \left(F_{X}(x), F_{Y}(x)\right)} \mathrm{d} \omega
$$

Это площадь, если просуммировать вдоль оси $\boldsymbol{x}$; давайте попробуем поменять местами интегралы.
Рассмотрим какое-нибудь $\boldsymbol{\omega}$; есть два симметричных случая. В первом $\boldsymbol{F}_{\boldsymbol{X}}(\boldsymbol{x}) \leq \boldsymbol{\omega} \leq \boldsymbol{F}_{\boldsymbol{Y}}(\boldsymbol{x})$. Ну тогда из второго неравенства $\boldsymbol{F}_{\boldsymbol{Y}}^{\boldsymbol{- 1}}(\boldsymbol{\omega}) \leq \boldsymbol{x}$. Теперь заметим, что $\boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}(\boldsymbol{\omega}) \geq \boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}(\hat{\boldsymbol{\omega}})$ для любого $\hat{\boldsymbol{\omega}} \leq \boldsymbol{\omega}$ в силу неубывания $\boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}$; поэтому возьмём в качестве $\hat{\boldsymbol{\omega}}:=\boldsymbol{F}_{\boldsymbol{X}}(\boldsymbol{x})$, который по первому неравенству не больше $\boldsymbol{\omega}$, и получим

$$
F_{X}^{-1}(\omega) \geq F_{X}^{-1}(\hat{\omega})=F_{X}^{-1}\left(F_{X}(x)\right)=x
$$

Мы получили, что $\boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}(\boldsymbol{\omega}) \leq \boldsymbol{x} \leq \boldsymbol{F}_{\boldsymbol{Y}}^{\boldsymbol{- 1}}(\boldsymbol{\omega})$. В симметричном случае, когда $\boldsymbol{F}_{\boldsymbol{Y}}(\boldsymbol{x}) \leq \boldsymbol{\omega} \leq \boldsymbol{F}_{\boldsymbol{X}}(\boldsymbol{x})$, мы получили бы аналогично $\boldsymbol{F}_{\boldsymbol{Y}}^{\boldsymbol{- 1}}(\boldsymbol{\omega}) \leq \boldsymbol{x} \leq \boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}(\boldsymbol{\omega})$. Таким образом, при данном $\boldsymbol{\omega}$ для подсчёта площади переменной $\boldsymbol{x}$ нужно пробежать от $\min \left(\boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}(\boldsymbol{x}), \boldsymbol{F}_{\boldsymbol{Y}}^{\boldsymbol{- 1}}(\boldsymbol{x})\right)$ до $\max \left(\boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}(\boldsymbol{x}), \boldsymbol{F}_{\boldsymbol{Y}}^{\boldsymbol{- 1}}(\boldsymbol{x})\right)$; мы получаем равенство

$$
\left(\int_{\mathbb{R}} \mathrm{d} x \int_{\min \left(F_{X}(x), F_{Y}(x)\right)}^{\max (F} \mathrm{~d} \boldsymbol{\omega}\right)=\left(\int_{0}^{1} \mathrm{~d} \boldsymbol{\omega} \int_{\min \left(F_{X}^{-1}(\omega), F_{Y}^{-1}(\omega)\right)}^{\max (F} \mathrm{~d} \boldsymbol{x}\right)
$$

Ну а это в точности равно $\mathcal{W}_{1}$, то есть выинтегрированию модуля разности между $\boldsymbol{F}_{\boldsymbol{X}}^{\boldsymbol{- 1}}$ и $\boldsymbol{F}_{\boldsymbol{Y}}^{\boldsymbol{- 1}}$.
Замечание. Утверждение верно только для $\boldsymbol{p}=\mathbf{1}$ : иначе существенно, вдоль какой оси мы растягиваем разность функций возведением в $\boldsymbol{p}$-ую степень (физический смысл «площади между графиками» пропадает). Поэтому метрика Крамера, которая есть L2-pacстояние между c.d.f., не равна $\mathcal{W}_{2}$, которая есть L2-pacстояние между квантильными функциями.

Пример 78: Расстояние $\boldsymbol{\mathcal { W } _ { 1 }}$ между двумя распределениями неспроста имеет второе название Earth Moving Distance. Аналогия такая: нам даны две кучи песка. Объём песка в кучах одинаков, но у них разные конфигурации, они «насыпаны» по-разному. Чтобы перенести каждую песчинку массы $\boldsymbol{m}$ на расстояние $\boldsymbol{x}$, нам нужно затратить «работы» объёмом $\boldsymbol{m} \boldsymbol{x}$. Расстояние Вассерштайна $\boldsymbol{\mathcal { W } _ { 1 }}$ замеряет, какое минимальное количество работы нужно совершить, чтобы перевести конфигурацию первой кучи песка во вторую кучу; объём песка в каждой кучи одинаков. Для дискретных распределений, когда функции распределения (и, соответственно, квантильные функции) - «ступеньки», минимальная работа полностью соответствует площади между функциями распределений.

---

Посчитаем $\boldsymbol{\mathcal { W } _ { 1 }}$ между двумя следующими распределениями. Первое распределение (синие на картинке) - честная монетка с исходами 0 и 1. Вторая случайная величина (красная на картинке) принимает значение 0.4 с вероятностью $\boldsymbol{\theta}<0.5$ и 0.8 с вероятностью $1-\boldsymbol{\theta}$. Можно нарисовать функции распределения и посчитать площадь между ними. А можно рассуждать так: давайте «превратим» вторую кучу песка в первую. Посмотрим на песок объёма $\boldsymbol{\theta}$ в точке 0.4 . Куда его переносить? Наверное, в точку 0 , куда его тащить ближе. Перенесли; совершили работы объёмом $0.4 \theta$. Посмотрим на песок объёма $1-\boldsymbol{\theta}$ в точке 0.8 . Его удобно тащить в точку 1 , но там для получения первой конфигурации нужно только 0.5 песка. Поэтому 0.5 песка из точки 0.8 мы можем перевести в точку 1 , совершив работу $0.2 \cdot 0.5$, а оставшийся объём $1-\boldsymbol{\theta}-\mathbf{0 . 5}$ придётся переводить в точку 0 , совершая работу $0.8(0.5-\boldsymbol{\theta})$. Итого расстояние Вассерштайна равно:

$$
\mathcal{W}_{1}=0.4 \theta+0.8(0.5-\theta)+0.1
$$



Утверждение 42: Максимальная форма метрики Вассерштайна $\boldsymbol{\mathcal { W } _ { p } ^ { \text { max } }}$ есть метрика в пространстве Z функций.

Теорема 40: По метрике $\mathcal{W}_{p}^{\max }\left(\mathcal{Z}_{1}, \mathcal{Z}_{2}\right)$ оператор $\mathfrak{B}_{D}$ является сжимающим.
Доказательство для $\mathcal{W}_{1}$. Воспользуемся формой расстояния через c.d.f. (4.14), тогда

$$
\mathcal{W}_{1}^{\max }\left(\mathfrak{B}_{D} \mathcal{Z}_{1}, \mathfrak{B}_{D} \mathcal{Z}_{2}\right)=\sup _{s, a} \int_{\mathbb{R}}\left|\mathbf{P}\left(r+\gamma \mathcal{Z}_{1}\left(s^{\prime}, a^{\prime}\right) \leq x\right)-\mathbf{P}\left(r+\gamma \mathcal{Z}_{2}\left(s^{\prime}, a^{\prime}\right) \leq x\right)\right| \mathrm{d} x=(*)
$$

где внутри вероятностей $s^{\prime}, a^{\prime}$ - тоже случайные величины! Ну, сначала заметим, что добавление награды не изменяет значение интеграла. Давайте сделаем такую замену: $\hat{x}=\frac{x-r}{\gamma}$. Получаем:

$$
(*)=\sup _{s, a} \gamma \int_{\mathbb{R}}\left|\mathbf{P}\left(\mathcal{Z}_{1}\left(s^{\prime}, a^{\prime}\right) \leq \hat{x}\right)-\mathbf{P}\left(\mathcal{Z}_{2}\left(s^{\prime}, a^{\prime}\right) \leq \hat{x}\right)\right| \mathrm{d} \hat{x}=(* *)
$$

Осталось справиться со случайностью $s^{\prime}, a^{\prime}$; к счастью, для функций распределений это несложно. Пусть $s, a$ - фиксированы, тогда просто по формуле полной вероятности:

$$
\mathbf{P}\left(\mathcal{Z}\left(s^{\prime}, a^{\prime}\right) \leq \hat{x}\right)=\int_{\mathcal{S}} \int_{\mathcal{A}} p\left(s^{\prime} \mid s, a\right) \pi\left(a^{\prime} \mid s^{\prime}\right) F_{\mathcal{Z}\left(s^{\prime}, a^{\prime}\right)}(\hat{x}) \mathrm{d} s^{\prime} \mathrm{d} a^{\prime}
$$

где $\boldsymbol{F}_{\boldsymbol{Z}\left(s^{\prime}, a^{\prime}\right)}(\hat{x})$ - вероятность, что $\mathcal{Z}\left(s^{\prime}, a^{\prime}\right)$ не превзойдёт $\hat{x}$ при фиксированных $s^{\prime}, a^{\prime}$. Подставляем:

$$
\begin{aligned}
(* *) & =\gamma \sup _{s, a} \int_{\mathbb{R}}\left|\int_{\mathcal{S}} p\left(s^{\prime} \mid s, a\right) \pi\left(a^{\prime} \mid s^{\prime}\right)\left(F_{\mathcal{Z}_{1}\left(s^{\prime}, a^{\prime}\right)}-F_{\mathcal{Z}_{2}\left(s^{\prime}, a^{\prime}\right)}\right) \mathrm{d} s^{\prime} \mathrm{d} a^{\prime}\right| \mathrm{d} \hat{x} \leq \\
& \leq\left\{\text { наше любимое } \mathbb{E}_{a} f(x) \leq \max _{x} f(x)\right\} \leq \gamma \sup _{s, a} \mathcal{W}_{1}^{\max }\left(\mathcal{Z}_{1}, \mathcal{Z}_{2}\right)
\end{aligned}
$$

Это значит, что для систем уравнений (4.11) выполняется теорема Банаха 3.3.1!
Утверждение 43: Существует единственная функция $\mathcal{S} \times \mathcal{A} \rightarrow \mathbf{P}(\mathbb{R})$, являющаяся решением уравнений (4.11), и метод простой итерации сходится к ней из любого начального приближения по метрике Вассерштайна.

Пример 79: Попробуем найти $\mathcal{Z}^{\pi}$ для случайной $\boldsymbol{\pi}$ (выбирающей из двух действий всегда равновероятно) для MDP с рисунка и $\gamma=0.5$.

Сначала попробуем понять, в каких границах может лежать наша награда за весь эпизод. Если, например, мы всё время выбираем $\square$ то получим в итоге ноль; меньше, понятно, получить нельзя. Если же мы всё время выбираем $\square$ то получим в итоге $1+\gamma+\gamma^{2}+$ $+\ldots+=\frac{1}{1-\gamma}=2$. Значит, вероятные исходы размазаны на отрезке $[0,2]$.

Попробуем посмотреть на $\mathcal{Z}^{\pi}(\mathbb{R})$. По определению, мы предполагаем, что на первом шаге выбирается действие и значит, на первом шаге мы гарантированно получим +0 . Тогда, проводя аналогичные рассуждения, можно заключить, что дальнейшая возможная


---

награда лежит в отрезке $[0,1]$. Но что именно это за распределение? Можно рассмотреть распределение случайной величины $\sum_{t=0}^{T} \gamma^{t} r_{t} \mid a_{0}=\square$ не при $\boldsymbol{T}=+\infty$, а при меньших $\boldsymbol{T}$. Например, при $\boldsymbol{T}=\mathbf{1}$ мы получим +0 , затем в качестве $\boldsymbol{r}_{1}$ с равными вероятностями получим $+\mathbf{0}$ или $+\frac{1}{2}$. Получится равновероятное распределение с исходами $0,+\frac{1}{2}$. При $\boldsymbol{T}=\mathbf{2}$ мы получим уже равновероятное распределение с исходами $\mathbf{0},+\frac{1}{4},+\frac{1}{2},+\frac{3}{4}$. Продолжая рассуждение дальше, можно увидеть, что при $\boldsymbol{T} \rightarrow+\infty$ распределение продолжает равномерно размазывать вероятности по $[0,1]$. Видимо, в пределе получится просто равномерное распределение на $[0,1]$. Как можно строго доказать, что это правильный ответ?

Попробуем подставить в уравнения Беллмана (4.11) в качестве $\mathcal{Z}^{\boldsymbol{\pi}}(\mathbf{0})$ равномерное распределение на отрезке $[0,1]$, а в качестве $\mathcal{Z}^{\boldsymbol{\pi}}(\mathbf{0})$ равномерное распределение на отрезке $[\mathbf{1}, \mathbf{2}]$ (так как тут мы гарантированно получим +1 на первом же шаге). Что мы получим? Рассмотрим первое уравнение:

$$
\mathcal{Z}^{\pi}(\mathbf{0}) \stackrel{\text { e.d. } \mathbf{f} .}{\simeq}+1+\underset{\gamma}{0,5} \mathcal{Z}^{\pi}\left(a^{\prime}\right)
$$

где $\boldsymbol{a}^{\prime}$ - случайная величина, с равной вероятностью принимающая оба возможных значения.
Вот мы выбрали $\square$ с одной стороны левая часть уравнения говорит, что мы получим равномерное распределение на $[1,2]$. С другой стороны правая часть уравнения рассматривает «одношаговое приближение»: мы точно получим +1 , затем кинем кубик; с вероятностью 0.5 выберем на следующем шаге $\square$ и получим равномерное из $[1,2]$, а с вероятностью 0.5 выберем $\square$ и получим равномерное из $[0,1]$. Значит, начиная со второго шага мы получим сэмпл из равномерного на $[0,2]$; он будет дисконтирован на гамму и получится сэмпл из $[0,1]$; дальше мы его сдвинем на +1 , который мы получили на первом шаге, и в итоге как раз получится равномерное из $[1,2]$ ! Сошлось; в левой и правой стороне уравнения получается одно и то же распределение! Аналогично проверяется, что сходится второе distributional-уравнение. Из доказанного нами свойства сжатия следует, что это решение - единственное, и, значит, является истинной $\mathcal{Z}^{\boldsymbol{\pi}}$.

Итак, мы с каждым шагом алгоритма становимся всё ближе к $\mathcal{Z}^{\boldsymbol{\pi}}$, однако только если мы понимаем близость в терминах Вассерштайна. Это не единственная метрика в $\mathbf{P}(\mathbb{R})$, по максимальной форме которой была доказана сжимаемость $\mathfrak{B}$; например, ещё она доказана для максимальной формы метрики Крамера. Важно, что есть примеры метрик, для максимальных форм которых свойства сжатия нет (например, Total Variation). Для нас важен более практический пример: в реальности нам с Вассерштайном обычно неудобно работать, и мы предпочитаем более удобные дивергенции ${ }^{8}$, например, KL-дивергенцию. Чисто теоретически мы можем задаться вопросом, как ведёт себя расстояние от $\mathcal{Z}_{k}:=\mathfrak{B}_{D}^{k} \mathcal{Z}_{0}$ до истинной $\mathcal{Z}^{\boldsymbol{\pi}}$ в терминах KL-дивергенции, то есть стремится ли оно хотя бы к нулю? Оказывается, не просто не стремится, а вообще полное безобразие происходит: KL-дивергенция не умеет адекватно мерить расстояние между распределениями с несогладающим доменом (disjoint support).

Теорема 41: Расстояние между $\mathfrak{B}_{D}^{k} \mathcal{Z}_{0}$ и истинным $\mathcal{Z}^{\boldsymbol{\pi}}$ по максимальной форме $\mathbf{K L}$-дивергенции может быть равно бесконечности для всех $\boldsymbol{k}$.

Пример. Пусть в MDP с одним состоянием, одним действием и нулевой функцией награды мы проинициализировали $\mathcal{Z}_{0}$ вырожденной случайной величиной, всегда принимающей значение 1 . Тогда на первом шаге метода мы получим случайную величину, с вероятностью 1 равную $\gamma$; на $\boldsymbol{k}$-ом шаге, по индукции, случайную величину, с вероятностью 1 равную $\gamma^{k}$. При этом $\mathbf{K L}$-дивергенция между ней и истинным распределением $\mathcal{Z}^{\boldsymbol{\pi}}$ - вырожденной в нуле - равна бесконечности!


Так, ну ладно: оно же сходится по Вассерштайну, который лишён этой проблемы. Мы показали, что мы чисто теоретически умеем конструктивно находить $\mathcal{Z}^{\boldsymbol{\pi}}$, запустив метод простой итерации из произвольного начального приближения (в том числе, кстати, можем стартовать из вырожденных распределений). От практического алгоритма нас пока отделяет тот факт, что даже в табличном сеттинге мы не умеем в ячейках таблицы хранить «полностью» распределения на $\mathbb{R}$; мы займёмся этой проблемой чуть позже.

Пока что ответим на такой вопрос: а вот когда мы учим так $\mathcal{Z}^{\boldsymbol{\pi}}$, что там происходит с их мат.ожиданиями, то есть, по сути, с нашими представлениями о $\boldsymbol{Q}^{\boldsymbol{\pi}}$ ? Может, они там как-то быстрее сходятся за счёт того, что мы начали дополнительную информацию о распределении учить? Нет: их поведение в точности совпадает с тем, что получилось бы в методе простой итерации для обучения Q-функции непосредственно.

Пусть $\mathfrak{B}$ - обычный оператор Беллмана из пространства Q-функций в пространство Q-функций, а $\mathfrak{B}_{D}$, как и раньше, оператор Беллмана из пространства Z-функций в пространство Z-функций.

Теорема 42: Пусть инициализации $\mathcal{Z}_{0}$ и $Q_{0}$ удовлетворяют $\mathbb{E} \mathcal{Z}_{0}=Q_{0}$, и рассматривается два метода простой

[^0]
[^0]:    ${ }^{8}$ снимается требование симметричности и неравенства треугольника.

---

итерации:

$$
\begin{aligned}
Q_{k} & :=\mathfrak{B}^{k} Q_{0} \\
\mathscr{Z}_{k} & :=\mathfrak{B}_{D}^{k} \mathscr{Z}_{0}
\end{aligned}
$$

Тогда:

$$
Q_{k}=\mathbb{E} \mathcal{Z}_{k}
$$

Доказательство. По индукции. Пусть это верно для $\boldsymbol{k}$-ой итерации, покажем для $\boldsymbol{k}+\mathbf{1}$-ой:

$$
\mathbb{E} \mathcal{Z}_{k+1}=\mathbb{E}\left[\mathfrak{B}_{D} \mathcal{Z}_{k}\right](s, a)=\mathbb{E} r(s, a)+\gamma \mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)=(*)
$$

Заметим, что мат.ожидание в последнем выражении берётся по $s^{\prime}, a^{\prime}$ и случайности в $\mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)$ (случайности по хвосту траектории). По предположению индукции:

$$
\mathbb{E} \mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)=Q_{k}\left(s^{\prime}, a^{\prime}\right)
$$

Подставляем:

$$
(*)=\mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime}} r(s, a)+\gamma \mathbb{E} \mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)=\mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime}}\left[r(s, a)+\gamma Q_{k}\left(s^{\prime}, a^{\prime}\right)\right]=\mathfrak{B} Q_{k}=Q_{k+1}
$$

# 4.3.5. Distributional Value Iteration 

По аналогии с традиционным случаем, очень хочется ввести оптимальную оценочную функцию в distributional-форме как Z-функцию оптимальных стратегий:

$$
\mathcal{Z}^{*}(s, a) \stackrel{\text { e.d.f. }}{\rightleftharpoons} \mathcal{Z}^{\pi^{*}}(s, a)
$$

Мы начинаем спотыкаться уже на этом моменте, и дальше будет только хуже.

Теорема 43: Определение (4.15) неоднозначно.
Доказательство. Рассмотрим MDP, где агент может выбрать действие $\boldsymbol{a}=$ $=$ и получить нулевую награду с вероятностью 1 , или $\boldsymbol{a}=$ и получить +1 или -1 с вероятностями 0.5 (эпизод в обоих случаях заканчивается). Все стратегии будут оптимальными, хотя все Z-функции различны.


С уравнением оптимальности Беллмана для $\mathcal{Z}^{*}$ тоже внезапно есть тонкости. Для любой оптимальной стратегии $\pi^{*}$ вследствие (4.10) верно, что

$$
Q^{*}(s, a)=\mathbb{E} \mathcal{Z}^{\pi^{*}}(s, a)
$$

и мы знаем, что, в частности, среди оптимальных есть стратегия

$$
\pi^{*}(s)=\underset{a}{\operatorname{argmax}} Q^{*}(s, a)=\underset{a}{\operatorname{argmax}} \mathbb{E} \mathcal{Z}^{\pi^{*}}(s, a)
$$

В принципе, можно взять (4.11) для этой $\pi^{*}(s)$ и использовать её вид.

$$
\mathcal{Z}^{*}(s, a) \stackrel{\text { e.d.f. }}{=} r(s, a)+\gamma \mathcal{Z}^{*}\left(s^{\prime}, \pi^{*}\left(s^{\prime}\right)\right), \quad s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)
$$

Здесь справа мы для данных $s, \boldsymbol{a}$ описываем следующий процесс генерации случайной величины: генерируем $s^{\prime}$ из функции переходов, определяем однозначно ${ }^{9} a^{\prime}=\underset{a^{\prime}}{\operatorname{argmax}} \mathbb{E} \mathcal{Z}^{*}\left(s^{\prime}, a^{\prime}\right)$, после чего генерируем сэмпл $\mathcal{Z}^{*}\left(s^{\prime}, a^{\prime}\right)$ и выдаём $r(s, a)+\gamma \mathcal{Z}^{*}\left(s^{\prime}, a^{\prime}\right)$ в качестве результата.

Заметим, что взятие мат.ожидания справа и слева в уравнении (4.16) приведёт к традиционному уравнению оптимальности Беллмана для Q (3.17).

Определение 68: Введём оператор оптимальности Беллмана в distributional-форме $\mathfrak{B}_{D}^{*}$ :

$$
\left[\mathfrak{B}_{D}^{*} \mathcal{Z}\right](s, a) \stackrel{\text { e.d.f. }}{=} r(s, a)+\gamma \mathcal{Z}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} \mathbb{E} \mathcal{Z}\left(s^{\prime}, a^{\prime}\right)\right), \quad s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)
$$

Пусть также $\mathfrak{B}^{*}$ - обычный оператор оптимальности Беллмана из пространства Q-функций в пространство Q-функций.

[^0]
[^0]:    ${ }^{9}$ здесь есть нюанс с выбором жадного действия в случае равного среднего для нескольких вариантов: для полной корректности, множество действий должно быть упорядочено, и в «спорных» ситуациях следует выбирать действие с наименьшим порядком. Это существенно, поскольку выбор разных оптимальных действий приводит к одному и тому же среднему, но другие статистики могут быть различны.

---

Теорема 44: Пусть инициализации $\mathcal{Z}_{0}$ и $\boldsymbol{Q}_{0}$ удовлетворяют $\mathbb{E} \mathcal{Z}_{0}=\boldsymbol{Q}_{0}$ и рассматривается два метода простой итерации:

$$
\begin{aligned}
Q_{k} & \left.=\left(\mathfrak{B}^{*}\right)^{k} Q_{0}\right. \\
\mathcal{Z}_{k} & \left.=\left(\mathfrak{B}_{D}^{*}\right)^{k} \mathcal{Z}_{0}\right.
\end{aligned}
$$

Тогда:

$$
Q_{k}=\mathbb{E} \mathcal{Z}_{k}
$$

Доказательство. По индукции. Пусть это верно для $\boldsymbol{k}$-ой итерации, покажем для $\boldsymbol{k}+\mathbf{1}$-ой:

$$
\mathbb{E} \mathcal{Z}_{k+1}=\mathbb{E}\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{k}\right](s, a)=\mathbb{E}\left[r(s, a)+\gamma \mathcal{Z}_{k}\left(s^{\prime}, \underset{a^{\prime}}{\arg \max } \mathbb{E} \mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)\right)\right]=(\boldsymbol{*}
$$

Заметим, что внутренний аргмакс эквивалентен аргмаксу по $\boldsymbol{Q}$-функции, так что здесь мы тоже можем воспользоваться предположением индукции:

$$
\mathbb{E} \mathcal{Z}_{k}\left(s^{\prime}, a^{\prime}\right)=Q_{k}\left(s^{\prime}, a^{\prime}\right)
$$

Подставляем:

$$
\begin{aligned}
(*) & =\mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime}}\left[r(s, a)+\gamma \mathbb{E} \mathcal{Z}_{k}\left(s^{\prime}, \underset{a^{\prime}}{\arg \max } Q_{k}\left(s^{\prime}, a^{\prime}\right)\right)\right]= \\
& =r(s, a)+\gamma Q_{k}\left(s^{\prime}, \underset{a^{\prime}}{\arg \max } Q_{k}\left(s^{\prime}, a^{\prime}\right)\right)= \\
& =r(s, a)+\gamma \max _{a^{\prime}} Q_{k}\left(s^{\prime}, a^{\prime}\right)=\mathfrak{B}^{*} Q_{k}=Q_{k+1}
\end{aligned}
$$

Итак, мы показали, что в методе простой итерации с оператором $\mathfrak{B}_{D}^{*}$ средние движутся точно также, как и $\boldsymbol{Q}^{*}$ в обычном подходе. Однако, хвосты распределений при этом могут вести себя довольно нестабильно, и понятно, почему.

Теорема 45: Оператор $\mathfrak{B}_{D}^{*}$ может не являться непрерывным.
Доказательство. Пусть после выполнения $\boldsymbol{s}, \boldsymbol{a}$ мы точно попадаем в некоторое $s^{\prime}$, для которого наше приближение $\mathcal{Z}$ указано как на рисунке; варьируя $\epsilon$, мы можем получать близкие (по любой непрерывной метрике) Z-функции. Рассмотрим $\boldsymbol{\epsilon} \rightarrow \mathbf{0}$ и поймём, что оператор $\mathfrak{B}_{D}^{*}$ выдаёт совершенно разные Z-функции в зависимости от того, приближаемся ли мы к нулю с положительной полуоси или отрицательной.

Смотрим на $\left[\mathfrak{B}_{D}^{*} \mathcal{Z}\right](s, a) \stackrel{\text { e.d. } \boldsymbol{\epsilon}}{=} \gamma \mathcal{Z}\left(s^{\prime}, \pi^{*}\left(s^{\prime}\right)\right)$, где $\pi^{*}\left(s^{\prime}\right)$ - жадная. Если $\epsilon>\mathbf{0}$, жадная политика выдаст $a^{\prime}=$ и результат оператора будет вырожденным: $\left[\mathfrak{B}_{D}^{*} \mathcal{Z}\right](s, a)$ скажет, что с вероятностью 1 будет получена награда $\gamma \epsilon$. Если $\epsilon<\mathbf{0}$, то результат оператора будет дискретным распределением с двумя атомами $\gamma$ и $-\gamma$ (с вероятностями $\frac{1}{2}$ ). Расстояние между этими двумя вариантами по любой метрике не будет нулевым. Иными словами, при переходе $\boldsymbol{\epsilon}$ через ноль при непрерывном изменении аргумента оператора значение оператора может сколько угодно сильно измениться.


Утверждение 44: Оператор $\mathfrak{B}_{D}^{*}$ может не являться сжимающим.
Пояснение. По определению, любой сжимающий оператор Липшицев, и, значит, обязан быть непрерывным.

Мы, тем не менее, показали, что средние сходятся к $\boldsymbol{Q}^{*}$, поэтому не совсем ясно, насколько страшно, что хвосты распределений могут начать вести как-то нестабильно.

---

# 4.3.6. Категориальная аппроксимация Z-функций 

Пока что мы не получили даже табличный алгоритм в рамках distributional-подхода: даже если пространства $\mathcal{S}$ и $\mathcal{A}$ конечны, а функция переходов известна, хранить в памяти точное распределение $\mathcal{Z}(s, a)$ мы не умеем. Нам придётся выбрать некоторую параметрическую аппроксимацию. Хорошая новость заключается в том, что награда - скаляр, и распределения, с которыми мы хотим работать, одномерны. Более того, в силу (1.3) домен распределений ограничен. Мы можем этим воспользоваться и придумать какую-нибудь «сеточную» аппроксимацию.

Определение 69: Зададимся набором атомов (atoms) $r^{\min }=z_{0}<z_{1}<z_{2} \cdots<z_{A}=r^{\max }$, где $A+1$ - число атомов. Обозначим семейство категориальных распределений $\mathcal{C} \subset \mathbf{P}(\mathbb{R})$ как множество дискретных распределений на домене $\left\{z_{0}, z_{1} \ldots z_{A}\right\}$ : если $\mathcal{Z}(s, a) \in \mathcal{C}$, то

$$
\mathbf{P}\left(\mathcal{Z}(s, a)=z_{i}\right)=p_{i}
$$

где $\boldsymbol{p}$ - набор из $\boldsymbol{A}+\mathbf{1}$ чисел, суммирующихся в единицу.
Пример 80 - c51: Типично атомы образуют просто равномерную сетку, для задания которой требуется три гиперпараметра: число атомов, минимальное и максимальное значение награды. Распространённый дефолтный вариант для Atari игр - 51 атом на отрезке $[-10,10]$. В честь такой параметризации (categorical with 51 atoms) иногда алгоритм Categorical DQN, к построению которого мы приближаемся, называют с51.

Итак, для каждой пары $s, a$ мы будем хранить в табличке $\boldsymbol{A}+\mathbf{1}$ неотрицательное число $\boldsymbol{p}_{0}, \boldsymbol{p}_{1} \ldots \boldsymbol{p}_{\boldsymbol{A}}$, суммирующиеся в единицу, и полагать, что $\boldsymbol{A}+\mathbf{1}$ узлов нашей сетки $z_{0}, z_{1} \ldots z_{A}$ являются единственно возможными исходами будущей награды. Такова наша аппроксимация.

Возникает следующая проблема: мы, в принципе, можем посчитать


распределения $\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}$, но что, если оно «не попадёт» в рассматриваемое семейство аппроксимаций? То есть что, если для какой-то пары $s, a\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}\right](s, a) \notin \mathcal{C}$, то есть что, если оно не является категориальным распределением на домене $\left\{z_{0}, z_{1} \ldots z_{A}\right\}$ ? Нам придётся как-то проецировать полученный результат на нашу сетку...

Утверждение 45: В табличном сеттинге если $\mathcal{Z}(s, a) \in \mathcal{C}$ для всех $s, a$, то $\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}\right](s, a)$ - дискретное распределение с конечным множеством исходов.

Доказательство. Распределение является смесью не более чем $|\mathcal{S}| \mid \mathcal{A} \mid$ категориальных распределений с $\boldsymbol{A}$ исходами, поэтому у него не может быть более $|\mathcal{S}| \mid \mathcal{A} \mid \boldsymbol{A}$ различных исходов.

Значит, нам нужно научиться проецировать лишь дискретные распределения.
Определение 70: Введём оператор проекции П, действующий из пространства произвольных дискретных распределений в $\mathcal{C}$ следующим образом. Пусть $\tilde{\mathcal{Z}}(s, a)$ - произвольное дискретное распределение с исходами $\tilde{z}_{i}$ с соответствующими вероятностями $\tilde{p}_{i}$ (суммирующимися в единицу). Изначально инициализируем все $\boldsymbol{p}_{\boldsymbol{i}}$ для результата работы оператора нулями.

Дальше перебираем исходы $\tilde{z}_{i}$; если очередной исход меньше $r^{\min }=z_{0}$, всю его вероятностную массу отправляем в $\boldsymbol{p}_{0}$, то есть увеличиваем $\boldsymbol{p}_{0}$ на $\tilde{z}_{i}$. Аналогично поступаем если $\tilde{z}_{i}>r^{\max }=z_{A}$. В остальных случаях найдётся два соседних атома нашей сетки, такие что $z_{j} \leq \tilde{z}_{i} \leq z_{j+1}$. Распределим вероятностную массу между ними обратно пропорционально расстоянию до них, а то есть:

$$
\begin{gathered}
p_{j} \leftarrow p_{j}+\frac{z_{j+1}-\tilde{z}_{i}}{z_{j+1}-z_{j}} \tilde{p}_{i} \\
p_{j+1} \leftarrow p_{j+1}+\frac{\tilde{z}_{i}-z_{j}}{z_{j+1}-z_{j}} \tilde{p}_{i}
\end{gathered}
$$

Почему именно так мы ввели оператор проекции? Наш метод простой итерации теперь «подкорректированный», после каждого шага мы применяем проекцию:

$$
\mathcal{Z}_{k+1} \stackrel{\text { c.d.f. }}{=} \Pi \mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{k}
$$

где применение П к Z-функции означает проецирование всех распределений $\mathcal{Z}(s, a)$ для всех $s, a$. Мы очень хотели бы сохранить гарантии теоремы 44 о том, что средние в таком подправленном процессе продолжают вести себя как аппроксимации Q-функции в Value Iteration!

---

Теорема 46: Пусть $\mathcal{Z}(s, a)$ дискретно и выдаёт исходам вне отрезка $\left[r_{\min }, r_{\max }\right]$ нулевую вероятность. Тогда оператор проекции (4.17) сохраняет мат.ожидание, $\forall \mathcal{Z}, s, a$ :

$$
\mathbb{E} \Pi \mathcal{Z}(s, a)=\mathbb{E} \mathcal{Z}(s, a)
$$

Доказательство. Рассмотрим один исход $\mathcal{Z}(s, a)$; он вносил в итоговое среднее вклад $\tilde{z} \tilde{p}$, где $\tilde{p}$ - его вероятность, $\tilde{z}$ значение исхода. По условию, вся вероятностная масса размазывалась между двумя соседними узлами $z_{j} \leq \tilde{z} \leq z_{j+1}$, и в $[\Pi \mathcal{Z}](s, a)$ соответственно появляется два слагаемых:

$$
\begin{aligned}
& \overbrace{z_{j+1}-\tilde{z}}^{+\text { левый узел }} \overbrace{\tilde{z}_{j}+\tilde{z}_{j}+}^{+\text { правый узел }} \overbrace{\tilde{z}_{j+1}-z_{j}}^{+\text { правый узел }} \tilde{p} z_{j+1}= \\
& =\overbrace{\tilde{z}_{j+1} z_{j}-\tilde{z}_{j}+\tilde{z}_{j+1}-z_{j} z_{j+1}}^{z_{j+1}-z_{j}} \tilde{p}= \\
& =\tilde{z} \tilde{p}
\end{aligned}
$$



# 4.3.7. Categorical DQN 

Попробуем составить уже полностью практический алгоритм. Вопервых, обобщим алгоритм на случай произвольных пространств состояний, моделируя $\mathcal{Z}_{\theta} \approx \mathcal{Z}^{*}$ (а точнее - её распределение) при помощи нейросети с параметрами $\boldsymbol{\theta}$. Для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ такая нейросеть выдаёт $\boldsymbol{A}+\mathbf{1}$ неотрицательное число $\boldsymbol{p}_{0}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\theta}), \boldsymbol{p}_{\mathbf{1}}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\theta}) \ldots \boldsymbol{p}_{\boldsymbol{A}}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\theta})$, суммирующиеся в единицу, и мы предполагаем категориальную аппроксимацию

$$
\mathbf{P}\left(\mathcal{Z}_{\theta}(s, a)=z_{i}\right) \equiv p_{i}(s, a, \theta)
$$

Как и в DQN, считаем, что у нас есть таргет-сеть с параметрами $\boldsymbol{\theta}^{-}-\mathrm{Z}$ функция $\mathcal{Z}_{\theta^{-}}$с предыдущего (условно, $\boldsymbol{k}$-го) шага метода простой итерации, а мы хотим обучать $\boldsymbol{\theta}$ так, чтобы получить Z-функцию на $\boldsymbol{k}+\mathbf{1}$-ом шаге: наша цель - выучить $\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}$.

В model-free режиме, без доступа к функции переходов, мы не то чтобы


посчитать $\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}$не можем, нам даже недоступна большая часть информации о нём. Для данной пары $s, a$ из реплей буфера мы можем получить только один сэмпл $s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$, и нам нужен какой-то «аналог» метода временных разностей.

Первое соображение: мы умеем сэмплировать из $\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)$. Действительно: пусть дано $s, a$; берём сэмпл $s^{\prime}$ из, например, буфера; смотрим на нашу таргет сеть $\mathcal{Z}_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$ для всех действий $a^{\prime}$, считаем для каждого действия $a^{\prime}$ мат.ожидание (для ситуации $\mathcal{Z}_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right) \in \mathcal{C}$ это, очевидно, не проблема) и выбираем «наилучшее» действие $a^{\prime}=\underset{a^{\prime}}{\operatorname{argmax}} \mathbb{E} \mathcal{Z}_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$. Выбираем такое $a^{\prime}$, и дальше у нас есть даже не сэмпл, а целая компонента искомого распределения $\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)$ в виде распределения $\boldsymbol{r}(s, a)+\gamma \mathcal{Z}_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$, которое мы и будем использовать в качестве таргета.

Итак, пусть $\mathbb{T} \equiv(s, a, r, s^{\prime})-$ четвёрка из буфера. Введём целевую переменную (таргет) следующим образом:

$$
y(\mathbb{T}) \stackrel{\text { c.d.t. }}{=} r+\gamma \mathcal{Z}_{\theta^{-}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} \mathbb{E} \mathcal{Z}_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)\right)
$$

где $s^{\prime}$ в формуле берётся из $\mathbb{T}$, то есть взято из буфера. Такой таргет является дискретным распределением с, очевидно, $\boldsymbol{A}$ атомами, но из-за того, что мы взяли лишь один сэмпл $s^{\prime}$, он является лишь компонентой из $\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)$.

Второе соображение: допустим, для данной пары $s, a$ мы сможем оптимизировать следующий функционал для некоторой дивергенции $\mathcal{D}$, используя лишь сэмплы из первого распределения:

$$
\mathcal{D}\left(\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a) \| \mathcal{Z}_{\theta}(s, a)\right) \rightarrow \min _{\theta}
$$

Если бы мы могли моделировать произвольные Z-функции, минимум достигался бы в нуле на совпадающих распределениях, и наша цель была бы достигнута. Однако мы ограничены нашим аппроксимирующим категориальным семейством $\mathcal{C}$, и при оптимизации такого функционала даже чисто теоретически мы получим лишь проекцию на $\boldsymbol{\mathcal { C }}$; здесь возникает вопрос, а не потеряем ли мы свойство сохранения мат.ожидания. Мы знаем, что наш оператор проекции (4.17) обладает этим свойством: мы могли бы приближать наше распределение сразу к «хорошей» проекции:

$$
\mathcal{D}\left(\left[\Pi \mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a) \| \mathcal{Z}_{\theta}(s, a)\right) \rightarrow \min _{\theta}
$$

---

Тогда мы будем учить категориальное распределение с сохранённым мат.ожиданием.
Вопрос: не потеряли ли мы возможность сэмплировать из целевого распределения? То есть можем ли мы получить сэмпл из $\left[\Pi \mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$ ?

Теорема 47:

$$
\left[\Pi \mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a}) \stackrel{\text { e.d. } \dagger}{\text { e.d. }} \Pi y(\mathbb{T}), \quad \boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)
$$

Пояснение. Сначала разберёмся, что здесь написано. Мы можем (теоретически) посчитать полностью одношаговую аппроксимацию $\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}-}$ и спроецировать полученное распредление (случ. величина слева); а можем взять случайный $\boldsymbol{s}^{\prime}$, посмотреть на распределение $\boldsymbol{r}+\gamma \mathcal{Z}_{\boldsymbol{\theta}^{-}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}\right)$ для жадного $\boldsymbol{a}^{\prime}$ и спроецировать его (случ. величина справа). Утверждается эквивалентность этих процедур: мы можем проецировать лишь компоненты $\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$. Таким образом, сэмплы из $\boldsymbol{\Pi} y(\mathbb{T})$ при случайных $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ есть сэмплы $\left[\Pi \mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$.

Доказательство. Следует, в общем-то, из определения нашего оператора проекции (4.17), который с каждым возможным исходом работает «независимо» от всех остальных. Пусть для некоторого $\boldsymbol{s}^{\prime} \boldsymbol{p}$ - вероятность исхода $\boldsymbol{z}$, тогда в правой части эта вероятность будет размазана между соседними узлами $\boldsymbol{z}_{\boldsymbol{i}} \leq \boldsymbol{z} \leq \boldsymbol{z}_{\boldsymbol{i}+1}$ с некоторыми весами $\boldsymbol{w}_{\boldsymbol{i}}, \boldsymbol{w}_{\boldsymbol{i}+1}$. Тогда в силу того, что $\boldsymbol{s}^{\prime}$ случайно, эта вероятностная масса в итоговом распределении в правой части уравнения будет домножена на $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. В распределении в левой части вероятностная масса будет сначала домножена на $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, а только потом размазана между теми же $\boldsymbol{z}_{\boldsymbol{i}}, \boldsymbol{z}_{\boldsymbol{i}+1}$ с теми же весами $\boldsymbol{w}_{\boldsymbol{i}}, \boldsymbol{w}_{\boldsymbol{i}+1}$ (которые по определению зависят только от значения исхода $\boldsymbol{z}$ ); очевидно, эти процедуры эквивалентны.

Итак, $\boldsymbol{\Pi} y(\mathbb{T})$ есть компонента $\left[\boldsymbol{\Pi} \mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$, то есть у нас, условно, есть сэмплы из этого распределения. Для каких метрик $\mathcal{D}$ мы умеем получать несмещённую оценку градиента (4.20) по сэмплам? Теория подсказывает, что в пространстве Z-функций осмысленной метрикой является Вассерштайн (4.13). И тут случается облом.

Теорема 48: Градиенты расстояния Вассерштайна до сэмплов не являются несмещёнными оценками градиента расстояния Вассерштайна до полного распределения.

Контрпример. Контрпримером будет являться практически любая ситуация, где $\boldsymbol{s}^{\prime}$ недетерминировано, а наше параметрическое семейство $\boldsymbol{Z}_{\boldsymbol{\theta}}$ может моделировать невырожденные случайные величины; так в принципе устроено расстояние Вассерштайна.

Разберём какой-нибудь пример, для простоты для $\boldsymbol{\mathcal { V } _ { 1 }}$. Пусть для данных $\boldsymbol{s}, \boldsymbol{a}$ с вероятностью $0.5 \boldsymbol{s}^{\prime}$ такого, что $\boldsymbol{y}_{\mathbf{1}}(\mathbb{T})$ - вырожденная в нуле, а с вероятностью $0.5 \boldsymbol{s}^{\prime}$ такого, что $\boldsymbol{y}_{\mathbf{2}}(\mathbb{T})$ - вырожденная в единице. Тогда понятно, что целиком распределение $\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$


есть распределение с двумя равновероятными исходами, 0 и 1.

Пусть $\boldsymbol{Z}_{\boldsymbol{\theta}}$ равна 0.4 с вероятностью $\boldsymbol{\theta}$ и 0.8 с вероятностью $\mathbf{1}-\boldsymbol{\theta}$, других значений не принимает. Будем смотреть на точку $\boldsymbol{\theta}<\mathbf{0 . 5}$. Мы как раз считали подобные расстояния Вассерштайна в примере 78. Тогда:

$$
\begin{gathered}
\boldsymbol{\mathcal { V } _ { 1 }}\left(\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a}) \| \mathcal{Z}_{\boldsymbol{\theta}}\right)=0.4 \boldsymbol{\theta}+0.8(0.5-\boldsymbol{\theta})+0.1 \\
\boldsymbol{\mathcal { V } _ { 1 }}\left(y_{1}(\mathbb{T}) \| \mathcal{Z}_{\boldsymbol{\theta}}\right)=0.4 \boldsymbol{\theta}+0.8(1-\boldsymbol{\theta}) \\
\boldsymbol{\mathcal { V } _ { 1 }}\left(y_{2}(\mathbb{T}) \| \mathcal{Z}_{\boldsymbol{\theta}}\right)=0.6 \boldsymbol{\theta}+0.2(1-\boldsymbol{\theta})
\end{gathered}
$$

Получаем:

$$
\begin{gathered}
\nabla_{\theta} \mathcal{W}_{1}\left(\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right)\right] \| \mathcal{Z}_{\boldsymbol{\theta}}\right)=-0.4 \\
\nabla_{\theta} \mathbb{E}_{\boldsymbol{s}^{\prime}} \mathcal{W}_{p}\left(y(\mathbb{T}) \| \mathcal{Z}_{\boldsymbol{\theta}}\right)=\frac{1}{2}(-0.4)+\frac{1}{2}(+0.4)=0
\end{gathered}
$$

Не сходится.
Как мы сейчас покажем, псевдометрикой, которую можно оптимизировать по сэмплам, является наша любимая KL-дивергенция. Мы понимаем, что, с одной стороны, теория подсказывает нам, что в пространстве Z-функций KL-дивергенция потенциально не приближает нас к истинной оптимальной Z-функции, но зато мы сможем оптимизировать её в model-free режиме.

Итак, рассмотрим в (4.20) в качестве $\boldsymbol{\mathcal { D }}$ KL-дивергенцию (значит, будет важен порядок аргументов). Для неё вылезает ещё одна проблема: домен сравниваемых распределений должен совпадать, иначе KL-дивергенция по определению бесконечность и не оптимизируется. К счастью, мы уже решили, что мы будем в качестве целевого распределения использовать $\boldsymbol{\Pi} \boldsymbol{y}(\mathbb{T})$, которое имеет тот же домен - сетку $\boldsymbol{z}_{\boldsymbol{0}}<\boldsymbol{z}_{\mathbf{1}}<\ldots<\boldsymbol{z}_{\boldsymbol{A}}$.

---

Теорема 49: Градиент $\mathbf{K L}$-дивергенции до целевой переменной $\boldsymbol{\Pi} \boldsymbol{y}(\mathbb{T})$ есть несмещённая оценка градиента $(4.20):$

$$
\nabla_{\theta} \mathrm{KL}\left(\left[\Pi \mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)\left\|\mathcal{Z}_{\theta}(s, a)\right)=\mathbb{E}_{s^{\prime}} \nabla_{\theta} \mathrm{KL}\left(\Pi y(\mathbb{T}) \mid \mathcal{Z}_{\theta}(s, a)\right)
$$

Доказательство.

$$
\begin{aligned}
\nabla_{\theta} \mathrm{KL}\left(\left[\Pi \mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)\left\|\mathcal{Z}_{\theta}(s, a)\right)\right. & =\nabla_{\theta} \operatorname{const}(\theta)- \\
& -\nabla_{\theta} \mathbb{E}_{s \sim}\left[\Pi \mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a) \log \mathrm{P}\left(\mathcal{Z}_{\theta}(s, a)=z\right)= \\
=\{(4.21)\} & =-\nabla_{\theta} \mathbb{E}_{s^{\prime}} \mathbb{E}_{s \sim \Pi y(\mathbb{T})} \log \mathrm{P}\left(\mathcal{Z}_{\theta}(s, a)=z\right)= \\
& =\mathbb{E}_{s^{\prime}} \nabla_{\theta} \mathrm{KL}\left(\Pi y(\mathbb{T}) \mid \mathcal{Z}_{\theta}(s, a)\right)
\end{aligned}
$$

Итак, градиент KL-дивергенции - мат.ожидание по целевому распределению, и значит, мы можем вместо мат.ожидания по $\left[\boldsymbol{\Pi} \mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)$ использовать Монте-Карло оценку по сэмплам. При этом поскольку у нас есть даже не просто сэмплы, а целая компонента $\boldsymbol{\Pi} y(\mathbb{T})$ целевого распределения, то по ней интеграл мы можем взять просто целиком (он состоит всего из $\boldsymbol{A}$ слагаемых, как видно, поскольку $\boldsymbol{\Pi} y(\mathbb{T}) \in \mathcal{C}$ ).

Получается следующее: для данного перехода мы в качестве функции потерь возьмём $\operatorname{KL}\left(\boldsymbol{\Pi} y(\mathbb{T}) \| \mathcal{Z}_{\theta}(s, a)\right)$, где таргет $y(\mathbb{T})$ вычисляется по формуле (4.19). Раз мы используем категориальную аппроксимацию (4.18), и $\boldsymbol{\Pi} y(\mathbb{T})$ - категориальное распределение на той же сетке, то эта KL-дивергенция считается явно и (с точностью до константы, не зависящей от $\boldsymbol{\theta}$ ) равна

$$
-\sum_{i=0}^{A} \mathrm{P}\left(\Pi y(\mathbb{T})=z_{i}\right) \log p_{i}\left(s_{i}, a_{i}, \theta\right)
$$

Как видно из этой формулы, мы по сути начинаем решать задачу классификации, где у нас есть для данного входа $s, a$ сразу целая компонента «целевого» распределения. Минимизация KL-дивергенции, хоть и является стандартной функций потерь в таких ситуациях, сейчас имеет для нас побочный эффект: мы отчасти потеряли «физический смысл» наших «классов». KLдивергенция смотрит на каждый узел $z_{i}$ нашей сетки отдельно и сравнивает вероятность, которую мы выдаём сейчас, с вероятностью $z_{i}$ в таргете. Она не учитывает, находится ли разница


в вероятностной массе на соседнем узле, например, $z_{i+1}$ (в «соседнем» исходе) или на противоположном конце сетки в условном $z_{0}$; в обоих случаях KL-дивергенция будет выдавать одно и то же значение. Адекватные метрики в пространстве распределений, например, Вассерштайн, продифференцировали бы эти случаи. Причём заметим, что мы, вообще говоря, могли бы посчитать того же Вассерштайна между $\boldsymbol{y}(\mathbb{T})$ и $\boldsymbol{Z}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$ (эти распределения дискретны, и мы разбирали, как считать расстояние Вассерштайна в таком случае в примере 78), но градиенты такой функции потерь в силу теоремы 48 не были бы несмещёнными оценками градиента для минимизации (4.20), и такой алгоритм был бы некорректен.

Тем не менее, мы получили первый полноценный Distributional алгоритм. Соберём c51, он же Categorical DQN, целиком.

# Алгоритм 10: Categorical DQN (c51) 

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{V}_{\text {max }}, \boldsymbol{V}_{\text {min }}, \boldsymbol{A}$ - параметры категориальной аппроксимации, $\boldsymbol{K}$ - периодичность обновления таргет-сети, $\boldsymbol{\varepsilon}(\boldsymbol{t})$ - стратегия исследования, $\boldsymbol{p}_{\boldsymbol{i}}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\theta})$ - нейросетка с параметрами $\boldsymbol{\theta}$, SGD-оптимизатор

Предпосчитать узлы сетки $z_{i}:=\boldsymbol{V}_{\min }+\frac{i}{A}\left(\boldsymbol{V}_{\max }-\boldsymbol{V}_{\min }\right)$
Инициализировать $\boldsymbol{\theta}$ произвольно
Положить $\boldsymbol{\theta}^{-}:=\boldsymbol{\theta}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. выбрать $\boldsymbol{a}_{\boldsymbol{t}}$ случайно с вероятностью $\boldsymbol{\varepsilon}(\boldsymbol{t})$, иначе $\boldsymbol{a}_{\boldsymbol{t}}:=\underset{a_{t}}{\operatorname{argmax}} \sum_{i=0}^{A} z_{i} p_{i}\left(s_{t}, a_{t}, \theta\right)$
2. пронаблюдать $\boldsymbol{r}_{t}, s_{t+1}, \operatorname{done}_{t+1}$
3. добавить пятёрку $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right.$, done $\left._{t+1}\right)$ в реплей буфер

---

4. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера
5. для каждого перехода $\mathbb{T}:=\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right.$, done $)$ посчитать таргет:

$$
\mathbf{P}\left(\boldsymbol{y}(\mathbb{T})=\boldsymbol{r}+\gamma(1-\text { done }) \boldsymbol{z}_{\boldsymbol{t}}\right):=\boldsymbol{p}_{\boldsymbol{t}}\left(s^{\prime}, \underset{\boldsymbol{a}^{\prime}}{\operatorname{argmax}} \sum_{i=0}^{A} \boldsymbol{z}_{\boldsymbol{t}} \boldsymbol{p}_{\boldsymbol{t}}\left(s^{\prime}, \boldsymbol{a}^{\prime}, \boldsymbol{\theta}^{-}\right), \boldsymbol{\theta}^{-}\right)
$$

6. спроецировать таргет на сетку $\left\{z_{0}, z_{1} \ldots z_{A}\right\}: y(\mathbb{T}) \leftarrow \Pi y(\mathbb{T})$
7. посчитать лосс:

$$
\operatorname{Loss}(\boldsymbol{\theta}):=-\frac{1}{B} \sum_{\mathrm{T}} \sum_{t=0}^{A} \mathbf{P}\left(\boldsymbol{y}(\mathbb{T})=\boldsymbol{z}_{\boldsymbol{t}}\right) \log \boldsymbol{p}_{\boldsymbol{t}}\left(s_{t}, a_{t}, \boldsymbol{\theta}\right)
$$

8. сделать шаг градиентного спуска по $\boldsymbol{\theta}$, используя $\nabla_{\boldsymbol{\theta}} \operatorname{Loss}(\boldsymbol{\theta})$
9. если $\boldsymbol{t} \bmod \boldsymbol{K}=\mathbf{0}: \boldsymbol{\theta}^{-} \leftarrow \boldsymbol{\theta}$

# 4.3.8. Квантильная аппроксимация Z-функций 

В с51 мы воспользовались тем, что KL-дивергенция - это мат.ожидание по одному из сравниваемых распределений. Только это позволило нам несмещённо оценивать градиенты, используя лишь один сэмпл $\boldsymbol{s}^{\prime}$. Иначе говоря, у нас не ложатся карты: по адекватным метрикам такой фокус не прокатывает - их нельзя так просто «оптимизировать по сэмплам» - и к тому же у нас есть сложности с доменом распределения, нам необходим оператор проекции и аккуратный подбор неудобных гиперпараметров $\boldsymbol{V}_{\text {max }}, \boldsymbol{V}_{\text {min }}$, которые критично подобрать более-менее правильно.

Оказывается, карты сложатся, если мы выберем другую аппроксимацию распределений в $\mathbf{P}(\mathbb{R})$. Если раньше мы зафиксировали домен (узлы сетки) и подбирали вероятности, то теперь мы зафиксируем вероятности и будем подбирать узлы сетки. На первый взгляд это может показаться странно (как можно отказываться от предсказания вероятностей?), однако на самом деле это весьма гибкое семейство распределений с интересными свойствами. Итак:


Определение 71: Обозначим семейство квантильных распределений $\mathcal{Q} \subset \mathrm{P}(\mathbb{R})$ с $\boldsymbol{A}$ атомами как множество равномерных дискретных распределений с $\boldsymbol{A}$ произвольными исходами: если $\mathcal{Z}(\boldsymbol{s}, \boldsymbol{a}) \in \mathcal{Q}$, то для некоторых $\boldsymbol{A}$ чисел $z_{0}, z_{1} \ldots z_{A-1}$ :

$$
\mathbf{P}\left(\mathcal{Z}(s, a)=z_{t}\right)=\frac{1}{A}
$$

Сразу хорошо то, что нам понадобится всего один гиперпараметр - число атомов $\boldsymbol{A}$ - и не понадобится подбирать верхнюю-нижнюю границу ручками. Также заметим, что вырожденное распределение принадлежит $\mathcal{Q}$ : просто все $z_{i}$ в этом случае совпадают.
$\boldsymbol{\mathfrak { M }}_{D}^{*} \mathcal{Z}_{\boldsymbol{\theta}-}$, тем не менее, может снова выпадать из такого семейства представлений, и нам всё равно понадобится какая-то проекция. Но на этот раз мы сможем сделать куда более естественную проекцию. На очередном шаге для заданной пары $\boldsymbol{s}, \boldsymbol{a}$ мы будем оптимизировать расстояние Вассерштайна $\boldsymbol{\mathcal { W } _ { 1 }}$ между правой частью уравнения Беллмана и тем, что мы выдаём:

$$
\boldsymbol{\mathcal { W } _ { 1 }}\left(\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a}) \| \mathcal{Z}\right) \rightarrow \min _{\mathcal{Z} \in \mathcal{Q}}
$$

Если бы умели выдавать произвольные распределения, мы бы искали $\mathcal{Z}$, условно, среди всех распределений и тогда выдали бы $\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$, получив точный шаг метода простой итерации. Но поскольку мы ограничены семейством квантильных распределений, то лучшее, что мы можем сделать, это спроецировать шаг метода простой итерации в него.

Ключевой момент: оказывается, задача (4.23) имеет аналитическое решение. Введём следующее обозначение («середины отрезков сетки»):

$$
\tau_{i}:=\frac{\frac{i}{A}+\frac{i+1}{A}}{2}
$$

Теорема 50: Пусть $\boldsymbol{F}$ - функция распределения $\left[\mathfrak{B}_{\boldsymbol{D}}^{*} \mathcal{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$. Тогда решение $\mathcal{Z} \in \mathcal{Q}$ задачи (4.23) имеет домен $z_{0}, z_{1} \ldots z_{A-1}$ :

$$
z_{i}=F^{-1}\left(\tau_{i}\right)
$$

---

Доказательство. Задача минимизации выглядит так:

$$
\int_{0}^{1}\left|F^{-1}(\omega)-U_{z_{0}, z_{1} \ldots x_{A-1}}^{-1}(\omega)\right| \mathrm{d} \omega \rightarrow \min _{z_{0}, z_{1} \ldots x_{A-1}}
$$

где $\boldsymbol{U}_{\boldsymbol{z}_{0}, \boldsymbol{z}_{1} \ldots \boldsymbol{z}_{A-1}}^{-1}$ - функция распределения равномерного дискретного распределения на домене $\left\{z_{0}, z_{1} \ldots z_{A-1}\right\}$. Чему она равна? Ну, понятно, что это такая «лесенка»:

$$
\boldsymbol{U}_{\boldsymbol{z}_{0}, \boldsymbol{z}_{1} \ldots \boldsymbol{z}_{A-1}}^{-1}(\omega)=\left\{\begin{array}{ll}
z_{0} & 0 \leq \omega<\frac{1}{A} \\
z_{1} & \frac{1}{A} \leq \omega<\frac{1}{A} \\
\vdots & \\
z_{A-1} & \frac{A-1}{A} \leq \omega<1
\end{array}\right.
$$

Подставляем это в (4.24):

$$
\sum_{i=0}^{A-1} \int_{\frac{1}{A}}^{z+1}\left|\boldsymbol{F}^{-1}(\omega)-z_{i}\right| d \omega \rightarrow \min _{z_{0}, z_{1} \ldots x_{A-1}}
$$



Видим, что задача распадается на $\boldsymbol{A}$ отдельных задач оптимизации:

$$
\int_{\frac{1}{A}}^{z+1}\left|\boldsymbol{F}^{-1}(\omega)-z_{i}\right| d \omega \rightarrow \min _{z_{i}}
$$

Продифференцируем по $\boldsymbol{z}_{\boldsymbol{i}}$ и приравняем к нулю. Функция $\boldsymbol{F}$ монотонна, поэтому сначала будет кусок интеграла, где градиент равен -1 , затем кусок, где +1 :

$$
\int_{\frac{1}{A}}^{F\left(z_{i}\right)}-1 \mathrm{~d} \omega+\int_{F\left(z_{i}\right)}^{z+1} 1 \mathrm{~d} \omega=0
$$

Берём интегралы от константы:

$$
-\left(F\left(z_{i}\right)-\frac{i}{A}\right)+\frac{i+1}{A}-F\left(z_{i}\right)=0
$$

Откуда видим $\boldsymbol{F}\left(\boldsymbol{z}_{\boldsymbol{i}}\right)=\tau_{i}$ и получаем доказываемое.

# 4.3.9. Quantile Regression DQN 

Мы получили, что нам достаточно уметь искать лишь $\boldsymbol{A}$ определённых квантилей распределения $\left[\boldsymbol{\mathfrak { B }}_{\boldsymbol{D}}^{*} \boldsymbol{Z}_{\boldsymbol{\theta}^{-}}\right](\boldsymbol{s}, \boldsymbol{a})$ для вычисления аппроксимации правой части уравнения Беллмана. Можем ли мы это сделать, используя только сэмплы? Конечно.

Квантильная регрессия (quantile regression) - способ получить $\boldsymbol{\tau}$-ый квантиль некоторого распределения, из которого доступна только лишь выборка. В частном случае, мы получим известный факт о том, что для получения медианы ( $\frac{1}{2}$-го квантиля)


нужно минимизировать MAE между константным прогнозом и сэмплами из распределения.

Определение 72: Для заданного $\tau \in(0,1)$ квантильной функиией потерь (quantile loss) называется:

$$
\operatorname{Loss}_{\tau}(\boldsymbol{c}, \boldsymbol{X}):=\left\{\begin{array}{ll}
\tau(\boldsymbol{c}-\boldsymbol{X}) & \boldsymbol{c} \geq \boldsymbol{X} \\
(1-\tau)(\boldsymbol{X}-\boldsymbol{c}) & \boldsymbol{c}<\boldsymbol{X}
\end{array}\right.
$$

Теорема 51 - Квантильная регрессия: Решением задачи

$$
\mathbb{E}_{\boldsymbol{X}} \operatorname{Loss}_{\tau}(\boldsymbol{c}, \boldsymbol{X}) \rightarrow \min _{c \in \mathbb{R}}
$$

будет $\boldsymbol{\tau}$-ый квантиль распределения случайной величины $\boldsymbol{X}$.
Доказательство. Пусть $\boldsymbol{F}$ - функция распределения $\boldsymbol{X}$. Расшишем (4.27): для заданной точки $\boldsymbol{c}$ интеграл будет состоять из двух слагаемых, где в первом $\boldsymbol{c}<\boldsymbol{X}$, а во втором $\boldsymbol{c} \geq \boldsymbol{X}$ :

$$
\int_{0}^{\boldsymbol{F}^{-1}(c)}(1-\tau)(\boldsymbol{X}-\boldsymbol{c}) \mathrm{d} \boldsymbol{\omega}+\int_{\boldsymbol{F}^{-1}(c)}^{\boldsymbol{1}} \tau(\boldsymbol{c}-\boldsymbol{X}) \mathrm{d} \boldsymbol{\omega}=\mathbf{0}
$$

---

Дифференцируем по $\boldsymbol{c}$ и приравниваем к нулю:

$$
\int_{0}^{F^{-1}(c)}(\tau-1) \mathrm{d} \boldsymbol{\omega}+\int_{F^{-1}(c)}^{1} \tau \mathrm{~d} \boldsymbol{\omega}=0
$$

Берём константные интегралы:

$$
\boldsymbol{F}^{-1}(c)(\tau-1)+\left(1-\boldsymbol{F}^{-1}(c)\right) \tau=-\boldsymbol{F}^{-1}(c)+\tau=0
$$

Отсюда получаем, что $\boldsymbol{F}^{-1}(c)=\boldsymbol{\tau}$, то есть $\boldsymbol{c}-\boldsymbol{\tau}$-ый квантиль.

Утверждение 46: Формулу (4.26) можно переписать «в одну строчку» в следующем виде:

$$
\operatorname{Loss}_{\tau}(c, X)=(\tau-\mathbb{I}[c<X])(c-X)
$$

Итак, соберём всё вместе. У нас есть нейросеть $z_{i}(s, a, \theta)$ с параметрами $\boldsymbol{\theta}$, которая для данного состояния-действия выплёвывает $\boldsymbol{A}$ произвольных вещественных чисел, которые мы интерпретируем как $\boldsymbol{A}$ равновероятных возможных исходов случайной величины $\mathcal{Z}_{\theta}(s, a)$. Обозначим за $\boldsymbol{\theta}^{-}$веса таргет-сети, как обычно. Для очередного перехода $\mathbb{T}:=\left(s, a, r, s^{\prime}\right)$ из буфера мы хотим провести оптимизацию

$$
\mathcal{W}_{1}\left(\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)\left\|\mathcal{Z}_{\theta}(s, a)\right) \rightarrow \min _{\theta}\right.
$$

и мы поняли, что это эквивалентно поиску квантилей распределения $\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)$, поэтому для оптимизации $\boldsymbol{i}$-го выхода нейросетки будем оптимизировать квантильный лосс (4.26) (по $\boldsymbol{i}$ просто просуммируем - хотим учить все $\boldsymbol{A}$ интересующих нас квантилей):

$$
\sum_{i=0}^{A-1} \mathbb{E}_{x \sim\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)} \operatorname{Loss}_{\tau_{i}}\left(z_{i}(s, a, \theta), x\right) \rightarrow \min _{\theta}
$$



Конечно же, мы не будем доводить этот процесс оптимизации до конца, а сделаем всего один шаг обновления весов $\boldsymbol{\theta}$ по градиенту, а затем сразу же возьмём из буфера другой мини-батч. Мы лишь получили способ получать несмещённые оценки градиентов, указывающих в сторону минимизации (4.28).

Опять же заметим, что $\mathbb{E}_{x \sim\left[\mathfrak{B}_{D}^{*} \mathcal{Z}_{\theta^{-}}\right](s, a)}$ распадается в сэмплирование $s^{\prime}$ и интегрирование по возможным исходам $\mathcal{Z}_{\theta^{-}}\left(s^{\prime}, \pi^{*}\left(s^{\prime}\right)\right)$, где $\pi^{*}\left(s^{\prime}\right)$ выбирает действие жадно. Мат.ожидание по $\mathcal{Z}_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$ при данных $s^{\prime}, a^{\prime}$ есть просто усреднение по $\boldsymbol{A}$ равновероятным исходам, поэтому его мы посчитаем явно. Итого:

$$
\sum_{\substack{i=0 \\ \text { учии } \boldsymbol{A} \\ \text { квантилей }}^{\substack{A-1 \\ \text { вероятиости }}} \sum_{j=0}^{A-1} \sum_{j=0}^{A-1} \operatorname{Loss}_{\tau_{i}}\left(\underbrace{z_{i}(s, a, \theta)}_{\text {протноз }}, \overbrace{r+\gamma z_{j}\left(s^{\prime}, a^{\prime}, \theta^{-}\right)}^{\text {сэмпл }}\right) \rightarrow \min _{\theta}
$$

Закося внешнюю сумму под мат.ожидание по $s^{\prime}$, получаем функцию потерь, градиент которой можно оценивать по Монте-Карло, используя лишь сэмплы $s^{\prime}$ из функции переходов.

# Алгоритм 17: Quantile Regression DQN (QR-DQN) 

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{A}$ - число атомов, $\boldsymbol{K}$ - периодичность обновления таргетсети, $\varepsilon(t)$ - стратегия исследования, $z_{i}(s, a, \theta)$ - нейросетка с параметрами $\boldsymbol{\theta}$, SGD-оптимизатор

Предпосчитать середины отрезков квантильной сетки $\tau_{i}:=\frac{\dot{A}+\frac{t+1}{2}}{2}$
Инициализировать $\boldsymbol{\theta}$ произвольно
Положить $\boldsymbol{\theta}^{-}:=\boldsymbol{\theta}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. выбрать $\boldsymbol{a}_{\boldsymbol{t}}$ случайно с вероятностью $\boldsymbol{\varepsilon}(\boldsymbol{t})$, иначе $\boldsymbol{a}_{\boldsymbol{t}}:=\underset{a_{t}}{\operatorname{argmax}} \sum_{i=0}^{A-1} z_{i}\left(s_{t}, a_{t}, \boldsymbol{\theta}\right)$
2. пронаблюдать $\boldsymbol{r}_{t}, s_{t+1}$, done $_{t+1}$

---

3. добавить пятёрку $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right.$, done $\left._{t+1}\right)$ в реплей буфер
4. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера
5. для каждого перехода $\mathbb{T}:=\left(s, a, r, s^{\prime}\right.$, done $)$ посчитать таргет:

$$
y(\mathbb{T})_{j}:=r+(1-\text { done }) \gamma z_{j}\left(s^{\prime}, \underset{a^{\prime}}{\arg \max } \sum_{i} z_{i}\left(s^{\prime}, a^{\prime}, \theta^{-}\right), \theta^{-}\right)
$$

6. посчитать лосс:

$$
\operatorname{Loss}(\theta):=\frac{1}{B A} \sum_{\mathbb{T}} \sum_{i=0}^{A-1} \sum_{j=0}^{A-1}\left(\tau_{i}-\mathbb{I}\left[z_{i}(s, a, \theta)<y(\mathbb{T})_{j}\right]\right)\left(z_{i}(s, a, \theta)-y(\mathbb{T})_{j}\right)
$$

7. сделать шаг градиентного спуска по $\boldsymbol{\theta}$, используя $\nabla_{\theta} \operatorname{Loss}(\theta)$
8. если $t \bmod K=0: \theta^{-} \leftarrow \theta$

# 4.3.10. Implicit Quantile Networks 

B QR-DQN мы фиксировали «равномерную сетку» на оси квантилей: говорили, что наше аппроксимирующее распределение есть равномерное на домене из $\boldsymbol{A}$ атомов. Идея: давайте будем уметь в нашей нейросети выдавать произвольные квантили, каким-то образом задавая $\tau \in(0,1)$ дополнительно на вход. Тогда наша модель $\boldsymbol{z}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\tau}, \boldsymbol{\theta})$ будет неявно (implicit) задавать, вообще говоря, произвольное распределение на $\mathbb{R}$. По сути, мы моделируем квантильную функцию «целиком»; очень удобно:

$$
\boldsymbol{F}_{\boldsymbol{Z}_{\theta}(\boldsymbol{s}, \alpha)}^{-1}(\tau):=z(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\tau}, \boldsymbol{\theta})
$$

Поймём, как тогда считать мат.ожидание (или Q-функцию) в такой модели.
Теорема 52: Пусть $\boldsymbol{F}$ - функция распределения случайной величины $\boldsymbol{X}$. Тогда, если $\boldsymbol{\tau} \sim \boldsymbol{U}[\mathbf{0}, \mathbf{1}]$, случайная величина $\boldsymbol{F}^{-1}(\boldsymbol{\tau})$ имеет то же распределение, что и $\boldsymbol{X}$.

Доказательство. Заметим, что функция распределения равномерной случайной величины при $\boldsymbol{x} \in[0,1]$ равна $\mathbf{P}(\boldsymbol{\tau}<\boldsymbol{x})=\boldsymbol{x}$. Теперь посмотрим на функцию распределения случайной величины $\boldsymbol{F}^{-1}(\boldsymbol{\tau})$ :

$$
\mathbf{P}\left(\boldsymbol{F}^{-1}(\boldsymbol{\tau})<\boldsymbol{x}\right)=\mathbf{P}(\boldsymbol{\tau}<\boldsymbol{F}(\boldsymbol{x}))=\boldsymbol{F}(\boldsymbol{x})
$$

Итак, мы можем аппроксимировать жадную стратегию примерно так:

$$
\pi^{*}(s):=\underset{a}{\arg \max } \sum_{i=0}^{N} z\left(s, a, \tau_{i}, \theta\right), \quad \tau_{i} \sim U[0,1]
$$

В качестве функции потерь предлагается использовать тот же квантильный лосс, что и в QR-DQN, только если в QR-DQN нам были нужны определённые $\boldsymbol{A}$ квантилей, то теперь предлагается засэмплировать $\boldsymbol{N}^{\prime}$ каких-то квантилей и посчитать лосс для них. Для подсчёта лосса нам было нужно брать мат.ожидание по $\boldsymbol{Z}_{\boldsymbol{\theta}-\left(s^{\prime}, a^{\prime}\right)}$, для чего в формуле мы пользовались тем, что это распределение в нашей модели равномерно. Теперь же этот интеграл мы заменяем на МонтеКарло оценку с произвольным числом сэмплов $\boldsymbol{N}^{\prime \prime}$, а для сэмплирования опять же используем теорему 52:


$$
\operatorname{Loss}(\mathbb{T}, \boldsymbol{\theta}):=\sum_{i=0}^{N^{\prime}} \frac{1}{N^{\prime \prime}} \sum_{j=0}^{N^{\prime \prime}} \operatorname{Loss}_{\tau_{i}}\left(z\left(s, a, \tau_{i}, \theta\right), r+\gamma z\left(s^{\prime}, \pi^{*}\left(s^{\prime}\right), \tau_{j}, \theta^{-}\right)\right)
$$

где $\tau_{i}, \tau_{j} \sim U[0,1]$.

Возможно так обучать стратегию, которая в меньшей или большей степени предпочитает рисковать. Если при расчёте $\boldsymbol{\pi}$ сэмплировать квантили $\boldsymbol{\tau}$ не из равномерного распределения, а чаще брать квантили, близкие к 1 , агент будет в большей степени смотреть на награду, которую он может получить при везении. Если же чаще сэмплировать квантили, близкие к 0 , агент будет избегать рискованных ситуаций, когда есть вероятность получить низкую награду, и это может быть полезно в Safe RL. Поэтому в общем случае можно считать, что квантили генерируются в этой схеме из некоторого распределения $\boldsymbol{\beta}(\boldsymbol{\tau})$.

---

Архитектура сети предлагается такая. Входное состояние сжимается в некоторый эмбеддинг при помощи основной части сети $\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{s})$. Параллельно строится некоторый эмбеддинг $\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\tau})$, описывающий квантиль $\boldsymbol{\tau} \in$ $(0,1)$ : тут могут быть разные варианты, но авторы остановились на следующем: число $\boldsymbol{\tau}$ «описывается» несколькими признаками, где $\boldsymbol{i}$-ый признак равен $\boldsymbol{\operatorname { c o s }}(\boldsymbol{\pi} \boldsymbol{i} \boldsymbol{\tau})$, и преобразуется одним полносвязным слоем (с обучаемыми параметрами) для получения $\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\tau})$. Дальше финальное преобразование $\boldsymbol{h}$ должно взять $\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{s})$ и $\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\tau})$ и выдать по одному числу для каждого действия $\boldsymbol{a}$; чтобы не городить ещё слои, взаимодействие этих двух эмбеддингов предлагается организовать при помощи поэлементного перемножения:

$$
z(s, a, \tau, \theta):=h_{\theta}\left(f_{\theta}(s) \odot g_{\theta}(\tau)\right)
$$

# 4.3.11. Rainbow DQN 

В разделе про модификации 4.2 были рассмотрены весьма разные улучшения DQN, нацеленные на решения очень разных проблем. Хорошо видно, что все эти модификации «ортогональны» и могут включатьсявыключаться, так сказать, независимо в алгоритм. Distributional-подход, вообще говоря, не решает какую-то проблему внутри DQN, но может рассматриваться как ещё одна модификация базового алгоритма DQN.

Rainbow DQN совмещает 6 модификаций алгоритма DQN:

- Double DQN (раздел 4.2.3)
- Dueling DQN (раздел 4.2.4)
- Noisy DQN (раздел 4.2.5)
- Prioritized Experience Replay (раздел 4.2.6)
- Multi-step DQN (раздел 4.2.7)
- Distributional RL

В последнем пункте исторически в Rainbow используется Categorical DQN (раздел 4.3.7), однако понятно, что можно использовать любой другой алгоритм; в частности, QR-DQN (раздел 4.3.9) или IQN (раздел 4.3.10). Обсудим только пару нюансов, которые возникают при совмещении некоторых пар из этих идей, а дальше приведём полный-полный алгоритм.

Совмещение приоритизированного реплея и Distributional-подхода требует введения приоритета перехода $\mathbb{T}$ : в его качестве, аналогично обычному случаю (4.6), берётся значение функции потерь (4.22):

$$
\rho(\mathbb{T}):=\operatorname{KL}(y(\mathbb{T}) \| \mathcal{Z}_{\theta}(s, a))
$$

При совмещении шумных сетей с эвристикой Double DQN, шум сэмплируется заново на каждом проходе через сеть или таргет-сеть (то есть генерится отдельный сэмпл шума для выбора действия $\boldsymbol{a}^{\prime}$, отдельный для оценивания при построении таргета и отдельный для прохода через сеть для подсчёта градиентов).

Забивать костыльми приходится совмещение Categorical DQN с Dueling DQN. Здесь остаётся только идея о том, что при обновлении модели для пары $\boldsymbol{s}, \boldsymbol{a}$ мы должны «легче обобщаться» на все остальные действия $\hat{\boldsymbol{a}} \in \mathcal{A}$, для чего вычисление $\boldsymbol{p}_{\boldsymbol{i}}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\theta})$ проходит в два потока: «типа» V-поток $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})$, который выдаёт $\boldsymbol{A}$ атомов как бы общих для всех действий, и «типа» А-поток $\boldsymbol{A}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$, который выдаёт $\boldsymbol{A}$ атомов для каждого из $|\mathcal{A}|$ действий. Дальнейшая формула «взята» из обычного Dueling DQN (4.4); софтмакс, необходимый, чтобы получить на выходе валидное категориальное распределение, применяется в самом конце:

$$
p_{i}(s, a, \theta):=\operatorname{softmax}_{i}\left(V_{\theta}(s)_{i}+A_{\theta}(s, a)_{i}-\frac{1}{|\mathcal{A}|} \sum_{a} A_{\theta}(s, a)_{i}\right)
$$

Последнее слагаемое - «типа» централизация А-потока к нулю, снова со средним вместо максимума (хотя эта логика к вероятностям исходов $\mathcal{Z}$ уже плохо применима).

Ablation study там показывал, что убирание Dueling DQN, в отличие от остальных 5 модификаций, к особому падению качества итогового алгоритма не приводит. Вероятно, это связано с тем, что «семантика» потоков ещё больше теряется. Стоит отметить, что если использовать QR-DQN вместо c51, то софтмакс становится не нужен, и формула становится «более логичной».

## Алгоритм 1.6: Rainbow DQN

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{V}_{\text {max }}, \boldsymbol{V}_{\text {min }}, \boldsymbol{A}$ - параметры категориальной аппроксимации, $\boldsymbol{K}$ - периодичность обновления таргет-сети, $\boldsymbol{N}$ - количество шагов в оценке, $\boldsymbol{\alpha}$ - степень приоритизации, $\beta(\boldsymbol{t})$ - параметр importance sampling коррекции для приоритизированного реплея, $\boldsymbol{p}_{\boldsymbol{i}}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{\theta}, \boldsymbol{\varepsilon})$ - нейросетка с параметрами $\boldsymbol{\theta}$, SGD-оптимизатор

---

Предпосчитать узлы сетки $z_{i}:=\boldsymbol{V}_{\min }+\frac{t}{A}\left(V_{\max }-V_{\min }\right)$
Инициализировать $\boldsymbol{\theta}$ произвольно
Положить $\boldsymbol{\theta}^{-}:=\boldsymbol{\theta}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. выбрать $a_{t}:=\underset{a_{t}}{\operatorname{argmax}} \sum_{i=0}^{A} z_{i} p_{i}\left(s_{t}, a_{t}, \theta, \varepsilon\right), \quad \varepsilon \sim \mathcal{N}(0, I)$
2. пронаблюдать $\boldsymbol{r}_{t}, s_{t+1}$, done $_{t+1}$
3. построить $\boldsymbol{N}$-шаговый переход $\mathbb{T}:=\left(s, a, \sum_{n=0}^{N} \gamma^{n} r^{(n)}, s^{(N)}\right.$, done $)$, используя последние $\boldsymbol{N}$ наблюдений, и добавить его в реплей буфер с максимальным приоритетом
4. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера, используя вероятности $\mathbf{P}(\mathbb{T}) \propto \rho(\mathbb{T})^{\alpha}$
5. посчитать веса для каждого перехода:

$$
w(\mathbb{T}):=\frac{1}{\rho(\mathbb{T})^{\beta(t)}}
$$

6. для каждого перехода $\mathbb{T}:=(s, a, \bar{r}, \bar{s}$, done $)$ посчитать таргет:

$$
\begin{gathered}
\varepsilon_{1}, \varepsilon_{2} \sim \mathcal{N}(0, I) \\
\mathbf{P}\left(y(\mathbb{T})=\bar{r}+\gamma^{N}(1-\text { done }) z_{i}\right):=p_{i}\left(\bar{s}, \underset{\bar{s}}{\operatorname{argmax}} \sum_{i=0}^{A} z_{i} p_{i}\left(\bar{s}, \bar{a}, \theta, \varepsilon_{1}\right), \theta^{-}, \varepsilon_{2}\right)
\end{gathered}
$$

7. спроецировать таргет на сетку $\left\{z_{0}, z_{1} \ldots z_{A}\right\}: y(\mathbb{T}) \leftarrow \Pi y(\mathbb{T})$
8. посчитать для каждого перехода лосс:

$$
L(\mathbb{T}, \theta):=-\sum_{i=0}^{A} \mathrm{P}\left(y(\mathbb{T})=z_{i}\right) \log p_{i}\left(s_{t}, a_{t}, \theta, \varepsilon\right) \quad \varepsilon \sim \mathcal{N}(0, I)
$$

9. обновить приоритеты всех переходов из буфера: $\rho(\mathbb{T}) \leftarrow L(\mathbb{T}, \theta)$
10. посчитать суммарный лосс:

$$
\operatorname{Loss}(\theta):=\frac{1}{B} \sum_{\mathbb{T}} w(\mathbb{T}) L(\mathbb{T}, \theta)
$$

11. сделать шаг градиентного спуска по $\boldsymbol{\theta}$, используя $\nabla_{\theta} \operatorname{Loss}(\boldsymbol{\theta})$
12. если $t \bmod K=0: \theta^{-} \leftarrow \theta$

При решении задач дискретного управления в off-policy режиме имеет смысл выбирать Rainbow DQN, но задумываться о том, какие модули нужны, а какие можно отключить. Дело в том, что каждая модификация DQN несколько увеличивает вычислительную сложность каждой итерации. Использование тех модулей, которые несущественны для решаемой задачи, может ускорить работу алгоритма; однако, на практике часто сложно сказать, какие модули важны в том или ином случае. Полезно помнить, какую проблему решает каждая модификация, и пытаться отслеживать, возникает ли она.

Несмотря на увесистую теорию, на практике код Distributional RL алгоритмов отличается от кода DQN буквально несколькими строчками: нужно лишь поменять размерность выхода нейронной сети и поменять функцию потерь. Поэтому их имеет смысл обязательно попробовать при экспериментировании с value-based подходом; обучение всего распределения будущей награды вместо среднего может значительно повысить sample efficiency алгоритма.

---

# Policy Gradient подход 

В данной главе будет рассмотрен третий, Policy Gradient подход к решению задачи, в котором целевой функционал будет оптимизироваться градиентными методами. Для этого мы выведем формулу градиента средней награды по параметрам стратегии и обсудим различные способы получения его стохастических оценок. В итоге мы сможем получить общие алгоритмы, основным ограничением которых будет жёсткий on-policy режим.

## §5.1. Policy Gradient Theorem

### 5.1.1. Вывод первым способом

Часто говорят, что функционал в задаче обучения с подкреплением не дифференцируем. Имеется в виду, что функция награды $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ не дифференцируема по действиям $\boldsymbol{a}$; например, просто потому что пространство действий дискретно, например, в состоянии $\boldsymbol{s}$ агент выбрал действие $\boldsymbol{a}=\mathbf{0}$ и значение полученной награды можно лишь сравнивать со значениями для других действий. Однако, мы уже видели в примере 9 , что это не мешает дифференцируемости по параметрам стратегии в ситуации, когда стратегия ищется в семейсте стохастичных стратегий. Фактически, оптимизация в пространстве стохастичных стратегий является этакой «релаксацией» нашей задачи.

Пусть политика $\pi_{\theta}(a \mid s)$ параметризована $\boldsymbol{\theta}$ и дифференцируема по параметрам. Тогда наш оптимизируемый функционал $\boldsymbol{J}\left(\pi_{\theta}\right)=\boldsymbol{V}^{\pi_{\theta}}\left(s_{0}\right)$ тоже дифференцируем по $\boldsymbol{\theta}$, и далее мы будем выводить формулу этого градиента. Для этого нам понадобится стандартная техника вычисления градиента мат.ожидания по распределению, зависящего от параметров; мы уже встречались с ней при обсуждении эволюционных стратегий в главе 2.2.5. Сейчас в оптимизируемом функционале у нас стоит целая цепочка вложенных мат.ожиданий, и наш вывод будет заключаться просто в последовательном применении той же техники к каждому стоящему там мат.ожиданию $\mathbb{E}_{a \sim \pi(a \mid s)} \mid \cdot \mid$.

Заранее оговоримся, что при минимальных технических условиях регулярности ${ }^{1}$ мы имеем право менять местами знаки градиента, мат.ожиданий, сумм и интегралов.

Теорема 53:

$$
\nabla_{\theta} \boldsymbol{V}^{\pi_{\theta}}(s)=\mathbb{E}_{a}\left[\nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\nabla_{\theta} Q^{\pi_{\theta}}(s, a)\right]
$$

Доказательство.

$$
\begin{aligned}
\nabla_{\theta} \boldsymbol{V}^{\pi_{\theta}}(s) & =\{(3.6)\}=\nabla_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} Q^{\pi_{\theta}}(s, a)= \\
& =\{\text { мат.ожидание }- \text { это интеграл }\}= \\
& =\nabla_{\theta} \int_{\mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi_{\theta}}(s, a) \mathrm{d} a= \\
& =\{\text { проносим градиент внутрь интеграла }\}= \\
& =\int_{\mathcal{A}} \nabla_{\theta}\left[\pi_{\theta}(a \mid s) Q^{\pi_{\theta}}(s, a)\right] \mathrm{d} a= \\
& =\{\text { правило градиента произведения }\}
\end{aligned}
$$

[^0]
[^0]:    ${ }^{1}$ это следует из наших условий регулярности на MDP и предположения интегрируемости всех функций; тогда все оценочные функции и награды ограничены, следовательно, все интегралы и ряды в рассуждении сходятся абсолютно и равномерно по параметрам $\boldsymbol{\theta}$; мы просто не «связываемся» в контексте нашей задачи с бесконечностью, на которой в теории математического анализа и возникают ситуации, когда так делать нельзя.

---

$$
\begin{aligned}
& =\int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \mathrm{d} a+\int_{\mathcal{A}} \pi(a \mid s) \nabla_{\theta} Q^{\pi_{\theta}}(s, a) \mathrm{d} a= \\
& =\{\text { второе слагаемое - это мат.ожидание }\}= \\
& =\int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \mathrm{d} a+\mathbb{E}_{a} \nabla_{\theta} Q^{\pi_{\theta}}(s, a)= \\
& =\{\text { log-derivative trick }(2.7)\}= \\
& =\int_{\mathcal{A}} \pi_{\theta}(a \mid s) \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \mathrm{d} a+\mathbb{E}_{a} \nabla_{\theta} Q^{\pi_{\theta}}(s, a)= \\
& =\{\text { первое слагаемое тоже стало мат.ожиданием }\}= \\
& =\mathbb{E}_{a}\left[\nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\nabla_{\theta} Q^{\pi_{\theta}}(s, a)\right]
\end{aligned}
$$

Эта техника вычисления градиента через «стохастичный узел нашего вычислительного графа», когда мы сэмплируем $\boldsymbol{a} \sim \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$, носит название REINFORCE. Как видно, эта техника универсальна: применима всегда для любых пространств действий, а также в ситуации, когда функция $Q^{\pi}(s, a)$ не дифференцируема по действиям. Заметим, что в глубоком обучении при некоторых дополнительных условиях может быть также применим другой способ; его мы обсудим позже в главе 6 , когда пространство действий будет непрерывно, а $Q^{\pi}(s, a)$ - дифференцируема по действиям, и альтернативный метод будет применим.

Мы смогли выразить градиент V-функции через градиент Q-функции, попробуем сделать наоборот. Для этого нам нужно посчитать градиент от мат.ожидания по функции переходов, не зависящей от параметров нашей стратегии, поэтому здесь всё тривиально.

# Утверждение 47: 

$$
\nabla_{\theta} Q^{\pi_{\theta}}(s, a)=\gamma \mathbb{E}_{s^{\prime}} \nabla_{\theta} V^{\pi_{\theta}}\left(s^{\prime}\right)
$$

Доказательство.

$$
\begin{aligned}
\nabla_{\theta} Q^{\pi_{\theta}}(s, a) & =\{(3.5)\}=\nabla_{\theta}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi_{\theta}}\left(s^{\prime}\right)\right]= \\
& =\left\{r(s, a) \text { не зависит от } \theta, p\left(s^{\prime} \mid s, a\right) \text { тоже }\right\}= \\
& =\gamma \mathbb{E}_{s^{\prime}} \nabla_{\theta} V^{\pi_{\theta}}\left(s^{\prime}\right)
\end{aligned}
$$

Подставляя (5.2) в (5.1), получаем:
Утверждение 48:

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{a} \mathbb{E}_{s^{\prime}}\left[\nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\gamma \nabla_{\theta} V^{\pi_{\theta}}\left(s^{\prime}\right)\right]
$$

Следовательно, мы получили рекурсивное выражение градиента $V^{\pi_{\theta}}(s)$ через него же само. Очень похоже на уравнение Беллмана, кстати: в правой части стоит мат.ожидание по выбранному в $\boldsymbol{s}$ действию $\boldsymbol{a}$ и следующему состоянию.

Осталось раскрутить эту рекурсивную цепочку, продолжая раскрывать $\nabla_{\theta} V^{\pi_{\theta}}\left(s^{\prime}\right)$ в будущее до бесконечности. Аккуратно собирая слагаемые, а также собирая из мат.ожиданий мат.ожидание по траектории, получаем следующее:

Утверждение 49 - Policy Gradient Theorem: Выражение для градиента оптимизируемого функционала можно записать следующим образом:

$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) Q^{\pi}\left(s_{t}, a_{t}\right)
$$

### 5.1.2. Вывод вторым способом

Прежде, чем мы обсудим физический смысл полученной формулы, выведем её альтернативным способом. Применим REINFORCE не к мат.ожиданиям по отдельным действиям, а напрямую к распределению всей траектории следующим образом:

Теорема 54:

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R(\mathcal{T})
$$

---

Доказательство.

$$
\begin{aligned}
\nabla_{\theta} V^{\pi_{\theta}}(s) & =\nabla_{\theta} \mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}=s} R(\mathcal{T})= \\
& =\{\text { рассмотрим мат.ожидание как интеграл по всевозможным траекториям }\}= \\
& =\nabla_{\theta} \iint_{\mathcal{T}} p\left(\mathcal{T} \mid \pi, s_{0}=s\right) R(\mathcal{T}) \mathrm{d} \mathcal{T}= \\
& =\{\text { проносим градиент внутрь интеграла }\}= \\
& =\iint_{\mathcal{T}} \nabla_{\theta} p\left(\mathcal{T} \mid \pi, s_{0}=s\right) R(\mathcal{T}) \mathrm{d} \mathcal{T}= \\
& =\left\{\text { log-derivative trick }(2.7)\right\}= \\
& =\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}=s} \nabla_{\theta} \log p\left(\mathcal{T} \mid \pi, s_{0}=s\right) R(\mathcal{T})= \\
& =\{\text { вспоминаем, что вероятность траектории есть произведение вероятностей }\}= \\
& =\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R(\mathcal{T})
\end{aligned}
$$

В последнем переходе логарифмы вероятностей переходов $\sum_{t \geq 0} \nabla_{\theta} \log p\left(s_{t+1} \mid s_{t}, a_{t}\right)$ сократились как не зависящие от параметров стратегии.

Видим, что мы более простым способом получили очень похожую формулу, но с суммарной наградой за игры вместо Q-функции из первого доказательства (5.4). В силу корректности всех вычислений, уже можно утверждать равенство между этими формулами, что наводит на мысль, что градиент можно записывать в нескольких математически эквивалентных формах. Математически эти формы будут эквивалентны, то есть равны, как интегралы, но их Монте-Карло оценки могут начать вести себя совершенно по-разному. Может быть, мы можем как-то «хорошую форму» выбрать для наших алгоритмов.

Рассмотрим, как можно этим вторым способом рассуждений дойти формально до формы градиента из первого способа. Раскрывая $R(\mathcal{T})$ по определению, мы сейчас имеем произведение двух сумм под интегралом:

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}=s}\left(\sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\right)\left(\sum_{t \geq 0} \gamma^{t} r_{i}\right)
$$

Давайте перемножим эти два ряда:

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{\mathcal{T} \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \gamma^{t} r_{i}
$$

Видим странную вещь: на градиент по параметрам за решение выбрать $\boldsymbol{a}_{\boldsymbol{t}}$ в момент времени $\boldsymbol{t}$ влияет награда, собранная при $\hat{\boldsymbol{t}}<\boldsymbol{t}$, то есть величина, на которую наше решение точно не могло повлиять, поскольку принималось после её получения. Но почему формально это так?

Теорема 55: Для произвольного распределения $\pi_{\theta}(a)$ с параметрами $\boldsymbol{\theta}$, верно:

$$
\mathbb{E}_{a \sim \pi_{\theta}(a)} \nabla_{\theta} \log \pi_{\theta}(a)=0
$$

Доказательство.

$$
\begin{aligned}
\mathbb{E}_{a \sim \pi_{\theta}(a)} \nabla_{\theta} \log \pi_{\theta}(a) & =\{\text { производная логарифма }\}=\mathbb{E}_{a \sim \pi_{\theta}(a)} \frac{\nabla_{\theta} \pi_{\theta}(a)}{\pi_{\theta}(a)}= \\
& =\int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a) \mathrm{d} a=\nabla_{\theta} \int_{\mathcal{A}} \pi_{\theta}(a) \mathrm{d} a=\nabla_{\theta} 1=0
\end{aligned}
$$

Следующее утверждение формализует этот тезис о том, что «будущее не влияет на прошлое»: выбор действий в некоторый момент времени никак не влияет на те слагаемые из награды, которые были получены в прошлом.

[^0]
[^0]:    Теорема 56 - Принцип причинности (causality): При $t>\hat{t}$ :

    $$
    \mathbb{E}_{\mathcal{T} \sim \pi} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \gamma^{t} r_{i}=0
    $$

---

# Доказательство. 

$$
\begin{aligned}
& \mathbb{E}_{\boldsymbol{\tau} \sim \pi} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \gamma^{\boldsymbol{t}} r_{\boldsymbol{t}}= \\
& =\{\text { представляем мат.ожидание по траектории как вложенные мат.ожидания }\}= \\
& =\mathbb{E}_{a_{1}, s_{1} \ldots s_{t}, a_{t}} \mathbb{E}_{s_{t+1}, a_{t+1} \ldots s_{t}, a_{t} \ldots} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \gamma^{\boldsymbol{t}} r_{\boldsymbol{t}}= \\
& =\{\text { выносим константу из мат.ожидания }\}= \\
& =\mathbb{E}_{a_{1}, s_{1} \ldots s_{t}, a_{t}} \gamma^{\boldsymbol{t}} r_{\boldsymbol{t}} \mathbb{E}_{s_{t+1}, a_{t+1} \ldots s_{t}, a_{t} \ldots} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)= \\
& =\{\text { мат.ожидание градиента логарифма вероятности есть иоль }(5.6)\}= \\
& =\mathbb{E}_{a_{1}, s_{1} \ldots s_{t}, a_{t}} \gamma^{\boldsymbol{t}} r_{\boldsymbol{t}} \cdot \mathbf{0}=\mathbf{0}
\end{aligned}
$$

Значит, вместо полной награды за игру можно в весах оставить только reward-to-go (3.1), поскольку из всех слагаемых в (5.5) слагаемые для $\hat{\boldsymbol{t}}<\boldsymbol{t}$ запулятся. Понятно, что дисперсия Монте-Карло оценки такого интеграла будет меньше: мы убрали некоторую зашумляющую часть нашей стохастической оценки, которая, как мы теперь поняли, в среднем равна нулю.

При этом дисконтирование в сумме наград шло с самого начала игры, поэтому для того, чтобы записать формулу в терминах reward-to-go, нужно вынести $\gamma^{t}$ :

$$
\sum_{\hat{t} \geq t} \gamma^{\hat{t}} r_{\hat{t}}=\gamma^{t} \sum_{\hat{t} \geq t} \gamma^{\hat{t}-t} r_{\hat{t}}=\gamma^{t} R_{t}
$$

## Утверждение 50:

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{\boldsymbol{\tau} \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R_{t}
$$

Reward-to-go очень похож на Q-функцию, так как является Монте-Карло оценкой Q-функции, а мат.ожидание по распределениям всё равно берётся. Формальное обоснование эквивалентности выглядит так:

Утверждение 51: Формула (5.7) эквивалентна

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{\boldsymbol{\tau} \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) Q^{\pi}\left(s_{t}, a_{t}\right)
$$

Доказательство.

$$
\begin{aligned}
\nabla_{\theta} V^{\pi_{\theta}}(s) & =\mathbb{E}_{\boldsymbol{\tau} \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R_{t}= \\
& =\{\text { меняем местами сумму по } \boldsymbol{t} \text { и мат.ожидание по траекториям }\}= \\
& =\sum_{t \geq 0} \mathbb{E}_{\boldsymbol{\tau} \sim \pi \mid s_{0}=s} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R_{t}= \\
& =\{\text { представляем мат.ожидание по траектории как вложенные мат.ожидания }\}= \\
& =\sum_{t \geq 0} \mathbb{E}_{a_{0}, s_{1} \ldots s_{t}, a_{t}} \mathbb{E}_{s_{t+1}, a_{t+1} \ldots} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R_{t}= \\
& =\{\text { выносим константу из мат.ожидания }\}= \\
& =\sum_{t \geq 0} \mathbb{E}_{a_{0}, s_{1} \ldots s_{t}, a_{t}} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \mathbb{E}_{s_{t+1}, a_{t+1} \ldots} R_{t}= \\
& =\{\text { видим определение Q-функции }\}= \\
& =\sum_{t \geq 0} \mathbb{E}_{a_{0}, s_{1} \ldots s_{t}, a_{t}} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) Q^{\pi}\left(s_{t}, a_{t}\right)= \\
& =\{\text { формально, берём фиктивное мат.ожидание по } s_{t+1}, a_{t+1}, \cdots\}= \\
& =\sum_{t \geq 0} \mathbb{E}_{\boldsymbol{\tau} \sim \pi \mid s_{0}=s} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) Q^{\pi}\left(s_{t}, a_{t}\right)= \\
& =\{\text { снова меняем местами сумму и мат.ожидание }\}=
\end{aligned}
$$

---

Итак, мы получили формулу (5.4) вторым способом.

# 5.1.3. Физический смысл 

Обсудим, а в каком направлении, собственно, указывает полученная формула градиента (5.4). Оказывается, градиент нашего функционала имеет вид градиента взвешенных логарифмов правдоподобий. Чтобы ещё лучше увидеть это, рассмотрим суррогатную функиию (surrogate objective) - другой функционал, который будет иметь в точке текущих значений параметров стратегии $\boldsymbol{\pi}$ такой же градиент, как и $\boldsymbol{J}(\boldsymbol{\theta})$ :

$$
\mathcal{L}_{\tilde{\pi}}(\theta):=\mathbb{E}_{\mathcal{T} \sim \tilde{\pi}(s)} \sum_{t \geq 0} \gamma^{t} \log \pi_{\theta}(a \mid s) Q^{\tilde{\pi}}(s, a)
$$

Это суррогатная функция от двух стратегий: стратегии $\pi_{\theta}$, которую мы оптимизируем, и ещё одной стратегии $\tilde{\pi}$. Давайте рассмотрим эту суррогатную функцию в точке $\boldsymbol{\theta}$ такой, что эти две стратегии совпадают: $\tilde{\pi} \equiv \pi_{\theta}$, и посмотрим на градиент при изменении $\boldsymbol{\theta}$, только одной из них. То есть что мы сказали: давайте «заморозим» оценочную Q-функцию, и «заморозим» распределение, из которого приходят пары $\boldsymbol{s}, \boldsymbol{a}$. Тогда:

## Утверждение 52:

$$
\left.\nabla_{\theta} \mathcal{L}_{\tilde{\pi}}(\theta)\right|_{\tilde{\pi}=\pi_{\theta}}=\nabla_{\theta} J(\theta)
$$

Доказательство. Поскольку мат.ожидание по траекториям не зависит в этой суррогатной функции от $\boldsymbol{\theta}$, то градиент просто можно пронести внутрь:

$$
\left.\nabla_{\theta} \mathcal{L}_{\tilde{\pi}}(\theta)\right|_{\tilde{\pi}=\pi_{\theta}}=\mathbb{E}_{\mathcal{T} \sim \tilde{\pi}(s)} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}(a \mid s)\left|_{\tilde{\pi}=\pi_{\theta}} Q^{\tilde{\pi}}(s, a)\right.
$$

В точке $\boldsymbol{\theta}: \tilde{\pi}=\pi_{\theta}$ верно, что $\boldsymbol{p}(\mathcal{T} \mid \tilde{\pi}) \equiv \boldsymbol{p}(\mathcal{T} \mid \pi)$ и $\boldsymbol{Q}^{\tilde{\pi}}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$; следовательно, значение градиента в этой точке совпадает со значением формулы (5.4).

Значит, направление максимизации $\boldsymbol{J}(\boldsymbol{\theta})$ в текущей точке $\boldsymbol{\theta}$ просто совпадает с направлением максимизации этой суррогатной функции! Это принципиально единственное свойство введённой суррогатной функции. Таким образом, можно считать, что в текущей точке мы на самом деле «как бы» максимизируем (5.8), а это уже в чистом виде логарифм правдоподобия каких-то пар $(\boldsymbol{s}, \boldsymbol{a})$, для каждой из которых дополнительно выдан «вес» в виде значения $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$.

Эта суррогатная функция очень удобна для подсчёта градиента $\nabla_{\theta} J(\boldsymbol{\pi})$, поскольку она представляет его «в терминах лосса»:

$$
\nabla_{\theta} J(\pi)=\nabla_{\theta} \operatorname{Loss}^{\operatorname{netor}}(\theta)
$$

Это полезно в средствах автоматического дифференцирования, где нужно задать некоторый вычислительный граф для получения градиентов; также её можно считать некой «функцией потерь» для актёра, хотя это название очень условно хотя бы потому, что значение этой функции вовсе не должно убывать и может вести себя довольно хаотично.

Проведём такую аналогию с задачей обучения с учителем: если в машинном обучении в задачах регрессии и классификации мы для данной выборки $(\boldsymbol{x}, \boldsymbol{y})$ максимизировали правдоподобие

$$
\sum_{(x, y)} \log p(y \mid x, \theta) \rightarrow \max _{\theta} \mathbf{x}
$$

то теперь в RL, когда выборки нет, мы действуем по-другому: мы сэмплируем сами себе входные данные $\boldsymbol{s}$ и примеры выходных данных $\boldsymbol{a}$, выдаём каждой паре какой-то «кредит доверия» (credit), некую скалярную оценку хорошести, выраженную в виде $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, и идём в направлении максимизации

$$
\sum_{(s, a)} \log \pi(a \mid s, \theta) Q^{\pi}(s, a) \rightarrow \max _{\theta}
$$

---

# 5.1.4. REINFORCE 

Попробуем сразу построить какой-нибудь практический RL алгоритм при помощи формулы (5.4). Нам достаточно лишь несмещённой оценки на градиент, чтобы воспользоваться методами стохастической градиентной оптимизации, и поэтому мы просто попробуем всё неизвестное в формуле заменить на Монте-Карло оценки.

Во-первых, для оценки мат.ожидания по траекториям просто сыграем несколько полных игр при помощи текущей стратегии $\boldsymbol{\pi}$. Сразу заметим, что мы тогда требуем эпизодичности среды и сразу ограничиваем себя on-policy режимом: для каждого следующего шага нам требуется сыграть эпизоды при помощи именно текущей стратегии. Во-вторых, воспользуемся Монте-Карло оценкой для приближения $\boldsymbol{Q}^{\boldsymbol{\pi}}\left(s_{t}, a_{t}\right)$, заменив его просто на reward-to-go:

$$
Q^{\pi}(s, a) \approx R(\mathcal{T}), \quad \mathcal{T} \sim \pi \mid s_{t}=s, a_{t}=a
$$

Можно сказать, что мы воспользовались формулой градиента в форме (5.7).

## Алгоритм 19: REINFORCE

Гиперпараметры: $\boldsymbol{N}$ - количество игр, $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{\theta})$ - стратегия с параметрами $\boldsymbol{\theta}$, SGD-оптимизатор.
Инициализировать $\boldsymbol{\theta}$ произвольно
На очередном шаге $t$ :

1. играем $N$ игр $\mathcal{T}_{1}, \mathcal{T}_{2} \ldots \mathcal{T}_{N} \sim \pi$
2. для каждого $\boldsymbol{t}$ в каждой игре $\mathcal{T}$ считаем reward-to-go: $\boldsymbol{R}_{\boldsymbol{t}}(\mathcal{T}):=\sum_{\boldsymbol{i}=\boldsymbol{t}} \gamma^{\boldsymbol{i}-\boldsymbol{t}} \boldsymbol{r}_{\boldsymbol{i}}$
3. считаем оценку градиента:

$$
\nabla_{\theta} J(\pi):=\frac{1}{N} \sum_{\mathcal{T}} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \log \pi\left(a_{t} \mid s_{t}, \theta\right) R_{t}(\mathcal{T})
$$

4. делаем шаг градиентного подъёма по $\boldsymbol{\theta}$, используя $\nabla_{\theta} J(\pi)$

Первая беда такого алгоритма очевидна: для одного шага градиентного подъёма нам необходимо играть несколько игр целиком (!) при помощи текущей стратегии. Такой алгоритм просто неэффективен в плане сэмплов, и это негативная сторона on-policy режима. Чтобы как-то снизить этот эффект, хотелось бы научиться как-то делать шаги обучения, не доигрывая эпизоды до конца.

Вторая проблема алгоритма - колоссальная дисперсия нашей оценки градиента. На одном шаге направление оптимизации указывает в одну сторону, на следующем - совсем в другую. В силу корректности нашей оценки все гарантии стохастичной оптимизации лежат у нас в кармане, но на практике дождаться каких-то результатов от такого алгоритма в сколько-то сложных задачах не получится.

Чтобы разобраться с этими двумя проблемами, нам понадобится чуть подробнее познакомиться с конструкцией (5.4).

### 5.1.5. State visitation frequency

Мы получили, что градиент оптимизируемого функционала по параметрам стратегии (5.4) имеет вид мат.ожиданий по траекториям стратегии $\boldsymbol{\pi}$. Казалось бы, для его оценки нам придётся играть полные эпизоды. Однако видно, что внутри интеграла по траекториям и суммы по времени стоит нечто, зависящее только от пар состояние-действие:

$$
f(s, a):=\log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)
$$

Нельзя ли как-то сказать, что если мы делаем стратегией $\boldsymbol{\pi}$ лишь несколько шагов в среде, то собранные пары $s, a$ приходят из того самого распределения, которое мы хотим оценить?

Допустим, мы взаимодействуем со средой при помощи стратегии $\boldsymbol{\pi}$. Из какого распределения нам приходят состояния, которые мы встречаем?

Утверждение 53: Состояния, которые встречает агент со стратегией $\boldsymbol{\pi}$, приходят из некоторой стационарной марковской цепи.

Доказательство. Выпишем вероятность оказаться на очередном шаге в состоянии $s^{\prime}$, если мы используем стратегию $\boldsymbol{\pi}$ :

$$
p\left(s^{\prime} \mid s\right)=\int_{\mathcal{A}} \pi(a \mid s) p\left(s^{\prime} \mid s, a\right) \mathrm{d} a
$$

---

Эта вероятность не зависит от времени и от истории, следовательно, цепочка состояний образует марковскую цепь.

Допустим, начальное состояние $\boldsymbol{s}_{\boldsymbol{0}}$ фиксировано. Обозначим вероятность оказаться в состоянии $\boldsymbol{s}$ в момент времени $\boldsymbol{t}$ при использовании стратегии $\boldsymbol{\pi}$ как $\boldsymbol{p}\left(\boldsymbol{s}_{\boldsymbol{t}}=\boldsymbol{s} \mid \boldsymbol{\pi}\right)$. Мы могли бы попробовать посчитать, сколько раз мы в среднем оказываемся в некотором состоянии $s$, просто просуммировав по времени:

$$
\sum_{t \geq 0} p\left(s_{t}=s \mid \pi\right)
$$

однако, как легко видеть, такой ряд может оказаться равен бесконечности (например, если в MDP всего одно состояние). Поскольку мы хотели получить распределение, мы можем попробовать отнормировать этот «счётчик»:

Определение 73: Для данного MDP и политики $\boldsymbol{\pi}$ state visitation frequency называется

$$
\mu_{\pi}(s):=\lim _{T \rightarrow \infty} \frac{1}{T} \sum_{t \geq 0}^{T} p\left(s_{t}=s \mid \pi\right)
$$

Вообще говоря, мы мало что можем сказать про это распределение. При некоторых технических условиях у марковской цепи встречаемых состояний может существовать некоторое стационарное распределение $\lim _{t \rightarrow \infty} p\left(s_{t}=s \mid \pi\right)$, из которого будут приходить встречающиеся состояния, условно, через бесконечное количество шагов взаимодействия; если так, то (5.9) совпадает с ним, поскольку, интуитивно, начиная с некоторого достаточно большого момента времени $\boldsymbol{t}$ все слагаемые в ряде будут очень похожи на стационарное распределение.

Можно примерно считать, что во время обучения при взаимодействии со средой состояния приходят из $\boldsymbol{\mu}_{\boldsymbol{\pi}}(\boldsymbol{s})$, где $\boldsymbol{\pi}$ - стратегия взаимодействия. Конечно, это верно лишь в предположении, что марковская цепь уже «разогрелась» и распределение действительно похоже на стационарное; то есть, в предположении, что обучение продолжается достаточно долго (обычно это так), и предыдущие стратегии, использовавшиеся для взаимодействия, менялись достаточно плавно.

Естественно, надо помнить, что сэмплы из марковской цепи скоррелированы, и соседние состояния будут очень похожи - то есть независимости в цепочке встречаемых состояний, конечно, нет. При необходимости набрать мини-батч независимых сэмплов из $\boldsymbol{\mu}_{\boldsymbol{\pi}}(\boldsymbol{s})$ для декорреляции необходимо воспользоваться параллельными средами: запустить много сред параллельно и для очередного мини-батча собирать состояния из разных симуляций взаимодействия. Вообще, если в батч попадает целая цепочка состояний из одной и той же среды, то это нарушает условие независимости сэмплов и мешает обучению.

# 5.1.6. Расцепление внешней и внутренней стохастики 

Итак, давайте попробуем формально понять, из какого распределения приходят состояния в формуле градиента (5.4), и отличается ли оно от $\boldsymbol{\mu}_{\boldsymbol{\pi}}(\boldsymbol{s})$. Для этого мы сейчас придумаем, как можно записывать функционалы вида

$$
\mathbb{E}_{T \sim \pi} \sum_{t \geq 0} \gamma^{t} f\left(s_{t}, a_{t}\right)
$$

где $\boldsymbol{f}$ - какая-то функция от пар состояние-действие, немного по-другому.
B MDP есть два вида стохастики:

- внешняя (extrinsic), связанная со случайностью в самой среде и неподконтрольная агенту; она заложена в функции переходов $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$.
- внутренняя (intrinsic), связанная со случайностью в стратегии самого агента; она заложена в $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$. Это стохастика нам подконтрольна при обучении.

Мат. ожидание по траектории (1.1) плохо тем, что мат.ожидания по внешней и внутренней стохастике чередуются. При этом во время обучения из внешней стохастики мы можем только получать сэмплы, поэтому было бы здорово переписать наш функционал как-то так, чтобы он имел вид мат.ожидания по всей внешней стохастике.

Введём ещё один, «дисконтированный счётчик посещения состояний» для стратегии взаимодействия $\boldsymbol{\pi}$. При дисконтировании отпадают проблемы с нормировкой.

Определение 74: Для данного MDP и политики $\boldsymbol{\pi}$ discounted state visitation distribution называется

$$
d_{\pi}(s):=(1-\gamma) \sum_{t \geq 0} \gamma^{t} p\left(s_{t}=s \mid \pi\right)
$$

---

Утверждение 54: State visitation distribution есть распределение на множестве состояний, то есть:

$$
\int_{\mathcal{S}} d_{\pi}(s) \mathrm{d} s=1
$$

Доказательство.

$$
\int_{\mathcal{S}} d_{\pi}(s) \mathrm{d} s=\int_{\mathcal{S}}(1-\gamma) \sum_{t \geq 0} \gamma^{t} p\left(s_{t}=s\right) \mathrm{d} s=(1-\gamma) \sum_{t \geq 0} \gamma^{t} \int_{\mathcal{S}} p\left(s_{t}=s\right) \mathrm{d} s=(1-\gamma) \sum_{t \geq 0} \gamma^{t}=1
$$

State visitation distribution (5.10) является важным понятием ввиду следующей теоремы, благодаря которой мы можем чисто теоретически (!) расцепить (decouple) внешнюю и внутреннюю стохастику:

Теорема 57: Для произвольной функции $f(s, a)$ :

$$
\mathbb{E}_{\boldsymbol{T} \sim \pi} \sum_{t \geq 0} \gamma^{t} f\left(s_{t}, a_{t}\right)=\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi}(s)} \mathbb{E}_{a \sim \pi(a \mid s)} f(s, a)
$$

Доказательство.

$$
\begin{aligned}
\mathbb{E}_{\boldsymbol{T} \sim \pi} \sum_{t \geq 0} \gamma^{t} f\left(s_{t}, a_{t}\right) & = \\
\{\text { меняем местами сумму и интеграл }\} & =\sum_{t \geq 0} \gamma^{t} \mathbb{E}_{\boldsymbol{T} \sim \pi} f\left(s_{t}, a_{t}\right)= \\
\{\text { расписываем мат.ожидание по траектории }\} & =\sum_{t \geq 0} \gamma^{t} \int_{\mathcal{S}} \int_{\mathcal{A}} p\left(s_{t}=s, a_{t}=a \mid \pi\right) f(s, a) \mathrm{d} a \mathrm{~d} s= \\
\{\text { по определению процесса }\} & =\sum_{t \geq 0} \gamma^{t} \int_{\mathcal{S}} \int_{\mathcal{A}} p\left(s_{t}=s \mid \pi\right) \pi(a \mid s) f(s, a) \mathrm{d} a \mathrm{~d} s= \\
\{\text { выделяем мат.ожидание по стратегии }\} & =\sum_{t \geq 0} \gamma^{t} \int_{\mathcal{S}} p\left(s_{t}=s \mid \pi\right) \mathbb{E}_{\pi(a \mid s)} f(s, a) \mathrm{d} s= \\
\{\text { заносим сумму по времени обратно }\} & =\int_{\mathcal{S}} \sum_{t \geq 0} \gamma^{t} p\left(s_{t}=s \mid \pi\right) \mathbb{E}_{\pi(a \mid s)} f(s, a) \mathrm{d} s= \\
\{\text { выделяем state visitation distribution }(5.10)\} & =\int_{\mathcal{S}} \frac{d_{\pi}(s)}{1-\gamma} \mathbb{E}_{\pi(a \mid s)} f(s, a) \mathrm{d} s= \\
\{\text { выделяем мат.ожидание по состояниям }\} & =\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi}(s)} \mathbb{E}_{\pi(a \mid s)} f(s, a)
\end{aligned}
$$

Итак, мы научились переписывать мат.ожидание по траекториям в другом виде. В будущем мы будем постоянно пользоваться формулой (5.11) для разных $\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})$, поэтому полезно запомнить эту альтернативную форму записи. Например, мы можем получить применить этот результат к нашему функционалу:

Утверждение 55: Оптимизируемый функционал (1.5) можно записать в таком виде:

$$
J(\pi)=\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi}(s)} \mathbb{E}_{\pi(a \mid s)} r(s, a)
$$

Интерпретация у полученного результата может быть такая: нам не столько существенна последовательность принимаемых решений, сколько частоты посещений «хороших» состояний с высокой наградой.

Теорема 57 позволяет переписать и формулу градиента:
Утверждение 56: Выражение для градиента оптимизируемого функционала можно записать следующим образом:

$$
\nabla_{\theta} J(\pi)=\frac{1}{1-\gamma} \mathbb{E}_{d_{\pi}(s)} \mathbb{E}_{\pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)
$$

Доказательство. Применить теорему 57 для $\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a}):=\nabla_{\theta} \log \pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s}) Q^{\pi}(\boldsymbol{s}, \boldsymbol{a})$.

---

Итак, множитель $\gamma^{t}$ в формуле (5.4) имеет смысл «дисконтирования частот посещения состояний». Для нас это представляет собой, мягко говоря, очень странную проблему. Если мы рассмотрим формулу градиента $\boldsymbol{J}(\boldsymbol{\theta})$, то есть зафиксируем начальное состояние в наших траекториях, то все слагаемые, соответствующие состояниям, встречающиеся в эпизодах только после, условно, 100 -го шага, будут домножены на $\boldsymbol{\gamma}^{100}$. То есть одни слагаемые в нашем оптимизируемом функционале имеют один масштаб, а другие - домножаются на близкий к нулю $\gamma^{100}$, совершенно иной. Градиентная оптимизация с такими функционалами просто не справится: в градиентах будет доминировать информация об оптимизации наших решений в состояниях около начального, имеющих большой вес.

Откуда это лезет? Давайте посмотрим на то, что мы оптимизируем: $\boldsymbol{J}(\boldsymbol{\theta})$ (1.5). Ну действительно: награда, которую мы получим после сотого шага, дисконтируется на $\boldsymbol{\gamma}^{100}$. Мы уже поняли, что для того, чтобы промаксимизировать её, нам всё равно придётся промаксимизировать $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ для всех состояний, то есть эти слагаемые «в микро-масштабе» для достижения глобального оптимума тоже должны оптимизироваться, и задача оптимизации поставлена «корректно». Но для градиентной оптимизации такая «форма» просто не подходит.

Сейчас случится неожиданный поворот событий. На практике во всех Policy Gradient методах от дисконтирования частот посещения отказываются. Это означает, что мы заменяем $\boldsymbol{d}_{\boldsymbol{\pi}}(\boldsymbol{s})$ на $\boldsymbol{\mu}_{\boldsymbol{\pi}}(\boldsymbol{s})(5.9)$ :

$$
\nabla_{\theta} J(\pi) \approx \frac{1}{1-\gamma} \mathbb{E}_{\mu_{\pi}(s)} \mathbb{E}_{\pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)
$$

В формуле (5.4) это эквивалентно удалению множителя $\gamma^{t}$ :

$$
\nabla_{\theta} J(\pi) \approx \mathbb{E}_{\boldsymbol{\tau} \sim \pi} \sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) Q^{\pi}\left(s_{t}, a_{t}\right)
$$

Мы вовсе не отказались от дисконтирования вовсе: оно всё ещё сидит внутри оценочной функции и даёт приоритет ближайшей награде перед получением той же награды в будущем, как мы и задумывали изначально.

Итак, при таком соглашении мы можем получить как бы оценку Монте-Карло на градиент:

$$
\nabla_{\theta} J(\pi) \approx \frac{1}{1-\gamma} \mathbb{E}_{a \sim \pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a), \quad s \sim \mu_{\pi}(s)
$$

Такую оценку можно считать условно несмещённой (с учётом нашего забивания на множитель $\gamma^{t}$ и приближённого сэмплирования из $\boldsymbol{\mu}_{\boldsymbol{\pi}}(\boldsymbol{s})$ ), если у нас на руках есть точная (или хотя бы несмещённое) оценка «кредита» $Q^{\pi}(s, a)$.

# 5.1.7. Связь с policy improvement 

Формула градиента в форме (5.12) даёт ещё одну интересную интерпретацию того, в каком направлении указывает полученная формула градиента. Давайте введём ещё одну суррогатную функиию (surrogate objective). Как и суррогатная функция (5.8), это будет ещё один функционал, который имеет в точке текущих значений параметров стратегии $\boldsymbol{\pi}$ такой же градиент, как и $\boldsymbol{J}(\boldsymbol{\theta})$ :

$$
\mathcal{L}_{\tilde{\pi}}(\theta):=\frac{1}{1-\gamma} \mathbb{E}_{d_{\tilde{\pi}}(s)} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} Q^{\tilde{\pi}}(s, a)
$$

Эта суррогатная функция, опять же, от двух стратегий: стратегии $\pi_{\theta}$, которую мы оптимизируем, и ещё одной стратегии $\tilde{\pi}$. Снова смотрим на эту суррогатную функцию в такой точке $\boldsymbol{\theta}$, что две стратегии совпадают: $\tilde{\pi} \equiv \pi_{\theta}$; будем «шевелить» $\boldsymbol{\theta}$ и смотреть на градиент. То есть теперь мы «заморозили» распределение частот посещений состояний и, как и в прошлый раз, оценочную функцию. Тогда:

## Утверждение 57:

$$
\left.\nabla_{\theta} \mathcal{L}_{\tilde{\pi}}(\theta)\right|_{\tilde{\pi}=\pi_{\theta}}=\nabla_{\theta} J(\theta)
$$

Доказательство. Поскольку $d_{\tilde{\pi}}(s)$ и $Q^{\tilde{\pi}}(s, a)$ не зависят от $\boldsymbol{\theta}$, то для дифференцирования суррогатной функции достаточно лишь пронести градиент через мат.ожидание по выбору стратегии при помощи REINFORCE:

$$
\begin{aligned}
\nabla_{\theta} \frac{1}{1-\gamma} \mathbb{E}_{d_{\tilde{\pi}}(s)} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} Q^{\tilde{\pi}}(s, a) & =\frac{1}{1-\gamma} \mathbb{E}_{d_{\tilde{\pi}}(s)} \int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\tilde{\pi}}(s, a) \mathrm{d} a= \\
& =\left\{\text { log-derivative trick }(2.7)\right\}=\frac{1}{1-\gamma} \mathbb{E}_{d_{\tilde{\pi}}(s)} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\tilde{\pi}}(s, a)
\end{aligned}
$$

В точке $\boldsymbol{\theta}: \tilde{\pi}=\pi_{\theta}$ верно, что $d_{\tilde{\pi}}(s)=d_{\pi}(s)$ и $Q^{\tilde{\pi}}(s, a)=Q^{\pi}(s, a)$; следовательно, значение градиента в этой точке совпадает со значением формулы (5.12).

---

Это значит, что в текущей точке градиенты указывают туда же, куда и при максимизации $\mathcal{L}_{\#}(\theta)$ по $\theta$. А куда указывает направление градиентов для неё? Выражение $\mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} \boldsymbol{Q}^{\#}(s, a)$ говорит максимизировать Q-функцию по выбираемым нами действиям: это в чистом виде направление policy improvement-a из теоремы 1 ?!

Но если в табличном сеттинге policy improvement мы делали, так сказать, «жёстко», заменяя целиком стратегию на жадную по действиям, то формула градиента теперь говорит нам, что это лишь направление максимального увеличения $\boldsymbol{J}(\boldsymbol{\theta})$; после любого шага в этом направлении наша Q-функция тут же, формально, меняется, и в новой точке направление уже должно быть скорректировано.


Второе уточнение, которое дарит нам эта формула, это распределение, из которого должны приходить состояния. В табличном случае мы не знали, в каких состояниях проводить improvement «важнее», теперь же мы видим, что $\boldsymbol{s}$ должно приходить из $\boldsymbol{d}_{\boldsymbol{\pi}}(\boldsymbol{s})$. Если какое-то состояние посещается текущей стратегией часто, то улучшать стратегию в нём важнее, чем в состояниях, которые мы посещаем редко.

Но ещё это наблюдение, что градиент будущей награды и policy improvement связаны, в частности, даёт одно из оправданий тому, что мы отказались от дисконтирования частот посещения состояний. Мы используем формулу градиента для максимизации $\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}_{\boldsymbol{0}}\right)$. Но, в общем-то, мы хотим максимизировать $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ сразу для всех $\boldsymbol{s}$, и мы можем делать это с некоторыми весами (и эти веса, в целом, наш произвол). Тогда, выходит, частоты появления $\boldsymbol{s}$ в оптимизируемом функционале определяются в том числе этими весами. Другими словами, $\boldsymbol{d}_{\boldsymbol{\pi}}(\boldsymbol{s})$ указывает на «самое правильное» распределение, из которого должны приходить состояния, которые дадут направление именно максимального увеличения функционала, но теория policy improvement-a подсказывает нам, что теоретически корректно выбрать и любое другое. Даже если мы совсем другое какое-то распределение выберем для сэмплирования состояний $s$, то функционал

$$
\mathbb{E}_{s} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} \boldsymbol{Q}^{\#}(s, a) \rightarrow \max _{\theta}
$$

будет давать какое-то направление подъёма, какое-то годное направление оптимизации.
Получается, исходя из этого рассуждения, что нам для использования формулы градиента даже не нужны on-policy сэмплы, и мы можем брать состояния просто из буфера! Можем ли мы так делать? Допустим, мы готовы заменить распределение состояний на произвольное. Всё равно остаётся существенным сэмплировать действия $\boldsymbol{a}$ именно из текущей версии $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$; значит, брать действие $\boldsymbol{a}$ из буфера нельзя. Следовательно, мы не можем получить $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$; но если у нас как-то есть на руках для любой пары $\boldsymbol{s}, \boldsymbol{a}$ какое-то значение «кредита» $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, то для градиента актёра нам, согласно формуле, больше ничего и не нужно.

Итак, если у нас есть модель Q-функции, мы даже с оговорками можем обучать стратегию с буфера, если будем брать из него только состояния, а мат.ожидание по $\boldsymbol{a}$ (или его Монте-Карло оценку) считать, используя текущую стратегию $\boldsymbol{\pi}$. Однако, коли уж мы хотим off-policy алгоритм, то и эту Q-функцию тогда нужно учить с буфера. В итоге, мы получим алгоритм, очень похожий на DQN, со схожими недостатками и преимуществами. Мы обсудим такой подход позже в главе 6 , а пока что мы не будем гоняться за возможностью обучаться с буфера и за счёт этого сможем получить некоторые другие полезные преимущества, которые открываются в on-policy режиме.

В частности, раз уж формула градиента подсказывает сэмплировать $\boldsymbol{s}$ из частот посещения состояний, то мы будем стараться так делать, стараться моделировать именно оценку градиента: на практике мы возьмём состояния из недисконтированных частот $\boldsymbol{\mu}_{\boldsymbol{\pi}}(\boldsymbol{s})$, но тем не менее.

# 5.1.8. Бэйзлайн 

При стохастической оптимизации ключевым фактором является дисперсия оценки градиента. Когда мы заменяем мат.ожидания на Монте-Карло оценки, дисперсия увеличивается. Понятно, что замена Q-функции выинтегрированных будущих наград - на её Монте-Карло оценку в REINFORCE (5.7) повышало дисперсию. Однако, в текущем виде основной источник дисперсии заключается в другом.

Пример 81: Допустим, у нас два действия $\mathcal{A}=\{0,1\}$. Мы породили траекторию, в котором выбрали $\boldsymbol{a}=\mathbf{0}$. Выданное значение «кредита доверия» - пусть это точное значение $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a}=\mathbf{0})$ - допустим, равно 100. Это значит, что градиент для данной пары $\boldsymbol{s}, \boldsymbol{a}$ указывает на направление изменение параметров, которые увеличат вероятность $\boldsymbol{\pi}(\boldsymbol{a}=\mathbf{0} \mid \boldsymbol{s})$, и это направление войдёт в итоговую оценку градиентов с весом 100 . Но 100 - это много или мало?

Рассмотрим один вес $\boldsymbol{\theta}_{i} \in \mathbb{R}$ в нашей параметризации стратегии. Без ограничения общности будем считать, что для повышения $\boldsymbol{\pi}(\boldsymbol{a}=\mathbf{0} \mid \boldsymbol{s})$ вес $\boldsymbol{\theta}_{\boldsymbol{i}}$ нужно повышать. Тогда если на следующем шаге оптимизации мы в том же $\boldsymbol{s}$ засэмплировали $\boldsymbol{a}=\mathbf{1}$, то для повышения $\boldsymbol{\pi}(\boldsymbol{a}=\mathbf{1} \mid \boldsymbol{s})$ вес $\boldsymbol{\theta}_{\boldsymbol{i}}$, очевидно, нужно уменьшать (так как сумма $\boldsymbol{\pi}(\boldsymbol{a}=\mathbf{0} \mid \boldsymbol{s})+\boldsymbol{\pi}(\boldsymbol{a}=\mathbf{1} \mid \boldsymbol{s})$ обязана равняться 1). С каким весом мы будем уменьшать $\boldsymbol{\theta}_{\boldsymbol{i}}$ ? C весом $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a}=\mathbf{1})$. Что, если оно равно 1000 ? На прошлом шаге мы шли в одну сторону с весом 100 , на текущем - в другую с весом 1000 . За счёт разницы весов мы в среднем движемся в правильную сторону, но

---

Обобщим описанную в примере ситуацию. Для этого вспомним утверждение (5.6): градиент логарифма правдоподобия в среднем равен нулю. Это значит, что если для данного $\boldsymbol{s}$ мы выдаём некоторое распределение $\boldsymbol{\pi}(\boldsymbol{a} \mid$ $|\boldsymbol{s})$, для увеличения вероятностей в одной области $\mathcal{A}$ нужно данный вес $\boldsymbol{\theta}_{\boldsymbol{i}}$ параметризации увеличивать, а в другой области - уменьшать. В среднем «магнитуда изменения» равна нулю. Но у нас в Монте-Карло оценке только один сэмпл $\boldsymbol{a} \sim \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$, и для него направление изменения домножится на кредит, на нашу оценку $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. Если эта оценка в одной области 100 , а в другой 1000 - дисперсия получаемых значений $\boldsymbol{\nabla}_{\boldsymbol{\theta}} \log \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s}) \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ становится колоссальной. Было бы сильно лучше, если бы «кредит» - вес примеров - был в среднем центрирован, и тоже колебался возле нуля. Тогда для «плохих действий» мы правдоподобие этих действий уменьшаем, а для «хороших действий» - увеличиваем, что даже чисто интуитивно логичнее. И для центрирования весов правдоподобия в Policy Gradient методах всегда вводится бэйзлайн (baseline), без которого алгоритмы обычно не заведутся.

Утверждение 58: Для произвольной функции $\boldsymbol{b}(\boldsymbol{s}): \mathcal{S} \rightarrow \mathbb{R}$, называемой бэйзлайном, верно:

$$
\nabla_{\theta} J(\pi)=\frac{1}{1-\gamma} \mathbb{E}_{d_{\pi}(s)} \mathbb{E}_{\pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s)\left(Q^{\pi}(s, a)-b(s)\right)
$$

Доказательство. Добавленное слагаемое есть ноль в силу формулы (5.6).
Это верно для произвольной функции от состояний и становится неверно, если вдруг бэйзлайн начинает зависеть от $\boldsymbol{a}$. Мы вольны выбрать бэйзлайн произвольно; он не меняет среднего значения оценок градиента, но изменяет дисперсию.

Теорема 58: Бэйзлайном, максимально снижающим дисперсию Монте-Карло оценок формулы градиентов (5.14), является

$$
b^{*}(s):=\frac{\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi(a \mid s)\right\|_{2}^{2} Q^{\pi}(s, a)}{\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi(a \mid s)\right\|_{2}^{2}}
$$

Доказательство. Рассмотрим одно состояние $\boldsymbol{s}$ и попробуем вычислить оптимальное значение $\boldsymbol{b}(\boldsymbol{s})$. Пусть для данного состояния $\boldsymbol{m}$ - среднее значение оценки градиента, которое, как мы поняли ранее, не зависит от значения $\boldsymbol{b}$ :

$$
m:=\mathbb{E}_{a \sim \pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)
$$

Для нас будет важно, что $\boldsymbol{m}$ - просто какой-то фиксированный вектор той же размерности, что и $\boldsymbol{\theta}$. Распишем дисперсию и будем минимизировать её по $\boldsymbol{b}$ :

$$
\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\left(Q^{\pi}(s, a)-b\right)-m\right\|_{2}^{2} \rightarrow \min _{b}
$$

Дифференцируем по $\boldsymbol{b}$ и приравниваем к нулю:

$$
2 \mathbb{E}_{a}\left(\nabla_{\theta} \log \pi_{\theta}(a \mid s)\left(Q^{\pi}(s, a)-b\right)-m\right)^{T}\left(-\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right)=0
$$

Выделяем норму градиента логарифма правдоподобия:

$$
-\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2} Q^{\pi}(s, a)+\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2} b+\mathbb{E}_{a} m^{T}\left(\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right)=0
$$

Осталось заметить, что третье слагаемое есть ноль. Это обобщение нашей теоремы о бэйзлайне (формулы (5.6)): условно, бэйзлайн может быть свой для каждой компоненты вектора $\boldsymbol{\theta}$, опять же, до тех пор, пока он не зависит от действий. В данном случае $\boldsymbol{m}$ - некоторый фиксированный вектор, одинаковый для всех $\boldsymbol{a}$; поэтому, если $\boldsymbol{d}$ - размерность вектора параметров $\boldsymbol{\theta}$, то:

$$
\mathbb{E}_{a} m^{T}\left(\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right)=\mathbb{E}_{a} \sum_{i=0}^{d} m_{i} \nabla_{\theta_{i}} \log \pi_{\theta}(a \mid s)=\sum_{i=0}^{d} m_{i} \underbrace{\mathbb{E}_{a} \nabla_{\theta_{i}} \log \pi_{\theta}(a \mid s)}_{0 \text { по формуле }(5.6)}=0
$$

Убирая это нулевое третье слагаемое из (5.15), получаем равенство между первыми двумя:

$$
b \mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2}=\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2} Q^{\pi}(s, a)
$$

Выражая из него $\boldsymbol{b}$, получаем доказываемое.

---

Пример 82: Представить себе интуицию этой формулы можно следующим образом. Рассмотрим одно $\boldsymbol{s}$. В нём мы сэмплируем одно какое-то действие $\boldsymbol{a}$ с вероятностями $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$. Допустим, действий четыре: засэмплировалось какое-то одно. Для каждого действия есть свой градиент, показывающий, как повысить вероятность выбора этого действия $\boldsymbol{\nabla}_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$. У него есть некоторая норма $\left\|\boldsymbol{\nabla}_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})\right\|_{2}^{2}$ - его длина. Мы в качестве градиента берём такой вектор, отмасптабированный на $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ - произвольное, в общем-то, число. Как снизить дисперсию получающихся градиентов?

Мы можем вычесть из значения «кредитов» $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ какое-то общее для всех действий число b. И формула говорит: возьмите среднее значение $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ с учётом двух вещей: что, во-первых, какието вектора мелькают чаще других, и поэтому нужно брать среднее с учётом вероятностей $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$, а во-вторых, какие-то вектора длиннее по норме, чем другие, и поэтому также нужно перевзвесить значения $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ пропорционально норме градиента $\left\|\boldsymbol{\nabla}_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})\right\|_{2}^{2}$. Второе указывает сделать бэйзлайн таким, чтобы отцентрированный «кредит» для самых «длинных» векторов был поближе к нулю.


Практическая ценность результата невысока. Знать норму градиента для всех действий $\boldsymbol{a}$ вычислительно будет труднозатратно даже в дискретных пространствах действий. Поэтому мы воспользуемся небольшим предположением: мы предположим, что норма градиента примерно равна для всех действий. Тогда:

$$
b^{*}(s)=\frac{\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2} Q^{\pi}(s, a)}{\mathbb{E}_{a}\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2}}=\left\{\left\|\nabla_{\theta} \log \pi_{\theta}(a \mid s)\right\|_{2}^{2} \approx \operatorname{const}(a)\right\} \approx \mathbb{E}_{a} Q^{\pi}(s, a)=V^{\pi}(s)
$$

Эта аппроксимация довольно интуитивная: по логике, для дисперсии хорошо, если значения функции $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})-\boldsymbol{b}(\boldsymbol{s})$ вертятся вокруг нуля, то есть в среднем дают ноль, и поэтому хорошим (но неоптимальным) бэйзлайном будет

$$
\mathbb{E}_{a \sim \pi(a \mid s)}\left(Q^{\pi}(s, a)-b(s)\right):=0 \quad \Rightarrow \quad b(s):=\mathbb{E}_{a \sim \pi(a \mid s)} Q^{\pi}(s, a)=V^{\pi}(s)
$$

Итак, всюду далее будем в качестве бэйзлайна использовать $\boldsymbol{b}(\boldsymbol{s}):=\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$. Подставляя и вспоминая определение (3.19) Advantage-функции, получаем:

# Утверждение 59: 

$$
\nabla_{\theta} J(\pi)=\frac{1}{1-\gamma} \mathbb{E}_{d_{\sigma}(s)} \mathbb{E}_{\pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) A^{\pi}(s, a)
$$

Таким образом, «кредит», который мы выдаём каждой паре $\boldsymbol{s}, \boldsymbol{a}$, будет являться оценкой Advantage, и состоять из двух слагаемых: оценки Q-функции и бэйзлайна. Именно поэтому этот вес и называется «кредитом»: задача оценки Advantage и есть в точности тот самый credit assingment, который мы обсуждали в главе 3.5!

## §5.2. Схемы «Актёр-критик»

### 5.2.1. Введение критика

Мы хотели научиться оптимизировать параметры стратегии при помощи формулы градиента, не доигрывая эпизоды до конца. Мы уже поняли, что мат.ожидание по траекториям не представляет для нас проблемы. Тогда осталось лишь придумать, как, не доигрывая эпизоды до конца, проводить credit assignment, то есть определять для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ оценку Advantage-функции.

Раз знание функции $\boldsymbol{Q}^{\boldsymbol{\pi}}$ позволит обучаться, не доигрывая эпизоды до конца, а функцию в готовом виде нам никто не даст, то возникает логичная идея - аппроксимировать её. Итак, введём вторую сетку, которая будет «оценивать» наши собственные решения - критика (critic). Нейросеть, моделирующую стратегию, соответственно будем называть актёром (actor), и такие алгоритмы, в которых обучается как модель критика, так и модель актёра, называются Actor-Critic.

Здесь возникает принципиальный момент: мы не умеем обучать модели оценочных функций $\boldsymbol{Q}^{\boldsymbol{\pi}}$ или $\boldsymbol{V}^{\boldsymbol{\pi}}$, так чтобы они оценивали будущую награду несмещённо («выдавали в среднем правильный ответ»). Что это влечёт? При смещённой оценке Q-функции любые гарантии на несмещённость градиентов мгновенно теряются. Единственный способ оценивать Q-функцию несмещённо - Монте-Карло, но она требует полных эпизодов и имеет более высокую дисперсию. Любое замешивание нейросетевой аппроксимации в оценку - и мы сразу теряем несмещённость оценок на градиент, и вместе с ними - любые гарантии на сходимость стохастической оптимизации к локальному оптимуму или даже вообще хоть куда-нибудь!

---

В машинном обучении в любых задачах оптимизации всегда необходимо оценивать градиент оптимизируемого функционала несмещённо, и важной необычной особенностью Policy Gradient методов в RL является тот факт, что в них внезапно используются именно смещённые оценки градиента. Надежда на работоспособность алгоритма после замены значения $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ на смещённую оценку связана с тем, что формула градиента говорит проводить policy improvement во встречаемых состояниях (см. физический смысл суррогатной функции (5.13)). Коли градиент есть просто policy improvement, то мы знаем из общей идеи алгоритма 9 Generalized Policy Iteration, что для улучшения политики вовсе не обязательно использовать идеальную $\boldsymbol{Q}^{\boldsymbol{\pi}}$, достаточно любой аппроксимации критика $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, которая параллельным процессом движется в сторону идеальной $\boldsymbol{Q}^{\boldsymbol{\pi}}$ для текущей стратегии $\boldsymbol{\pi}$. Фактически, все Actor-Critic методы моделируют именно эту идею.

Следующий существенный момент заключается в том, что в качестве критика обычно учат именно Vфункцию. Во-первых, если бы мы учили модель Q-функции и подставили бы её, то градиент

$$
\nabla_{\theta} \log \pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s}) Q(s, \boldsymbol{a})
$$

направил бы нашу стратегию в $\operatorname{argmax} \boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$. Помимо того, что это выродило бы стратегию, это бы привело к тому, что качество обучения актёра выродилось бы в качество обучения критика: пока модель $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$ не похожа на истинную $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, у нас нет надежды, что актёр выучит хорошую стратегию.

Можем ли мы в качестве критика напрямую учить сетку, выдающую аппроксимацию $\boldsymbol{A}_{\boldsymbol{\phi}}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{A}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, и использовать её выход в качестве нашей оценки $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$ ? Это не очень удобно хотя бы потому, что в отличие от Q-функций и V-функций, advantage - не абстрактная произвольная функция: она должна подчиняться теореме 22. При этом восстановить по advantage-y Q-функцию без знания V-функции нельзя, а значит, для advantage не получится записать аналога уравнения Беллмана, использующего только advantage-функции.

Возможность не обучать сложную $\boldsymbol{Q}^{\boldsymbol{\pi}}$ является одним из преимуществ подхода прямой оптимизации $\boldsymbol{J}(\boldsymbol{\theta})$. B DQN было обязательным учить именно Q-функцию, так как, во-первых, мы хотели выводить из неё оптимальную стратегию, во-вторых, для уравнения оптимальности Беллмана для оптимальной V-функции фокус с регрессией не прокатил бы - там мат.ожидание стоит внутри оператора взятия максимума. Сейчас же мы из соображений эффективности алгоритма (желания не играть полные эпизоды и задачи снижения дисперсии оценок) не сможем обойтись совсем без обучения каких-либо оценочных функций, но нам хватит лишь $\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s}) \approx \boldsymbol{V}^{\boldsymbol{\pi}}$, поскольку оценить Q-функцию мы можем хотя бы так:

$$
Q^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V^{\pi}\left(s^{\prime}\right) \approx r(s, a)+\gamma V_{\phi}\left(s^{\prime}\right), \quad s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)
$$

Иначе говоря, если у нас есть приближение V-функции, то мы можем использовать её как для бэйзлайна, так и для оценки Q-функции. Важно, что V-функция намного проще, чем Q-функция: ей не нужно дифференцировать между действиями, достаточно лишь понимать, какие области пространства состояний - хорошие, а какие плохие. И поэтому раз можно обойтись ей, то почему бы так не сделать и не упростить критику задачу.

# 5.2.2. Bias-variance trade-off 

Обсудим сначала, как критик $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s}) \approx \boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ с параметрами $\boldsymbol{\phi}$ будет использоваться в формуле градиента по параметрам стратегии (5.16). Мы собираемся вместо честного advantage подставить некоторую его оценку (advantage estimator) и провести таким образом credit assignment:

$$
\nabla_{\theta} J(\pi) \approx \frac{1}{1-\gamma} \mathbb{E}_{d_{\boldsymbol{s}}(s)} \mathbb{E}_{a \sim \pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) \underbrace{\Psi(s, a)}_{\approx A^{\pi}(s, a)}
$$

Мы столкнулись ровно с тем же самым bias-variance trade-off, который мы обсуждали 3.5, который как раз и сводится к оцениванию Advantage и определению того, какие действия хорошие или плохие. Только теперь, в контексте policy gradient, речь напрямую идёт о дисперсии и смещении оценок градиента. Мы уже встречались с двумя самыми «крайними» вариантами выбора функции $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$ : Монте-Карло и одношаговая оценка через V-функцию (5.17):

| $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$ | Дисперсия | Смещение |
| :--: | :--: | :--: |
| $\boldsymbol{R}_{\boldsymbol{t}}-\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$ | высокая | нету |
| $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})+\gamma \boldsymbol{V}_{\boldsymbol{\phi}}\left(\boldsymbol{s}^{\prime}\right)-\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$ | низкая | большое |

В этой таблице речь идёт именно о дисперсии и смещении оценки градиента $\boldsymbol{J}(\boldsymbol{\pi})$ при использованной аппроксимации! А то есть, в первом случае оценка Q-функции несмещённая, оценка V-функции смещённая, но поскольку в качестве бэйзлайна в силу (5.6) может использоваться совершенно произвольная функция от состояний, совершенно несущественно, насколько наша аппроксимация $\boldsymbol{V}^{\boldsymbol{\pi}}(\boldsymbol{s})$ вообще похожа на истинную оценочную

---

функцию. Да, если наша аппроксимация V-функции будет неточной, мы, вероятно, не так сильно собьём дисперсию оценок градиента, как могли бы, но ровно на этом недостатки использования смещённой аппроксимации V-функции в качестве бэйзлайна заканчиваются.

Во втором же случае, аппроксимация $\boldsymbol{V}^{\boldsymbol{\pi}}\left(\boldsymbol{s}^{\boldsymbol{t}}\right)$ используется для оценки Q-функции и вызывает смещение в том числе в оценке градиента. Дисперсия же снижается, поскольку в Q-функции аккумулированы мат.ожидания по всему хвосту траектории; она в обоих случаях существенно ниже, чем до введения бэйзлайна, но замена Монте-Карло оценки на бутстрапированную оценку снижает её ещё сильнее.

Вспомним, как можно интерполировать между этими крайними вариантами. Для начала, у нас есть ещё целое семейство промежуточных вариантов - многопаговых (multi-step) оценок Q-функции, использующих N -шаговое уравнение Беллмана (3.25):

$$
Q^{\pi}(s, a) \approx \sum_{t=0}^{N-1} \gamma^{t} r^{(t)}+\gamma^{N} V_{\phi}\left(s^{(N)}\right)
$$

Тогда мы пользуемся для credit assignment-a $\boldsymbol{N}$-шаговой оценкой Advantage, или $\boldsymbol{N}$-шаговой временной разностью (3.41).

$$
\boldsymbol{\Psi}_{(N)}(s, a):=\sum_{t=0}^{N-1} \gamma^{t} r^{(t)}+\gamma^{N} \boldsymbol{V}_{\phi}\left(s^{(N)}\right)-\boldsymbol{V}_{\phi}(s)
$$

С ростом $\boldsymbol{N}$ дисперсия такой оценки увеличивается: всё больший фрагмент траектории мы оцениваем по Монте-Карло, нам становятся нужны сэмплы $\boldsymbol{a}_{t+1} \sim \pi\left(a_{t+1} \mid s_{t=1}\right), s_{t+2} \sim \pi\left(s_{t+2} \mid s_{t+1}, a_{t+1}\right), \ldots$, $s_{t+N} \sim \pi\left(s_{t+N} \mid s_{t+N-1}, a_{t+N-1}\right)$. а при $\boldsymbol{N} \rightarrow \infty$ оценка в пределе переходит в полную Монте-Карло оценку оставшейся награды, где дисперсия большая, но зато исчезает смещение в силу отсутствия смещённой аппроксимации награды за хвост траектории.

Также с ростом $\boldsymbol{N}$ мы начинаем всё меньше опираться на модель критика. Используя сэмплы наград, мы ценой увеличения дисперсии напрямую учим связь между хорошими действиями и высоким откликом от среды, меньше опираясь на промежуточный этап в виде выучивания оценочной функции. Это значит, что нам не нужно будет дожидаться, пока наш критик идеально обучиться: в оценку Advantage попадает сигнал в том числе из далёкого будущего при больших $\boldsymbol{N}$, и актёр поймёт, что удачно совершённое действие надо совершать чаще. Математически это можно объяснить тем, что, увеличивая $\boldsymbol{N}$, мы снижаем смещение наших оценок градиента. Значит, нам в целом даже и не потребуется, чтобы критик оценивал состояния с высокой точностью, поскольку главное, чтобы в итоге получалась более-менее адекватная оценка совершённых нашей стратегией действий.

Trade-off заключается в том, что чем дальше в будущее мы заглядываем, тем выше дисперсия этих оценок; помимо этого, для заглядывания в будущее на $\boldsymbol{N}$ шагов нужно же иметь это самое будущее, то есть из каждой параллельно запущенной среды понадобится собрать для очередного мини-батча не по одному сэмплу, а собрать целый длинный фрагмент траектории. Поэтому, регулируя длину оценок, мы «размениваем» смещение на дисперсию, и истина, как всегда, где-то посередине.

Возможность разрешать bias-variance trade-off и выбирать какую-то оценку с промежуточным смещением и дисперсией - чуть ли не главное преимущество on-policy режима обучения. Напомним, что сэмплы фрагментов траекторий из буфера получить не удастся (действия должны генерироваться из текущей стратегии), и использование многошаговых оценок в off-policy режиме было невозможно.

Пока что в нашем алгоритме роллауты непрактично делать сильно длинными. Дело в том, что пары $\boldsymbol{s}, \boldsymbol{a}$ из длинных роллаутов будут сильно скоррелированы, что потребуется перебивать числом параллельно запущенных сред, а тогда при увеличении длины роллаута начинает раздуваться размер мини-батча. Потом собранные переходы будут использованы всего для одного шага градиентного подъёма, и переиспользовать их будет нельзя; это расточительно и поэтому большой размер мини-батча невыгоден. Поэтому пока можно условно сказать, что самая «выгодная» оценка Advantage-а для не очень длинных роллаутов - оценка максимальной длины: для $\boldsymbol{s}_{\boldsymbol{t}}$ мы можем построить $\boldsymbol{N}$-шаговую оценку, её и возьмём; для $\boldsymbol{s}_{\boldsymbol{t}+1}$ уже не более чем $\boldsymbol{N}-\mathbf{1}$-шаговую; наконец, для $\boldsymbol{s}_{\boldsymbol{t}+\boldsymbol{N}-1}$ нам доступна лишь одношаговая оценка, просто потому что никаких сэмплов после $\boldsymbol{s}_{\boldsymbol{t}+\boldsymbol{N}}$ мы ещё не получали.

Определение 75: Для пар $s_{t}, a_{t}$ из роллаута $s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, \ldots, s_{N}$ длины $N$ будем называть оиенкой максимальной длины (max trace estimation) оценку с максимальным заглядыванием в будущее: для Q функции

$$
y^{\operatorname{MaxTrace}}\left(s_{t}, a_{t}\right):=\sum_{\bar{t}=t}^{N-1} \gamma^{\bar{t}-t} r_{\bar{t}}+\gamma^{N-t} \boldsymbol{V}^{\pi}\left(s_{N}\right)
$$

для Advantage функции, соответственно:

$$
\boldsymbol{\Psi}^{\operatorname{MaxTrace}}\left(s_{t}, a_{t}\right):=y^{\operatorname{MaxTrace}}\left(s_{t}, a_{t}\right)-\boldsymbol{V}^{\boldsymbol{\pi}}\left(s_{t}\right)
$$

Заметим, что мы вовсе не обязаны использовать для всех пар $\boldsymbol{s}, \boldsymbol{a}$ оценку одной и той же длины $\boldsymbol{N}$. То есть мы не должны брать для $s_{t}, a_{t}$ из $\boldsymbol{N}$-шагового роллаута $\boldsymbol{N}$-шаговую оценку, а остальные пары $\boldsymbol{s}, \boldsymbol{a}$ из роллаута не использовать в обучении лишь потому, что для них $\boldsymbol{N}$-шаговая оценка невозможна; вместо этого для них следует

---

просто использовать ту многошаговую оценку, которая доступна. Это полностью корректно, поскольку любая $\boldsymbol{N}$-шаговая оценка является оценкой Advantage-а для данного слагаемого в нашей формуле градиента. Именно это и говорит оценка максимальной длины: если мы собрали роллаут $s_{0}, a_{0}, s_{1}, a_{1} \ldots s_{5}$ длины 5 , то одну пару $s_{4}, a_{4}$, самую последнюю, нам придётся оценить одношаговой оценкой (поскольку для неё известен сэмпл лишь следующего состояния $s_{5}$ и только, никаких альтернатив здесь придумать не получится), предпоследнюю пару $s_{3}, a_{3}$ - двухшаговой, и так далее. То есть «в среднем» длина оценок будет очень маленькой, такая оценка с точки зрения bias-variance скорее смещена, чем имеет большую дисперсию.

# 5.2.3. Generalized Advantage Estimation (GAE) 

Пока $\boldsymbol{N}$ не так велико, чтобы дисперсия раздувалась, оценка максимальной длины - самое разумное решение trade-off-а, и о более умном решении думать не нужно. Однако, впоследствии мы столкнёмся с ситуацией, что опpolicy алгоритмы будут собирать достаточно длинные роллауты (порядка тысячи шагов), и тогда брать оценку наибольшей длины уже будет неразумно; с этой же проблемой можно столкнуться и в контексте обсуждаемой Actor-Critic схемы, если длина собираемых роллаутов достаточно большая.

Решение дилеммы bias-variance trade-off подсказывает теория $\operatorname{TD}(\boldsymbol{\lambda})$ оценки из главы 3.5. Нужно применить формулу (3.51) и просто заансамблировать $\boldsymbol{N}$-шаговые оценки разной длины:
Определение 76: GAE-оценкой Advantage-функции называется ансамбль многошаговых оценок, где оценка длины $\boldsymbol{N}$ (3.41) берётся с весом $\boldsymbol{\lambda}^{\boldsymbol{N}-\mathbf{1}}$, где $\boldsymbol{\lambda} \in(\mathbf{0}, \mathbf{1})$ - гиперпараметр:

$$
\boldsymbol{\Psi}_{\mathrm{GAE}}(s, a):=(1-\lambda) \sum_{N>0} \lambda^{N-1} \boldsymbol{\Psi}_{(N)}(s, a)
$$

Как мы помним, при $\boldsymbol{\lambda} \boldsymbol{\rightarrow 0}$ такая GAE-оценка соответствует одношаговой оценке; при $\boldsymbol{\lambda}=\mathbf{1}$ GAEоценка соответствует Монте-Карло оценке Q-функции, которой мы фактически воспользовались, например, в REINFORCE.

Как и при дисконтировании, геометрическая прогрессия затухает очень быстро; это означает, что $\boldsymbol{\lambda} \approx \mathbf{0 . 9}$ больше предпочитает короткие оценки длинным. Поэтому часто $\boldsymbol{\lambda}$ всё-таки близка к 1 , типичное значение $-0.95$.

В текущем виде в формуле суммируются все $\boldsymbol{N}$-шаговые оценки вплоть до конца эпизода. В реальности собранные роллауты могут прерваться в середине эпизода: допустим, для данной пары $\boldsymbol{s}, \boldsymbol{a}$ через $\boldsymbol{M}$ шагов роллаут «обрывается». Тогда на практике используется чуть-чуть другим определением GAE-оценки: если мы знаем $s^{(M)}$, но после этого эпизод ещё не доигран до конца, мы пользуемся формулой (3.51) и оставляем от суммы только «доступные» слагаемые:

$$
\boldsymbol{\Psi}_{\mathrm{GAE}}(s, a):=\sum_{t \geq 0}^{M-1} \gamma^{t} \lambda^{t} \boldsymbol{\Psi}_{(1)}\left(s^{(t)}, a^{(t)}\right)
$$

Напомним, что это корректно, поскольку соответствует просто запулению следа на $\boldsymbol{M}$-ом шаге, или, что тоже самое, ансамблированию (взятию выпуклой комбинации) первых $\boldsymbol{M}$ многошаговых оценок, где веса «пропавших» слишком длинных оценок просто перекладываются в вес самой длинной доступной $\boldsymbol{M}$-шаговой оценки:

Утверждение 60: Формула (5.21) эквивалентна следующему ансамблю $\boldsymbol{N}$-шаговых оценок:

$$
\boldsymbol{\Psi}_{\mathrm{GAE}}(s, a)=(1-\lambda) \sum_{N>0}^{M-1} \lambda^{N-1} \boldsymbol{\Psi}_{(N)}(s, a)+\lambda^{M-1} \boldsymbol{\Psi}_{(M)}(s, a)
$$

Доказательство. Следует из доказательства теоремы 32.
В такой «обрезанной» оценке $\boldsymbol{\lambda}=\mathbf{1}$ соответствует оценке максимальной длины (75), а $\boldsymbol{\lambda}=\mathbf{0}$ всё ещё даст одношаговую оценку.

В коде формула (5.21) очень удобна для рекурсивного подсчёта оценки; также для практического алгоритма осталось учесть флаги done $_{t}$. Формулы подсчёта GAE-оценки для всех пар $(s, a)$ из роллаута $s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, \cdots s_{N}$ приобретают такой вид:

$$
\begin{aligned}
\boldsymbol{\Psi}_{\mathrm{GAE}}\left(s_{N-1}, a_{N-1}\right) & :=\boldsymbol{\Psi}_{(1)}\left(s_{N-1}, a_{N-1}\right) \\
\boldsymbol{\Psi}_{\mathrm{GAE}}\left(s_{N-2}, a_{N-2}\right) & :=\boldsymbol{\Psi}_{(1)}\left(s_{N-2}, a_{N-2}\right)+\gamma \boldsymbol{\lambda}\left(1-\text { done }_{N-2}\right) \boldsymbol{\Psi}_{\mathrm{GAE}}\left(s_{N-1}, a_{N-1}\right) \\
\vdots & \\
\boldsymbol{\Psi}_{\mathrm{GAE}}\left(s_{0}, a_{0}\right) & :=\boldsymbol{\Psi}_{(1)}\left(s_{0}, a_{0}\right)+\gamma \boldsymbol{\lambda}\left(1-\text { done }_{0}\right) \boldsymbol{\Psi}_{\mathrm{GAE}}\left(s_{1}, a_{1}\right)
\end{aligned}
$$

---

Заметим, что эти формулы очень похожи на расчёт кумулятивной награды за эпизод, где «наградой за шаг» выступает $\boldsymbol{\Psi}_{(1)}(s, a)$. В среднем наши оценки Advantage должны быть равны нулю, и, если наша аппроксимация этому не удовлетворяет, мы получаем направление корректировки стратегии.

Теория говорит, что любые off-policy алгоритмы должны быть более эффективны в плане числа использованных сэмплов, чем on-policy, но на практике может оказаться так, что предвинутые policy gradient алгоритмы, которые мы обсудим позднее, обойдут DQN-подобные алгоритмы из главы 4 и по sample efficiency. Вероятно, это происходит в ситуациях, когда DQN страдает от проблемы распространения сигнала. При использовании GAE замешивание далёких будущих наград в таргеты позволяет справляться с этой проблемой; если награда разреженная, то имеет смысл выставлять $\mathbf{A}$ ближе к единице, если плотная - ближе к, допустим, 0.9. Распространённый дефолтный вариант 0.95 можно рассматривать как отчасти «универсальный». Если же награда информативная, на каждом шаге поступает какой-то информативный сигнал, то осмыслены и одношаговые обновления; и в таких случаях off-policy алгоритмы на основе value-based подхода скорее всего окажутся более эффективными.

# 5.2.4. Обучение критика 

Как обучать критика $\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s}) \approx \boldsymbol{V}^{\boldsymbol{\pi}}$ ? Воспользуемся идеей перехода к регрессии, которую мы обсуждали раньше в контексте DQN (раздел 4.1.2). Нам нужно просто решать методом простой итерации уравнение Беллмана $(3.3):$

$$
V_{\phi_{k+1}}(s) \leftarrow \mathbb{E}_{\boldsymbol{a}}\left[r+\gamma \mathbb{E}_{s^{\prime}} V_{\phi_{k}}\left(s^{\prime}\right)\right]
$$

Мы можем получить несмещённую оценку правой части $\boldsymbol{y}:=\boldsymbol{r}+\gamma \boldsymbol{V}_{\phi_{k}}\left(s^{\prime}\right)$ и с таким таргетом минимизировать MSE. Однако, давайте воспользуемся преимуществами on-policy режима и поймём, что мы можем поступить точно также, как с оценкой Q-функции в формуле градиента: решать многошаговое уравнение Беллмана (3.25) вместо одношагового. Например, можно выбрать любое $\boldsymbol{N}$-шаговое уравнение и строить целевую переменную как

$$
y:=r+\gamma r^{\prime}+\gamma^{2} r^{\prime \prime}+\cdots+\gamma^{N} V_{\phi_{k}}\left(s^{(N)}\right)
$$

Конечно же, определение того, насколько далеко в будущее заглядывать при построении таргета - это снова всё тот же самый bias-variance trade-off, и очередное ключевое преимущество on-policy подхода - разрешать его в том числе при обучении критика.

В чём заключается bias-variance trade-off при обучении критика? C ростом $\boldsymbol{N}$ таргет (5.22) в задаче регрессии становится всё более и более шумным, зато мы быстрее распространяем сигнал и меньше опираемся в таргете на свою же собственную аппроксимацию. Это позволяет бороться с проблемой накапливающейся ошибки, от которой страдают off-policy алгоритмы вроде DQN. В пределе - при $\boldsymbol{N}=+\infty$ - мы решаем задачу регрессии, где целевая переменная есть reward-to-go, и начинаем учить V-функцию просто по определению (3.2). Такая задача регрессии уже является самой обычной задачей регрессии из машинного обучения, целевая переменная будет являться «ground truth»: именно теми значениями, среднее которых мы и хотим выучить. Но такая задача регрессии будет обладать очень шумными целевыми переменными, плюс для сбора таких данных понадобится, опять же, полные эпизоды играть.

Мы можем для оценки Q-функции для обучения политики и для построения целевой переменной для критика использовать разные подходы (скажем, оценки разной длины), но особого смысла в этом немного: хороший вариант для одного будет хорошим вариантом и для другого. Соответственно, можно считать решение trade-off одинаковым для актёра и критика. Тогда если мы оцениваем Advantage как

$$
\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{y}-\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})
$$

где $\boldsymbol{y}$ - некоторая оценка Q-функции, то $\boldsymbol{y}$ же является и таргетом для V-функции, и наоборот. Используя функцию потерь MSE с таким таргетом, мы как раз и учим среднее значение наших оценок Q-функции, то есть бэйзлайн.

Конечно же, мы можем использовать и GAE-оценку (5.21) Advantage, достаточно «убрать бэйзлайн»:

$$
Q^{\pi}(s, a)=A^{\pi}(s, a)+V^{\pi}(s) \approx \Psi_{\mathrm{GAE}}(s, a)+V_{\phi}(s)
$$

При этом мы как бы будем решать «заансамблированные» N -шаговые уравнения Беллмана для V-функции:
Утверждение 61: Таргет $\boldsymbol{\Psi}_{\mathbf{G A E}}(s, a)+\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$ является несмещённой оценкой правой части «ансамбля» уравнений Беллмана:

$$
\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})=(\mathbf{1}-\boldsymbol{\lambda}) \sum_{N>0} \lambda^{N-1}\left[\mathfrak{B}^{N} \boldsymbol{V}_{\boldsymbol{\phi}}\right](\boldsymbol{s})
$$

где $\mathfrak{B}$ - оператор Беллмана для V-функции (3.23).
Доказательство. По определению, поскольку $\boldsymbol{\Psi}_{(N)}(\boldsymbol{s}, \boldsymbol{a})+\boldsymbol{V}(\boldsymbol{s})$ является несмещённой оценкой правой

---

части $\boldsymbol{N}$-шагового уравнения Беллмана (т. е. несмещённой оценкой $\left[\mathfrak{B}^{N} V^{\pi}\right](\boldsymbol{s})$ ), а

$$
(1-\lambda) \sum_{N>0} \lambda^{N-1}\left(\Psi_{(N)}(s, a)+V(s)\right)=\Psi_{\mathrm{GAE}}(s, a)+V(s)
$$

по определению (5.20) GAE-оценки.
Брать для обучения критика набор выходных состояний $\boldsymbol{s}$ мы можем откуда угодно. Поэтому для удобства будем брать $s \sim \boldsymbol{\mu}_{\pi}(s)$, чтобы можно было использовать для обучения критика и актёра один и тот же минибатч. Итого получаем следующее: делаем несколько шагов взаимодействия со средой, собирая таким образом роллаут некоторой длины $\boldsymbol{N}$; считаем для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ некоторую оценку Q-функции $\boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})$, например, оценку максимальной длины (5.19); оцениваем Advantage каждой пары как $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a}) \vDash \boldsymbol{y}(\boldsymbol{s}, \boldsymbol{a})-\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$; далее по Монте-Карло оцениваем градиент по параметрам стратегии

$$
\nabla_{\theta} J(\pi) \approx \frac{1}{N} \sum_{s, a} \nabla_{\theta} \log \pi_{\theta}(a \mid s) \Psi(s, a)
$$

и градиент для оптимизации критика (допустим, критик - Q-функция):

$$
\operatorname{Loss}^{\text {critic }}(\phi)=\frac{1}{N} \sum_{s, a}\left(y(s, a)-V_{\phi}(s)\right)^{2}
$$

Естественно, для декорреляции нужно собрать несколько независимых роллаутов из параллельно запущенных сред.

Когда состояния представлены в виде картинок, понятно, что и критику, и актёру нужны примерно одни и те же фичи с изображений (положение одних и тех же распознанных объектов). Поэтому кажется, что если критик и актёр будут двумя разными сетками, их первые слои будут обучаться одному и тому же. Логично для ускорения обучения объединить экстрактор фич (feature extractor) для критика и актёра и сделать им просто свои индивидуальные головы. Конечно, тогда нужно обучать всю сеть синхронно, но мы специально для этого считаем градиенты для актёра и критика по одному и тому же мини-батчу. Есть у такого общего позвоночника (shared backbone) и свои минусы: лоссы придётся отмасштабировать при помощи скалярного гиперпараметра $\boldsymbol{\alpha}$ так, чтобы одна из голов не забивала градиентами другую:

$$
\operatorname{Loss}^{\text {ActorCritic }}(\boldsymbol{\theta})=\operatorname{Loss}^{\text {actor }}(\boldsymbol{\theta})+\boldsymbol{\alpha} \operatorname{Loss}^{\text {critic }}(\boldsymbol{\theta})
$$

# 5.2.5. Advantage Actor-Critic (A2C) 

Мы собрали стандартную схему Advantage Actor Critic (A2C): алгоритма, который работает в чистом виде on-policy режиме. Из-за того, что роллауты в этом алгоритме не очень длинные, используются оценки максимальной длины (или, что тоже самое, GAE с $\boldsymbol{\lambda}=\mathbf{1}$ ).

## Алгоритм 20: Advantage Actor-Critic (A2C)

Гиперпараметры: $\boldsymbol{M}$ - количество параллельных сред, $\boldsymbol{N}$ - длина роллаутов, $\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$ - нейросеть с параметрами $\boldsymbol{\phi}, \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ - нейросеть для стратегии с параметрами $\boldsymbol{\theta}, \boldsymbol{\alpha}$ - коэф. масштабирования лосса критика, SGD оптимизатор.

Инициализировать $\boldsymbol{\theta}, \boldsymbol{\phi}$
На каждом шаге:

1. в каждой параллельной среде собрать роллаут длины $\boldsymbol{N}$, используя стратегию $\boldsymbol{\pi}_{\boldsymbol{\theta}}$ :

$$
s_{0}, a_{0}, r_{0}, s_{1}, \ldots, s_{N}
$$

2. для каждой пары $s_{t}, a_{t}$ из каждого роллаута посчитать оценку Q-функции максимальной длины, игнорируя зависимость оценки от $\boldsymbol{\phi}$ :

$$
Q\left(s_{t}, a_{t}\right) \vDash \sum_{i=t}^{N-1} \gamma^{i-t} r_{i}+\gamma^{N-t} V_{\phi}\left(s_{N}\right)
$$

3. вычислить лосс критика:

$$
\operatorname{Loss}^{\text {critic }}(\phi):=\frac{1}{M N} \sum_{s_{t}, a_{t}}\left(Q\left(s_{t}, a_{t}\right)-V_{\phi}\left(s_{t}\right)\right)^{2}
$$

---

4. делаем шаг градиентного спуска по $\phi$, используя $\nabla_{\phi} \operatorname{Loss}^{\text {critic }}(\phi)$
5. вычислить градиент для актёра:

$$
\nabla_{\theta}^{\text {actor }}:=\frac{1}{M N} \sum_{s_{t}, a_{t}} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\left(Q\left(s_{t}, a_{t}\right)-V_{\phi}\left(s_{t}\right)\right)
$$

6. сделать шаг градиентного подъёма по $\boldsymbol{\theta}$, используя $\nabla_{\theta}^{\text {actor }}$

Для того, чтобы поощрить исследование среды, к суррогатной функции потерь актёра добавляют ещё одно слагаемое, «регулиризатор», поощряющий высокую энтропию. Обычно, энтропию распределения $\boldsymbol{\pi}_{\theta}\left(\cdot \mid s_{t}\right)$ для выбранной параметризации распределения можно посчитать аналитически и дифференцировать по $\boldsymbol{\theta}$. Естественно, этот «дополнительный лосс» тоже нужно грамотно взвесить на скалярный коэффициент. Позже в разделе 6.2.1 мы встретимся с постановкой задачи Maximum Entropy RL, в рамках которой появление этого слагаемого можно объяснить небольшим изменением в самом оптимизируемом функционале.

Известно, что Policy Gradient алгоритмы особенно чувствительны к инициализации нейросетей. Рекомендуют ортогональную инициализацию слоёв. Также крайне существенна обрезка градиентов, которая защищает от взрывов в градиентной оптимизации. При обучении policy gradient алгоритмов важно логировать норму градиентов; большое значение нормы сигнализирует о неудачном подборе параметров.

Пример 83: Перерыв на чтение комиксов.
Сравним A2C с value-based алгоритмами на основе DQN. Когда мы моделировали Value Iteration, мы целиком и полностью «полагались» на этап оценивания стратегии, то есть на обучение критика: модели актёра в явном виде даже не было в алгоритме, поскольку текущая стратегия всё время считалась жадной по отношению к текущему критику. Это означало, что качество стратегии упиралось в качество критика: пока Q-функция не научится адекватно приближать истинную $\boldsymbol{Q}^{*}$, стратегия хорошей не будет.

Достаточно интересно, что идея model-free алгоритмов, отказывающихся обучать модель функции переходов в том числе из соображений end-to-end схемы, пришла к тому, что мы иногда сводимся к обучению оценочных функций - промежуточных величин, вместо того, чтобы напрямую матчить состояния и действия, понимать, какие действия стоит выбирать чаще других. Вообще, интуиция подсказывает, что обучать актёра проще, чем оценочную функцию ${ }^{2}$ : в реальных задачах человек редко когда думает в терминах будущей награды.

Пример 84: Представьте, что вы стоите перед кофеваркой и у вас есть два действия: получить кофе и не получить. Вы прекрасно знаете, какое действие лучше другого, но при этом не оцениваете, сколько кофе вы сможете выпить в будущем при условии, например, что сейчас вы выберете первый или второй вариант: то есть не строите в явном виде прогноз значения оценочной функции.

И формула градиента, идея policy gradient методов, как раз предоставляет возможность обучать актёра напрямую, минуя промежуточный этап в виде критика: так, в алгоритме 19 REINFORCE мы могли использовать Монте-Карло оценки Q-функции и обходиться без обучения в явном виде модели критика, то есть полностью опираться на этап policy improvement-a. Поэтому policy gradient алгоритмы ещё иногда называют policy-based. Таким образом, DQN и REINFORCE - это два «крайних случая», первый алгоритм полностью опирается на критика, а второй алгоритм - на актёра. Только у первого недостатки компенсируются возможностью обучаться в off-policy режиме и использовать реплей буфер, а вот недостатки алгоритма REINFORCE - высокая дисперсия и необходимость играть целые эпизоды - не имеют аналогичного противовеса.

Важно, что в Policy Gradient алгоритмах мы за счёт использования метода REINFORCE при расчёте градиента можем заменять значение Q-функции (необходимую для улучшения политики) на какую-то заглядывающую в будущее оценку. И насколько сильно при обучении актёра опираться на критика («насколько далеко в будущее заглядывать») - это и есть bias-variance trade-off, который в on-policy алгоритмах возможно разрешать. За счёт

[^0]а для критика мы идём по градиенту (продифференцируйте MSE, чтобы убедиться в этом):

$$
\nabla_{\phi} V_{\phi}(s) \Psi(s, a)
$$

Как видно, это как будто бы один и тот же градиент: он просто говорит увеличивать соответствующий выход нейронной сети, а кредит предоставляет скаляр, сообщающий масштаб изменения и, главное, знак.


[^0]:    ${ }^{2}$ есть, однако, и ряд теоретических наблюдений, которые говорит, что эти задачи скорее эквивалентны по сложности. На это указывают формулы градиентов для обучения критика и актёра: так, для актёра мы идём по градиенту

    $$
    \nabla_{\theta} \log \pi_{\theta}(s \mid s) \Psi(s, a)
    $$

---

этого A2C в отличие от DQN может использовать и в качестве таргета для обучения критика, и в качестве оценки для обучения Q-функции GAE-оценку (5.21). Да, пока что в A2C роллауты (зачастую) получаются слишком короткими, и GAE ансамбль «бедный», приходится $\boldsymbol{\lambda}=\mathbf{1}$ использовать, но главное, что есть такая возможность технически, и даже короткие роллауты уже позволяют существенно справиться с проблемой распространения сигнала: это помогает и критику лучше учиться, и актёр может не полностью на критика опираться. В частности, нам не нужна модель Q-функции, достаточно более простой V-функции. Всё это делает Policy Gradient алгоритмы куда более эффективными в средах с сильно отложенным сигналом.

Наконец, ещё одно небольшое, но важное преимущество Policy Gradient - обучение стохастичной политики. Хотя мы знаем, что оптимальная политика детерминирована, обучение стохастичной политики позволяет использовать её для сбора данных, и стохастичность - ненулевая вероятность засэмплировать любое действие частично решает проблему исследования. Да, это, конечно, далёкое от идеала решение, но намного лучшее, чем, например, $\varepsilon$-жадная стратегия: можно рассчитывать на то, что если градиент в какой-то области пространства состояний постоянно указывает на то, что одни действия хорошие, а другие плохие, актёр выучит с вероятностью, близкой к единице, выбирать хорошие действия; если же о действиях в каких-то состояниях приходит противоречивая информация, или же такие состояния являются для агента новыми, то можно надеяться, что стратегия будет близка к равномерной, и агент будет пробовать все действия.

Все эти преимущества неразрывно связаны с on-policy режимом. За счёт «свежести» данных, можно делать, так сказать, наилучшие возможные шаги обучения стратегия, практически идти по градиенту оптимизируемого функционала. Но из него проистекает и главный недостаток подхода: неэффективность по количеству затрачиваемых сэмплов. Нам всё время нужны роллауты, сгенерированные при помощи текущей политики с параметрами $\boldsymbol{\theta}$, чтобы посчитать оценку градиента в точке $\boldsymbol{\theta}$ и сделать шаг оптимизации. После этого будут нужны уже сэмплы из новой стратегии, поэтому единственное, что мы можем сделать с уже собранными данными - почистить оперативную память.

Поиятно, что это полное безобразие: допустим, агент долго вёл себя псевдослучайно и наконец наткнулся на какой-то хороший исход с существенным откликом среды. Схема сделает по ценному роллауту всего один градиентный шаг, что скорее всего не поможет модели выучить удачное поведение или значение отклика. После этого ценная информация никак не может быть использована и придётся дожидаться следующей удачи, что может случиться нескоро. Это очень sample inefficient и основная причина, почему от любого on-policy обучения в чистом виде нужно пытаться отойти.

Пример 85: Допустим, вы играете в видео-игру, и в начале обучения мало что умеете, всё время действуя примерно случайно и падая в первую же яму. Среда выдаёт вам всё время ноль, и вы продолжаете вести себя случайно. Вдруг в силу стохастичности стратегии вы перепрыгиваете первую яму и получаете монетку +1 . В DQN этот ценнейший опыт будет сохранён в буфере, и постепенно критик выучит, какие действия привели к награде. В A2C же агент сделает один малюююсенький шаг изменения весов моделей и тут же выкинет все собранные данные в мусорку, потому что на следующих итерациях он никак не может переиспользовать их. Агенту придётся ждать ещё много-много сессий в самой игре, пока он не перепрыгнет яму снова, чтобы сделать следующий шаг обучения перепрыгиванию ям.

Частично с этой проблемой нам удастся побороться в следуюшей главе, что позволит построить алгоритмы лучше A2C. Но важно, что полностью решить эту проблему, полностью перейти в off-policy режим, нам не удастся точно также, как в off-policy мы не смогли адекватно разрешать bias-variance trade-off (и там максимум наших возможностей была Retrace-оценка). Поэтому построить алгоритм, берущий «лучшее от двух миров», нельзя, и поэтому среди model-free алгоритмов выделяют два класса алгоритмов: off-policy, работающие с реплей буфером и потому потенциально более sample efficient, и on-policy алгоритмы, где есть ряд вышеуказанных преимуществ.

# §5.3. Продвинутые Policy Gradient 

### 5.3.1. Суррогатная функция

Как можно хотя бы частично побороться с ключевой проблемой Policy Gradient подхода - с on-policy режимом? Есть два соображения, как это можно попробовать сделать.

Во-первых, можно попробовать вместо градиентного подъёма использовать какой-то более тяжеловесный метод оптимизации, например, методы оптимизации второго порядка. Такие методы обычно требуют меньше обращений к оракулу, делая меньше итераций, за счёт более высокой вычислительной сложности каждого шага. В нашем случае это может быть ровно то, что нам нужно - вызов оракула для нас это сбор данных, и какие-то дополнительные вычисления «на компьютере» можно считать дешёвой процедурой.

Вторая идея - можно как-то попробовать посчитать оценку градиента в точке текущих параметров, при этом используя сэмплы, полученные при помощи другой стратегии. Полностью перейти в off-policy режим с сохранением всех преимуществ on-policy у нас не получится, поэтому придётся наложить ограничение на эту другую стратегию сбора данных: она должна быть очень похожей версией оцениваемой стратегии, то есть, можно считать, недавней версии актуальной стратегии. То, что стратегия сбора данных «похожа» на текущую,

---

означает, что её траектории тоже в некотором смысле «похожи»: понятно, что это важно, ведь много информации о градиенте политики по около-оптимальным параметрам с траекторий случайного агента собрать не получится, просто потому что такие траектории у около-оптимальной стратегии почти не встретятся.

На самом деле эти две идеи примерно об одном и том же, что мы и увидим далее.
Итак, допустим мы хотим оптимизировать стратегию $\pi_{\theta}$ по параметрам $\theta$, используя только сэмплы из другой стратегии $\pi^{\text {old }}$ (в итоговой схеме это будет недавняя версия стратегии). Что нам тогда мешает оценить значение (5.16)?

$$
\nabla_{\theta} J(\pi)=\frac{1}{1-\gamma} \mathbb{E}_{d_{\pi}(s)} \mathbb{E}_{\pi(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) A^{\pi}(s, a)
$$

Во-первых, у нас нет сэмплов из $d_{\pi}(s)$, ведь в данных есть только ${ }^{3}$ состояния из $d_{\pi^{\text {old }}}(s)$. Мы уже обсуждали, что поскольку формула говорит проводить policy improvement, мы можем подменить это распределение на любое другое, и схема останется рабочей. Однако ключевой является вторая проблема. У нас нет оценки $A^{\pi}(s, a)$. Если мы попробуем оценить Advantage для одной стратегии по данным из другой, то нам придётся вводить все те коррекции, которые мы обсуждали в разделе 3.5.7 про Retrace оценку, которая скорее всего схлопнется в смещённую. Очень хотелось бы всё-таки оставить «чистую» GAE-оценку, то есть как-то использовать «несвежего» критика $A^{\pi^{\text {old }}}(s, a)$ в формуле градиента.

Как сделать так, чтобы у нас где-то в формулах образовался «несвежий» критик? На помощь приходит RPI, формула (3.20): мы можем сменить функцию награды на Advantage функцию любой другой стратегии! Давайте запишем это в следующем виде:

# Утверждение 62: 

$$
J\left(\pi_{\theta}\right)=J\left(\pi^{\mathrm{old}}\right)+\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_{\theta}}(s)} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} A^{\pi^{\mathrm{old}}}\left(s_{t}, a_{t}\right)
$$

Доказательство. Из RPI (3.20) следует, что для любых двух стратегий верно

$$
J\left(\pi_{\theta}\right)=J\left(\pi^{\mathrm{old}}\right)+\mathbb{E}_{\boldsymbol{T} \sim \pi_{\theta}} \sum_{t \geq 0} \gamma^{t} A^{\pi^{\mathrm{old}}}\left(s_{t}, a_{t}\right)
$$

Для получения формулы достаточно переписать мат.ожидание по траекториям из $\pi_{\theta}$ через state visitation distribution, подставив в теореме 57 в качестве $\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})=A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$.

Тогда градиент исходного функционала $\nabla_{\theta} J\left(\pi_{\theta}\right)$ есть градиент правой части, и в ней уже как-то «замешана» оценка Advantage для стратегии $\pi^{\text {old }}$, которое мы сможем посчитать при помощи GAE-оценки:

$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\nabla_{\theta} \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_{\theta}}(s)} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} A^{\pi^{\mathrm{old}}}\left(s_{t}, a_{t}\right)
$$

Мы также можем справиться с мат.ожиданием $\mathbb{E}_{a \sim \pi_{\theta}(a \mid s)}$ при помощи importance sampling. Да, этот коэффициент может быть ужасен (сильно большим единицы или близким к нулю), но это коррекция всего лишь за один шаг, и такая дробь будет терпимой.

$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\nabla_{\theta} \frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_{\theta}}(s)} \mathbb{E}_{a \sim \pi^{\mathrm{old}}(a \mid s)} \frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)} A^{\pi^{\mathrm{old}}}\left(s_{t}, a_{t}\right)
$$

Осталась последняя проблема: $d_{\pi_{\theta}}(s)$. В роллаутах, сгенерированных при помощи $\pi^{\text {old }}$, состояния всё-таки будут приходить из $d_{\pi^{\text {old }}}(s)$, и тут мы importance sampling не сделаем даже при большом желании, так как просто не можем внятно оценить эти величины: из частот посещения состояний мы можем только сэмплировать.

Рассмотрим аппроксимацию: что, если мы в формуле (5.24) заменим $d_{\pi_{\theta}}(s)$ на $d_{\pi^{\text {old }}}(s)$ ? Это, вообще говоря, будет какая-то другая функция.

Определение 77: Введём суррогатную функцию:

$$
\boldsymbol{L}_{\pi^{\mathrm{old}}}(\theta):=\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi^{\mathrm{old}}}(s)} \mathbb{E}_{a \sim \pi^{\mathrm{old}}(a \mid s)} \frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)} A^{\pi^{\mathrm{old}}}(s, a)
$$

Утверждение 63: Сэмплы из мат.ожиданий в суррогатной функции - это сэмплы из роллаутов, сгенериро-

[^0]
[^0]:    ${ }^{3}$ с учётом забивания на $\gamma^{t}$ и наших прочих оговорок; но для корректности выкладок будем в этой главе писать везде дисконтированные частоты посещения состояний $\boldsymbol{d}$

---

ванных при помощи $\pi^{\text {old }}$ :

$$
\boldsymbol{L}_{\pi^{\mathrm{old}}}(\theta)=\mathbb{E}_{\boldsymbol{T} \sim \pi^{\mathrm{old}}} \sum_{i \geq 0} \gamma^{i} \frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)} A^{\pi^{\mathrm{old}}}(s, a)
$$

Пояснение. Применить теорему 57 в обратную сторону для $\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})=\frac{\pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})}{\pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})} A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$
Какой у этой суррогатной функции $\boldsymbol{L}_{\pi^{\text {old }}}(\theta)$ физический смысл? Мы сказали, что наши настраиваемые параметры $\theta$ не влияют на частоты посещения состояний, то есть выбор действий не влияет на то, какие состояния мы будем посещать в будущем. Это очень сильное допущение, поэтому аппроксимация не самая удачная. Давайте поймём, что будет происходить, если мы будем оптимизировать вместо честного, исходного функционала, такую суррогатную функцию при фиксированной $\pi^{\text {old }}$ :

$$
\boldsymbol{L}_{\pi^{\mathrm{old}}}(\theta) \rightarrow \max _{\theta}
$$

Утверждение 64: Решением оптимизационной задачи (5.26) является жадная стратегия по отношению к $Q^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$ (или, что тоже самое, к Advantage-функции стратегии $\pi^{\text {old }}$ ):

$$
\pi_{\theta}(s)=\underset{a}{\operatorname{argmax}} A^{\pi^{\mathrm{old}}}(s, a)
$$

Доказательство. Эта оптимизационная задача распадается на оптимизационную задачу для каждого $s$, а решением задачи

$$
\mathbb{E}_{a \sim \pi^{\mathrm{old}}(a \mid s)} \frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)} A^{\pi^{\mathrm{old}}}(s, a)=\mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} A^{\pi^{\mathrm{old}}}(s, a) \rightarrow \max _{\theta}
$$

является «жадная» стратегия.
Другими словами, такая суррогатная функция просто говорит проводить policy improvement стратегии $\pi^{\text {old }}$, но в состояниях, приходящих из частот посещения состояний $d_{\pi^{\text {old }}}(\boldsymbol{s})$. Такая очередная форма нам сейчас будет удобна, поскольку такая суррогатная функция является локальной аппроксимацией нашего оптимизируемого функционала, причём эта аппроксимация - не просто, скажем, линейное приближение оптимизируемой функции, а какое-то более умное, учитывающее особенности задачи.

# 5.3.2. Нижняя оценка 

Итак, у нас есть аппроксимация нашего оптимизируемого функционала через суррогатную функцию, с которой мы можем работать:

$$
J(\pi) \approx J\left(\pi^{\mathrm{old}}\right)+\boldsymbol{L}_{\pi^{\mathrm{old}}}(\theta)=J\left(\pi^{\mathrm{old}}\right)+\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi^{\mathrm{old}}}(s)} \mathbb{E}_{a \sim \pi^{\mathrm{old}}(a \mid s)} \frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)} A^{\pi^{\mathrm{old}}}(s, a)
$$

Насколько эта аппроксимация хороша? Наша интуиция была в том, что если $\pi$ «похожа» на $\pi^{\text {old }}$, то частоты посещения состояний у них тоже наверняка будут похожи. Формализовать «похожесть» стратегий можно, например, так:

Определение 78: Введём расстояние между стратегиями $\pi^{\text {old }}, \pi_{\theta}$ как среднюю KL-дивергенцию между ними по состояниям из частот посещения первой стратегии:

$$
\mathbf{K L}\left(\pi^{\mathrm{old}} \| \pi_{\theta}\right):=\mathbb{E}_{s \sim d_{\pi^{\mathrm{old}}}(s)} \mathbf{K L}\left(\pi^{\mathrm{old}}(a \mid s) \| \pi_{\theta}(a \mid s)\right)
$$

Теорема 59:

$$
\left|J\left(\pi_{\theta}\right)-J\left(\pi^{\mathrm{old}}\right)-\boldsymbol{L}_{\pi^{\mathrm{old}}}(\theta)\right| \leq C \sqrt{\mathrm{KL}\left(\pi^{\mathrm{old}} \| \pi_{\theta}\right)}
$$

где $\boldsymbol{C}$ - константа, равная $\boldsymbol{C}=\frac{\sqrt{2} \pi}{(1-\gamma)^{2}} \max _{s, a}\left|A^{\pi^{\text {old }}}(s, a)\right|$
Без доказательства; интересующиеся могут обратиться к статье Constrained Policy Optimization.
Конечно, это очень грубая оценка, хотя бы потому, что она верна для произвольных MDP и произвольных двух стратегий. Но ключевой момент в том, что мы теперь формально можем вывести нижнюю оченку (lower

---

bound) на оптимизируемый функционал ${ }^{4}$ :

# Теорема 60 - Performance Lower Bound: 

$$
J\left(\pi_{\theta}\right)-J\left(\pi^{\text {old }}\right) \geq \boldsymbol{L}_{\pi^{\text {old }}}(\theta)-\boldsymbol{C} \sqrt{\mathrm{KL}\left(\pi^{\text {old }}\left\|\pi_{\theta}\right)\right.}
$$

Возникает любопытнейшая идея: возможно, мы можем работать не с исходным функционалом, а с нижней оценкой.

Теорема 61: Процедура оптимизации

$$
\theta_{k+1}:=\underset{\theta}{\operatorname{argmax}}\left[L_{\pi_{\theta_{k}}}(\theta)-\boldsymbol{C} \sqrt{\mathrm{KL}\left(\pi_{\theta_{k}} \| \pi_{\theta}\right)}\right]
$$

гарантирует монотонное неубывание функционала: $\boldsymbol{J}\left(\pi_{\theta_{k+1}}\right) \geq \boldsymbol{J}\left(\pi_{\theta_{k}}\right)$
Доказательство. В точке $\boldsymbol{\theta}=\boldsymbol{\theta}_{\boldsymbol{k}}$ суррогатная функция $\boldsymbol{L}_{\pi_{\theta_{k}}}(\boldsymbol{\theta})$ равна нулю, поскольку

$$
\begin{aligned}
\boldsymbol{L}_{\pi_{\theta_{k}}}\left(\theta_{k}\right) & = \\
=\{\text { по определению (5.25) }\} & =\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_{\theta_{k}}}(s)} \mathbb{E}_{a \sim \pi_{\theta_{k}}(a \mid s)} \frac{\pi_{\theta_{k}}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)} A^{\pi_{\theta_{k}}}(s, a)= \\
& =\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_{\theta_{k}}}(s)} \mathbb{E}_{a \sim \pi_{\theta_{k}}(a \mid s)} A^{\pi_{\theta_{k}}}(s, a)= \\
\{\text { следствие RPI }(5.23)\} & =\boldsymbol{J}\left(\pi_{\theta_{k}}\right)-\boldsymbol{J}\left(\pi_{\theta_{k}}\right)=\mathbf{0}
\end{aligned}
$$

Понятно, что $\mathbf{K L}\left(\pi_{\theta_{k}} \| \pi_{\theta}\right)$ в точке $\boldsymbol{\theta}=\boldsymbol{\theta}_{\boldsymbol{k}}$ тоже равна 0 , так как во всех состояниях $\mathbf{K L}$ между одинаковыми стратегиями равна нулю. Значит, максимум нижней оценки не меньше нуля, а она есть нижняя оценка на $\boldsymbol{J}\left(\pi_{\theta_{k+1}}\right)-\boldsymbol{J}\left(\pi_{\theta_{k}}\right)$.

Итак, в воздухе витает идея заняться типичным minorization-maximization алгоритмом. Сначала мы подтягиваем нашу нижнюю оценку так, чтобы в текущей точке $\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}$ она в точности совпадала с оптимизируемой функцией (minorization). Затем мы начинаем при фиксированном $\theta^{\text {old }}$ оптимизировать по $\theta$ не наш функционал (который мы не сможем оптимизировать без новых сэмплов из новой стратегии $\pi_{\theta}$ с текущими значениями параметров), а нижнюю оцен-


ку, с которой умеем работать (maximization).

То есть, что мы получили: мы «можем» оптимизировать не $\boldsymbol{J}\left(\pi_{\theta}\right)-\boldsymbol{J}\left(\pi^{\text {old }}\right)$ по формуле (5.24), а нашу суррогатную функцию $\boldsymbol{L}_{\pi^{\text {old }}}(\boldsymbol{\theta})$, то есть проводить policy improvement стратегии $\pi^{\text {old }}$, но добавив штраф регуляризатор - за различие между $\pi_{\theta}$ и стратегией, из которой приходят данные $\pi^{\text {old }}$ :

$$
\boldsymbol{L}_{\pi^{\text {old }}}(\theta)-\boldsymbol{C} \sqrt{\mathrm{KL}\left(\pi^{\text {old }} \| \pi_{\theta}\right)} \rightarrow \max _{\theta}
$$

Мы получили процедуру, гарантирующую улучшение стратегии, что звучит подозрительно хорошо. Очень похожие гарантии улучшения стратегии у нас были в policy improvement. Интересно порассуждать, в чём отличие. Мы уже увидели в утверждении 64, что оптимизация суррогатной функции без регуляризатора соответствует policy improvement-у стратегии $\pi^{\text {old }}$. Однако как мы помним, жадный policy improvement вовсе не является наилучшим: если мы в некотором состоянии $\boldsymbol{s}$ перекладываем вероятностную массу в какое-то действие, чтобы увеличить среднее значение $A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$, мы попадаем чаще в те состояния, где стратегия $\pi^{\text {old }}$ набирает больше, да, но на самом деле мы таким изменением стратегии можем помешать себе добираться чаще до тех состояний, где $A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$ принимает большие положительные значения!

[^0]
[^0]:    ${ }^{4}$ исторически в статьях по TRPO и PPO использовалась чуть более грубая нижняя оценка, в которой ошибка между суррогатной функцией и честным функционалом оценивалась сверху при помощи KL-дивергенции в максимальной форме:

    $$
    \mathbf{K L}^{\max }\left(\pi^{\text {old }} \| \pi\right):=\max _{\boldsymbol{s}} \mathbf{K L}\left(\pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s}) \| \pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})\right)
    $$

    Однако такое выражение посчитать в практических алгоритмах нельзя, поэтому далее её приходилось эвристически заменять на среднее по $\boldsymbol{s} \sim d_{\pi^{\text {old }}}(\boldsymbol{s})$. Уточнённая нижняя оценка обосновывает этот переход и указывает, что из KL-дивергенции также нужно взять корень; в остальном на ход дальнейших рассуждений это не влияет.

---

Пример 86: Что говорит policy improvement для MDP с картинки и заданной стратегией $\pi^{\text {old }} ?$ В начальном состоянии $Q^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{\square})=+\mathbf{1 0}, Q^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{\square})=-\mathbf{1 0 0}$, и поэтому улучшение будет заключаться в том, что новая стратегия должна чаще выбирать именно действие $\boldsymbol{\square}$ выбирать +10 . Это связано с тем, что policy improvement игнорирует влияние действий на частоты посещения состояний, и оптимизирует «новую награду» $A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$ независимо в каждом состоянии жадным образом. Он не видит, что выбор в начальном состоянии действия $\square$ позволит ему попасть в такое состояние, где для одного из действий $A^{\pi^{\text {old }}}\left(\boldsymbol{s}_{2}, \square^{\prime}\right)=+\mathbf{2 0 0 !}$


Поэтому формула policy gradient - «наилучшего policy improvement-a» - говорит, что как только параметры $\boldsymbol{\theta}$ хоть чуть-чуть изменяются, сразу же стоит улучшать новую стратегию, то есть использовать свежего критика. Мы же свежего критика получить не можем, хотим пользоваться лишь «несвежим» $A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$, и нижняя оценка (5.29) даёт промежуточную альтернативу, что тогда можно делать: добавить регуляризитор, запрещающий сильное изменение исходной стратегии $\pi^{\text {old }}$.

Однако чтобы оптимизировать (5.29), нужно как-то определить значение константы $\boldsymbol{C}$ : мы не умеем считать выражение для неё из теоремы 59 , поскольку там присутствует максимум Advantage-функции по всем парам состояние-действие ${ }^{5}$. Мы можем заменить его на гиперпараметр, но, чтобы не потерять теоретические гарантии, он должен быть достаточно большим, чтобы превосходить значение из теоремы. Вообще, скорее всего, даже если бы мы знали эту константу, она была бы колоссальной: чего стоит только $(\mathbf{1}-\gamma)^{\mathbf{2}}$ в знаменателе формулы из теоремы 59. Поэтому практической пользы от такой нижней оценки всё равно много бы не было: её оптимизация делала бы слишком консервативные шаги обновления политики.

# 5.3.3. Trust Region Policy Optimization (TRPO) 

В TRPO предлагается воспользоваться полученной теорией, чтобы построить на основе идеи суррогатной функции более «мощный» метод оптимизации, чем обычный градиентный спуск. Как обычно устроены методы оптимизации? В текущей точке для рассматриваемой функции строится какая-то модель, какая-то локально верная аппроксимация (например, линейная или квадратичная). Далее эта модель либо оптимизируется, и алгоритм сдвигает текущее решение в сторону оптимума модели (такие методы относят к line search подходу), либо модель оптимизируется в некотором «регионе доверия» (trust region), где эта модель, считается, более-менее похожа на исходный функционал. Подход на основе регионов доверия считается более тяжеловесным, поскольку требует решать на каждом шаге работы алгоритма условную задачу оптимизации, зато более эффективным по числу итераций.

Основная идея TRPO заключается в переходе от оптимизации (5.29) без ограничений к задаче оптимизации с ограничением в trust region форме:

$$
\left\{\begin{array}{l}
\boldsymbol{L}_{\pi^{\text {old }}}(\boldsymbol{\theta}) \rightarrow \max _{\boldsymbol{\theta}} \\
\mathbf{K L}\left(\pi^{\text {old }}\left\|\pi_{\theta}\right\rangle\right\rangle \boldsymbol{\delta}
\end{array}\right.
$$

То есть, суррогатная функция является локальной аппроксимацией нашего функционала, поэтому на каждом шаге работы алгоритма мы будем работать с ней. При этом


второе слагаемое оптимизируемой нижней оценки (5.29) подсказывает, что с ростом KL-дивергенции «нечестный» функционал всё меньше похож на настоящий, и к нему «всё меньше доверия». Условная задача оптимизации говорит, что оптимизировать суррогатную функцию вместо настоящего функционала можно, но с жёстким ограничением на длину шага.

Вообще говоря, мы получили задачу для оптимизации $\boldsymbol{L}_{\pi^{\text {old }}}(\boldsymbol{\theta})$ методом натуральных градиентов (natural gradient): мы оптимизируем функцию, разрешая не шаг некоторой длины по градиенту в пространстве параметров, а шаг некоторой длины в пространстве параметрически заданных распределений. Подробнее о натуральном градиенте можно прочитать в приложении А.1. Если раньше градиентный шаг мог сильно поменять стратегию (как распределение в пространстве действий), при том что для других небольших изменений распределения было бы необходимо сильно менять параметры $\boldsymbol{\theta}$, то здесь $\boldsymbol{\delta}$ ограничивает изменение самой стратегии в терминах KL-дивергенции. Ограничение $\boldsymbol{\delta}$ нам при этом всё равно нужно будет выбрать, это аналог learning rate в «trust region формах» методов оптимизации.

Как будем задачу (5.30) решать? В контексте нашей задачи мы собрали при помощи текущей стратегии некоторое количество данных для Монте-Карло оценок всех мат.ожиданий (в этот момент $\boldsymbol{\pi}^{\text {old }}=\boldsymbol{\pi}_{\boldsymbol{\theta}}$ ) и хотим решить задачу (5.30), зафиксировав $\pi^{\text {old }}$ и оптимизируя параметры $\boldsymbol{\theta}$. Обозначим параметры $\pi^{\text {old }}$ как $\boldsymbol{\theta}^{\text {old }}$. Аппроксимируем оптимизируемый функционал $\boldsymbol{L}_{\pi^{\text {old }}}(\boldsymbol{\theta})$ разложением Тейлора до первого порядка с центром в точке $\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}$, а ограничение - до второго. До второго - потому что слагаемое первого порядка ноль.

[^0]
[^0]:    ${ }^{5}$ мы могли бы оценить его сверху как $\mathbf{2} \boldsymbol{R}^{\text {max }}$, где $\boldsymbol{R}^{\text {max }}$ - максимальная награда, но это было бы непрактично грубой оценкой.

---

Утверждение 65: В точке $\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}$ первый член разложения ограничения в ряд Тейлора равен нулю:

$$
\left.\forall s: \nabla_{\theta} \mathbf{K L}\left(\pi^{\text {old }}\left\|\pi_{\theta}\right)\right\|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}}=\mathbf{0}
$$

Доказательство. KL-дивергенция в этой точке равна 0 как среднее по состояниям дивергенций между одинаковыми распределениями, следовательно как функция от $\boldsymbol{\theta}$ она достигает в этой точке глобального минимума $\Rightarrow$ градиент равен нулю.

Итак, введём обозначения для предложенного разложения. Пусть $\boldsymbol{g}$ - градиент $\boldsymbol{L}_{\pi \text { old }}(\boldsymbol{\theta})$ по параметрам $\boldsymbol{\theta}$ в точке $\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}$, а $\boldsymbol{F}$ - гессиан ограничения в точке $\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}$ :

$$
\begin{gathered}
\boldsymbol{g}:=\left.\nabla_{\theta} \boldsymbol{L}_{\pi \text { old }}(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}} \\
\boldsymbol{F}:=\left.\nabla_{\theta}^{2} \mathbf{K L}\left(\pi^{\text {old }}\left\|\pi_{\theta}\right)\right\|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}}
\end{gathered}
$$

В этих обозначениях аппроксимация задачи получается следующая:

$$
\left\{\begin{array}{l}
\left\langle g, \theta-\theta^{\text {old }}\right\rangle \rightarrow \max _{\theta} \\
\frac{1}{2}\left(\theta-\theta^{\text {old }}\right)^{T} \boldsymbol{F}\left(\theta-\theta^{\text {old }}\right) \leq \delta
\end{array}\right.
$$

Градиент $\boldsymbol{g}$, на самом деле, весьма любопытен:
Утверждение 66: Градиент $\boldsymbol{g}$ совпадает с градиентом из алгоритма Advantage Actor Critic (5.16).
Доказательство.

$$
\begin{aligned}
& \boldsymbol{g}=\left.\nabla_{\theta} \boldsymbol{L}_{\pi \text { old }}(\boldsymbol{\theta})\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}}=\frac{1}{1-\gamma} \mathbb{E}_{\boldsymbol{s} \sim \boldsymbol{d}_{\pi \text { old }}(\boldsymbol{s})} \mathbb{E}_{\boldsymbol{a} \sim \pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})} \frac{\nabla_{\theta} \pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}}}{\pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})} A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})= \\
& =\left\{\begin{array}{l}
\text { замечаем определение } \\
\text { производной логарифма }
\end{array}\right\}=\frac{1}{1-\gamma} \mathbb{E}_{\boldsymbol{s} \sim \boldsymbol{d}_{\pi \text { old }}(\boldsymbol{s})} \mathbb{E}_{\boldsymbol{a} \sim \pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})}\left.\nabla_{\theta} \log \pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}} A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})
\end{aligned}
$$

что в точности есть градиент обычного ActorCritic с бэйзлайном в точке $\boldsymbol{\theta}=\boldsymbol{\theta}^{\text {old }}$.

Теорема 62: Решение аппроксимированной задачи (5.33) есть

$$
\theta-\theta^{\text {old }}=k F^{-1} g
$$

где скалярный коэффициент пропорциональности $\boldsymbol{k}$ можно посчитать по формуле $\boldsymbol{k}=\sqrt{g^{T} F^{-1} g}$
Доказательство. Составляем лагранжиан (оптимизируемый функционал входит с минусом, т.к. максимум поменяем на минимум):

$$
\mathcal{L}(\boldsymbol{\lambda}, \boldsymbol{\theta})=-g^{T}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\text {old }}\right)+\lambda\left(\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\text {old }}\right)^{T} \boldsymbol{F}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\text {old }}\right)-\boldsymbol{\delta}\right)
$$

Дифференцируем лагранжиан и приравниваем к нулю:

$$
\nabla_{\theta} \mathcal{L}(\lambda, \boldsymbol{\theta})=-g+\lambda \boldsymbol{F}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\text {old }}\right)=\mathbf{0}
$$

Отсюда получаем:

$$
\theta-\theta^{\text {old }}=\frac{1}{\lambda} F^{-1} g
$$

Осталось найти значение $\boldsymbol{\lambda}$. Поскольку решение должно быть допустимой точкой, а оптимизируемый функционал линеен, понятно, что ограничение из задачи превратится в равенство и будет достигнуто на границе. То есть:

$$
\frac{1}{2}\left(\theta-\theta^{\text {old }}\right)^{T} F\left(\boldsymbol{\theta}-\theta^{\text {old }}\right)=\delta
$$

Подставляем найденное решение (5.34):

$$
\frac{1}{2}\left(\frac{1}{\lambda} \boldsymbol{F}^{-1} g\right)^{T} \boldsymbol{F}\left(\frac{1}{\lambda} \boldsymbol{F}^{-1} g\right)=\frac{1}{2 \lambda^{2}} g^{T} \boldsymbol{F}^{-1} g=\boldsymbol{\delta}
$$

Отсюда находим коэффициент ${ }^{*}$ :

$$
\lambda=\sqrt{\frac{g^{T} F^{-1} g}{2 \delta}}
$$

---

Обратная дробь $\frac{1}{\boldsymbol{s}}$, соответственно, является коэффициентом пропорциональности.

* однозначно в силу положительности коэф. Лагранжа; число $\boldsymbol{g}^{\boldsymbol{T}} \boldsymbol{F}^{\boldsymbol{- 1}} \boldsymbol{g}$ положительно в силу того, что гессиан $\mathrm{KL}$-дивергенции является матрицей Фишера и поэтому является положительно определённой матрицей (см. приложение А.1.2). Естественно, усреднение по состояниям не нарушает этого факта.

Итак, что мы делаем на практике. Во-первых, собираем большой-большой роллаут (порядка 1024 переходов суммарно по средам), чтобы возможно было хоть сколько-то адекватно оценивать гессиан $\boldsymbol{F}$ (5.32). Оцениваем Advantage собранных пар $\boldsymbol{s}, \boldsymbol{a}$ при помощи GAE (5.21). Затем считаем градиент $\boldsymbol{g}$ (5.31) аналогично Actor-Critic методу. Далее мы хотим решить систему линейных уравнений:

$$
\boldsymbol{F}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\mathrm{old}}\right)=\boldsymbol{g}
$$

Хранить в памяти гессиан и тем более обращать для нейросеток мы, конечно, не будем и воспользуемся методом сопряжсённых градиентов (conjugate gradient method), который позволяет решать систему линейных уравнений итеративно, на каждой итерации требуя лишь вычислять $\boldsymbol{F} \boldsymbol{h}$ для некоторых векторов $\boldsymbol{h}$ (прочитать про него можно, например, в википедии). Нам придётся для каждой итерации метода сопряжённых градиентов сделать два обратных прохода по вычислительному графу (дважды вычислять все производные: сначала по функции, для которой мы хотим посчитать гессиан, чтобы получить градиент $\boldsymbol{f}:=\nabla_{\theta} \mathbf{K L}\left(\pi^{\text {old }}\left\|\pi_{\theta}\right)\right\|_{\boldsymbol{\theta} \approx \boldsymbol{\theta}^{\text {ны }}}$, затем для градиента функции $\langle\boldsymbol{f}, \boldsymbol{h}\rangle$ ), но до сходимости сводить алгоритм не будем и остановим после порядка 10 итераций. Процедура получается дороговатой всё равно, но поскольку в деле замешан гессиан, то что поделать - мы практически выходим в методы оптимизации второго порядка.

Посчитали приближение $\boldsymbol{F}^{-\mathbf{1}} \boldsymbol{g}$, дальше возникает проблема: нам нужно домножить вектор на коэффициент $\boldsymbol{k}$, который напрямую зависит от $\boldsymbol{\delta}$. Мы могли бы поставить $\boldsymbol{\delta}$ гиперпараметром как learning rate в обычном градиентном спуске, но всё-таки мы боремся за то, чтобы делать шаги «правильного» размера. Мы уже знаем направление $\boldsymbol{F}^{\boldsymbol{- 1}} \boldsymbol{g}$, в котором будем менять параметры политики, но не знаем, насколько. И вот тут становится важно, что когда мы приблизили локально $\boldsymbol{J}(\boldsymbol{\theta})$ суррогатной функцией, мы для суррогатной функции можем посчитать значение в любой точке.

Когда мы делаем шаг градиентного спуска, мы рискуем сделать слишком длинный шаг, даже если он делается в верном направлении. B Policy Gradient методах это особенно критично: если сделать неудачное обновление политики, и $\boldsymbol{\pi}$ сломается, то на следующем шаге собранные данные будут ужасными, поскольку они собираются on-policy, при помощи стратегии $\boldsymbol{\pi}$ ! Поэтому важно не сломать стратегию ни на одном шаге, и для этого learning rate приходится выбирать маленьким. Естественно, это приводит к тому, что модель будет обучаться очень долго (а у нас тут сэмплы на каждый шаг сжигаются!). При этом, как это обычно бывает при обучении нейронных сетей, запускать какую-нибудь процедуру автоматического подбора learning rate, дороговато - для этого нужно уметь вычислять значение оптимизируемой функции в разных точках. Тем более в случае с RL-ем, где, чтобы оценить $\boldsymbol{J}(\boldsymbol{\theta})$ и проверить, не сломалось ли чего, нужно отправлять на каждом шаге несколько раз какие-то стратегии в среду и играть целые эпизоды, это совершенно не вариант.

Но здесь, в TRPO, мы применяли аппроксимацию дважды: сначала приблизили функционал на суррогатную функцию, а затем суррогатную функцию приблизили линейной моделью. Можно было бы сказать, что мы просто исходный функционал приблизили линейно (получился бы тот же самый градиент $\boldsymbol{g}$, как мы разобрали), но для промежуточной задачи (5.30), в которой мы работаем с суррогатной функцией, значение которой мы можем посчитать в любой точке, мы на самом деле можем проводить бэктрэкинга для адаптивного подсчёта $\boldsymbol{\delta}$ аналога learning rate в trust region подходе. Его можно проводить немного по-разному, рассмотрим конкретный вариант из стандартных реализаций TRPO.

Выставляем какое-нибудь большое значение $\boldsymbol{\delta}$ (это начальное значение - гиперпараметр) и считаем коэффициент пропорциональности $\boldsymbol{k}=\sqrt{\frac{2 \delta}{g^{\boldsymbol{T}} \boldsymbol{F}^{-\mathbf{1}} \boldsymbol{g}}}$ (заметим, что $\boldsymbol{F}^{-\mathbf{1}} \boldsymbol{g}$ приближённо мы уже посчитали в методе сопряжённых градиентов). Дальше, во-первых, проверяется, а правда ли мы остались внутри trust region-a, то есть соблюли ли условие $\mathbf{K L}\left(\pi^{\text {old }}\left\|\pi_{\theta}\right\rangle \leq \boldsymbol{\delta}\right.$. Мы могли нарушить это условие как из-за перехода к приближённой задаче (5.33), так и из-за приближённого её решения через метод сопряжённых градиентов; если таки нарушили, то уменьшаем $\boldsymbol{\delta}$ в, допустим, два раза и перепроверяем. Если же условие соблюдено, то проверяем, что значение $\boldsymbol{L}_{\text {qodd }}(\boldsymbol{\theta})$, оценённое по имеющимся данным, больше нуля (что хотя бы для эмпирических данных удалось увеличить значение суррогатной функции): если нет, то продолжаем уменьшать регион доверия, а если да, то мы можем заканчивать процедуру бэктрэкинга и делать таким образом «максимально длинный» шаг.

После такой процедуры мы уже шагнули на границу нашего региона доверия, максимально отступив от $\pi^{\text {old }}$, использовавшихся для сбора данных. Соответственно, смысла «продолжать» оптимизировать нижнюю оценку дальше, разложив функционал в $\boldsymbol{\theta} \neq \boldsymbol{\theta}^{\text {old }}$, нет; нужно перестраивать нижнюю оценку, то есть собирать новые данные при помощи обновлённой стратегии. Можно сказать, что переиспользовать данные мы не научились, но мы научились делать тяжёлые и дорогие, но хорошие шаги оптимизации.

---



В текущем виде в TRPO критика всё ещё нужно оптимизировать также, как в А2C, то есть собирая маленькие роллауты и делая небольшие шаги. Не очень удобно делать это параллельно со сбором больших роллаутов для актёра, да и оптимизировать актёра и критика в этой схеме, получается, придётся раздельно (иметь для них две раздельные сетки). Поскольку роллауты собирают большие, в имплементациях на критика иногда вообще забивают и используют Монте-Карло оценку как в REINFORCE, просто доигрывая игры до конца (если игры относительно короткие, по 20-100 шагов, то это может быть разумно: всё равно нам длинные роллауты нужны шагов по 100). Конечно, если всё-таки обучать критика, то можно существенно выиграть от использования GAE-оценок.

# 5.3.4. Proximal Policy Loss 

Считается, что TRPO практически всегда работает лучше A2C, с единственным основным недостатком сложностью самого алгоритма. Всё-таки в глубоком обучении привычнее работать с какими-то датасетами, из которых сэмплируются батчи, вычисляется какая-то функция потерь или оптимизируемый функционал, считается градиент и отправляется в условный Adam. Настраивать такой алгоритм тяжело и неприятно.

Proximal Policy Optimization (PPO) - альтернативный способ рассуждения после вывода нижней оценки (5.29). Давайте не будем прибегать к методам оптимизации, требующим какие-либо гессианы, и будем оптимизировать нижнюю оценку напрямую обычным градиентным спуском, где $\boldsymbol{C}$ - гиперпараметр:

$$
\mathbb{E}_{\boldsymbol{s} \sim d_{\boldsymbol{\sigma}} \text { old }(\boldsymbol{s})} \mathbb{E}_{\boldsymbol{a} \sim \pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})}\left[\frac{\pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})}{\pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})} A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})-\boldsymbol{C} \sqrt{\mathbf{K L}\left(\pi^{\text {old }} \| \pi_{\theta}\right)}\right] \rightarrow \max _{\theta}
$$

Сделать так напрямую в лоб не получится; давайте поймём, почему. Какого размера датасет нам понадобится для такого пайплайна? Собрать данных на мини-батч или несколько и «воспользоваться ими несколько раз» не получится: Монте-Карло оценки мат.ожиданий в наших функциях потерь просто будут скоррелированы. Чтобы по данным из $\pi^{\text {old }}$ прооптимизировать $\theta$ несколькими шагами градиентного спуска, нужно, чтобы мини-батчи были достаточно разными. Это значит, что при помощи $\pi^{\text {old }}$ придётся собрать датасет достаточно большого размера.


---

Чтобы что-то выиграть от такого процесса, нужно пройтись по датасету несколькими эпохами (иначе тот же А2С был бы выгоднее за счёт свежести данных). Это значит, что шагов градиентной оптимизации понадобится сделать достаточно много. Следовательно, стратегия потенциально обновляется за время оптимизации на одном датасете относительно сильно: вместе с тем, что небольшое изменение в пространстве параметров может приводить к большому изменению стратегии $\boldsymbol{\pi}$ (в пространстве распределений) без введения жёсткого региона доверия стратегия может начать сколь угодно сильно подстраиваться под те оценки критика, которые мы выдадим па-


рам в датасете.

Действительно, на практике наш критик $A^{\pi^{\text {old }}}$ неидеален и его оценка $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a}) \approx A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})$ будет смещённой. Но важно, что в оценке $\boldsymbol{\Psi}$ будут заложены сэмплы из собранных при помощи $\pi^{\text {old }}$ данных: награды и состояния за будущие шаги. Для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ мы увидим лишь по одному сэмплу будущего, и как бы мы оценку $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$ ни строили, эти сэмплы $\boldsymbol{s}^{\prime}, \boldsymbol{a}^{\prime}, \ldots$ у нас ровно в одном экземпляре. Мы не хотим, ест-но, несколько раз $\pi^{\text {old }}$ в среду отправлять - это бессмысленно, в on-policy режиме всегда выгоднее если и отправлять в среду, то самую свежую стратегию, переходя таким образом к очередному шагу алгоритма. А значит, что при фиксированных данных при оптимизации суррогатной функции мы не жадный policy improvement стратегии $\pi^{\text {old }}$ проведём, а полную ерунду: будем искать $\underset{a}{\operatorname{argmax}} \boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$, то есть перекладывать всю вероятность в те действия, где оценка (!) Advantage случилась положительная, и убирать вероятность оттуда, где оценка Advantage случилась отрицательная. Этот эффект можно охарактеризовать как «переобучение под сэмплы».

Пример 87: Утрированный пример. Текущая стратегия сделала два шага в среде, собрала $\boldsymbol{s}_{1}, \boldsymbol{a}_{1}, \boldsymbol{s}_{2}, \boldsymbol{a}_{2}$. Оценки Advantage получились следующими: $\boldsymbol{\Psi}\left(\boldsymbol{s}_{1}, \boldsymbol{a}_{1}\right)=\mathbf{1}, \boldsymbol{\Psi}\left(\boldsymbol{s}_{2}, \boldsymbol{a}_{2}\right)=-\mathbf{1}$. Теперь если мы по сэмплам оценили суррогатную функцию (5.25) и начали её оптимизировать по параметрам стратегии, то получится:

$$
\frac{\pi_{\theta}\left(a_{1} \mid s_{1}\right)}{\pi^{\mathrm{old}}\left(a_{1} \mid s_{1}\right)}(+1)+\frac{\pi_{\theta}\left(a_{2} \mid s_{2}\right)}{\pi^{\mathrm{old}}\left(a_{2} \mid s_{2}\right)}(-1) \rightarrow \max _{\theta}
$$

Знаменатели дробей можно считать какими-то положительными числами, поэтому в итоге $\pi_{\theta}\left(a_{2} \mid s_{2}\right)$ полетит в ноль, $\pi_{\theta}\left(a_{1} \mid s_{1}\right)$ - или в единицу, если пространство действий дискретно, или вообще в бесконечность, если пространство действий непрерывно.

То есть на тех парах $\boldsymbol{s}, \boldsymbol{a}$, где оценка Advantage положительна, стратегия начнёт улетать к вырожденной, а вероятности на парах $\boldsymbol{s}, \boldsymbol{a}$ с отрицательной оценкой Advantage начнут уплывать в ноль. В итоге начинают регулярно встречаться взрывающиеся или затухающие importance sampling коэффициенты $\frac{\pi_{\theta}(a \mid s)}{\pi^{\text {old }}(a \mid s)}$, мини-бауч становится несбалансированным и в плане «весов» объектов. И регуляризатор в виде KL-дивергенции из нижней оценки, поставленный в виде жёсткого ограничения («trust region»-a) в задачу оптимизации (5.30), защищал нас от этого эффекта в TRPO.

Предлагается полечить костылём: обрезать. Обозначим importance sampling коэффициент как

$$
\rho(\theta):=\frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)}
$$

и обрежем его как

$$
\rho^{\mathrm{clip}}(\theta):=\operatorname{clip}(\rho(\theta), 1-\epsilon, 1+\epsilon)
$$

где $\boldsymbol{\epsilon} \in(\mathbf{0}, \mathbf{1})$ - гиперпараметр (типичный выбор -0.1 или 0.2 ). Рассмотрим альтернативную функцию потерь:

$$
\mathbb{E}_{\theta \sim \mathrm{~d}_{\text {aold }}(s)} \mathbb{E}_{\boldsymbol{a} \sim \pi^{\mathrm{old}}(a \mid s)}\left[\rho^{\mathrm{clip}}(\theta) A^{\pi^{\mathrm{old}}}(\boldsymbol{s}, \boldsymbol{a})-\boldsymbol{C} \sqrt{\mathrm{KL}\left(\pi^{\mathrm{old}} \| \pi_{\theta}\right)}\right] \rightarrow \max _{\theta}
$$

Что случилось с градиентами при таком изменении? Если происходит обрезка, то есть если $\boldsymbol{\rho}(\theta)$ не попадает в указанный диапазон [1- $\boldsymbol{\epsilon}, \mathbf{1}+\boldsymbol{\epsilon}$ ], то градиент основного слагаемого, как легко видеть, зануляется. Иначе он остаётся без изменений:

$$
\nabla_{\theta} \rho^{\mathrm{clip}}(\theta)= \begin{cases}0 & \rho(\theta) \notin[1-\epsilon, 1+\epsilon] \\ \nabla_{\theta} \rho(\theta) & \rho(\theta) \in[1-\epsilon, 1+\epsilon]\end{cases}
$$

Таким образом, подобный клиппинг - «мягкий trust region»: как только стратегия слишком отдаляется от $\pi^{\text {old }}$ на данной паре $\boldsymbol{a}, \boldsymbol{a}$, градиенты «перестают» обновлять эту пару. Это не означает, что стратегия в этой паре не продолжит меняться: с ней потенциально в градиентном спуске может


---

происходить всё что угодно, она может продолжать изменяться за счёт «схожих» пар $\boldsymbol{s}, \boldsymbol{a}$, например, которые ещё остаются «внутри trust region-a» или, в конце концов, из-за моментума в алгоритме стохастической оптимизации ${ }^{6}$.

Замена функции потерь лишила нас в очередной раз «гарантий нижней оценки»: хотелось бы, чтобы при достаточно большой константе $\boldsymbol{C}$ эти гарантии оставались. Поэтому мы не просто заменим функцию потерь на версию с обрезкой, а возьмём минимум между ними: так мы сохраним свойство нижней оценки и гарантируем, что функционал (5.35) не увеличился:

$$
\mathbb{E}_{s \sim d_{\pi \text { old }}(\boldsymbol{s})} \mathbb{E}_{a \sim \pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})}\left[\min \left(\rho(\theta) A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{\rho}^{\text {clip }}(\theta) A^{\pi^{\text {old }}}(\boldsymbol{s}, \boldsymbol{a})\right)-\boldsymbol{C} \sqrt{\mathbf{K L}\left(\pi^{\text {old }} \| \pi_{\theta}\right)}\right] \rightarrow \max _{\theta}
$$

Интуиция нижней оценки здесь, на самом деле, под очень большим вопросом. Авторы алгоритма на этом этапе в ablation studies внезапно обнаружили, что на слагаемое с KL-дивергенцией можно внезапно забить (выставить $\boldsymbol{C}=\mathbf{0}$ ), и эмпирические результаты не изменятся. Обычно в имплементациях РРО KL-дивергенции в функционале по дефолту нет ${ }^{7}$, и итого оптимизируемый функционал выглядит просто вот так:

$$
\mathbb{E}_{s \sim d_{\pi \text { old }}(s)} \mathbb{E}_{a \sim \pi^{\text {old }}(a \mid s)} \min \left(\rho(\theta) A^{\pi^{\text {old }}}(s, a), \rho^{\text {clip }}(\theta) A^{\pi^{\text {old }}}(s, a)\right) \rightarrow \max _{\theta}
$$

На этом месте гробик связи между алгоритмом и теорией нижней оценки засыпается землёй. Но что же произошло? Мы ввели обрезку (имеющую некоторый смысл trust region-a) и взятие минимума между двумя функциями потерь; если обрезка так хорошо «сходелировала» trust region, что регуляризатор (слагаемое с KL-дивергенцией) и не нужен, то какой физический смысл имеет минимум?

Итак, давайте поймём, какую роль в интуиции trust region-a играет взятие минимума, посмотрев на градиенты для одной пары $\boldsymbol{s}, \boldsymbol{a}$. Введение минимума всё ещё «выкидывает» из градиентов функционала пары $\boldsymbol{s}, \boldsymbol{a}$, на которых коэффициент $\boldsymbol{r}(\boldsymbol{\theta})$ не близок к 1 ; или же оставляет градиент без изменений. Запуление градиента происходит в случае, если происходит сразу два события: минимум достигается на «обрезанной» версии градиента, и importance sampling вес вышел за границы. Рассмотрим всевозможные случаи:

$$
\nabla_{\theta} \min \left(\rho(\theta) A^{\pi^{\text {old }}}(s, a), \rho^{\text {clip }}(\theta) A^{\pi^{\text {old }}}(s, a)\right)= \begin{cases}0 & \rho(\theta)>1+\epsilon, \quad A^{\pi^{\text {old }}}(s, a)>0 \\ \nabla_{\theta} \rho(\theta) & \rho(\theta)<1+\epsilon, \quad A^{\pi^{\text {old }}}(s, a)>0 \\ 0 & \rho(\theta)<1-\epsilon, \quad A^{\pi^{\text {old }}}(s, a)<0 \\ \nabla_{\theta} \rho(\theta) & \rho(\theta)>1-\epsilon, \quad A^{\pi^{\text {old }}}(s, a)<0\end{cases}
$$

Внимательно вглядевшись в эту «таблицу», становится понятно, что происходит. Если оценка критика положительна, градиенты говорят увеличивать $\pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})$; importance sampling вес повышается, и в какой-то момент случится обрезка по $\mathbf{1}+\boldsymbol{\epsilon}$. Но если оценка критика положительна, и градиенты указывают увеличивать вероятность, а она по какой-то причине уменьшилась (такое вполне возможно в процессе оптимизации) и даже «вылетела за границы trust region», то градиенты не зануляются: она вылетела «не с той стороны»! Симметричная ситуация случится при отрицательной оценке критика: вероятность должна уменьшаться, но сильно за счёт обрезки уменьшиться она не может, а за счёт оператора минимума при случайном увеличении градиенты продолжат тянуть вероятности «к барьеру». Мы получили этакий «полуоткрытый trust region».

# 5.3.5. Clipped Value Loss 

Применим аналогичную идею «полуоткрытого trust region-a» для лосса критика. Пусть для данного состояния $\boldsymbol{s}$ наша аппроксимация V-функции выдаёт $\boldsymbol{V}_{\phi}(\boldsymbol{s})$, а таргет равен $\boldsymbol{y}$. Обычно мы бы минимизировали MSE:

$$
\operatorname{Loss}_{1}(\phi):=\left(V_{\phi}(s)-y\right)^{2}
$$



Однако если таргет $\boldsymbol{y}$ содержит в себе в том числе какие-то Монте-Карло оценки и переиспользуется «несколько раз» из небольшого датасета, то мы не хотим на него переобучаться. Пусть на момент сбора датасета критик выдавал для данного состояния $\boldsymbol{V}^{\text {old }}(\boldsymbol{s})$; таргет указывает лишь направление, в котором мы хотим изменить значение выхода критика, но, как и в любой стохастической аппроксимации, мы не хотим «заменять жёстко» выход $\boldsymbol{V}_{\phi}(\boldsymbol{s})$ на $\boldsymbol{y}$, а лишь сдвинуть в его сторону. Для этого добавим и вычтем в MSE $\boldsymbol{V}^{\text {old }}(\boldsymbol{s})$ :

$$
\operatorname{Loss}_{1}(\phi)=\left(V_{\phi}(s)-V^{\text {old }}(s)-\left(y-V^{\text {old }}(s)\right)\right)^{2}
$$

[^0]
[^0]:    ${ }^{6}$ в RL, как и в глубоком обучении, обычный стохастический градиентный спуск не используется, а вместо этого используется, например, Adam.
    ${ }^{7}$ если же его всё-таки оставляют, то обычно без квадратного корня.

---

Теперь мы сравниваем не выход сети с таргетом, а изменение значения выхода критика с «желаемым изменением» (на самом деле, просто оценкой Advantage; $\boldsymbol{y}-\boldsymbol{V}^{\text {old }}(\boldsymbol{s})$ используется в качестве аппроксимации $\boldsymbol{\Psi}(\boldsymbol{s}, \boldsymbol{a})$ ). Введём другую функцию потерь, с «обрезкой», которая построит «мягкий trust-region» и будет занулять градиенты, как только $\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$ станет непохожим на $\boldsymbol{V}^{\text {old }}(\boldsymbol{s})$ :

$$
\operatorname{Loss}_{2}(\phi) \equiv\left(\operatorname{clip}\left(V_{\phi}(s)-V^{\text {old }}(s), \epsilon,-\epsilon\right)-\left(y-V^{\text {old }}(s)\right)\right)^{2}
$$

где $\boldsymbol{\epsilon}$ - гиперпараметр. Аналогично лоссу актёра, градиенты в такой функции потерь просто будут зануляться в ситуациях, когда происходит обрезка.

Наконец, чтобы «открыть» trust-region с одной стороны, нужно просто, по аналогии с лоссом актёра, взять максимум от этих двух функций потерь (для актёра мы брали минимум, поскольку там велась максимизация, а здесь лосс минимизируется):


$$
\operatorname{Loss}(\phi) \equiv \max \left(\operatorname{Loss}_{1}(\phi), \operatorname{Loss}_{2}(\phi)\right)
$$

Непосредственной проверкой легко убедиться, что градиенты будут зануляться, только если наш критик вышел из trust-region-a «с правильной стороны»; а так, градиенты будут тянуть нашего критика к допустимому барьеру.

Считается, что обрезка функционала для критика менее существенна, чем для актёра. Если мы не обрежем функционал для актёра, и тот «сломается», переобучившись под сэмплы, это может сломать весь оптимизационный процесс, поскольку стратегия используется для сбора данных. Но если немного сломался критик, то это не так сильно отразится на процессе обучения, потому что актёр не опирается целиком на оценки критика. Тем не менее, раз такая возможность «защищиться» от переобучения под сэмплы в критике есть, то стоит ей пользоваться. Недостаток - в необходимости подобрать соответствующее масштабу V -функции значение гиперпараметра обрезки $\boldsymbol{\epsilon}$.

# 5.3.6. Proximal Policy Optimization (PPO) 

Итоговая процедура работы алгоритма следующая. Собираем большой роллаут (порядка хотя бы 1000 maгов). Версия стратегии, использовавшаяся для сбора, обозначается как $\pi^{\text {old }}$, и вероятности, с которыми она выбирала действия, сохраняются. Для всех пар $\boldsymbol{s}, \boldsymbol{a}$ высчитываются оценки Q-функции и Advantage методом GAE (5.21). K собранным данным относимся как к датасету, из которого можно брать пары $s \sim d_{\text {apold }}(s), a \sim$ $\sim \pi^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})$ и считать Монте-Карло оценку градиента (5.36). Размер батча при сэмплировании из датасета при этом обычный для градиентных методов первого порядка, условно такой же, как был бы в А2С. По датасету нужно пройтись при этом несколько раз, теоретически - хочется как можно больше, но понятно, что чем больше расходится $\pi_{\theta}$ и $\pi^{\text {old }}$, тем менее эффективна «нижняя оценка» и тем больше данных будет резать наш клиппинг. Соответственно, количество эпох - сколько раз пройтись по датасету - является ключевым гиперпараметром. Существенно, сколько именно шагов градиентного спуска будет сделано по одному датасету; это более важно, чем размер мини-батчей, поскольку именно первое больше влияет на то, когда мы начнём «вываливаться» из trust region-a, то есть когда отступим от стратегии сбора данных достаточно далеко.

## Алгоритм 21: Proximal Policy Optimization (PPO)

Гиперпараметры: $\boldsymbol{M}$ - количество параллельных сред, $\boldsymbol{N}$ - длина роллаутов, $\boldsymbol{B}$ - размер минибатчей, n_epochs - количество эпох, $\boldsymbol{\lambda}$ - параметр GAE-оценки, $\boldsymbol{\epsilon}$ - параметр обрезки для актёра, $\hat{\boldsymbol{\epsilon}}$ - параметр обрезки для критика, $\boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$ - нейросеть с параметрами $\boldsymbol{\phi}, \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ - нейросеть для стратегии с параметрами $\boldsymbol{\theta}, \boldsymbol{\alpha}$ - коэф. масштабирования лосса критика, SGD оптимизатор.

## Инициализировать $\boldsymbol{\theta}, \boldsymbol{\phi}$

## На каждом шаге:

1. в каждой параллельной среде собрать роллаут длины $\boldsymbol{N}$, используя стратегию $\boldsymbol{\pi}_{\boldsymbol{\theta}}$, сохраняя вероятности выбора действий как $\boldsymbol{\pi}^{\text {old }}(\boldsymbol{a} \mid \boldsymbol{s})$, а выход критика на встреченных состояниях как $\boldsymbol{V}^{\text {old }}(\boldsymbol{s}) \leftarrow \boldsymbol{V}_{\boldsymbol{\phi}}(\boldsymbol{s})$
2. для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ из роллаутов посчитать одношаговую оценку Advantage:

$$
\boldsymbol{\Psi}_{(1)}(s, a) \equiv r+\gamma\left(1-\text { done }^{\prime}\right) V_{\phi}\left(s^{\prime}\right)-V_{\phi}(s)
$$

3. посчитать GAE-оценку:

$$
\boldsymbol{\Psi}_{\mathrm{GAE}}\left(s_{N-1}, a_{N-1}\right) \equiv \boldsymbol{\Psi}_{(1)}\left(s_{N-1}, a_{N-1}\right)
$$

---

4. для $t$ от $N-2$ до 0 :

- $\Psi_{\mathrm{GAE}}\left(s_{t}, a_{t}\right):=\Psi_{(1)}\left(s_{t}, a_{t}\right)+\gamma \lambda\left(1-\operatorname{done}_{t}\right) \Psi_{\mathrm{GAE}}\left(s_{t+1}, a_{t+1}\right)$

5. посчитать таргет для критика:

$$
y(s):=\Psi_{\mathrm{GAE}}(s, a)+V_{\phi}(s)
$$

6. составить датасет из шестёрок $(s, a, \Psi_{\mathrm{GAE}}(s, a), y(s), \pi^{\text {old }}(a \mid s), V^{\text {old }}(s))$
7. выполнить n_epochs проходов по роллауту, генерируя мини-батчи пятёрок $\mathbb{T} \equiv$ $(s, a, \Psi_{\mathrm{GAE}}(s, a), y(s), \pi^{\text {old }}(a \mid s), V^{\text {old }}(s))$ размером $B$; для каждого мини-батча:

- вычислить лосс критика:

$$
\begin{gathered}
\operatorname{Loss}_{1}(\mathbb{T}, \phi):=\left(y(s)-V_{\phi}(s)\right)^{2} \\
\operatorname{Loss}_{2}(\mathbb{T}, \phi):=\left(y(s)-V^{\text {old }}(s)-\operatorname{clip}\left(V_{\phi}(s)-V^{\text {old }}(s),-\hat{\epsilon}, \hat{\epsilon}\right)\right)^{2} \\
\operatorname{Loss}^{\text {critic }}(\phi):=\frac{1}{B} \sum_{\mathbb{T}} \max \left(\operatorname{Loss}_{1}(\mathbb{T}, \phi), \operatorname{Loss}_{2}(\mathbb{T}, \phi)\right)
\end{gathered}
$$

- сделать шаг градиентного спуска по $\phi$, используя $\nabla_{\phi} \operatorname{Loss}^{\text {critic }}(\phi)$
- нормализовать $\Psi_{\mathrm{GAE}}(s, a)$ по батчу, чтобы в среднем значения равнялись 0 , а дисперсия -1 .
- посчитать коэффициенты в importance sampling:

$$
r_{\theta}(\mathbb{T}):=\frac{\pi_{\theta}(a \mid s)}{\pi^{\mathrm{old}}(a \mid s)}
$$

- посчитать обрезанную версию градиентов:

$$
r_{\theta}^{\mathrm{clip}}(\mathbb{T}):=\operatorname{clip}\left(r_{\theta}(\mathbb{T}), 1-\epsilon, 1+\epsilon\right)
$$

- вычислить градиент для актёра:

$$
\nabla_{\theta}^{\text {actor }}:=\frac{1}{B} \sum_{\mathbb{T}} \nabla_{\theta} \min \left(r_{\theta}(\mathbb{T}) \Psi_{\mathrm{GAE}}(s, a), r_{\theta}^{\mathrm{clip}}(\mathbb{T}) \Psi_{\mathrm{GAE}}(s, a)\right)
$$

- сделать шаг градиентного подъёма по $\boldsymbol{\theta}$, используя $\nabla_{\theta}^{\text {actor }}$

В Atari играх хорошим значением гиперпараметра «число эпох» считается 3 , а для задач непрерывного управления - 10. Возможность несколько раз «увидеть» переход (пусть даже всего 3) вместо одного даёт существенный прирост эффективности алгоритма PPO по сравнению с A2C, поэтому на практике осмысленно использовать именно его, если нужен on-policy алгоритм.

РРО использовался для великих достижений в Dota и считается более-менее устоявшейся SOTA в RL: если нет проблем с симулятором и можно использовать on-policy алгоритмы - стоит начать с PPO, но причины, по которым он так круто работает, полностью не ясны. В частности, последовавшие исследования указывают на то, что клиппинг - не основная причина успеха, а кроется она в целом наборе удачных инженерных хаков в официальной реализации алгоритма от OpenAI. Ключевая из них приведена в описании алгоритма - нормализация $\Psi_{\mathrm{GAE}}(s, a)$ по батчу; мы знаем, что в среднем Advantage должен быть равен нулю, и поэтому можем его центрировать. Деление же на стандартное отклонение Advantage-оценок позволяет «отнормировать» масштаб функции награды внутри самого алгоритма.

---

# ГЛАВА 6 

## Continuous control

В этой главе будут рассмотрены алгоритмы для задачи непрерывного управления $(|\mathcal{A}| \subseteq[-1,1]^{\boldsymbol{A}})$, развивающие идеи Value-based и Policy gradient подходов. Мы увидим, что эти два подхода имеют много общего и отчасти даже эквивалентны, в обоих случаях получив схемы, моделирующие алгоритм Policy Iteration.

## §6.1. DDPG

### 6.1.1. Вывод из Deep Q-learning

Схема DQN 15 имела принципиальный недостаток: мы не могли работать с непрерывными пространствами действий в силу необходимости постоянно считать операторы максимума и аргмаксимума

$$
\max _{\boldsymbol{a}} Q_{\theta}(s, a)
$$

как для жадного выбора действия, так и для построения таргета в задаче регрессии.
Единственная возможная архитектура модели - приём действий на вход вместе с состояниями, тогда поиск аргмаксимума проводить, в целом, можно, но дорого: инициализируем $\boldsymbol{a}^{\mathbf{0}}$ случайно, и устраиваем градиентный подъём по входу в модель:

$$
a^{k+1}=a^{k}+\alpha \nabla_{\alpha} Q_{\theta}(s, a)\left.\right|_{\boldsymbol{a}=\boldsymbol{a}^{k}}
$$



Понятно, что такую процедуру устраивать по несколько раз за шаг дороговато. Однако, в глубинном обучении для таких проблем есть мега-универсальное решение: давайте задачу поиска аргмаксимума тоже аппроксимируем другой нейросетью! Максимум тогда можно будет считать, просто подставив в $Q_{\theta}$ вместо действия приближение этого аргмаксимума.

Итак, пусть $\boldsymbol{\pi}_{\boldsymbol{\omega}}(\boldsymbol{s})$ принимает на вход состояние $\boldsymbol{s}$ и выдаёт аргмаксимум текущей аппроксимации Qфункции, то есть будем добиваться $\boldsymbol{\pi}_{\boldsymbol{\omega}}(\boldsymbol{s}) \approx \underset{a}{\operatorname{argmax}} Q_{\theta}(s, a)$. Понятно, как обучать такую сеть:

$$
Q_{\theta}\left(s, \pi_{\omega}(s)\right) \rightarrow \max _{\omega}
$$

Весь алгоритм DQN оставляем неизменным с единственной модификацией, что на каждом батче также нужно сделать шаг оптимизации $\boldsymbol{\omega}$. При этом каждый раз, когда в схеме необходимо считать максимум или аргмаксимум $Q_{\theta}$, используется $\boldsymbol{\pi}_{\boldsymbol{\omega}}(\boldsymbol{s})$.

В стандартном алгоритме DQN нам было необходимо считать $\max _{\boldsymbol{a}^{\prime}} Q_{\boldsymbol{\theta}^{-}}\left(s^{\prime}, \boldsymbol{a}^{\prime}\right)$, и в дефолтной версии алгоритма использовалась таргет-сеть. Технически это означает, что для таргет-сети $Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$ нам тоже нужно знать аргмаксимум, поэтому можно хранить старую версию вспомогательной функции $\pi_{\omega^{-}}(s)$.

Считается, что это не так принципиально, поскольку использование «свежей» $\boldsymbol{\pi}_{\boldsymbol{\omega}}(\boldsymbol{s})$ при подсчёте таргета соответствует моделированию Double оценки (см. раздел 4.2.3). Все остальные модификации DQN также применимы.

Итого мы получили, что для жадного выбора действия используется $\boldsymbol{\pi}_{\boldsymbol{\omega}}(\boldsymbol{s})$ (отсюда такое обозначение этой «вспомогательной» функции - это фактически стратегия); а таргет для перехода $\mathbb{T}:=\left(s, a, r, s^{\prime}\right)$ вычисляется по формуле

$$
y(\mathbb{T}):=r+\gamma Q_{\theta^{-}}\left(s^{\prime}, \underset{a^{\prime}}{\operatorname{argmax}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)\right) \approx r+\gamma Q_{\theta^{-}}\left(s^{\prime}, \pi_{\omega^{-}}(s)\right)
$$

Такой алгоритм называется Deep Deterministic Policy Gradient (DDPG), и название может сбить с толку: а причём здесь policy gradient?

---

# 6.1.2. Вывод из Policy Gradient 

B Policy Gradient алгоритмах мы получили формулу градиента нашего функционала, «релаксировав» задачу и перейдя к оптимизации в пространстве стохастических стратегий. Если пространство действий непрерывно, то такая релаксация на самом деле не обязательна. Предположим ${ }^{1}$ дифференцируемость любых Q-функций $\boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ по действиям $\boldsymbol{a}$. Попробуем посчитать градиент по параметрам стратегии в случае детерминированной стратегии $\pi_{\theta}(s)$ :

Teорема 63 - Deterministic Policy Gradient: В непрерывных пространствах действий в предположении дифференцируемости Q-функций по действиям:

$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\frac{1}{1-\gamma} \mathbb{E}_{d_{\pi_{\theta}}(s)} \nabla_{\theta} \pi_{\theta}(s) \nabla_{a} Q^{\pi}(s, a) \mid a=\pi_{\theta}(s)
$$

Доказательство.

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\{\mathrm{VQ} \text { уравнение }(3.6)\}=\nabla_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(s)} Q^{\pi_{\theta}}(s, a)=\nabla_{\theta} Q^{\pi_{\theta}}\left(s, \pi_{\theta}(s)\right)=(\star)
$$

Заметим, что в последнем выражении при малом изменении $\boldsymbol{\theta}$ поменяется не только $\boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{s})$, но и сама оценочная функция $\boldsymbol{Q}^{\pi_{\theta}}$. Считая, что якобиан функции $\mathbb{R}^{\boldsymbol{n}} \rightarrow \mathbb{R}^{\boldsymbol{m}}$ имеет размерность $\boldsymbol{n} \times \boldsymbol{m}$, и обозначая размерность действий как $\boldsymbol{A}$, а размерность параметров $\boldsymbol{\theta}$ буквой $\boldsymbol{d}$, получаем следующие размерности градиентов:

$$
\nabla_{\theta} Q^{\pi_{\theta}}(s, a) \in \mathbb{R}^{d \times 1} \quad \nabla_{a} Q^{\pi_{\theta}}(s, a) \in \mathbb{R}^{A \times 1} \quad \nabla_{\theta} \pi_{\theta}(s) \in \mathbb{R}^{d \times A}
$$

Тогда продолжение вычисления выглядит так:

$$
(\star)=\left.\nabla_{\theta} \pi_{\theta}(s) \nabla_{a} Q^{\pi}(s, a)\right|_{a=\pi_{\theta}(s)}+\left.\nabla_{\theta} Q^{\pi_{\theta}}(s, a)\right|_{a=\pi_{\theta}(s)}
$$

где последнее слагаемое - якобиан $\boldsymbol{Q}^{\pi_{\theta}}$ при фиксированном $\boldsymbol{a}$ по параметрам стратегии $\boldsymbol{\pi}_{\boldsymbol{\theta}}$, которую он оценивает. Отдельно это слагаемое имеет вид:

$$
\nabla_{\theta} Q^{\pi_{\theta}}(s, a)=\{\mathrm{QV} \text { уравнение }(3.5)\}=\gamma \mathbb{E}_{s^{\prime}} \nabla_{\theta} V^{\pi_{\theta}}\left(s^{\prime}\right)
$$

Получаем рекурсивную формулу, аккуратно собирая которую, получим:

$$
\nabla_{\theta} V^{\pi_{\theta}}(s)=\mathbb{E}_{T \sim \pi \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \pi_{\theta}\left(s_{t}\right) \nabla_{a} Q^{\pi}\left(s_{t}, a\right) \mid a=\pi_{\theta}\left(s_{t}\right)
$$

Осталось только применить теорему 57 об эквивалентной форме мат.ожидания по траекториям для

$$
f(s, a)=\left.\nabla_{\theta} \pi_{\theta}(s) \nabla_{a} Q^{\pi}(s, a)\right|_{a=\pi_{\theta}(s)}
$$

Сразу построим суррогатную функцию для такой формулы градиента:

$$
\mathcal{L}_{\hat{\pi}}(\theta):=\frac{1}{1-\gamma} \mathbb{E}_{d_{\hat{\pi}}(s)} Q^{\hat{\pi}}\left(s, \pi_{\theta}(s)\right)
$$

Действительно, если мы посчитаем градиент этой функции по $\boldsymbol{\theta}$, то мы просто получим формулу chain rule для оптимизации параметров стратегии через градиент Q-функции по действиям. Иными словами, градиент по параметрам детерминированной стратегии указывает просто проводить policy improvement: выбирать те действия, для которых Q-функция больше, используя её градиент по действиям.

Если мы хотим построить Actor-Critic схему, воспользовавшись такой формулой, нам придётся аппроксимировать Q-функцию $\boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ и явно использовать её градиент по действиям, надеясь на то, что $\nabla_{a} \boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a}) \approx \nabla_{a} \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. Таким образом, обойтись обучением лишь V-функции или пользоваться многошаговыми оценками не получится, а качество обучения стратегии будет упираться в качество обучения критика, поэтому всеми преимуществами on-policy подхода в такой формуле воспользоваться не удастся.

Но можем ли мы воспользоваться формулой в off-policy режиме? Вообще говоря, нет, поскольку состояния должны приходить согласно формуле из распределения $\boldsymbol{d}_{\pi_{\theta}}(\boldsymbol{s})$. Однако, как мы обсуждали в разделе 5.1.7, мы понимаем, что если мы будем оптимизировать Q-функцию по действиям, то как бы мы это не делали (из какого бы распределения не брали состояния, в которых мы изменяем стратегию), мы всё равно сможем увеличить значение нашего функционала, поскольку мы проводим policy improvement. Это означает, что если мы в формуле (6.1) заменим $d_{\pi_{\theta}}(s)$ на что-либо другое, полученная формула «градиента» будет всё равно направлением улучшения стратегии, пусть и не направлением локально максимального увеличения функционала, что верно для

[^0]
[^0]:    ${ }^{1}$ даже если это не так, в будущем мы всё равно будем приближать эти Q-функции нейросетями, для которых всегда справедлива дифференцируемость по кюду.

---

честного градиента. Итого, будем сэмплировать батч состояний из реплей буфера и делать шаг градиентного подъёма:

$$
\theta \leftarrow \theta+\alpha \mathbb{E}_{s} \nabla_{\theta} \pi_{\theta}(s)\left.\nabla_{a} Q^{\pi}(s, a)\right|_{\alpha=\pi_{\theta}(s)}
$$

где состояния $\boldsymbol{s}$ приходят из произвольного распределения (например, из реплей буфера). Это эквивалентно одному шагу градиентной оптимизации суррогатной функции:

$$
\mathbb{E}_{s} Q^{\pi}\left(s, \pi_{\theta}(s)\right) \rightarrow \max _{\theta}
$$

Q-функцию $\boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a}) \approx \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, необходимую для такой оптимизации, будем тоже учить в off-policy режиме с одношаговых оценок: ему для данной пары $\boldsymbol{s}, \boldsymbol{a}$ требуется лишь сэмпл $\boldsymbol{s}^{\prime}$, поэтому такого критика можно обучать по переходам $\mathbb{T}:=\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right)$ из буфера на таргеты

$$
y(\mathbb{T}):=r+\gamma Q_{\omega^{-}}\left(s^{\prime}, \pi\left(s^{\prime}\right)\right)
$$

Одношаговые таргеты имеют сильное смещение (сильно опираются на выход нашей же нейросети), и поэтому для стабилизации процесса требуется использование таргет-сетей. Тут-то и можно, заметить, что...

# 6.1.3. Связь между схемами 

Теорема 64: Предыдущие две схемы (вывод через DQN и через Policy Gradient) эквивалентны полностью.
Доказательство. Методом пристального взгляда.
Итак, DQN для непрерывных действий и Policy Gradient для детерминированных стратегий - это одно и то же. Поймём, как так случилось.

Мы двумя путями ${ }^{2}$ пришли к Policy Iteration схеме 8. Действительно: мы параллельно ведём два оптимизационных процесса: оцениваем $\boldsymbol{Q} \approx \boldsymbol{Q}^{\boldsymbol{\pi}}$ для текущей стратегии $\boldsymbol{\pi}$ и учим $\boldsymbol{\pi}(\boldsymbol{s}) \leftarrow \underset{a}{\operatorname{argmax}} \boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a})$, то есть проводим Policy Evaluation и Policy Improvement. При этом на этапе Policy Improvement мы делаем андейт стратегии сразу для всех состояний, и никаких требований на «распределение» состояний мы можем не накладывать в силу теоремы 17 о Policy Improvement.

Таким образом, обоснование, почему в выводе через Policy Gradient мы можем забить на $\boldsymbol{d}_{\boldsymbol{\pi}}(\boldsymbol{s})$ и брать состояния из буфера, можно сформулировать так: мы отказываемся от Policy Gradient подхода, в котором мы оптимизируем функционал (1.5) напрямую, и переходим к Policy Iteration схеме 8.

А ещё подметим, что схема шибко похожа на GAN: критик в этом алгоритме «учит» функцию потерь для стратегии. Так что вполне естественно, что мы принципиально используем непрерывность пространства действий. Эта аналогия также объясняет, почему схема DDPG нестабильна; как только что-то ломается в одной из двух оптимизируемых функций (критике или актёре), другому тут же становится плохо. Поэтому схема жутко чувствительна к гиперпараметрам; пожалуй, это один из самых нестабильных алгоритмов RL.

### 6.1.4. Ornstein-Uhlenbeck Noise

В рассмотренной схеме из-за использования детерминированной стратегии, как и в DQN, возникает проблема exploration-exploitation-a. В непрерывных пространствах действий вместо $\boldsymbol{\varepsilon}$-жадной стратегии возможно добавлять к выбранным стратегией действиям шум из нормального распределения:

$$
a_{t}:=\pi\left(s_{t}\right)+\varepsilon_{t}, \quad \varepsilon_{t} \sim \mathcal{N}\left(0, \sigma^{2} I\right)
$$

Гиперпараметр $\boldsymbol{\sigma}$, контролирующий магнитуду впрыскиваемого шума, нужно подбирать, его также можно, например, постепенно затухать к нулю с ходом обучения. Однако, такое впрыскивание шума предполагает, что исследование в соседние моменты времени независимо.

Пример 88: Если действия робота - это направление движения (например, поворот руля управляемой машины), а один шаг в среде это доля секунды, странно проводить исследования, рандомно «подёргиваясь» пару раз в секунду. Хочется целенаправленно смещать траекторию: если мы решили в целях исследования повернуть руль чуть правее, чем говорит наша детерминированная стратегия, следует сохранить это смещение руля вправо и в дальнейшем. Для моделирования этого шум должен быть скоррелированным: поэтому вместо независимого шума имеет смысл добавлять случайный процесс, колеблящийся вокруг нуля.

[^0]
[^0]:    ${ }^{2}$ что в очередной раз означает, что между Value-based подходом и Policy Gradient подходом есть тесная связь.

---

Определение 79: Шум Орнитейна - Уленбека (Ornstein-Uhlenbeck noise), в начале эпизода инициализированный нулём, задаётся рекурсивно как:

$$
\varepsilon_{t+1}:=\alpha \varepsilon_{t}+\mathcal{N}\left(0, \sigma^{2} I\right)
$$

где $\boldsymbol{\alpha} \leq \mathbf{1}$ и $\boldsymbol{\sigma}-$ гиперпараметры.
По сути, это просто кумулятивный шум, который с коэффициентом $\boldsymbol{\alpha}$ прибивается к нулю. Если $\boldsymbol{\alpha}=\mathbf{0}$, получаем обычный независимый шум из нормального распределения. Одно из преимуществ такого эксплорейшна - считается, что его параметры можно со временем не менять, то есть даже при околооптимальном поведении такой шум будет исследовать разумные альтернативы вместо рандомных подёргиваний.

Пример 89: На рисунке приведён пример поведения процесса Орнштейна-Уленбека для $\boldsymbol{\alpha}=\mathbf{0 . 9}, \boldsymbol{\sigma}=\mathbf{1}$.


# 6.1.5. Deep Deterministic Policy Gradient (DDPG) 

Мы собрали алгоритм DDPG - off-policy алгоритм для непрерывных пространств действий. Несмотря на название, по свойствам этот алгоритм похож именно на DQN, и обладает аналогичными недостатками и преимуществами.

## Алгоритм 22c Deep Deterministic Policy Gradient (DDPG)

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{\beta}$ - коэф. экспоненциального сглаживания для таргетсеток, $\boldsymbol{\alpha}, \boldsymbol{\sigma}$ - параметры шума, $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$ - нейросетка с параметрами $\boldsymbol{\theta}, \boldsymbol{\pi}_{\boldsymbol{\omega}}(\boldsymbol{s})$ - детерминированная стратегия с параметрами $\boldsymbol{\omega}$, SGD-оптимизаторы.

Инициализировать $\boldsymbol{\theta}, \boldsymbol{\omega}$ произвольно
Положить $\boldsymbol{\theta}^{-}:=\boldsymbol{\theta}$
Положить $\boldsymbol{\omega}^{-}:=\boldsymbol{\omega}$
Инициализировать шум $\varepsilon:=\mathbf{0}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. обновить шум $\varepsilon \leftarrow \alpha \varepsilon+\hat{\varepsilon}$, где $\hat{\varepsilon} \sim \mathcal{N}\left(0, \sigma^{2} I\right)$
2. выбрать $a_{t}:=\pi_{\omega}\left(s_{t}\right)+\varepsilon$
3. пронаблюдать $\boldsymbol{r}_{t}, s_{t+1}$, done $_{t+1}$
4. добавить пятёрку $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right.$, done $\left._{t+1}\right)$ в реплей буфер
5. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера
6. сделать один шаг градиентного подъёма по $\boldsymbol{\omega}$ :

$$
\frac{1}{B} \sum_{s \in B} Q_{\theta}\left(s, \pi_{\omega}(s)\right) \rightarrow \max _{\omega}
$$

7. для каждого перехода $\mathbb{T}:=\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right.$, done $)$ посчитать таргет:

$$
y(\mathbb{T}):=r+\gamma(1-\text { done }) Q_{\theta^{-}}\left(s^{\prime}, \pi_{\omega^{-}}\left(s^{\prime}\right)\right)
$$

---

8. сделать один шаг градиентного спуска по $\boldsymbol{\theta}$ :

$$
\frac{1}{B} \sum_{\mathbb{T}}\left(Q_{\theta}(s, a)-y(\mathbb{T})\right)^{2} \rightarrow \min _{\theta}
$$

9. если $\boldsymbol{t} \bmod K=0$ :

$$
\begin{aligned}
\theta^{-} & \leftarrow(1-\beta) \theta^{-}+\beta \theta \\
\omega^{-} & \leftarrow(1-\beta) \omega^{-}+\beta \omega
\end{aligned}
$$

DDPG за счёт off-policy режима может оказаться эффективнее PPO в задаче локомоции 25. В этой задаче нет тех проблем, из-за которых off-policy обучение может сломаться или плохо работать: там нет сильно отложенного сигнала (плохое действие - и существо сразу падает, хорошее действие - и существо продвинется вперёд и получит положительное подкрепление), Q-функция как функция от действий не похожа на плато (интуитивно, в большинстве состояний есть «хорошие» действия с высоким значением $Q(s, a)$, которые позволяют существу продолжать бежать, и «плохие» с низким значением, которые нарушают баланс устойчивости существа и приводят к его падению), а функция награды очень плотная и информативная, из-за чего полезно иметь возможность все переходы много раз из буфера вспоминать и обучаться восстанавливать хотя бы $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$, содержащуюся в одношаговых таргетах, из разных пар $\boldsymbol{s}, \boldsymbol{a}$.

# 6.1.6. Twin Delayed DDPG (TD3) 

TD3 - набор из трёх эвристических костылей, которые можно навесить над DDPG для существенного повышения стабильности происходящих процессов.

Одна из главных проблем DDPG - унаследованная от DQN проблема переоценивания (см. раздел 4.2.1). Хотя сейчас в формулах в явном виде не присутствует оператор максимума, на самом деле он всё равно есть: наш актёр учится «взламывать» критика и находить те действия, где погрешность нашей аппроксимации истинной Q-функции положительна. Поэтому оценка $Q_{\theta}\left(s, \pi_{\omega}(s)\right)$ практически всегда завышенно оценивает $\max \boldsymbol{Q}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, и если это завышение попадает в целевую переменную для обучения модели Q-функции, начинается цепная реакция.

Более того, в пространстве действий могут обнаружиться узкие области, в которых наша неидеальная нейросетевая аппроксимация Qфункции имеет всплеск; актёр может научиться «взламывать» критика, используя эти всплески. Поэтому первая идея борьбы с этим эффектом заключается в том, чтобы при построении таргета зашумить выход нашей стратегии при помощи некоторого шума. Шум здесь не должен быть очень большим по модулю (важно, что этот шум не имеет смысл «исследований»): обычно сэмплируют зашумление из гауссианы и обрезают, чтобы получить что-то не очень большое по значению:


$$
\varepsilon^{\prime} \sim \operatorname{clip}(\mathcal{N}(0, \hat{\sigma} I),-c, c)
$$

Во-вторых, воспользуемся Twin-трюком, который мы обсуждали в разделе 4.2.2, а то есть будем обучать две Q-функции по общему буферу и использовать в таргетах минимум из двух оценок критиков. Таргет-сеть обычно заводят для каждой из двух копий; а вот стратегию предлагается оставить для них общую, то есть использовать одно и то же «приближение аргмакса» для обоих Q-функций, поскольку в такой схеме она моделируется отдельной нейросетью. Итого формула таргета получается общая для двух критиков:

$$
y(\mathbb{T}):=r+\gamma \min _{i \in\{1,2\}} Q_{\theta_{i}^{-}}\left(s^{\prime}, \pi_{\omega^{-}}\left(s^{\prime}\right)+\varepsilon^{\prime}\right)
$$

При обучении актёра можно как оставить оптимизацию при помощи первого из двух критиков

$$
Q_{\theta_{1}}\left(s, \pi_{\omega}(s)\right) \rightarrow \max _{\omega}
$$

и тогда актёр не будет видеть одну из Q-функций (и там, где у первого критика будет взломанное актёром завышение, у второго критика, мы надеемся, завышение или занижение будет условно равновероятно), так и использовать снова минимум из двух критиков:

$$
\min _{i \in\{1,2\}} Q_{\theta_{i}}\left(s, \pi_{\omega}(s)\right) \rightarrow \max _{\omega}
$$

Наконец, в-третьих, обновление весов стратегии будем делать реже, чем обновление весов Q-функции: это позволит «поучить» Q-функцию приближать $\boldsymbol{Q}^{\boldsymbol{\pi}}$ именно для текущей, свежей версии $\boldsymbol{\pi}$, прежде чем использовать её градиент для оптимизации стратегии; здесь наблюдается полная аналогия с GAN-ами, где тоже иногда помогают подобные фокусы.

---

Третья эвристика кажется наименее существенной, поскольку авторы предлагали делать $\boldsymbol{N}=\mathbf{2}$ шагов обучения критика на один шаг обучения актёра.

# Алгоритм 23: Twin Delayed DDPG (TD3) 

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{N}$ - периодичность обновления весов стратегии, $\boldsymbol{\alpha}, \boldsymbol{\sigma}$ параметры шума, $\hat{\sigma}, \boldsymbol{c}$ - параметры шума для добавки к действиям для таргета, $\boldsymbol{\beta}$ - коэф. экспоненциального сглаживания для таргет-сеток, $\boldsymbol{Q}_{\theta_{1}}(s, a), \boldsymbol{Q}_{\theta_{2}}(s, a)$ - нейросетки с параметрами $\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}, \boldsymbol{\pi}_{\boldsymbol{\omega}}(s)$ - детерминированная стратегия с параметрами $\boldsymbol{\omega}$, SGD-оптимизаторы.

Инициализировать $\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}, \boldsymbol{\omega}$ произвольно
Инициализировать таргет-сетки $\boldsymbol{\theta}_{1}^{-}:=\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2}^{-}:=\boldsymbol{\theta}_{2}, \boldsymbol{\omega}^{-}:=\boldsymbol{\omega}$
Инициализировать шум $\varepsilon_{0}:=\mathbf{0}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. посчитать шум $\varepsilon_{t}:=\alpha \varepsilon_{t-1}+\varepsilon$, где $\varepsilon \sim \mathcal{N}\left(0, \sigma^{2} I\right)$
2. выбрать $a_{t}:=\pi_{\omega}\left(s_{t}\right)+\varepsilon_{t}$
3. пронаблюдать $\boldsymbol{r}_{t}, s_{t+1}$, done $_{t+1}$
4. добавить пятёрку $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right.$, done $\left._{t+1}\right)$ в реплей буфер
5. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера
6. для каждого перехода $\mathbb{T}:=\left(s, a, r, s^{\prime}\right.$, done $)$ посчитать таргет:

$$
\begin{gathered}
\varepsilon^{\prime} \sim \operatorname{clip}\left(\mathcal{N}(0, \hat{\sigma} I),-c, c\right) \\
y(\mathbb{T}):=r+\gamma(1-\text { done }) \min _{i \in\{1,2\}} Q_{\theta_{i}^{-}}\left(s^{\prime}, \pi_{\omega^{-}}\left(s^{\prime}\right)+\varepsilon^{\prime}\right)
\end{gathered}
$$

7. сделать один шаг градиентного спуска по $\boldsymbol{\theta}_{1}$ и $\boldsymbol{\theta}_{2}$ :

$$
\begin{aligned}
& \frac{1}{B} \sum_{\mathbb{T}}\left(Q_{\theta_{1}}(s, a)-y(\mathbb{T})\right)^{2} \rightarrow \min _{\theta_{1}} \\
& \frac{1}{B} \sum_{\mathbb{T}}\left(Q_{\theta_{2}}(s, a)-y(\mathbb{T})\right)^{2} \rightarrow \min _{\theta_{2}}
\end{aligned}
$$

8. если $t \bmod N=0$ :

- сделать один шаг градиентного подъёма по $\boldsymbol{\omega}$ :

$$
\frac{1}{B} \sum_{s \in B} Q_{\theta_{1}}\left(s, \pi_{\omega}(s)\right) \rightarrow \max _{\omega}
$$

- обновить таргет-сети:

$$
\begin{aligned}
& \theta_{1}^{-} \leftarrow(1-\beta) \theta_{1}^{-}+\beta \theta_{1} \\
& \theta_{2}^{-} \leftarrow(1-\beta) \theta_{2}^{-}+\beta \theta_{2} \\
& \omega^{-} \leftarrow(1-\beta) \omega^{-}+\beta \omega
\end{aligned}
$$

---

# 6.1.7. Обучение стохастичных политик 

Среди преимуществ on-policy режима, которые мы потеряли в off-policy алгоритмах, мы упоминали возможность обучать стохастичные политики. Действительно, ряд наших проблем с нестабильностью в DDPG был связан с тем, что мы учим детерминированную стратегию: нужны костыли для решения проблемы exploration-a, актёр может «взламывать» нашу аппроксимацию критика и приводить к overestimation-y, и всё это приходится лечить очередной порцией эвристик. Когда же в on-policy подходе мы можем получить, если так можно сказать, «естественный» exploration за счёт обучения стохастичной стратегии. Обязательно ли в off-policy режиме учить именно детерминированную стратегию?

На самом деле нет. В теории есть возможность обучать и стохастического актёра, хотя точные формулы градиента будут немного отличаться в зависимости от выбранной параметризации политики. Итак, допустим мы моделируем актёра $\pi_{\theta}(a \mid s)$ в классе стохастичных стратегий.

Определение 80: Скажем, что для параметризации $\pi_{\theta}(a \mid s)$ применим репараметризационный трюк (reparameterization trick), если сэмплирование $\boldsymbol{a} \sim \pi_{\theta}(\boldsymbol{a} \mid \boldsymbol{s})$ эквивалентно сэмплированию шума из некоторого не зависящего от параметров распределения $\varepsilon \sim \boldsymbol{p}(\varepsilon)$ и его дальнейшего детерминированного преобразования $\boldsymbol{a}=\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{\varepsilon})$.

Пример 90: Пусть наша стратегия параметризована нормальным распределением:

$$
\pi_{\theta}(a \mid s):=\mathcal{N}\left(\mu_{\theta}(s), \sigma_{\theta}(s)^{2} I\right)
$$

Тогда для неё применим репараметризационный трюк: сэмплирование действий эквивалентно $\boldsymbol{a}:=\boldsymbol{\mu}_{\theta}(s)+$ $+\varepsilon \odot \sigma_{\theta}(s)$, где $\varepsilon \sim \mathcal{N}(0, I), \odot-$ поэлементное перемножение.

Пример 91: Семейство детерминированных стратегий $\pi_{\theta}(s)$ тоже можно считать таким «вырожденным» примером параметризаций, для которой можно проворачивать репараметризационный трюк: просто шум $\varepsilon$ считаем «пустым».

Можно заметить, что если для семейства стратегий применим репараметризационный трюк, то в выводе формулы Policy Gradient можно пользоваться им вместо REINFORCE: фактически, в теореме 63 мы этим воспользовались.

Теорема 65: Если для $\pi_{\theta}(a \mid s)$ применим репараметризационный трюк, то:

$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\frac{1}{1-\gamma} \mathbb{E}_{d_{\sigma_{\theta}}(s)} \mathbb{E}_{\varepsilon \sim p(\varepsilon)} \nabla_{\theta} f_{\theta}(s, \varepsilon) \nabla_{a} Q^{\pi}(s, a) \mid{ }_{a=f_{\theta}(s, \varepsilon)}
$$

Доказательство. Полностью полностью повторяет вывод теоремы 63.
Таким образом, все идеи DDPG расширяются на этот случай. Убирая из формулы градиента частоты посещения состояний и переходя к policy iteration схеме, получаем следующий функционал:

$$
\mathbb{E}_{s} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} Q_{\omega}(s, a) \rightarrow \max _{\theta}
$$

где состояния берутся из буфера. В силу репараметризационного трюка мы легко справимся со взятием градиента на практике:

$$
\nabla_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} Q_{\omega}(s, a)=\mathbb{E}_{\varepsilon \sim p(\varepsilon)} \nabla_{\theta} Q_{\omega}\left(s, f_{\theta}(s, \varepsilon)\right)
$$

Теперь стохастическая оценка градиента по $\boldsymbol{\theta}$ получается напрямую. Понятно, что мы получили обобщение формулы (6.2); мы оптимизируем актёра при помощи градиентов из критика, но дополнительно впрыскиваем шум в действия для получения стохастичной стратегии.

Недостатком гауссианы является то, что она унимодальна, что может приводить к бедам.

Пример 92: Представьте, что вы хотите объехать дерево. Вы можете объехать его справа, можете слева. Критик сообщает вам высокие значения и слева, и справа, и оптимизируя (6.4) в классе гауссиан, можно получить стратегию, которая с наибольшей вероятностью выбирает действие «врезаться в дерево».


Поэтому мы можем захотеть выбрать более сложную параметризацию стратегии, для которой не применим репараметризационный трюк.

---

Пример 93: Например, можно использовать смесь гауссиан. Тогда при использовании $\boldsymbol{K}$ компонент смеси актёр для данного состояния $\boldsymbol{s}$ выдаёт следующие величины: $\boldsymbol{K}$ суммирующихся в единицу чисел $\boldsymbol{w}_{i}(s, \theta)$, а также $\boldsymbol{K}$ векторов $\boldsymbol{\mu}_{i}(s, \theta), \boldsymbol{\sigma}_{i}(s, \theta)$, где $i \in 1,2, \ldots, \boldsymbol{K}$. Итоговое распределение полагается

$$
\pi_{\theta}(a \mid s):=\sum_{i=1}^{K} \boldsymbol{w}_{i}(s, \theta) \mathcal{N}\left(\boldsymbol{\mu}_{i}(s, \theta), \boldsymbol{\sigma}_{i}(s, \theta)^{2} I\right)
$$



Тогда для оптимизации (6.4) нам придётся использовать REINFORCE, обладающий более высокой дисперсией оценок, для сбивания которой необходимо использование бэйзлайна:

Утверждение 67: Градиент (6.4) по параметрам актёра $\boldsymbol{\theta}$ равен

$$
\nabla_{\theta}=\mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s)\left(Q_{\omega}(s, a)(s, a)-b(s)\right)
$$

где $\boldsymbol{b}(\boldsymbol{s})$ - бэйзлайн, произвольная функция от состояний.
Естественно, эту формулу можно интерпретировать как то, что в обычной формуле policy gradient (5.14) мы аналогично DDPG «отказались» от сэмплирования состояний из частот посещения текущей стратегии ради offpolicy режима работы. Ну и хорошим бэйзлайном будет, соответственно, $\boldsymbol{V}_{\boldsymbol{\omega}}(\boldsymbol{s}):=\mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} \boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a})$. Чтобы посчитать такое значение, можно либо использовать Монте-Карло оценку, либо учить отдельную нейросеть, аппроксимирующую V-функцию (целевой переменной для входа $\boldsymbol{s}$ тогда будет $\boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a})$, где $\boldsymbol{a} \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ ).

Обсудим ещё одну тонкость. В задачах непрерывного управления мы обычно работаем с пространством действий $|\boldsymbol{A}| \subseteq[-1,1]^{A}$, и хотелось бы, чтобы наши семейства стратегий тоже выдавали действия именно из этого диапазона. Когда мы работали с детерминированными стратегиями, можно было учесть это в параметризации простым навешиванием гиперболического тангенса $\tanh$ на последний слой сети. А что, если мы используем гауссиану или смесь гауссиан?

Как ни странно, игнорирование этого нюанса может всё равно сработать на практике. То есть, актёр моделирует гауссиану, и считает, что он отправил сэмплы $\boldsymbol{a}$ в среду, которые могут выходить за требуемый диапазон. Неожиданное преимущество такого подхода в том, что иногда в задачах «оптимальные» действия находятся на границе и равны +1 или -1 в каких-то компонентах, и тогда такая необрезанная гауссиана может довольно удобно «часто» сэмплировать подобные действия. Тем не менее, чаще имеет смысл всё же учесть домен в параметризации.

Тогда можно применять гиперболический тангенс к сэмплам из «необрезанной» модели. То есть, стратегия объявляется следующей: $\pi_{\theta}(a \mid s):=\tanh (u)$, где $u \sim \mu_{\theta}(u \mid s)$, и $u \in \mathbb{R}^{A}$. Понятно, как тогда применять репараметризационный трюк, но если мы пользуемся этой идеей вместе с REINFORCE (например, для обучения смеси гауссиан или в on-policy алгоритмах), то нам нужно уметь считать $\log \pi_{\theta}(a \mid s)$. Для этого нужно вспоминать правило замены переменных в плотностях (см., например, википедию).

Ткорема 66 - Формула замены переменной в плотности: Пусть $\pi_{\theta}(a \mid s):=g(u)$, где $g: \mathbb{R}^{A} \rightarrow \mathbb{R}^{A}$, и $u \sim$ $\sim \mu_{\theta}(u \mid s)$. Тогда:

$$
\log \pi_{\theta}(a \mid s)=\log \mu_{\theta}(a \mid s)-\log \left|\operatorname{det} \nabla_{u} g\right|
$$

Без доказательства.
Пример 94: Например, для функции $\boldsymbol{g}(\boldsymbol{u}):=\tanh (\boldsymbol{u})$ якобиан $\nabla_{\boldsymbol{u}} g$ есть диагональная матрица (поскольку преобразование поэлементное), и его определитель равен покомпонентному произведению:

$$
\begin{aligned}
\operatorname{det} \nabla_{u} g(u) & =\prod_{i=1}^{A} \nabla_{u_{i}} \tanh \left(u_{i}\right)= \\
& =\{\text { производная гиперболического тангенса }\}=\prod_{i=1}^{A}\left(1-\tanh ^{2}\left(u_{i}\right)\right)
\end{aligned}
$$

Заметим, что все компоненты положительные $\left(\tanh \left(u_{i}\right) \leq 1\right)$, поэтому модуль из формулы (6.5) брать не нужно. Подставляя в (6.5), получаем окончательно:

$$
\log \pi_{\theta}(a \mid s)=\log \mu_{\theta}(a \mid s)-\sum_{i=1}^{A} \log \left(1-\tanh ^{2}\left(u_{i}\right)\right)
$$

---

Итак, теоретически возможность обучать стохастичные стратегии в off-policy режиме есть. Что тогда мешает в DDPG воспользоваться этим и использовать гауссиану или смесь гауссиан? Дело в том, что мы помним, что направление оптимизации актёра - детерминированная стратегия: мы идём в сторону жадной стратегии $\pi_{\theta}(s)=\underset{\boldsymbol{a}}{\operatorname{argmax}} Q_{\boldsymbol{\omega}}(s, \boldsymbol{a})$. Схлопывание стохастичной стратегии в детерминированную чревато численными проблемами: например, для гауссианы дисперсия начнёт уходить в ноль, градиенты могут начать взрываться.

Можно, конечно, попытаться как-то побороться с этим эффектом, например, при помощи эвристики, которой часто пользуются в on-policy методах - добавкой энтропийного лосса. Напомним: чем больше значение энтропии для распределения, тем оно «менее вырожденное»:

Определение 81: Энтропией распределения $\boldsymbol{\pi}(\boldsymbol{a})$ называется

$$
\mathcal{H}(\pi(a)):=-\mathbb{E}_{\pi(a)} \log \pi(a)
$$

И в policy gradient алгоритмах часто в формулу градиента добавляют слагаемое $\nabla_{\theta} \mathcal{H}\left(\pi_{\theta}(\cdot \mid s)\right)$, которое поощряет высокую энтропию стратегии. Однако в on-policy режиме Q-функция заменялась на оценку и была стохастичной. В off-policy же актёр имеет куда больше шансов «переобучиться» под критика, и энтропийный лосс придётся тогда выставлять с большим коэффициентом.

Вместо подобных плясок с бубном хотелось бы, чтобы подобных проблем в принципе не возникало. Конечно, детерминированность оптимальной стратегии - особенность постановки задачи RL, и поэтому если мы хотим, чтобы таких эффектов в оптимизационных процессах не было, нам придётся найти какую-то альтернативную постановку задачи. Оказывается, такая альтернативная формулировка есть, и она бывает крайне удобна. В ней оптимальные стратегии уже будут стохастичны, и ряд численных проблем, а также проблем с exploration-ом, отпадёт; в частности, она «обоснует» добавку градиента энтропии в формулу градиента.

# §6.2. Soft Actor-Critic 

### 6.2.1. Maximum Entropy RL

Определение 82: Задачей Maximum Entropy RL является максимизация функционала

$$
J_{\text {soft }}(\pi):=\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0} \gamma^{t}\left[r_{t}+\alpha \mathcal{H}(\pi(\cdot \mid s))\right] \rightarrow \max _{\pi}
$$

где $\boldsymbol{\alpha}$ - гиперпараметр, называемый температурой (temperature).
Утверждение 68: Задача (6.7) эквивалентна

$$
J_{\text {soft }}(\pi):=\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0} \gamma^{t}\left[r_{t}-\alpha \log \pi\left(a_{t} \mid s_{t}\right)\right] \rightarrow \max _{\pi}
$$

Доказательство. Следует из определения энтропии (6.6); ведь мат.ожидание по действиям присутствует в мат.ожидании по траекториям.

Интуиция такого функционала проста: мы говорим, что хотим не просто оптимальную стратегию, а самую стохастичную около-оптимальную стратегию (коэффициент $\boldsymbol{\alpha}$ масштабирует добавку и определяет важность добавленного слагаемого). Цель - избегать локальных оптимумов в среде.

Пример 95: Представим, что у агента есть два пути, и по мере углубления награда на каждом пути одинаково растёт. Пусть первый путь заканчивается тупиком и суммарно позволяет набрать не более +100 , а на втором тупик стоит чуть дальше и даёт +110 . Во время обучения агент может уловить награду вдоль первого пути и учиться углубляться в него, игнорируя исследование второго пути, даже если агент умеет набирать там награду как на первом; за счёт бонуса за наиболее стохастичную стратегию агент мотивирован в течение обучения в начале эпизодов случайно выбирать между двумя путями. То есть, энтропийный бонус помогает


избегать подобных «локальных максимумов» в среде.

Можно считать, что в данном фреймворке мы на самом деле лишь чуть-чуть модифицировали награду в среде:

$$
r_{\text {soft }}(s, a):=r(s, a)-\alpha \log \pi(a \mid s)
$$

---

Построенную теорию теперь нужно перепроверять, поскольку награда $\boldsymbol{r}$ стала зависеть напрямую от вероятностей, выдаваемых стратегией (такого в формализме MDP не предполагалось), да и очевидно, что не все утверждения переносятся на новую постановку:

Утверждение 69: Оптимальной детерминированной стратегии может не существовать.
Доказательство. В MDP, где награда всегда 0 , оптимальна стратегия с максимальной энтропией.
Однако принцип Policy Iteration - чередование этапов оценивания и улучшения стратегии - остаётся для этой альтернативной постановки задачи неизменным. Чтобы подчеркнуть, что речь идёт именно о постановке Maximum Entropy RL, ко всем терминам из обычной теории добавляют слово soft («мягкие»), подразумевая, что энтропийный бонус «сглаживает» стратегию, предупреждая её вырождение в детерминированную. Поэтому будем обозначать оценочные функции в рамках данного фреймворка как $\boldsymbol{Q}_{\text {soft }}^{\boldsymbol{\pi}}, \boldsymbol{V}_{\text {soft }}^{\boldsymbol{\pi}}$. Также без ограничения общности далее будем считать $\boldsymbol{\alpha}=1$, так как всегда можно перемасштабировать награду $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ без изменения задачи.

Гиперпараметр температуры является основным недостатком идеи Maximum Entropy RL - на практике его обычно очень сложно подбирать, а он существенно влияет на то, какая стратегия получится в итоге обучения.

Мы также должны договориться о том, в какой момент во время взаимодействия приходит энтропийный бонус. Формула (6.8) предполагает, что после выбора действия $\boldsymbol{a}$ из состояния $\boldsymbol{s}$ помимо награды $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ агент ещё поощряется $\log \pi(\boldsymbol{a} \mid \boldsymbol{s})$; однако мы договоримся по-другому ${ }^{3}$. Будем считать, что агент, приходя в некоторое состояние $\boldsymbol{s}$, получает бонус в размере $\mathcal{H}(\pi(\cdot \mid \boldsymbol{s})) \equiv-\mathbb{E}_{a \sim \pi(a \mid s)} \log \pi(\boldsymbol{a} \mid \boldsymbol{s})$ (если состояние не терминальное), затем сэмплирует действие $\boldsymbol{a}$, получает бонус $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ и потом наблюддет $\boldsymbol{s}^{\prime}$. В такой договорённости уравнения для мягких оценочных функций выглядят так:

Теорема 67 - Мягкие уравнения связи:

$$
\begin{gathered}
Q_{\text {soft }}^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{\text {soft }}^{\pi}\left(s^{\prime}\right) \\
V_{\text {soft }}^{\pi}(s)=\mathbb{E}_{\pi(a \mid s)}[Q_{\text {soft }}^{\pi}(s, a)-\log \pi(a \mid s)]
\end{gathered}
$$

Доказательство. По определению с учётом договорённости.

Теорема 68 - Мягкие уравнения Беллмана (soft Bellman equations):

$$
\begin{gathered}
Q_{\text {soft }}^{\pi}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \mathbb{E}_{a^{\prime}}\left[Q_{\text {soft }}^{\pi}\left(s^{\prime}, a^{\prime}\right)-\log \pi\left(a^{\prime} \mid s^{\prime}\right)\right] \\
V_{\text {soft }}^{\pi}(s)=\mathbb{E}_{a}[r(s, a)-\log \pi(a \mid s)+\gamma \mathbb{E}_{s^{\prime}} V_{\text {soft }}^{\pi}\left(s^{\prime}\right)]
\end{gathered}
$$

# 6.2.2. Soft Policy Evaluation 

Имея на руках мягкие уравнения Беллмана, можно сразу же построить алгоритм оценивания стратегии (Soft Policy Evaluation), обучения мягкой Q- или V-функции по заданной стратегии $\boldsymbol{\pi}$. Технически, необходимо только проверить, что правые части мягких уравнений Беллмана являются сжатиями:

Теорема 69: Операторы, стоящие в правой части мягких уравнений Беллмана, являются сжимающими с коэффициентом $\gamma$ по метрике

$$
\rho\left(V_{1}, V_{2}\right):=\max _{s}\left|V_{1}(s)-V_{2}(s)\right|
$$

Доказательство. Покажем для мягкой V-функции. Пусть $\boldsymbol{\mathfrak { B }}_{\text {soft }}$ - оператор, стоящий в правой части (6.12), и пусть даны две V-функции $\boldsymbol{V}_{1}, \boldsymbol{V}_{2}$. Тогда:

$$
\left|\left[\boldsymbol{\mathfrak { B }}_{\text {soft }} V_{1}\right](s)-\left[\boldsymbol{\mathfrak { B }}_{\text {soft }} V_{2}\right](s)\right|=\gamma\left|\mathbb{E}_{a} \mathbb{E}_{s^{\prime}}\left[V_{1}\left(s^{\prime}\right)-V_{2}\left(s^{\prime}\right)\right]\right|
$$

поскольку энтропия стратегии $\boldsymbol{\pi}$ вместе с наградой за шаг одинакова для $\boldsymbol{V}_{1}$ и $\boldsymbol{V}_{2}$ и потому сокращается. Дальше, как и для обычных V-функций, можно просто оценить это выражение сверху $\gamma \rho\left(V_{1}, V_{2}\right)$, заканчивая доказательство.

[^0]
[^0]:    ${ }^{3}$ такая договорённость удобнее, поскольку после сэмплирования действия $\boldsymbol{a}$ в состоянии $\boldsymbol{s}$ неудобно учитывать зависимость награды от распределения $\boldsymbol{\pi}(\cdot \mid \boldsymbol{s})$.

---

Утверждение 70: Метод простой итерации сходится к единственному решению мягких уравнений Беллмана из любого начального приближения.

Итак, мы уже можем сразу построить процедуру обучения критика. Рассмотрим обучение $\boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a}) \approx$ $\approx \boldsymbol{Q}_{\text {soft }}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ с одношаговых оценок в off-policy режиме, то есть будем просто решать мягкое уравнение Беллмана (6.11). Тогда для заданного перехода $\mathbb{T}=\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right)$ целевая переменная строится как

$$
y(\mathbb{T}):=r+\gamma \mathbb{E}_{a^{\prime} \sim \pi\left(a^{\prime} \mid s^{\prime}\right)}\left[Q_{\omega}\left(s^{\prime}, a^{\prime}\right)-\log \pi\left(a^{\prime} \mid s^{\prime}\right)\right]
$$

В непрерывных пространствах действий взять мат.ожидание по $\boldsymbol{a}^{\prime}$ аналитически может не удастся (например, если $\boldsymbol{\pi}$ моделируется гаусспаной). Можно, как всегда, тоже заменить его на Монте-Карло оценку (по возможности, взяв аналитически энтропию). Но при желании есть возможность ещё немножко сбить дисперсию, заведя вспомогательную нейросеть для аппроксимации $\boldsymbol{V}_{\boldsymbol{\psi}}(\boldsymbol{s}) \approx \boldsymbol{V}_{\text {soft }}^{\boldsymbol{\pi}}(\boldsymbol{s})$ : то есть, фактически, просто учить мат.ожидание $\mathbb{E}_{a^{\prime}}$.

В таком варианте таргеты для критика (Q-функция с параметрами $\boldsymbol{\omega}$ ) и для вспомогательной V-функции выглядят следующим образом: для заданного перехода $\mathbb{T}=\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right)$, взятого из буфера, генерируем $\boldsymbol{a}_{\boldsymbol{\pi}}$ из текущей версии стратегии $\boldsymbol{\pi}$ и запоминаем вероятность $\boldsymbol{\pi}\left(\boldsymbol{a}_{\boldsymbol{\pi}} \mid \boldsymbol{s}\right)$, после чего вычисляем несмещённые оценки правых частей уравнений связи (6.10) и (6.9):

$$
\begin{gathered}
y_{Q}(\mathbb{T}):=r+\gamma V_{\psi}\left(s^{\prime}\right) \\
y_{V}(\mathbb{T}):=Q_{\omega}\left(s, a_{\pi}\right)-\log \pi\left(a_{\pi} \mid s\right)
\end{gathered}
$$

И с такими таргетами, как обычно, решаем задачу регрессии с функцией потерь MSE. Самое главное - соответствующее действие $\boldsymbol{a}_{\boldsymbol{\pi}}$ для таргета $\boldsymbol{y}_{\boldsymbol{V}}$ брать не из буфера, а сгенерировать из текущей версии стратегии, поскольку иначе мы некорректно оценим среднее $\mathbb{E}_{a \sim \pi}$.

Естественно, в любых одношаговых таргетах оценка очень смещённая, и поэтому для стабилизации полезны две стандартные эвристики - таргет-сети и clipped double оценки (см. раздел 4.2.2). В реализациях места, где используются таргет-сети, могут различаться, поскольку однозначно сложно сказать, где их лучше всего использовать, но они точно должны быть; без таргет-сетей эти алгоритмы не заработают из-за сильной смещённости всех оценок.

# 6.2.3. Soft Policy Improvement 

Теперь обсудим, как обучать актёра. Напомним, что вся теория оптимизации стратегии сводится к policy improvement и нам нужен аналог теоремы 17. Построить этот аналог можно из простых соображений: вот нам дана стратегия $\pi_{1}$ и её мягкая оценочная функция $Q_{\text {soft }}^{\pi_{1}}(\boldsymbol{s}, \boldsymbol{a})$. Как можно улучшить эту стратегию из состояния $\boldsymbol{s}$ ? Сейчас $\pi_{1}$ набирает из $\boldsymbol{s}$ в среднем $\boldsymbol{V}_{\text {soft }}^{\pi_{1}}(\boldsymbol{s})$. А сколько мы будем в среднем набирать, если сейчас в состоянии $\boldsymbol{s}$ сменим стратегию на $\pi_{2}$ (теперь с учётом энтропийного бонуса, который мы тогда получим в $\boldsymbol{s}$ за эту новую стратегию $\pi_{2}$ ), а в будущем будем вести себя ну по крайней мере не хуже, чем стратегия $\pi_{1}$ ?

Теорема 70 - Soft Policy Improvement: Пусть стратегии $\pi_{1}$ и $\pi_{2}$ таковы, что для всех состояний $\boldsymbol{s}$ выполняется:

$$
\mathbb{E}_{\pi_{2}(a \mid s)} Q_{\text {soft }}^{\pi_{1}}(s, a)+\mathcal{H}\left(\pi_{2}(\cdot \mid s)\right) \geq V_{\text {soft }}^{\pi_{1}}(s)
$$

тогда $\pi_{2} \succeq \pi_{1}$ с учётом энтропийного бонуса; если хотя бы для одного $\boldsymbol{s}$ неравенство выполнено строго, то $\pi_{2} \succ \pi_{1}$.

Доказательство. Полностью аналогично доказательству в обычном случае (теорема 17). Покажем, что $V_{\text {soft }}^{\pi_{2}}(s) \geq V_{\text {soft }}^{\pi_{1}}(s)$ для любого $\boldsymbol{s}:$

$$
\begin{aligned}
V_{\text {soft }}^{\pi_{1}}(s) \leq & \left\{\text { по построению } \pi_{2}\right\} \leq \mathbb{E}_{\pi_{2}(a \mid s)} Q_{\text {soft }}^{\pi_{1}}(s, a)+\mathcal{H}\left(\pi_{2}(\cdot \mid s)\right)= \\
& =\{\text { связь } \mathrm{QV}(6.9)\}=\mathbb{E}_{\pi_{2}(a \mid s)}\left[r+\mathcal{H}\left(\pi_{2}(\cdot \mid s)\right)+\gamma \mathbb{E}_{s^{\prime}} V_{\text {soft }}^{\pi_{1}}\left(s^{\prime}\right)\right] \leq \\
\leq & \{\text { применяем это же неравенство рекурсивно }\}=\mathbb{E}_{\pi_{2}(a \mid s)}\left[r+\mathcal{H}\left(\pi_{2}(\cdot \mid s)\right)+\right. \\
& \left.+\mathbb{E}_{s^{\prime}} \mathbb{E}_{\pi_{2}\left(a^{\prime} \mid s^{\prime}\right)}\left[\gamma r^{\prime}+\gamma \mathcal{H}\left(\pi_{2}\left(\cdot \mid s^{\prime}\right)\right)+\gamma^{2} \mathbb{E}_{s^{\prime \prime}} V_{\text {soft }}^{\pi_{1}}\left(s^{\prime \prime}\right)\right]\right] \leq \\
\leq & \{\text { раскручиваем цепочку далее }\} \leq \cdots \leq \mathbb{E}_{T \sim \pi_{2} \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} r_{t}+\gamma^{t} \mathcal{H}\left(\pi_{2}\left(\cdot \mid s_{t}\right)\right)= \\
= & \{\text { по определению мягкой V-функции }\}=V_{\text {soft }}^{\pi_{2}}(s)
\end{aligned}
$$

Если для какого-то $\boldsymbol{s}$ неравенство из условия теоремы было выполнено строго, то для него первое неравенство в этой цепочке рассуждений выполняется строго, и, значит, $V_{\text {soft }}^{\pi_{2}}(s)>V_{\text {soft }}^{\pi_{1}}(s)$.

Таким образом, аналог общей схемы Generalized Policy Iteration в задаче Maximum Entropy RL выглядит так:

---

- Soft Policy Evaluation заключается в обучении аппроксимации мягкой оценочной функции $\boldsymbol{Q}_{\boldsymbol{\omega}} \approx Q_{\text {soft }}^{\boldsymbol{\pi}}$ для текущей стратегии $\boldsymbol{\pi}$;
- Soft Policy Improvement заключается в оптимизации следующего функционала (для разных состояний $s):$

$$
\mathbb{E}_{\boldsymbol{\pi}(a \mid s)} \boldsymbol{Q}_{\boldsymbol{\omega}}(s, a)+\mathcal{H}(\pi(\cdot \mid s)) \rightarrow \max _{\boldsymbol{\pi}}
$$

Соответственно, для обучения актёра, заданного параметрической стратегией $\boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$, нужно просто оптимизировать не среднее значение Q-функции, а учесть дополнительно добавку энтропийного бонуса в рассматриваемом состоянии. У задачи оптимизации (6.13) есть один важный альтернативный вид:

Теорема 71: Задача (6.13) эквивалентна задаче

$$
\mathbf{K L}\left(\boldsymbol{\pi}_{\boldsymbol{\theta}}(\cdot \mid \boldsymbol{s}) \| \exp \boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \cdot)\right) \rightarrow \min _{\boldsymbol{\theta}} \mathbf{n}
$$

где $\exp Q_{\omega}(s, \cdot)$ - ненормированное распределение над действиями.
Доказательство. Обозначим нормировочную константу распределения $\exp Q_{\omega}(s, \cdot)$ как $Z_{\omega}(s):=$ $\vDash \int \exp Q \omega(s, a) \mathrm{d} a$. Тогда:

$$
\mathbf{K L}\left(\boldsymbol{\pi}_{\boldsymbol{\theta}}(\cdot \mid \boldsymbol{s}) \| \exp \boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \cdot)\right)=\mathbb{E}_{\boldsymbol{a} \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})} \log \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})-\boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a})+\log Z_{\boldsymbol{\omega}}(\boldsymbol{s})
$$

Осталось заметить, что при домножении на минус получим (6.13): первое слагаемое есть энтропия, а третье слагаемое не зависит от оптимизируемых параметров $\boldsymbol{\theta}$ :

$$
\mathbb{E}_{\boldsymbol{a} \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})} \log Z_{\boldsymbol{\omega}}(\boldsymbol{s})=\log Z_{\boldsymbol{\omega}}(\boldsymbol{s})=\operatorname{const}(\boldsymbol{\theta})
$$

Теорема 72 - Вид жадной стратегии: Максимальное значение (6.13) достигается на стратегии

$$
\pi(a \mid s) \propto \exp Q_{\omega}(s, a)
$$

Доказательство. Следует из теоремы 71, поскольку минимум KL-дивергенции достигается в нуле на совпадающих распределениях.

Таким образом мы показали, что актёр просто пытается выучить распределение $\boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s}) \propto \exp \boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \cdot)$ - больцмановскую стратегию по отношению к Q-функции (3.37). Мы сталкивались с ней, когда обсуждали способы исследования. Соответственно, в Maximum Entropy RL понятие жадной стратегии немножко другое: нужно не аргмакс по действиям брать, а софтмакс.

На практике второе слагаемое функционала (6.13) - энтропию $\boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ - обычно можно рассчитать аналитически для выбранной параметризации $\boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$. Первое же слагаемое берётся так, как мы обсуждали в разделе 6.1.7: при помощи репараметризационного трюка, если такая возможность есть, и при помощи REINFORCE иначе.

Пример 96: Пусть наша стратегия параметризована гауссианой (см. пример 90). Для неё можно проводить репараметризационный трюк и также можно аналитически посчитать энтропию:

$$
\mathcal{H}\left(\mathcal{N}\left(\mu, \sigma^{2} I\right)\right)=\sum_{i=1}^{A} \log \sigma_{i}
$$

Итого, формула (6.13) в таком случае получается следующей:

$$
\mathbb{E}_{\boldsymbol{z} \sim \mathcal{N}(0, I)} Q_{\boldsymbol{\omega}}\left(s, \mu_{\boldsymbol{\theta}}(s)+\sigma_{\boldsymbol{\theta}}(s) \odot \boldsymbol{\varepsilon}\right)+\sum_{i=1}^{A} \log \sigma_{i}(s, \boldsymbol{\theta}) \rightarrow \max _{\boldsymbol{\theta}}
$$

Пример 97: Пусть наша стратегия параметризована смесью гауссиан (см. пример 93). Тогда для неё не применим репараметризационный трюк, и сложно аналитически посчитать энтропию. Тогда придётся применять REINFORCE, и формула градиента (6.13) получается следующей:

$$
\nabla_{\boldsymbol{\theta}}=\mathbb{E}_{\boldsymbol{a} \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})} \nabla_{\boldsymbol{\theta}} \log \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})\left[Q_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a})-\log \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})-\boldsymbol{b}(\boldsymbol{s})\right]
$$

где $\boldsymbol{b}(\boldsymbol{s})$ - бэйзлайн, произвольная функция от состояний. Имеет смысл делать её близкой к среднему значению $\boldsymbol{Q}_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a})-\log \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$, то есть хорошо выбирать $\boldsymbol{b}(\boldsymbol{s}):=\boldsymbol{V}_{\boldsymbol{\omega}}(\boldsymbol{s})=\mathbb{E}_{\boldsymbol{a}}\left[Q_{\boldsymbol{\omega}}(\boldsymbol{s}, \boldsymbol{a})-\log \boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})\right]$.

---

# 6.2.4. Soft Actor-Critic (SAC) 

Мы уже можем собрать алгоритм-аналог DDPG для задачи Maximum Entropy RL. B этом алгоритме по аналогии с DDPG есть актёр и критик, критик учится оценивать актёра по одношаговым таргетам, а актёр обучается моделированием soft policy improvement-a. Таким образом, схема работает в off-policy режиме.

Перечислим ряд деталей, с которыми в целом можно играться в этом алгоритме ${ }^{4}$. При обучении критика используется вспомогательная V-функция, для стабилизации можно ограничиться таргет-сетью (с параметрами $\psi^{-}$) только для V-функции:

$$
y_{Q}(\mathbb{T}):=r+\gamma V_{\psi^{-}}\left(s^{\prime}\right)
$$

а для борьбы с overestimation bias в таргете для V-функции используется twin оценка аналогично алгоритму TD3 (см. раздел 6.1.6), то есть обучается две Q-функции с параметрами $\omega_{1}, \omega_{2}$, и таргет $y_{V}$ строится по следующей формуле:

$$
y_{V}(\mathbb{T}):=\min _{i=1,2} Q_{\omega_{i}}\left(s, a_{\pi}\right)-\log \pi(a \mid s)
$$

где $a_{\pi} \sim \pi_{\theta}(a \mid s)$.
Наконец, в функционале (6.13) актёру предлагается «взламывать» минимум из двух Q-функций, то есть использовать twin-oценку и в этой формуле:

$$
\mathbb{E}_{\pi_{\theta}(a \mid s)} \min _{i=1,2} Q_{\omega_{i}}(s, a)+\mathcal{H}(\pi(\cdot \mid s)) \rightarrow \max _{\pi}
$$

Далее схема приведена для простоты для случая, если актёр моделирует гаусснану (см. пример 96). Конечно, возможно использование и для смеси гаусснан (см. пример 93) при замене в функционале актёра репараметризационного трюка на REINFORCE.

## Алгоритм 24: Soft Actor-Critic (SAC)

Гиперпараметры: $\boldsymbol{B}$ - размер мини-батчей, $\boldsymbol{\beta}$ - параметр экспоненциального сглаживания таргетсети, $\boldsymbol{\alpha}$ - температура, $\boldsymbol{\pi}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s}):=\boldsymbol{\mathcal { N }}\left(\boldsymbol{\mu}_{\boldsymbol{\theta}}(\boldsymbol{s}), \boldsymbol{\sigma}_{\boldsymbol{\theta}}(\boldsymbol{s})^{\mathbf{2}} \boldsymbol{I}\right)$ - гауссова стратегия с параметрами $\boldsymbol{\theta}$, $\boldsymbol{Q}_{\omega_{1}}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{Q}_{\omega_{2}}(\boldsymbol{s}, \boldsymbol{a})$ - две нейросетки с параметрами $\boldsymbol{\omega}_{1}$ и $\boldsymbol{\omega}_{2}, \boldsymbol{V}_{\psi}(\boldsymbol{s})$ - нейросетка с параметрами $\psi$, SGD-оптимизаторы.

Инициализировать $\boldsymbol{\theta}, \boldsymbol{\omega}_{1}, \boldsymbol{\omega}_{2}, \boldsymbol{\psi}$ произвольно
Инициализировать таргет-сеть $\boldsymbol{\psi}^{-}:=\boldsymbol{\psi}$
Пронаблюдать $s_{0}$
На очередном шаге $t$ :

1. выбрать $a_{t} \sim \pi_{\theta}\left(a_{t} \mid s_{t}\right)$
2. проваблюдать $r_{t}, s_{t+1}$, done $_{t+1}$
3. добавить пятёрку $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right.$, done $\left._{t+1}\right)$ в реплей буфер
4. засэмплировать мини-батч размера $\boldsymbol{B}$ из буфера
5. для каждого $s$ из батча засэмплировать шума $\varepsilon(s) \sim \mathcal{N}(0, I)$ и посчитать $\boldsymbol{\mu}(s, \theta), \boldsymbol{\sigma}(s, \theta)$ стратегии $\pi_{\theta}$
6. посчитать оценку градиента по параметрам стратегии:

$$
\nabla_{\theta}:=\frac{1}{B} \sum_{s \in B} \nabla_{\theta}\left[\alpha \sum_{i=1}^{A} \log \sigma_{i}(s, \theta)+\min _{i=1,2} Q_{\omega_{i}}\left(s, \mu_{\theta}(s)+\sigma_{\theta}(s) \odot \varepsilon(s)\right)\right]
$$

7. делаем шаг градиентного подъёма по $\boldsymbol{\theta}$, используя $\nabla_{\theta}$
8. для каждого перехода $\mathbb{T}:=\left(s, a, r, s^{\prime}\right.$, done $)$ засэмплировать $a_{\pi} \sim \pi_{\theta}\left(a_{\pi} \mid s\right)$ и сохранить вероятности $\pi_{\theta}\left(a_{\pi} \mid s\right)$
9. посчитать таргеты:

$$
\begin{gathered}
y_{V}(\mathbb{T}):=\min _{i=1,2} Q_{\omega_{i}}\left(s, a_{\pi}\right)-\alpha \log \pi_{\theta}\left(a_{\pi} \mid s\right) \\
y_{Q}(\mathbb{T}):=r+\gamma V_{\psi^{-}}\left(s^{\prime}\right)
\end{gathered}
$$

[^0]
[^0]:    ${ }^{4}$ существует несколько версий этого алгоритма; в частности, есть версия, где Q-функции учатся через Q-функции, и вспомогательная V-сеть не используется. Тогда нужно заводить таргет-сети для двух Q-сетей.

---

10. посчитать лоссы:

$$
\begin{aligned}
\operatorname{Loss}_{V}(\psi) & :=\frac{1}{B} \sum_{\mathrm{T}}\left(V_{\psi}\left(s^{\prime}\right)-y_{V}(\mathbb{T})\right)^{2} \\
\operatorname{Loss}_{Q 1}\left(\omega_{1}\right) & :=\frac{1}{B} \sum_{\mathrm{T}}\left(Q_{\omega_{1}}(s, a)-y_{Q}(\mathbb{T})\right)^{2} \\
\operatorname{Loss}_{Q 2}\left(\omega_{2}\right) & :=\frac{1}{B} \sum_{\mathrm{T}}\left(Q_{\omega_{2}}(s, a)-y_{Q}(\mathbb{T})\right)^{2}
\end{aligned}
$$

11. делаем шаг градиентного спуска по $\psi, \omega_{1}$ и $\omega_{2}$, используя $\nabla_{\psi} \operatorname{Loss}_{V}(\psi), \nabla_{\omega} \operatorname{Loss}_{Q 1}\left(\omega_{1}\right)$ и $\nabla_{\omega_{2}} \operatorname{Loss}_{Q 2}\left(\omega_{2}\right)$ соответственно
12. обновляем таргет-сеть: $\psi^{-} \leftarrow(1-\beta) \psi^{-}+\beta \psi$

Можно провести sanity check, убедившись, что при $\boldsymbol{\alpha} \rightarrow \mathbf{0}$, мы получаем обычный DDPG (с twin-трюком и для стохастичной стратегии).

Консенсуса по поводу того, кто круче - TD3 (алгоритм 23) или SAC, нету. Считается, что они оба хорошо работают, по крайней мере лучше DDPG, и на разных задачах может победить как первый, так и второй. Недостатком TD3 является некоторая «востыльность» предложенных эвристик, а недостатком SAC неудобный гиперпараметр температуры.

# 6.2.5. Аналоги других алгоритмов 

В обычном RL оптимальной стратегией была жадная по отношению к оптимальной Q-функции, то есть та, для которой нельзя провести policy improvement ни в одном состоянии. Аналогичные рассуждения верны и для Maximum Entropy RL, разве что форма жадной стратегии (6.14) теперь другая.

Сформулируем критерий оптимальности Беллмана в задаче Maximum Entropy RL. По аналогии с обычным случаем, введём оптимальные оценочные функции:

$$
\begin{aligned}
V_{\text {soft }}^{*}(s) & :=\max _{\pi} V_{\text {soft }}^{\pi}(s) \\
Q_{\text {soft }}^{*}(s, a) & :=\max _{\pi} Q_{\text {soft }}^{\pi}(s, a)
\end{aligned}
$$

Теорема 73 - Вид оптимальной стратегии для Maximum Entropy RL: Оптимальной является единственная стратегия

$$
\pi(a \mid s) \propto \exp Q_{\text {soft }}^{*}(s, a)
$$

Доказательство. Проводим рассуждения аналогичные теореме 12. Откажемся от стационарности и будем рассматривать задачу поиска оптимальной стратегии $\pi_{t}(a \mid s_{0})$ в предположении, что в будущем мы сможем набрать максимально возможную награду $Q_{\text {soft }}^{*}(s, a, t)$ :

$$
\mathbb{E}_{\pi_{t}(a \mid s)}\left[Q_{\text {soft }}^{*}(s, a, t)-\log \pi_{t}(a \mid s)\right] \rightarrow \max _{\pi_{t}(a \mid s)}
$$

Аналогично теореме 71 про soft policy improvement, можно заметить, что с точностью до константы, не зависящей от $\pi_{t}$, оптимизируемое выражение есть:

$$
-\mathrm{KL}\left(\pi_{t}(a \mid s) \| \frac{\exp Q_{\text {soft }}^{*}(s, a, t)}{Z(s, t)}\right) \rightarrow \max _{\pi_{t}(a \mid s)}
$$

где $\boldsymbol{Z}(\boldsymbol{s}, \boldsymbol{t})$ - нормировочная константа $\exp \boldsymbol{Q}_{\text {soft }}^{*}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{t})$. Понятно, что оптимум достигается в нуле на $\boldsymbol{\pi}_{\boldsymbol{t}}(\boldsymbol{a} \mid$ $\mid s)$, совпадающей с этим распределением.

Дальнейшее рассуждение строится как раньше: $Q_{\text {soft }}^{*}$ от времени не зависит по определению (аналогично утв. 15), поэтому оптимальная стратегия получается стационарной, следовательно

$$
\pi(a \mid s) \propto \exp Q_{\text {soft }}^{*}(s, a)
$$

Заметим, что в силу однозначного определения $Q_{\text {soft }}^{*}(s, a)$, такая стратегия в принципе единственна в отличие от обычной задачи RL.

Имея на руках вид оптимальной стратегии, мы можем получить уравнения оптимальности:

---

Теорема 74:

$$
V_{\text {soft }}^{*}(s)=\log \int_{\mathcal{A}} \exp Q_{\text {soft }}^{*}(s, a) \mathrm{d} a
$$

Доказательство. Мы знаем, что оптимальная стратегия имеет вид $\pi^{*}(a \mid s)=\frac{\exp Q_{\text {soft }}^{*}(s, a)}{Z(s)}$, где

$$
Z(s):=\int_{\mathcal{A}} \exp Q_{\text {soft }}^{*}(s, a) \mathrm{d} a
$$

является нормировочной константой. Посчитаем энтропию такого распределения:

$$
-\mathbb{E}_{\pi^{*}(a \mid s)} \log \pi^{*}(a \mid s)=\int_{\mathcal{A}} \frac{\exp Q_{\text {soft }}^{*}(s, a)}{Z(s)}\left(\log Z(s)-Q_{\text {soft }}^{*}(s, a)\right) \mathrm{d} a=\log Z(s)-\mathbb{E}_{\pi^{*}(a \mid s)} Q_{\text {soft }}^{*}(s, a)
$$

Подставим оптимальную стратегию в мягкое VQ уравнение (6.10), которое справедливо в том числе и для оптимальной стратегии:

$$
\begin{aligned}
V_{\text {soft }}^{*}(s) & =\mathbb{E}_{\pi^{*}(a \mid s)}\left[Q_{\text {soft }}^{*}(s, a)-\log \pi^{*}(a \mid s)\right]= \\
& =\mathbb{E}_{\pi^{*}(a \mid s)} Q_{\text {soft }}^{*}(s, a)-\mathbb{E}_{\pi^{*}(a \mid s)} Q_{\text {soft }}^{*}(s, a)+\log Z(s)= \\
& =\log Z(s)
\end{aligned}
$$

Вспоминая определение $Z(s)$, получаем доказываемое.

# Утверждение 71: 

$$
Q_{\text {soft }}^{*}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} V_{\text {soft }}^{*}\left(s^{\prime}\right)
$$

Утверждение 72 - Мягкое уравнение оптимальности Беллмана (soft Bellman optimality equations):

$$
Q_{\text {soft }}^{*}(s, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} \log \int_{\mathcal{A}} \exp Q_{\text {soft }}^{*}\left(s^{\prime}, a^{\prime}\right) \mathrm{d} a^{\prime}
$$

Теорема 75: Оператор, стоящий в правой части уравнения (6.16), является сжимающим с коэффициентом $\gamma$, и, следовательно, метод простой итерации решения этой системы уравнений сходится из любого начального приближения к единственной неподвижной точке.

Доказательство. Пусть даны две Q-функции, $Q_{1}, Q_{2}$, и пусть

$$
\rho\left(Q_{1}, Q_{2}\right):=\max _{s, a}\left|Q_{1}(s, a)-Q_{2}(s, a)\right|<\varepsilon
$$

Тогда:

$$
\log \int_{\mathcal{A}} \exp Q_{1}(s, a) \mathrm{d} a \leq \log \int_{\mathcal{A}} \exp \left(Q_{2}(s, a)+\varepsilon\right) \mathrm{d} a=\varepsilon+\log \int_{\mathcal{A}} \exp Q_{2}(s, a) \mathrm{d} a
$$

Аналогично можно показать, что

$$
\log \int_{\mathcal{A}} \exp Q_{1}(s, a) \mathrm{d} a \geq-\varepsilon+\log \int_{\mathcal{A}} \exp Q_{2}(s, a) \mathrm{d} a
$$

Пусть $\mathfrak{B}_{\text {soft }}$ - оператор, стоящий в правой части (6.16). Тогда:

$$
\begin{aligned}
\left|\left[\mathfrak{B}_{\text {soft }} Q_{1}\right](s, a)-\left[\mathfrak{B}_{\text {soft }} Q_{2}\right](s, a)\right| & =\gamma\left|\mathbb{E}_{s^{\prime}}\left(\log \int_{\mathcal{A}} \exp Q_{1}\left(s^{\prime}, a^{\prime}\right) \mathrm{d} a^{\prime}-\log \int_{\mathcal{A}} \exp Q_{2}\left(s^{\prime}, a^{\prime}\right) \mathrm{d} a^{\prime}\right)\right| \leq \\
& \leq \gamma \varepsilon
\end{aligned}
$$

Таким образом, $\rho\left(\mathfrak{B}_{\text {soft }} Q_{1}, \mathfrak{B}_{\text {soft }} Q_{2}\right)$ уменьшилось по крайней мере в $\gamma$ раз по сравнению с $\rho\left(Q_{1}, Q_{2}\right)$.

---

Утверждение 73: Если $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}) \propto \exp \boldsymbol{Q}_{\text {soft }}^{\boldsymbol{\pi}}(s, \boldsymbol{a})$, то она оптимальна.
Доказательство. Q-функция такой стратегии удовлетворяет мягкому уравнению оптимальности Беллмана (6.16) и в силу единственности его решения совпадает с $\boldsymbol{Q}_{\text {soft }}^{\boldsymbol{s}}(s, \boldsymbol{a})$.

Теперь можно построить аналог DQN для задачи Maximum Entropy RL, называемым Soft Q-learning. B нём нет отдельной модели актёра, и текущая стратегия просто полагается жадной по отношению к текущему критику. Это также можно интерпретировать как моделирование Soft Value Iteration - решение мягкого уравнения оптимальности (6.16). Тогда таргет для перехода $\mathbb{T} \equiv(s, a, r, s^{\prime})$ строится как

$$
y(\mathbb{T}) \equiv r+\gamma \log \int_{\mathcal{A}} \exp Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right) \mathrm{d} a^{\prime}
$$

где $\boldsymbol{\theta}^{-}$- параметры таргет-сети. Интересно, что это соответствует «учёту» Больцмановской стратегии исследования внутри оценочной функции (нечто похожее мы делали в табличном алгоритме SARSA в разделе 3.4.8). Однако, такой подход применим, как и в DQN, только для дискретных пространств действий, поскольку иначе стоящий в формуле интеграл мы аналитически не возьмём. Если мы попробуем завести отдельную нейросеть, чтобы приближать больцмановскую стратегию - получим SAC.

И, наконец, обсудим аналоги Policy Gradient алгоритмов для Maximum Entropy RL сеттинга. Многошаговые уравнения Беллмана выводится тривиально - достаточно учесть в наградах энтропийные бонусы. Ну а аналог формулы policy gradient тоже можно легко угадать; мы же знаем, что если из формулы градиента удалить мат.ожидание по частотам посещения состояний текущей стратегии, мы получаем формулу policy improvement-a. Это свойство в Maximum Entropy RL сохраняется, то есть можно просто к формуле soft policy improvement (6.13) добавить мат.ожидание по частотам посещения состояний. Выпишем, что получится для метода REINFORCE:

# Теорема 76 - Soft Policy Gradient: 

$$
\nabla_{\theta} J_{\text {soft }}(\theta)=\frac{1}{1-\gamma} \mathbb{E}_{s \sim d_{\pi_{\theta}}(s)} \mathbb{E}_{a \sim \pi_{\theta}(a \mid s)} \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q_{\text {soft }}^{\pi}(s, a)+\alpha \nabla_{\theta} \mathcal{H}\left(\pi_{\theta}(\cdot \mid s)\right)
$$

Доказательство. Аналогично доказательству в обычном случае.
Другими словами, сеттинг Maximum Entropy RL «объясняет» добавку энтропийного лосса при обучении актёра, но здесь ваджно помнить, что поменялось определение Q-функции: мягкая оценочная функция учитывает получение в будущем энтропийных бонусов (поэтому добавка энтропийного регуляризатора в обычных policy gradient алгоритмах остаётся эвристикой).

---

# Model-based 

В этой главе мы рассмотрим ситуацию, когда нам дана или мы готовы учить модель динамики среды.

## §7.1. Бандиты

### 7.1.1. Задача многоруких бандитов

Рассмотрим сильно упрощённую задачу RL, где эпизод заканчивается после первого шага.
Пример 98: В игровом зале стоят в ряд $|\boldsymbol{A}|$ игровых автоматов («одноруких бандитов»): в каждый можно отправить монетку, дёрнуть за ручку, и тогда автомат с некоторой вероятностью выдаст вам приз. Вероятность приза фиксирована для каждого автомата (зависит от настроек, выставленных владельцами зала...), но может различаться между автоматами.

Игроку (агенту) доступна только информация о том, получил ли он приз или нет, и всё, что он может - это пробовать дёргать за ручки разных автоматов. Задача - как можно быстрее выучить, какую ручку дёргать наиболее выгодно на основе накапливаемого опыта. Однако, без знания истинных вероятностей у игрока возникает вопрос: действительно ли тот автомат, который согласно его опыту выдавал приз чаще, является самым хорошим, или ему просто везло? Или с автоматом, который редко выдавал приз, просто было сыграно слишком мало игр? Налицо проблема exploration-exploitation дилеммы, которую в таком упрощённом виде легче теоретически анализировать.


Формально мы рассмотрим задачу RL для MDP, в котором нет состояний («есть только одно состояние»). Агент выбирает действие при помощи стратегии $\boldsymbol{\pi}(\boldsymbol{a})$ и получает какой-то приз из автомата с выбранным номером, после чего эпизод заканчивается. Формально мы будем интерпретировать это как стохастичную функцию награды, которая зависит только от выбранного игроком действия.
| Определение 83: Многорукий бандит (multi-armed bandit) - это $\boldsymbol{A}$ распределений на вещественной оси $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a}), \boldsymbol{a} \in\{1,2,3 \ldots \boldsymbol{A}\}$.

Понятно, что в данной задаче Q-функция не зависит от стратегии и всегда равна

$$
Q(a)=\mathbb{E}_{p(r \mid a)} r
$$

а оптимальная V-функция - скаляр, равный Q-функции оптимального действия, «наилучшего автомата»:

$$
V^{*}=\max _{a} Q(a)
$$

После каждого эпизода (т.е. после каждого шага) мы вольны улучшить свою стратегию, поэтому с каждым эпизодом наша стратегия меняется. Полный процесс, соответственно, задан следующим образом: на $\boldsymbol{k}$-ом эпизоде мы сэмплируем $\boldsymbol{a}_{\boldsymbol{k}} \sim \boldsymbol{\pi}_{\boldsymbol{k}}(\boldsymbol{a})$ и наблюдаем награду $\boldsymbol{r}_{\boldsymbol{k}} \sim \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{a}_{\boldsymbol{k}}\right)$, затем как-то меняем свою стратегию и получаем $\pi_{k+1}$.

Допустим, всего проводится $\boldsymbol{T}$ итераций обучения (играется $\boldsymbol{T}$ эпизодов). Если бы мы знали оптимальную ручку, мы бы сыграли все $\boldsymbol{T}$ эпизодов именно с ней, получив $\boldsymbol{T} \boldsymbol{V}^{*}$ награды. Но нам придётся сколько-то итераций потратить на поиск этой самой ручки - на «исследования» - и за счёт этого мы проиграем за каждое неоптимальное действие $a_{k}$ в среднем объёме $\boldsymbol{V}^{*}-\boldsymbol{Q}\left(\boldsymbol{a}_{\boldsymbol{k}}\right)$.

---

Определение 84: Сожалением (regret) за $\boldsymbol{T}$ эпизодов называется величина

$$
\operatorname{Regret}_{T}:=T V^{*}-\sum_{k=0}^{T} Q\left(a_{k}\right)
$$

Задача многорукого бандита - поиск такой процедуры обучения $\boldsymbol{\pi}$, которая минимизировала бы наши средние сожаления. В первую очередь здесь интересны асимптотически оптимальные стратегии с точки зрения сожалений: что $\lim _{\boldsymbol{T} \rightarrow \infty} \mathbb{E} \operatorname{Regret}_{\boldsymbol{T}}$, где мат.ожидание берётся по $\boldsymbol{\pi}_{\mathbf{1}}, \boldsymbol{\pi}_{\mathbf{2}} \ldots$, ведёт себя в некотором смысле наилучшим возможным образом.

# 7.1.2. Простое решение 

Единственный способ оценивать $\boldsymbol{Q}(\boldsymbol{a})$ - по Монте-Карло. На $\boldsymbol{k}$-ом шаге мы можем оценить каждую Qфункцию как

$$
Q_{k}(a):=\frac{\sum_{k} r_{k}\left[a_{k}=a\right]}{\sum_{k}\left[a_{k}=a\right]}
$$

Как мы обсуждали в разделе про экспоненциальное сглаживание, обновление такой Монте-Карло оценки $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{a})$ через счётчики по сути и есть обучение: пусть $\boldsymbol{n}_{\boldsymbol{k}}(\boldsymbol{a})$ - счётчик, сколько раз мы выбирали действие $\boldsymbol{a}$ за все эпизоды, тогда:

$$
\begin{aligned}
Q_{k}\left(a_{k}\right) & =\left(1-\frac{1}{n_{k}\left(a_{k}\right)}\right) Q_{k-1}\left(a_{k}\right)+\frac{1}{n_{k}\left(a_{k}\right)} r_{k}= \\
& =Q_{k-1}\left(a_{k}\right)+\frac{1}{n_{k}\left(a_{k}\right)}\left(r_{k}-Q_{k-1}\left(a_{k}\right)\right)
\end{aligned}
$$

Весь вопрос заключается в том, как на основе этих оценок и знания, сколько раз какое действие было попробовано - величины $\boldsymbol{n}_{\boldsymbol{k}}\left(\boldsymbol{a}_{\boldsymbol{k}}\right)$ - выбирать действия. Понятно, что жадный выбор может завести нас в ситуацию, когда мы будем всё время играть с не самой оптимальной ручкой.

Утверждение 74: При любом алгоритме скорость роста сожалений не более чем линейная: для некоторой константы $\boldsymbol{C}$

$$
\mathbb{E} \operatorname{Regret}_{T} \leq C T
$$

Доказательство. Рассмотрим худшую стратегию, которая всегда выбирает худшее действие с наибольшими сожалениями $\max _{a} \left(V^{*}-Q(a)\right)$. Тогда сожаления за $T$ эпизодов будут равны

$$
\operatorname{Regret}_{T}=T \max _{a}\left(V^{*}-Q(a)\right)
$$

Понятно, что это верхняя оценка на сожаления любого алгоритма, и $\max _{a}\left(V^{*}-Q(a)\right)$ - константа $\boldsymbol{C}$ в линейной скорости роста.

Утверждение 75: При жадном выборе сожаления растут с линейной скоростью: для некоторой константы $\boldsymbol{C}$

$$
\mathbb{E} \operatorname{Regret}_{T} \geq C T
$$

Доказательство. С некоторой вероятностью $\boldsymbol{\alpha}$ жадный алгоритм застрянет на постоянном выборе автомата $\boldsymbol{a}$ с ненулевым регретом $\boldsymbol{V}^{*}-\boldsymbol{Q}(\boldsymbol{a})$ и продолжит выбирать его до бесконечности. Тогда в среднем регрете появится слагаемое $\boldsymbol{T} \boldsymbol{\alpha}\left(\boldsymbol{V}^{*}-\boldsymbol{Q}(\boldsymbol{a})\right)$, следовательно как минимум можно в качестве константы $\boldsymbol{C}$ выбрать $\boldsymbol{\alpha}\left(\boldsymbol{V}^{*}-\boldsymbol{Q}(\boldsymbol{a})\right)$.

Наивное решение - решать проблему эксплорейшна при помощи $\varepsilon$-жадной стратегии 3.36.

## Алгоритм 25: Наивный бандит

Гиперпараметры: $\varepsilon(k)$ - стратегия исследования
Инициализировать $\boldsymbol{Q}_{0}(\boldsymbol{a})$ произвольно
Обнулить счётчики $\boldsymbol{n}_{0}(\boldsymbol{a}):=\mathbf{0}$
На очередном шаге $k$ :

---

1. выбрать $\boldsymbol{a}_{\boldsymbol{k}}$ случайно с вероятностью $\boldsymbol{\varepsilon}(\boldsymbol{k})$, иначе $\boldsymbol{a}_{\boldsymbol{k}}:=\underset{\boldsymbol{a}_{\boldsymbol{k}}}{\operatorname{argmax}} \boldsymbol{Q}_{\boldsymbol{k}}\left(\boldsymbol{a}_{\boldsymbol{k}}\right)$
2. увеличить счётчик $\boldsymbol{n}_{\boldsymbol{k}}(\boldsymbol{a}):=\boldsymbol{n}_{\boldsymbol{k}-1}(\boldsymbol{a})+\left[\boldsymbol{a}_{\boldsymbol{k}}=\boldsymbol{a}\right]$
3. пронаблюдать $\boldsymbol{r}_{\boldsymbol{k}}$
4. обновить $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{a}):=\boldsymbol{Q}_{\boldsymbol{k}-1}(\boldsymbol{a})+\left[\boldsymbol{a}_{\boldsymbol{k}}=\boldsymbol{a}\right] \frac{1}{n_{\boldsymbol{k}}(\boldsymbol{a})}\left(r_{\boldsymbol{k}}-\boldsymbol{Q}_{\boldsymbol{k}-1}(\boldsymbol{a})\right)$

Утверждение 76: При $\boldsymbol{\varepsilon}$-жадном выборе сожаления всё равно растут с линейной скоростью: для некоторой константы $\boldsymbol{C}$

$$
\mathbb{E} \operatorname{Regret}_{T} \geq C T
$$

Доказательство. Поскольку на каждом шаге с вероятностью $\frac{5}{|\mathcal{A}|}$ мы выбираем некоторое неоптимальное действие $\boldsymbol{a}$ с ненулевым регретом $\boldsymbol{V}^{*}-\boldsymbol{Q}(\boldsymbol{a})$, в среднем регрете появится слагаемое $\boldsymbol{T}_{\frac{5}{|\mathcal{A}|}}\left(V^{*}-Q(a)\right)$, следовательно как минимум можно в качестве константы $\boldsymbol{C}$ выбрать $\frac{5}{|\mathcal{A}|}\left(V^{*}-Q(a)\right)$.

В случае нестационарных бандитов распределения $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$ тоже меняются со временем и куда-то плывут. Наивное решение в такой ситуации можно модифицировать костыльми. Во-первых, $\boldsymbol{\varepsilon}$ нельзя уменьшать к нулю, необходимо постоянно пробовать различные действия, поэтому можно выставить $\boldsymbol{\varepsilon}$ в константу. Во-вторых, вместо счётчиков будем обновлять информацию о Q-функции через экспоненциальное сглаживание:

$$
\boldsymbol{Q}_{k}\left(a_{k}\right)=\boldsymbol{Q}_{k-1}\left(a_{k}\right)+\boldsymbol{\alpha}\left(r_{k}-\boldsymbol{Q}_{k-1}\left(a_{k}\right)\right)
$$

где $\boldsymbol{\alpha}<\mathbf{1}$ - константный гиперпараметр.

# 7.1.3. Теорема Лаи-Роббинса 

Можно ли придумать что-то лучшее, чем линейная скорость роста регрета? Перепишем регрет чуть-чуть по-другому:

## Утверждение 77:

$$
\operatorname{Regret}_{T}=\sum_{a} n_{T}(a)\left(V^{*}-Q(a)\right)
$$

Поиятно, что действия с большим регретом $\boldsymbol{V}^{*}-\boldsymbol{Q}\left(\boldsymbol{a}_{\boldsymbol{k}}\right)$ нужно выбирать как можно меньше, то есть асимптотически уменьшать счётчик $\boldsymbol{n}_{\boldsymbol{T}}(\boldsymbol{a})$. С другой стороны, ясно, что если распределения $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{a}^{*}\right)$, где $\boldsymbol{a}^{*}-$ оптимальная ручка, и $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$ для некоторого другого действия $\boldsymbol{a}$ очень похожи, то различить эти два автомата будет тяжело (просто потому, что если распределения похожи, то и сэмплы из них будут очень похожи). Также ясно, что если распределения похожи, то есть надежда, что у них будут очень похожи средние, то есть регрет для такого действия $\boldsymbol{a}$ будет маленьким. И наоборот: если регрет за действие большой, у автомата наверняка сильно другое распределение, нежели чем у оптимального автомата, и тогда их наверняка можно как-то просто различить по выборкам сэмплов. Оказывается, можно получить следующую нижнюю оценку на асимптотическое поведение любого алгоритма:

Теорема 77 - Теорема Лаи-Роббинса (Lai-Robbins theorem):

$$
\mathbb{E} \operatorname{Regret}_{T} \geq \log T \sum_{a \neq a^{*}} \frac{V^{*}-Q(a)}{\operatorname{KL}\left(p(r \mid a) \| p\left(r \mid a^{*}\right)\right)}
$$

Бел доказательства.
Весьма сильное утверждение: оно говорит, что теоретически нельзя построить алгоритм с лучшим асимптотическим поведением регрета, чем $\log \boldsymbol{T}$. Константа имеет понятный физический смысл: каждый автомат вносит свой вклад в эту константу, пропорциональный неоптимальности действия (числитель) и обратно пропорциональный похожести распределения награды в этом автомате с оптимальным автоматом (знаменатель).
Определение 85: Будем говорить, что алгоритм асимптотически оптимален, если он имеет логарифмическую скорость роста среднего регрета.

---

# 7.1.4. Upper Confidence Bound (UCB) 

Попробуем поискать хорошую эвристику исследования среди алгоритмов следующего вида: на очередном шаге $\boldsymbol{k}$ будем выбирать действие по следующей формуле:

$$
a_{k}:=\underset{a}{\operatorname{argmax}}\left[Q_{k}(a)+U_{k}(a)\right]
$$

где $\boldsymbol{U}_{\boldsymbol{k}}(\boldsymbol{a})$ - некоторая положительная добавка, имеющая смысл бонуса за исследования (exploration bonus). То, что добавка должна быть положительна, следует из принципа оптимизма перед неопределённостъю (optimism in the face of uncertainty).

Пример 99: Представьте, что вы идёте мимо пещеры, в которую вы никогда не заходили, и ваша оценка Qфункции для действия «зайти в пещеру» ниже, чем оценка других действий. Если алгоритм исследования таков, что ваше значение Q-функции занижается, то может возникнуть ситуация, что вы никогда не зайдёте в пещеру и не узнаете, что там. Если бы вы были уверены в идеальности ваших оценок, вы бы имели гарантии неоптимальности действия «зайти в пещеру»; но в реальности оценки никогда не бывают точны, и поэтому алгоритму исследований следует «завышать» те значения Q-функции, которые потенциально посчитаны ошибочно. При этом, чем больше неопределённость в их значениях, тем больше должно быть завышение.

Строить добавку нужно из соображений, вытекающих из формы регрета (7.1). Добавка должна быть маленькая, если данное действие было выбрано уже много раз, и наша неопределённость в знаниях о среднем значении $\boldsymbol{Q}(\boldsymbol{a})$ достаточно точные, или же если нам кажется, что регрет для этого действия близок к нулю.

Идея upper confidence bounds (UCB) алгоритмов следующая: давайте выбором $\boldsymbol{U}_{\boldsymbol{k}}(\boldsymbol{a})$ прогарантируем, что

$$
Q(a) \leq Q_{k}(a)+U_{k}(a)
$$

с очень высокой вероятностью, близкой к единице, то есть, другими словами, построим доверительный интервал (confidence interval) и возьмём его верхнюю границу. Такой $\boldsymbol{U}_{\boldsymbol{k}}(\boldsymbol{a})$ будет обратно пропорционален $\boldsymbol{n}_{\boldsymbol{k}}(\boldsymbol{a})$, ведь граница будет сжиматься к эмпирическому среднему. Жадный выбор $\underset{a}{\operatorname{argmax}} \boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{a})$, интуитивно, будет выбираться часто; его счётчик будет увеличиваться, и exploration bonus для него будет уменьшаться; тогда с достаточно маленькой вероятностью мы выберем при помощи формулы (7.2) действительно неоптимальное действие, для которого добавка $\boldsymbol{U}_{\boldsymbol{k}}(\boldsymbol{a})$ в силу его редкого выбора большая, и эта вероятность будет тем меньше, чем меньше эмпирическое среднее для этого неоптимального действия.

Пример 100: На картинке справа изображены «свечки»: для каждого из четырёх автоматов указаны средние (оценки $\left.\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{a})\right)$, а также верхние и нижние границы доверительного интервала. Хотя автомат В кажется наиболее выгодным, мы уже достаточно часто играли с ним, поэтому его верхняя граница интервала доверия не так далека от среднего; а вот с автоматом А мы играли редко и поэтому на очередном шаге решим выбрать именно его.


Доверительный интервал можно построить при помощи следующего неравенства:
Теорема 78 - Неравенство Хёфдинга: Пусть $\boldsymbol{X}_{1} \ldots \boldsymbol{X}_{\boldsymbol{n}}$ - i.i.d выборка из распределения на домене ${ }^{a}[\mathbf{0 , 1}] \mathrm{c}$ истинным средним $\boldsymbol{\mu}, \hat{\boldsymbol{\mu}}:=\frac{1}{N} \sum_{i}^{N} \boldsymbol{X}_{i}-$ выборочная оценка среднего. Тогда $\forall \boldsymbol{u}>\mathbf{0}$ :

$$
\mathrm{P}(\mu \geq \hat{\mu}+u) \leq e^{-2 n u^{2}}
$$

Без доказательства.
${ }^{a}$ мы всегда предполагаем ограниченность наград; для удобства записи будем считать, что диапазон награды $-[\mathbf{0 , 1}]$.
Мы можем переформулировать эту теорему в терминах доверительного интервала:
Утверждение 78: Для любого $\boldsymbol{\delta}$ с вероятностью $\mathbf{1}-\boldsymbol{\delta}$ истинное значение $\boldsymbol{Q}(\boldsymbol{a})$ не превосходит $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{a})+\boldsymbol{U}_{\boldsymbol{k}}(\boldsymbol{a})$, где:

$$
U_{k}(a):=\sqrt{\frac{-\ln \delta}{2 n_{k}(a)}}
$$

---

Доказательство. В силу неравенства Хёфдинга:

$$
\mathbf{P}\left(Q(a) \geq Q_{k}(a)+U_{k}(a)\right) \leq \exp ^{-2 n_{k}(a) U_{k}(a)^{2}}
$$

Возьмём заданное $\boldsymbol{\delta}$ и прогарантируем, что $\exp ^{-2 n_{k}(a) U_{k}(a)^{2}}=\boldsymbol{\delta}$. Разрешая это равенство относительно $U_{k}(a)$, получаем доказываемое.

В качестве $\boldsymbol{\delta}$ на $\boldsymbol{k}$-ом шаге будем выбирать, например, $\frac{1}{k^{2}}$, где $\boldsymbol{c}$ - гиперпараметр. Таким образом, истинное значение будет всё с большей вероятностью оказываться внутри доверительного интервала. Итого в алгоритме UCB предлагается следующая формула выбора действия на очередном $\boldsymbol{k}$-ом шаге:

$$
a_{k}=\underset{a}{\operatorname{argmax}}\left[Q_{k}(a)+c \sqrt{\frac{\log k}{n_{k}(a)}}\right]
$$

Получается интерпретируемая формула: числитель $\sqrt{\log k}$ гарантирует, что если некоторое действие давно не выбиралось (счётчик $\boldsymbol{n}_{\boldsymbol{k}}(\boldsymbol{a})$ не меняется с увеличением числа итераций $\boldsymbol{k}$ ), то добавленное слагаемое будет неограниченно расти и в какой-то момент промотивирует попробовать данное действие ещё раз.

# Теорема 79: UCB-алгоритм (7.3) асимптотически оптимален. 

Без доказательства.

### 7.1.5. Сэмплирование Томпсона

Мы далее попробуем учить степень нашей неопределённости в знаниях об $\boldsymbol{Q}(\boldsymbol{a})$, моделируя всё неизвестное распределение $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$, а не только среднюю награду. Таким образом, мы перейдём к model-based подходу в RL, в котором динамику среды предлагается пытаться учить. В случае с бандитами под динамикой среды подразумеваются распределения награды для каждого автомата $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$, для чего мы воспользуемся стандартным байесовским подходом.

Введём предположение, что распределения наград $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$ принадлежат некоторому параметрическому семейству; для каждого автомата заданы значения параметров этого семейства $\boldsymbol{\theta}_{\boldsymbol{a}} \in \boldsymbol{\Theta}$, и награда генерируется из распределения $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{a}\right)$. Однако, истинные значения $\boldsymbol{\theta}_{a}$ нам неизвестны.

Вместо оценки максимального приядонодобия на $\boldsymbol{\theta}_{a}$ будем делать байесовский вывод. Зададимся некоторым априорным распределением («прайором») $\boldsymbol{p}\left(\boldsymbol{\theta}_{a}\right)$ для каждого действия $\boldsymbol{a}$ (сюда мы можем в том числе положить какую-то дополнительную априорную информацию о том, какие автоматы заведомо лучше), и после очередного эпизода игры с автоматом $\boldsymbol{a}$ с исходом $\boldsymbol{r}_{\boldsymbol{k}}$ будем обновлять распределение над $\boldsymbol{\theta}_{\boldsymbol{a}}$ по формуле Байеса на апостериорное распределение:

$$
p\left(\theta_{a}\right) \leftarrow p\left(\theta_{a} \mid r_{k}\right) \propto p\left(r_{k} \mid \theta_{a}\right) p\left(\theta_{a}\right)
$$

Таким образом мы аккумулируем всю информацию о значениях $\boldsymbol{\theta}_{a}$, полученную из всех имеющихся сэмплов награды и исходного априорного распределения. Средний выигрыш из автомата $\boldsymbol{a}$, соответственно равный $\mathbb{E} \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{a}\right)$, теперь для нас будет являться случайной величиной, поскольку $\boldsymbol{\theta}_{a}$ - случайное, с распределением $\boldsymbol{p}\left(\boldsymbol{\theta}_{a}\right)$.

Пример 101 - Bernoulli-бандиты: Положим, что автоматы выдают только награду 0 или 1 , то есть что истинное распределение есть распределение Бернулли с вероятностью $\boldsymbol{\theta}_{a}$ :

$$
p(r \mid a):=\operatorname{Bernoulli}\left(r \mid \theta_{a}\right)=\theta_{a}^{|[r=1]}\left(1-\theta_{a}\right)^{\langle[r=0]}=\theta_{a}^{r}\left(1-\theta_{a}\right)^{1-r}
$$

Априорное распределение зададим при помощи Бета-распределения ${ }^{1}$ распределения, а то есть:

$$
p\left(\theta_{a}\right):=\operatorname{Beta}\left(\theta_{a} \mid \alpha, \beta\right) \propto \theta_{a}^{\alpha-1}\left(1-\theta_{a}\right)^{\beta-1}
$$

где $\boldsymbol{\alpha}$ и $\boldsymbol{\beta}$ - некоторые параметры (свои для каждого автомата $\boldsymbol{a}$ ). Мы можем обновлять эти знания при помощи новых сэмплов $\boldsymbol{r}_{\boldsymbol{k}} \sim \boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$, проводя байесовский вывод. Применяем формулу Байеса:

$$
p\left(\theta_{a} \mid r\right) \propto \underbrace{\theta_{a}^{r_{k}}\left(1-\theta_{a}\right)^{1-r_{k}}}_{p\left(r_{k} \mid \theta_{a}\right)} \underbrace{\theta_{a}^{\alpha-1}\left(1-\theta_{a}\right)^{\alpha-1}}_{p\left(\theta_{a}\right)}=\operatorname{Beta}\left(\theta_{a} \mid \alpha+r_{k}, \beta+1-r_{k}\right)
$$

Таким образом, для обновления параметров $\boldsymbol{\alpha}, \boldsymbol{\beta}$ достаточно увеличить $\boldsymbol{\alpha}$ на $\boldsymbol{r}_{\boldsymbol{k}}$, а $\boldsymbol{\beta}$ - на $\mathbf{1}-\boldsymbol{r}_{\boldsymbol{k}}$.
При заданном $\boldsymbol{\theta}_{\boldsymbol{a}}$ мы можем посчитать среднее значение выигрыша, ценность автомата $\boldsymbol{a}$ : для случайной Бернуллиевской величины с параметром $\boldsymbol{\theta}_{\boldsymbol{a}}$ среднее, сообственно, совпадает с $\boldsymbol{\theta}_{\boldsymbol{a}}$ :

$$
\hat{Q}(a)=\mathbb{E} p\left(r \mid \theta_{a}\right)=\theta_{a}
$$

---

Тогда, имея $\boldsymbol{\alpha}$ и $\boldsymbol{\beta}$, мы всегда можем посчитать мат.ожидание выигрыша в нашей модели:

$$
\mathbb{E}_{\boldsymbol{\theta}_{a}} \tilde{\boldsymbol{Q}}(\boldsymbol{a})=\mathbb{E}_{\mathbf{B e t a}\left(\boldsymbol{\theta}_{a} \mid \boldsymbol{\alpha}, \boldsymbol{\beta}\right)} \boldsymbol{\theta}_{\boldsymbol{a}}=\frac{\boldsymbol{\alpha}}{\boldsymbol{\alpha}+\boldsymbol{\beta}}
$$

Как видно, оно не отличается от Монте-Карло оценки Q-функции: $\boldsymbol{\alpha}$ имеет смысл «успехов», когда Бернулливская величина выкинула нам единичку, а $\boldsymbol{\beta}$ - число неуспехов, нулей, и формула обновления этих параметров в точности соответствует подсчёту этих счётчиков; при выборе прайора $\boldsymbol{\alpha}=\boldsymbol{\beta}=\mathbf{0}$ мы получим в формуле мат.ожидания отношение числа успехов к общему числу попыток.

Однако теперь у нас есть не только оценка среднего значения $\boldsymbol{\theta}_{\boldsymbol{a}}$, но и вероятности для каждого его возможного значения. На рисунке далее приведены $\boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{a}}\right)$ для разных значений $\boldsymbol{\alpha}$ («успехов») и $\boldsymbol{\beta}$ («неудач») при прайоре $\boldsymbol{\alpha}=\boldsymbol{\beta}=\mathbf{1}$ :


Как пользоваться полученным апостериорным распределением? Мы можем реализовать идею, которая называется probability matching. Согласно текущим $\boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{a}}\right)$ жадный выбор действия имеет вид

$$
\boldsymbol{a}:=\underset{\boldsymbol{a}}{\operatorname{argmax}} \mathbb{E}_{\boldsymbol{\theta}_{\boldsymbol{a}} \sim \boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{a}}\right)} \mathbb{E} \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{\boldsymbol{a}}\right)
$$

но теперь в нашей модели есть некоторая вероятность и того, что такое действие на самом деле неоптимально. Мы можем посчитать эту вероятность, то есть с учётом распределений $\boldsymbol{\theta}_{\boldsymbol{a}}$ посчитать вероятность, что хотя бы для одного другого автомата $\hat{\boldsymbol{a}} \neq \boldsymbol{a}$

$$
\mathbb{E} \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{\hat{\boldsymbol{a}}}\right)>\mathbb{E} \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{\boldsymbol{a}}\right)
$$

и посмотреть, с какой вероятностью мы ошибёмся. Аналогично можно для любого действия посчитать вероятность того, что на самом деле оптимальным является оно. Предлагается ровно с такими вероятностями, собственно, и выбирать автомат на очередном шаге.

Определение 86: Сэмплированием Томпсона (Thompson Sampling) называется процедура, при котором на $\boldsymbol{k}$-ом шаге при решении задачи многоруких бандитов действие $\boldsymbol{a}$ выбирается с вероятностью того, что оно оптимально в рамках выученных моделей $\boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{a}}\right)$ :

$$
\pi(\boldsymbol{a}):=\mathbf{P}\left(\mathbb{E} \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{\boldsymbol{a}}\right)=\max _{\hat{\boldsymbol{a}}} \mathbb{E} \boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{\hat{\boldsymbol{a}}}\right)\right)
$$

Посчитать такие вероятности может быть нетривиально, зато легко из такого распределения сэмплировать (отсюда название ${ }^{2}$ ). Действительно: просто для каждого действия $\boldsymbol{a}$ засэмплируем $\boldsymbol{\theta}_{\boldsymbol{a}}$ из текущего $\boldsymbol{p}\left(\boldsymbol{\theta}_{\boldsymbol{a}}\right)$, и, соответственно, выберем автомат с наибольшим мат.ожиданием согласно выпавшим $\boldsymbol{\theta}_{\boldsymbol{a}}$.

Пример 102: Допустим, мы храним информацию о распределениях $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$ в Бета-распределениях, как в предыдущем примере. Для всех действий сэмплируем $\boldsymbol{\theta}_{\boldsymbol{a}} \sim \operatorname{Beta}\left(\boldsymbol{\theta}_{\boldsymbol{a}} \mid \boldsymbol{\alpha}_{\boldsymbol{a}}, \boldsymbol{\beta}_{\boldsymbol{a}}\right)$ при выученных параметрах $\boldsymbol{\alpha}_{\boldsymbol{a}}, \boldsymbol{\beta}_{\boldsymbol{a}}$, и считаем мат.ожидание соответствующего распределения Бернулли при выпавших параметрах $\boldsymbol{\theta}_{\boldsymbol{a}}$ -

[^0]
[^0]:    ${ }^{2}$ забавный факт: Томпсон вообще первый рассмотрел задачу многоруких бандитов, и в качестве эвристического решения предложил сэмплировать из моделей и брать аргмаксимум из сэмплов. То есть, можно сказать, сэмплирование Томпсона было первым придуманным способом решения задачи. А позже через почти век выяснилось, что это решение асимптотически оптимально.

---

для распределения Бернулли среднее совпадает с $\boldsymbol{\theta}_{a}$ в точности. Таким образом, мы выберем автомат, для которого выпало наибольшее значение сэмпла $\boldsymbol{\theta}_{a}$.

Идея в том, что для автоматов, которые мы пробовали часто, дисперсия апостериорного распределения будет маленькой и сэмпл почти всегда будет в точности равен текущей оценке Q-функции. Для автоматов, которые мы пробовали редко в силу маленького реварда, дисперсия будет большая и иногда сэмплы будут оказываться лучше текущего самого хорошего автомата, что заставит нас поэксплорить данный автомат ещё.

# Алгоритм 26: Beta-Pernotilli Бандит с сэмплированием Томпсона 

Гиперпараметры: $\boldsymbol{\alpha}, \boldsymbol{\beta}$ - параметры прайора.
Обнулить счётчики $\boldsymbol{\alpha}_{0}(a):=\boldsymbol{\alpha}, \boldsymbol{\beta}_{0}(a):=\boldsymbol{\beta}$
На очередном шаге $k$ :

1. сгенерировать $\boldsymbol{\theta}_{a} \sim \operatorname{Beta}\left(\boldsymbol{\alpha}_{k}(a), \boldsymbol{\beta}_{k}(a)\right)$
2. выбрать $a_{k}:=\underset{a}{\operatorname{argmax}} \theta_{a}$
3. пронаблюдать $r_{k} \in\{0,1\}$
4. обновить счётчик $\boldsymbol{\alpha}_{k}(a):=\boldsymbol{\alpha}_{k-1}(a)+\left[a_{k}=a\right]\left[r_{k}=1\right]$
5. обновить счётчик $\boldsymbol{\beta}_{k}(a):=\boldsymbol{\beta}_{k-1}(a)+\left[a_{k}=a\right]\left[r_{k}=0\right]$

Теорема 80: Сэмплирование Томпсона асимптотически оптимально.
Без доказательства.
Пример 103: Понграться с визуализацией этого алгоритма можно здесь.

### 7.1.6. Обобщение на табличные MDP

Теория бандитов подсказывает, как можно «правильно» разрешать exploration-exploitation дилемму. K сожалению, эти идеи плохо масштабируются на произвольные MDP. Подход на основе UCB-счётчиков (7.3) можно эвристически моделировать в сложных MDP, что мы обсудим позже в главе 8.2. Сэмплирование Томпсона же может оказаться удобным способом справиться с некоторыми практическими задачами табличного RL.

Итак, пусть задано табличное MDP с конечным числом состояний и действий. Будем в байесовском смысле учить функцию переходов и функцию награду, то есть моделировать вероятности $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{s, a}\right) \approx \boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a})$ и $\boldsymbol{p}\left(s^{\prime} \mid \phi_{s, a}\right) \approx \boldsymbol{p}\left(s^{\prime} \mid s, a\right)$ и обновлять $\boldsymbol{\theta}$ и $\boldsymbol{\phi}$ по правилу Байеса. Для этого для каждой пары $s, a$ мы будем хранить «текущее» распределение $\boldsymbol{p}\left(\boldsymbol{\theta}_{s, a}\right)$ и $\boldsymbol{p}\left(\phi_{s, a}\right)$, обусловленные на всю встретившуюся историю. Фактически это будут вероятности, с которыми истинное MDP живёт по тем или иным законам. После очередного перехода $\left(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right.$ ) будем обновлять распределения над параметрами по формулам

$$
\begin{gathered}
p\left(\theta_{s, a}\right) \leftarrow p\left(\theta_{s, a} \mid r\right) \propto p\left(r \mid \theta_{s, a}\right) p\left(\theta_{s, a}\right) \\
p\left(\phi_{s, a}\right) \leftarrow p\left(\phi_{s, a} \mid s^{\prime}\right) \propto p\left(s^{\prime} \mid \phi_{s, a}\right) p\left(\phi_{s, a}\right)
\end{gathered}
$$

Далее на очередном шаге мы будем из такого «распределения в пространстве MDP» сэмплировать MDP: $\forall s, a:$

$$
\theta_{s, a} \sim p\left(\theta_{s, a}\right), \quad \phi_{s, a} \sim p\left(\phi_{s, a}\right)
$$

Конкретные значения $\boldsymbol{\theta}$ и $\boldsymbol{\phi}$ теперь задают какое-то одно конкретное MDP, в котором мы можем любым алгоритмом динамического программирования (например, Value Iteration) найти оптимальную стратегию $\boldsymbol{\pi}^{*}(a \mid$ $\boldsymbol{s}, \boldsymbol{\theta}, \boldsymbol{\phi})$. Ей и будем пользоваться, чтобы сыграть, например, очередной эпизод игры и собирать новые данные для уточнения модели функции переходов и функции наград.

Такая процедура называется posterior sampling, и является аналогом сэмплирования Томпсона для табличных MDP: мы будем выбирать действие с той вероятностью, с которой оно является оптимальным, обусловленную на всю историю нашего взаимодействия с настоящим MDP. Понятнее идея может стать на следующем примере.

Пример 104 - Online Shortest Path: Каждый день с утра агент едет на машине на работу. Карта представляет собой граф из нескольких вершин, некоторые из которых соединены рёбрами. Агент хочет кратчайшим маршрутом добираться из вершины «дом» в вершину «работа». Такая задача решалась бы алгоритмом поиска

---

кратчайшего пути в графе, если бы агенту были известны точные значения, сколько времени нужно на проезд по каждому ребру. Но это какие-то случайные величины, и каждый раз, когда агент проезжает по тому или иному ребру $\boldsymbol{e}$, он тратит случайное время $\boldsymbol{r} \sim \boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{e})$. Как ездить на работу так, чтобы постепенно, с течением процесса суммарное время («регрет»), которое агент тратил на поездки, было всё меньше и меньше?

В этой задаче вершины можно считать состояниями $\boldsymbol{s}$, а рёбра можно считать парами $\boldsymbol{s}, \boldsymbol{a}$. Агент, допустим, знает карту, то есть знает, что «функция переходов» - детерминированная, и знает $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, но не знает распределение наград. Поэтому в этой задаче достаточно учить лишь второе.

Итак, что говорит сэмплирование Томпсона: а давайте мы для каждого ребра $\boldsymbol{e}$ заведём модель $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{e}\right)$, и будем в байесовском смысле учить $\boldsymbol{\theta}_{e}$. Здесь $\boldsymbol{r}$ - вещественное, поэтому понадобится проводить байесовский вывод для непрерывных распределений*. Допустим, мы выбрали какое-то семейство $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{e}\right)$, завели какой-то прайор $\boldsymbol{p}\left(\boldsymbol{\theta}_{e}\right)$ и научились обновлять апостериорное распределение по формуле Байеса.

Тогда перед очередным эпизодом мы сэмплируем $\boldsymbol{\theta}_{e} \sim \boldsymbol{p}\left(\boldsymbol{\theta}_{e}\right)$, и теперь у нас есть конкретное MDP с конкретной моделью наград. Мы можем посчитать среднее $\operatorname{Ep}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{e}\right)$ и считать, что в среднем ровно столько времени потребуется для проезда по ребру $\boldsymbol{e}$. Затем мы в графе можем найти кратчайший маршрут (поиск кратчайшего маршрута в общем-то является частным случаем общего алгоритма динамического программирования Value Iteration), и алгоритм предлагает отправиться именно по нему. Собранные награды за шаг затраченное время для проезда по каждому ребру - используются для уточнения распределений $\boldsymbol{p}\left(\boldsymbol{\theta}_{e}\right)$, и модель всё улучшается.

[^0]
# §7.2. Обучаемые модели окружения 

### 7.2.1. Планирование

Понятно, что model-free алгоритмы, рассмотренные ранее, действуют не совсем так, как размышляет человек. Принимая решения, мы принимаем в учёт наши предсказания о том, в какое будущее выльется то или иное действие, и для построения предсказаний используем наши представления о том, как работает окружающий нас мир.

Итак, вернёмся к общей постановке задачи RL, и сейчас допустим, что нам доступен симулятор $\boldsymbol{p}\left(\boldsymbol{s}^{\prime}, \boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ для произвольных $\boldsymbol{s}, \boldsymbol{a}$ в произвольный момент времени.

В табличном сеттинге знание $\boldsymbol{p}\left(\boldsymbol{s}^{\prime}, \boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ позволяет применять алгоритмы динамического программирования Value Iteration и Policy Iteration. Однако, в табличном сеттинге мы умеем считать все интегралы мат. ожидания по функции переходах; а в сложных средах знание функции переходов эти интегралы брать не позволяют.

Понятное дело, что, имея доступ к точному симулятору, мы можем, например, запустить какой-нибудь modelfree алгоритм для обучения $\boldsymbol{\pi}$, но нас интересует, можем ли мы непосредственно «воспользоваться» симулятором внутри самой стратегии, или даже придумать стратегию, использующую только симулятор. Раз мы полностью знаем правила игры, то, выбирая действие в некотором состоянии $\boldsymbol{s}_{0}$, мы можем играть «виртуальные» игры, заглядывая в своё будущее.
|| Определение 87: Для данного состояния $\boldsymbol{s}_{0}$ набор действий $\boldsymbol{a}_{0}, \boldsymbol{a}_{1}, \boldsymbol{a}_{2}, \ldots$ будем называть планом (plan).
Итак, допустим мы сидим в некотором $\boldsymbol{s}_{0}$ и хотим найти хорошее действие $\boldsymbol{a}_{0}$. Мы можем выбрать какое-нибудь $\boldsymbol{a}_{0}$, засэмплировать $\boldsymbol{s}_{1}$, выбрать как-то $\boldsymbol{a}_{1}$, и так построить какой-нибудь план, для которого у нас будет сэмпл будущей награды. Можем так сделать, скажем, много раз. Нас ждёт две проблемы: у нас не получится рассмотреть всевозможные варианты будущего, и мы не сможем посчитать интеграл средней награды для одного плана; мы можем получить лишь сэмпл или Монте-Карло оценку


по нескольким сэмплам.

Пример 105: Какой типичный способ построения ИИ для ботов в играх: для текущего состояния $\boldsymbol{s}$, в котором нужно выбрать действие, рассматривается несколько вариантов дальнейших действий на ближайшие шагов сто (при этом желательно заведомо неоптимальные варианты отсекать, чтобы сократить перебор); для каждого плана проводится симуляция (или несколько, если правила игры стохастичны и время позволяет). Играть до конца эпизода обычно дороговато, поэтому состояние $\hat{\boldsymbol{s}}$, на котором симуляцию игры прервали, придётся оценивать при помощи какой-то эвристичной оценки $\boldsymbol{V}^{*}(\hat{\boldsymbol{s}})$.

Подобные алгоритмы могут «спланировать» свои будущие действия, а не выучить непосредственно зависимость $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})$ («обучить стратегию»).
*например, если моделировать $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{e}\right)$ в виде нормального распределения с настраиваемыми средним и дисперсией, $\boldsymbol{p}(\boldsymbol{r} \mid$ $\boldsymbol{\theta}_{e}$ ) $:=\boldsymbol{\mathcal { N }}\left(\boldsymbol{\mu}_{e}, \boldsymbol{\sigma}_{e}^{*}\right)$, то тогда в качестве сопряжённого распределения, как подсказывает википедия, понадобится выбрать гамманормальное распределение. Формулы вывода тогда можно посмотреть, например, здесь.


[^0]:    *например, если моделировать $\boldsymbol{p}\left(\boldsymbol{r} \mid \boldsymbol{\theta}_{e}\right)$ в виде нормального распределения в дисперсией, $\boldsymbol{p}(\boldsymbol{r} \mid$ $\boldsymbol{\theta}_{e}$ ) $:=\boldsymbol{\mathcal { N }}\left(\boldsymbol{\mu}_{e}, \boldsymbol{\sigma}_{e}^{*}\right)$, то тогда в качестве сопряжённого распределения, как подсказывает википедия, понадобится выбрать гамманормальное распределение. Формулы вывода тогда можно посмотреть, например, здесь.

---

при доступной функции переходов $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ и функции награды называется планированием (planning).
В такой концепции понятия обучения нет. Для детерминированных MDP задача планирования по определению даёт оптимальный план, если задача (7.4) решена точно, но для стохастичных MDP это неверно:

Теорема 81 - Неоптимальность планирования: Пусть MDP стохастично и $\boldsymbol{a}_{0}, \boldsymbol{a}_{1}, \boldsymbol{a}_{2} \ldots$ - точное решение задачи (7.4) для $\boldsymbol{s}_{0}$. Тогда может быть такое, что действие $\boldsymbol{a}_{0}$ неоптимально $\left(\boldsymbol{Q}^{*}\left(\boldsymbol{s}_{0}, \boldsymbol{a}_{0}\right)<\boldsymbol{Q}^{*}\left(\boldsymbol{s}_{0}, \boldsymbol{a}\right)\right.$ для некоторого $\left.\boldsymbol{a} \neq \boldsymbol{a}_{0}\right)$.

Доказательство. Посмотрим, чему равна средняя награда для каждого плана для MDP с рисунка.


Другими словами, планирование - разумный подход в детерминированных средах, когда будущее предопределено. Поэтому большинство планировщиков, которые мы будем обсуждать далее, заточены именно под детерминированный случай. Важно, что даже в такой ситуации решать задачу (7.4) точно, конечно же, не получится, и наше решение будет лишь приближённым.

По этим двум причинам планировщик иметь смысл перезапускать на каждом шаге. Обычно алгоритм решения задачи планирования используется как просто «очень вычислительно дорогая» стратегия: сидя в $\boldsymbol{s}_{0}$, алгоритм планирует совершить действия $\boldsymbol{a}_{0}, \boldsymbol{a}_{1}, \boldsymbol{a}_{2} \ldots$; в реальной среде совершается лишь действие $\boldsymbol{a}_{0}$, после чего для $\boldsymbol{s}_{1} \sim \boldsymbol{p}\left(\boldsymbol{s}_{1} \mid \boldsymbol{s}_{0}, \boldsymbol{a}_{0}\right)$ алгоритм планирования запускается ещё раз (важно, что тут он может использовать какую-то информацию с прошлого процесса планирования) для определения $\boldsymbol{a}_{1}$ и так далее. Такая процедура называется планированием с обратной связью. Планировщик, в частности, может понимать, что он не рассмотрел всевозможные исходы будущего, и из этих соображений выдать вероятностное распределение $\boldsymbol{\pi}\left(\boldsymbol{a}_{0} \mid \boldsymbol{s}_{0}\right)$; тогда стратегия, полученная при помощи такого планировщика, будет стохастична.

Пример 106 - Мета-эвристики как планировщики: Для решения (7.4) можно использовать, например, случайный поиск. Сидя в $\boldsymbol{s}_{0}$, засэмплируем 100 случайных планов и при помощи симулятора посчитаем для каждого из них сэмплы $\boldsymbol{R}(\boldsymbol{T})$; выберем то $\boldsymbol{a}_{\mathbf{0}}$, для которого напласть траектория с максимальным сэмплом reward-to-go. Если среда детерминирована, то $\boldsymbol{a}_{0}$ гарантировано позволяет получить такую награду; что не означает, что мы нашли наилучший план, и оптимальным может оказаться другое действие. Если среда стохастична, то даже гарантий получить «найденный» reward-to-go нет, поскольку это лишь сэмпл и нам могло в симуляции просто повезти. Выбранное $\boldsymbol{a}_{0}$ отправляется в среду; для сэмпла $\boldsymbol{s}_{1}$ из реальной среды процесс планирования запускается заново.

То есть, обучения в такой концепции нет, но каждое принятие решения алгоритмом (каждый «запуск стратегии») - это целый процесс оптимизации, что обычно очень дорого.

# 7.2.2. Модели мира (World Models) 

В реальной жизни в рамках нашей общей постановки задачи функция переходов и награды $\boldsymbol{p}\left(\boldsymbol{s}^{\prime}, \boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ нам неизвестна. Тем не менее, у нас есть опция как-то попытаться её выучить и как-то использовать для улучшения наших алгоритмов. Открывается два вопроса: как учить и как использовать.
| Определение 89: Моделъю мира (world model) называется любая модель, явно или неявно обучающаяся модели динамики среды.

Под «явно или неявно» подразумевается, что это не обязательно должна быть функция, по состоянию и действию возвращающая следующее состояние, хотя, конечно, это самая стандартная опция.

Модель мира также может сжимать описание состояний в некоторое латентное пространство, которое далее может использовать стратегия. Ещё можно нарушить end-to-end парадигму и в целом отдельно выучить автокодировщик, который сожмёт описание состояний в некоторый компактный векторочек; RL

---

же применять только для обучения самой стратегии, которая будет принимать на вход такое компактное описание состояний. Пример подобного сжатия.

Простейший способ «использовать» выученную модель - взять в качестве стратегии какой-нибудь планировщик, который будет работать с обученной аппроксимацией модели мира будто это истинная модель. Нюанс такого подхода заключается в том, что планировщик по определению «хорош» только если модель мира точна. Если модель мира обучается только на тех примерах, которые мы встречаем в нашем опыте, то мы рискуем познать только тот мир, который обозревает наша текущая стратегия. До «генеральной совокупности» подобному опыту может быть далеко, и поэтому необходимо постоянно дообучать модель мира после каждого улучшения стратегии.

# Алгоритм 27: Общая схема Model-based подхода 

Гиперпараметры: Планировщик, модель мира.
Проинициализировать стратегию $\boldsymbol{\pi}_{0}(\boldsymbol{a} \mid \boldsymbol{s})$ случайно.
Проинициализировать модель мира случайно.
Проинициализировать датасет пустым множеством.
На $\boldsymbol{k}$-ом шаге:

1. Повзаимодействовать со средой при помощи $\boldsymbol{\pi}_{\boldsymbol{k}}$, добавив встреченные переходы $\left\{\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}\right\}$ в датасет.
2. Провести дообучение модели мира на собранном датасете:
3. Получить $\boldsymbol{\pi}_{\boldsymbol{k}}$ из планировщика, используя текущую модель мира.

Под словами «получить $\boldsymbol{\pi}_{\boldsymbol{k}}$ из планировщика» здесь понимается, что далее в качестве стратегии $\boldsymbol{\pi}_{\boldsymbol{k}}$ используется запуск планировщика, который кушает текущее состояние, рассматривает какие-то планы и выдаёт первое действие наилучшего плана $\boldsymbol{a}_{0}$ или какое-то распределение в пространстве действий, «вероятности того, что $\boldsymbol{a}_{0}$ соответствует оптимальному плану».

### 7.2.3. Модель прямой динамики

| Определение 90: Модель функции переходов $\boldsymbol{p}_{\boldsymbol{\theta}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ называется $\boldsymbol{\text { моделью прямой динамики }}$ (forward dynamics model).

Учить генеративную модель может быть дороговато, и простым удешевлением является обучение детерминированного приближения $s^{\prime} \approx f_{\theta}(s, a)$. Это можно делать по любым доступным траекториям, собранным любым способом:


$$
\begin{aligned}
& \sum_{s, a, s^{\prime}}\left\|f_{\theta}(s, a)-s^{\prime}\right\|^{2} \rightarrow \min _{\theta} \\
& \sum_{s, a, r}\left(r_{\psi}(s, a)-r\right)^{2} \rightarrow \min _{\psi}
\end{aligned}
$$

Этот лобовой подход многим плох. В сложных средах описание состояний обычно содержит огромное количество никак не связанной с задачами агента информацией; например, декоративные элементы в видеоиграх. Обучение подобной $\boldsymbol{f}_{\boldsymbol{\theta}}$ сопряжено с бессмысленным изучением этой информации. Наконец, если состояния $\boldsymbol{s}$ картинки, то в текущей формуле сгенерированное изображение сравнивается с реальным $\boldsymbol{s}^{\prime}$ по 12 -метрике; для изображений это не сильно осмысленно, конечно, и нужно придумывать что-то другое.

Иногда в задачах робототехники можно в модель прямой динамики внести «inductive bias», используя знания о том, что модель динамики представляет собой какое-то дифференциальное уравнение. Тогда могут искаться параметры функций, стоящих внутри уравнений связи, а в качестве итоговой модели использоваться какая-то дискретизация «обученного» диффура.

---

# 7.2.4. Сновидения 

Наличие модели прямой динамики $\boldsymbol{p}_{\boldsymbol{\theta}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ позволяет нам при помощи текущей стратегии $\boldsymbol{\pi}$ генерировать траектории, используя полностью наши «внутренние модели» и никак не используя реальную внешнюю среду. Это означает, что в случае обучаемой модели динамики мы теоретически можем начать симулировать опыт при помощи, условно, нашей генеративной модели, и обучать на нём model-free алгоритм.

Определение 91: Сновидениями (dreaming) называется обучение агента на опыте, собранном при помощи приближения динамики среды $\boldsymbol{p}_{\boldsymbol{\theta}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a}\right)$.

Далеко мы так, конечно, не уедем, поскольку наша генеративная модель вряд ли будет идеальна: регулярно нужно «просыпаться» и отправляться в реальную среду собирать сэмплы для улучшения как минимум модели мира. Тем не менее, использование снов может существенно повысить sample efficiency алгоритмов: потребуется меньше реальных взаимодействий со средой, и большую часть процесса обучения самой стратегии можно переложить на сны. Цена - в вычислительной неэффективности: траекторий во сне из-за отличий от реальных потребуется генерировать довольно много. То есть, хотя шагов взаимодействия с настоящей средой станет меньше, время обучения алгоритма (wall-clock time) наоборот увеличится.

## §7.3. Планирование для дискретного управления

### 7.3.1. Monte-Carlo Tree Search (MCTS)

Попробуем придумать алгоритм планирования (7.4) в предположении идеального симулятора для MDP с дискретным пространством действий.

Поиятно, что для детерминированных MDP с функцией переходов $\boldsymbol{s}^{\prime}=\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})$, чисто теоретически, мы можем просто построить полное дерево игры. Заведём дерево, в котором каждому узлу будет соответствовать состояние, дугам - действия, и скажем, что узел, соответствующий $\boldsymbol{s}^{\prime}$ - потомок $\boldsymbol{s}$ по дуге $\boldsymbol{a}$, если $\boldsymbol{s}^{\prime}=\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})$.

У нас есть две проблемы. Во-первых, в сложных MDP такое дерево экспоненциально большое и целиком мы его не построим. Во-вторых, наши MDP, вообще говоря, стохастичны. Мы могли бы ввести много рёбер из $\boldsymbol{s}$, соответствующих одному и тому же действию $\boldsymbol{a}$ и «подписать» над каждым вероятности перехода $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid\right.$ $\mid s, a$ ), однако наша функция переходов $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ может быть произвольным распределением, да и вообще пространства состояний $\mathcal{S}$ могут быть в том числе континуальные или очень большие. Поэтому давайте введём понятие дерева чуть-чуть по-другому ${ }^{3}$ :

Определение 92: Деревом MDP с корнем в состоянии $\boldsymbol{s}_{0}$ будем называть дерево, где каждой дуге соответствует действие $\boldsymbol{a}$; узел на $\boldsymbol{t}$-ом уровне дереве соответствуют плану $\boldsymbol{a}_{0}, \boldsymbol{a}_{1} \ldots \boldsymbol{a}_{\boldsymbol{t}-1}$, соответствующему пути от корня.

Нам нужно продумать эвристики, как сокращать перебор поиска по такому дереву, и как его строить в ситуациях, когда эта задача экспоненциально сложная и у нас ограничены вычислительные ресурсы. Мы сейчас рассмотрим некоторую очень общую схему, как можно такую эвристику строить.

Будем строить дерево MDP с корнем в $\boldsymbol{s}_{\boldsymbol{0}}$ постепенно. Изначально, на первой итерации, наше дерево пусто - есть только корень. Узлы дерева будем обозначать красивым символом $\boldsymbol{\aleph}$. Дуги тогда однозначно задаются парой $\boldsymbol{\aleph}, \boldsymbol{a}$, где $\boldsymbol{\aleph}$ - узел дерева, откуда исходит дуга, $\boldsymbol{a}$ - действие, которому дуга соответствует. Для каждой дуги $(\boldsymbol{\aleph}, \boldsymbol{a})$ мы будем также хранить некоторую вспомогательную информацию; самый типичный вариант это счётчики прохождения по данной дуге $\boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})$ и ценность ${ }^{4} \boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$, скаляр.

Один шаг процедуры Monte-Carlo Tree Search (MCTS) заключается в проведении четырёх этапов: Selection, Expansion, Simulation и Update. Введём их по порядку.

Определение 93: На шаге Selection стартуем в корне, которому соответствует текущее состояние в реальной игре $\boldsymbol{s}_{0}$. При помощи некоторой стратегии, которую будем называть TreePolicy, и которая использует данные на дугах, выбираем действие $\boldsymbol{a}_{0}$; спускаемся по дереву на уровень глубже по дуге, соответствующей $\boldsymbol{a}_{0}$, и сэмплируем $\boldsymbol{s}_{1}, \boldsymbol{r}_{1}$ из $\boldsymbol{p}\left(\boldsymbol{s}_{1}, \boldsymbol{r}_{1} \mid \boldsymbol{s}_{0}, \boldsymbol{a}_{0}\right)$. Повторяем процедуру до тех пор, пока не попадём в некоторый лист дерева на уровне $\boldsymbol{t}$. Мы запоминаем (знаем) всю траекторию от корня до листа, то есть фактически выбираем таким образом начало некоторого плана $\boldsymbol{a}_{0}, \boldsymbol{a}_{1} \ldots \boldsymbol{a}_{\boldsymbol{t}-1}$ для рассмотрения, и заодно по Монте-Карло генерируем начало возможной траектории, соответствующей этому плану; в результате этой процедуры, мы попадаем в некоторый лист нашего текущего дерева и заодно симулируем для него состояние $\boldsymbol{s}_{\boldsymbol{t}}$.

[^0]
[^0]:    ${ }^{3}$ распространены объяснения MCTS в контексте детерминированных MDP, и поэтому про узлы постоянно говорят как про состояния; однако на самом деле ход рассуждений в принципе обобщается на стохастические MDP с тем замечанием, что в силу утверждения 81 в стохастичных средах планирование (7.4) неоптимально.
    ${ }^{4}$ хотя принято обозначать ценности через $\boldsymbol{V}$ или $\boldsymbol{Q}$, это не ценности каких-то состояний: в контексте стохастичных MDP это ценность плана, который привёл нас в это состояние; если угодно, можно обозначить этот счётчик как $\boldsymbol{Q}\left(\boldsymbol{s}_{0}, \boldsymbol{a}_{0}, \boldsymbol{a}_{1}, \boldsymbol{a}_{2} \ldots \boldsymbol{a}_{\boldsymbol{t}}\right)=$ $=\mathbb{E}_{\boldsymbol{T} \mid \boldsymbol{s}_{0}, \boldsymbol{a}_{0}, \boldsymbol{a}_{1} \ldots \boldsymbol{a}_{\boldsymbol{t}}} \boldsymbol{R}(\boldsymbol{T})$. Но в детерминированных средах они, конечно же, будут соответствовать оценочным функциям в стандартном понимании.

---

На этом шаге задачей TreePolicy является выбор того плана, для которого мы будем дальше строить дерево. То есть, допустим, мы сидим в некотором узле $\boldsymbol{\aleph}$, не являющемся листом, из которого исходит $|\boldsymbol{A}|$ дуг, и для каждого возможного пути знаем приблизительную оценку качества $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$ и то, сколько раз мы уже пробовали ходить по данной ветке $-\boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})$. Задача этой части эвристики - найти самый перспективный путь в дереве, используя эти статистики. С одной стороны, нужно искать оптимальный план в той ветке, где оценка качества велика; однако, если по каким-то веткам мы ходили редко, то высока вероятность, что там нам не повезло с сэмплом награды или мы просто недостаточно раскрутили там дерево. Налицо проблема многорукого бандита (см. раздел 7.1); поэтому типичные TreePolicy вдохновлены ${ }^{5}$ решениями этой задачи.

Пример 107 - Пример TreePolicy: Воспользуемся UCB-бандитом (7.3), формула которого принимает следующий вид:

$$
a:=\underset{a}{\operatorname{argmax}}\left[Q(\boldsymbol{\aleph}, \boldsymbol{a})+\boldsymbol{C} \sqrt{\frac{\log n(\boldsymbol{\aleph})}{n(\boldsymbol{\aleph}, \boldsymbol{a})}}\right]
$$

где $\boldsymbol{n}(\boldsymbol{\aleph}):=\sum_{\boldsymbol{a}} \boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})-$ это счётчик посещений узла, $\boldsymbol{C}$ - гиперпараметр. Действительно: $\boldsymbol{n}(\boldsymbol{\aleph})$ - сколько «эпизодов игры» мы провели с этим многоруким бандитом, а $\boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})-$ сколько раз выбирали в этом месте действие $\boldsymbol{a}$.

Определение 94: На шаге Expansion в выбранном на предыдущем этапе листе создаём для каждого действия $a_{t} \in \mathcal{A}$ по новому листу, соответствующему выбору этого действия: таким образом, мы расширяем дерево вдоль выбранной ветки «на один шаг вперёд».

Если число действий $|\boldsymbol{A}|$ велико, то имеет смысл определять поднабор действий случайным образом и создавать листы только для него. Тогда этап Selection заканчивается как только, например, TreePolicy сэмплирует действие, для которого ещё не существует дуги.

Определение 95: Шаг Simulation или Evaluation заключается в построении некоторой эвристичной оценки reward-to-go для каждого нового построенного на предыдущем шаге листа.

В этом месте, конечно, можно использовать какиенибудь handcrafted-эвристики, оценивающие очень примерно reward-to-go до конца эпизода (поэтому у этого шага есть второе название Evaluation), но универсальный вариант получить её - это для каждого $\boldsymbol{a}_{t}$ просимулировать игру (или несколько) из $s_{t}, a_{t}$ до конца эпизода, выбирая действия при помощи некоторой другой стратегии, которую назовём DefaultPolicy, и которая уже не может использовать никакой информации в узлах дерева (эти узлы мы ещё


просто-напросто не построили). Итого, мы получаем сэмплы reward-to-go для $|\mathcal{A}|$ планов, которые начинаются с $\boldsymbol{a}_{\mathbf{0}}, \boldsymbol{a}_{\mathbf{1}}, \ldots \boldsymbol{a}_{\mathbf{t}-\mathbf{1}}$, на $\boldsymbol{t}$-ом месте действие варьируется, и для каждого варианта у нас есть одна (или несколько) новая Монте-Карло оценка награды.

Пример 108: Пример типичной DefaultPolicy - это банально случайная стратегия.

Определение 96: Шаг Update или Backpropagation: обновление счётчиков и оценок во всех дугах дерева, по которым мы проходили на данном шаге при помощи полученных на шаге симуляции новых Монте-Карло оценок.

Пример 109: Процесс обновления счётчиков прост: в свежераскрытом листе для новых дуг инициализируем $\boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a}):=\mathbf{1}, \boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a}):=\hat{\boldsymbol{V}}_{\boldsymbol{a}}$, где $\hat{\boldsymbol{V}}_{\boldsymbol{a}}-$ reward-to-go, полученный на шаге Simulation для действия $\boldsymbol{a}$. Далее, рассмотрим пару $\boldsymbol{d}, \boldsymbol{a}$ где-то в рассмотренной ветке дерева; во-первых, увеличиваем счётчик посещения этой дуги на единицу. Для обновления оценки $\boldsymbol{Q}$ просто учтём новый сэмпл Монте-Карло оценки: мы симулировали награды за шаг, пока шли по дереву, и поэтому можем посчитать reward-to-go до момента прихода в лист. В листе же можно считать, что мы выбирали действие случайным образом, и поэтому усредним все $\hat{\boldsymbol{V}}_{\boldsymbol{a}}$ по

[^0]
[^0]:    ${ }^{5}$ тем не менее, применение бандитов здесь - тоже эвристика, хотя бы потому, что из-за постепенного раскрытия дерева вдоль каждой ветки «распределение $\boldsymbol{p}(\boldsymbol{r} \mid \boldsymbol{a})$ », из которого приходит дальнейший reward-to-go, меняется.

---

действиям. Таким образом, если $\hat{\boldsymbol{V}}$ - полученный новый сэмпл reward-to-go, $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$ обновляется как:

$$
Q(\boldsymbol{\aleph}, a) \leftarrow Q(\boldsymbol{\aleph}, a)+\frac{1}{n(\boldsymbol{\aleph}, a)}(\hat{V}-Q(\boldsymbol{\aleph}, a))
$$

Интуиция, почему это работает: чем больше мы строим дерево, тем меньше зависим от эвристики для этапа Simulation; в пределе после бесконечного числа итераций мы условно построим дерево игры целиком. Чем меньше эта зависимость, тем чаще TreePolicy выбирает хорошие действия. Эвристика из Simulation же лишь указывает на те узлы, куда MCTS быстрее отправится детальнее строить подробное дерево разбора игры, и чем эта эвристика лучше, тем удачнее пройдёт ограниченный перебор.

Вполне можно считать, что расчёт $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$ при помощи Монте-Карло оценок - это Policy Evaluation, а поиск по дереву при помощи многоруких бандитов - Policy Improvement.

# 7.3.2. Применение MCTS 

MCTS можно «предобучить» перед использованием: проводим много шагов MCTS-процедуры, считая, что корню соответствует $\boldsymbol{s}_{\mathbf{0}}$ (если распределение на начальное состояние стохастично, придётся сэмплировать $\boldsymbol{s}_{\mathbf{0}}$ : но тогда, в силу теоремы 81 , мы будем искать план, хороший в среднем для возможных исходов). На каждом шаге MCTS-процедуры ровно один лист в дереве «раскрывается» и для каждого пути симулируется одна или несколько игр до конца эпизодов. Полностью дерево для всего MDP мы скорее всего не построим, и однажды придётся остановиться.

Дальше мы хотим нашу стратегию отправить «играть» с реальной средой, и тут возможны варианты. Вопервых, мы можем дерево больше не трогать и просто использовать, например, TreePolicy (возможно, с «выключенным» эксплорейшном): а то есть, изначально нам дали $\boldsymbol{s}_{\mathbf{0}}$, которое, условно, соответствует корню. Мы выбираем действие при помощи TreePolicy и отправляем его в реальную среду; после этого мы можем в нашем дереве спустится по дуге, соответствующей выбранному действию, и на следующем шаге воспользоваться TreePolicy в этом узле. Однако, во время «использования» подобной стратегии нам могут встретиться состояния, для которых в дереве ещё нет узлов; тогда либо придётся выбирать действия случайно, либо проводить новые шаги MCTS-процедуры.

По этой причине, так обычно не делают; считается, что в общем случае, MCTS не строит дерево игры один раз и потом использует его во всех играх, а сама по себе является стратегией выбора действия в данном состоянии: просто долгой и тяжёлой в связи с переборной природой. То есть, перед каждой отправкой итогового действия в среду, нужно провести «ещё парочку» итераций MCTS-процедуры. Это означает, что, во-первых, можно вообще предобучения не проводить, а перед каждым выбором действия для реальной среды делать, там, 1000 шагов MCTS-процедуры, а во-вторых, после очередного реального шага, спускаясь на один узел вниз в дереве, мы можем оставлять только поддерево того узла, в который пришли, для оптимизации (наверх по дереву мы никогда не идём - прошлое уже не изменить).

Итоговое действие для реальной среды не обязательно выбирать при помощи TreePolicy (в нём есть учёт эксплорейшна, который при использовании стратегии нам не нужен). Распространённый вариант следующий: выбирать с большей вероятностью те действия, которые MCTS исследовал чаще всего в ходе всей процедуры:

$$
\pi\left(a_{0} \mid s_{0}\right) \propto n\left(\aleph_{0}, a_{0}\right)^{T}
$$

где $\boldsymbol{T}$ - температура, очередной гиперпараметр.
При «использовании» стратегии в бою обычно используют жёсткий вариант: $\boldsymbol{a}:=\underset{a}{\operatorname{argmax}} n\left(\boldsymbol{\aleph}_{0}, \boldsymbol{a}\right)$.

## Алгоритм 28: Стратегия МСТS (одна из вариаций)

Вход: $\boldsymbol{s}_{\mathbf{0}}$ - текущее состояние реальной среды, $\boldsymbol{p}\left(\boldsymbol{s}^{\prime}, \boldsymbol{r} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ - симулятор, $\boldsymbol{C}$ - гиперпараметр UCBбандита, $\boldsymbol{\aleph}_{0}$ - корень текущего дерева в памяти с хранением счётков $\boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})$ и оценок $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$ в дугах, $\boldsymbol{K}$ - число шагов, $\boldsymbol{T}$ - температура.

## На $\boldsymbol{k}$-ом шаге из $\boldsymbol{K}$ :

1. садимся в корень: $\boldsymbol{\aleph}:=\boldsymbol{\aleph}_{0}, \boldsymbol{s}:=\boldsymbol{s}_{\mathbf{0}}$.
2. инициализируем траекторию симуляции: $\mathcal{T}:=\left(s_{0}\right)$
3. пока $\boldsymbol{\aleph}$ - не лист:

- выбираем ветку, куда пойти:

$$
n(\boldsymbol{\aleph}):=\sum_{\boldsymbol{a}} n(\boldsymbol{\aleph}, \boldsymbol{a})
$$

---

$$
a \equiv \underset{a}{\operatorname{argmax}}\left[Q(\aleph, a)+C \sqrt{\frac{\log n(\aleph)}{n(\aleph, a)}}\right]
$$

- генерируем $s^{\prime}, r \sim p\left(s^{\prime}, r \mid s, a\right)$
- сохраняем $a, r, s^{\prime}$ в $\mathcal{T}$
- спускаемся по дереву: $\boldsymbol{\aleph} \leftarrow \operatorname{child}(\boldsymbol{\aleph}, a), s \leftarrow s^{\prime}$

4. для каждого $a \in \mathcal{A}$ :

- создаём узел $\hat{\boldsymbol{\aleph}}-$ ребёнка $\boldsymbol{\aleph}$ с дугой для действия $\boldsymbol{a}$
- симулируем $\mathcal{T}_{a} \sim \pi^{\text {random }} \mid s, a$, где $\pi^{\text {random }}-$ случайная стратегия
- инициализируем $n(\hat{\boldsymbol{\aleph}}, a) \equiv 1, Q(\hat{\boldsymbol{\aleph}}, a) \equiv R\left(\mathcal{T}_{a}\right)$

5. для каждой посещённой дуги $\boldsymbol{\aleph}, \boldsymbol{a}$ :

- считаем $\hat{V}$ - суммарный reward-to-go в траектории $\mathcal{T}$; полученный после посещения данной дуги, где награда после посещения листа оценена как $\frac{1}{|\mathcal{A}|} \sum_{a} R\left(\mathcal{T}_{a}\right)$.
- $Q(\boldsymbol{\aleph}, a) \leftarrow Q(\boldsymbol{\aleph}, a)+\frac{1}{n(\boldsymbol{\aleph}, a)}(\hat{V}-Q(\boldsymbol{\aleph}, a))$
- $n(\boldsymbol{\aleph}, a) \leftarrow n(\boldsymbol{\aleph}, a)+1$

Выход: стратегия $\pi\left(a_{0} \mid s_{0}\right) \propto n\left(\aleph_{0}, a_{0}\right)^{T}$

Итого, мы получили очень дорогостоящую по времени, разумную по памяти, но работающую процедуру поиска хороших действий в игре.

Пример 110: Попробуем провизуализировать шаг игры при помощи MCTS стратегии. Перед выбором действия будем делать 4 шага MCTS процедуры; выбирать действие для реальной среды будем при помощи $\pi^{\text {MTCS }}(\boldsymbol{a} \mid$ $|s) \propto n(\boldsymbol{\aleph}, a)$, где $\boldsymbol{\aleph}-$ корень дерева.


---

# 7.3.3. Дистилляция MCTS 

Появляется прекрасная идея: $\boldsymbol{d} \boldsymbol{a} \boldsymbol{c} \boldsymbol{\text { тилллнроват } \boldsymbol { ь }}$ MCTS в нейронку. Практически, мы займёмся имитационным обучением с MCTS в качестве эксперта. Для этого мы уже будем проводить этап обучения. Для обучения мы сыграем при помощи вышеописанной стратегии MCTS много игр, и сохраним их траектории ${ }^{6} \boldsymbol{\top}$. Далее решаем задачу классификации: пытаемся по состоянию $\boldsymbol{s}$ нейросеткой выдавать действия, которые выбрала бы MCTS. В идеале мы получим стратегию, работающую не хуже MCTS, но при этом скорость прямого прохода по сети будет намного, намного выше.

Схожая альтернатива - пытаться выдавать нейросеткой-стратегией $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ то же распределение, которое выдаёт долгий MCTS перебор; назовём это «целевое распределение» $\pi^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s})$. Тогда функция потерь выглядит так:

$$
\mathbb{E}_{s} \operatorname{KL}\left(\pi^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s}) \| \boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})\right)=\mathbb{E}_{s} \sum_{\boldsymbol{a}} \pi^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s}) \log \boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})+\operatorname{const}(\boldsymbol{\theta}) \rightarrow \min _{\boldsymbol{\theta}}
$$

где мат.ожидание по $\boldsymbol{s}$ берётся из буфера, из кучи сыгранных при помощи MCTS игр.
И тут возникает желание сделать следующий шаг: если у нас есть нейросеть, которая знает, какие действия в каком состоянии хорошие за счёт моделирования результата MCTS-перебора, может быть мы можем в новой игре воспользоваться ею внутри самого MCTS, использовать эту нейросеть $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ для ускорения перебора? Тогда мы наверняка сможем построить ещё более хорошую стратегию, дистиллировать её в нейронку, построить ещё более хорошую стратегию, дистиллировать её в нейронку, и так далее. Итак, появляется идея объединить MCTS с нейросетками.

### 7.3.4. AlphaZero

В алгоритме AlphaZero ${ }^{7}$ вводится нейросеть с параметрами $\boldsymbol{\theta}$, принимающая на вход состояние $\boldsymbol{s}$, с двумя головами. Одна выдаёт распределение на действиях $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ («вспомогательную стратегию» или «дистилированную MCTS»), другая выдаёт скалярную оценку текущего состояния $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})$. Сразу скажем, что $\boldsymbol{p}$ - лишь вспомогательная стратегия, и хотя ею вполне можно будет пользоваться для игры по итогам обучения, но наша итоговая стратегия всё-таки будет перебирать ходы при помощи MCTS и поэтому будет потенциально лучше. Цель $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ - запоминать с прошлых игр, какие действия были хорошими, ускоряя перебор, а цель $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})-$ запоминать reward-to-go, чтобы не было необходимости проводить симуляции при помощи случайной стратегии на этапе Simulation; вместо симуляции мы теперь просто будем вызывать эту нейросетку.

Итак, за основу алгоритма берётся схема MCTS-стратегии, описанная в алгоритме 28. Как и раньше, в каждом узле $\boldsymbol{\aleph}$ для каждого действия хранится оценка ценности $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$ и счётчик, сколько раз какое действие было попробовано $\boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})$; при выборе очередного действия для состояния $\boldsymbol{s}$ повторяется, там, 1600 раз наши четыре этапа MCTS схемы, а каждый эпизод дерево начинает строиться с нуля.

Изменений в схеме ровно два: на этапе Simulation мы вместо того, чтобы играть случайной стратегией до конца игры, просто воспользуемся нашей моделью $\hat{\boldsymbol{V}}_{\boldsymbol{a}}:=\boldsymbol{V}_{\boldsymbol{\theta}}(\hat{\boldsymbol{s}})$ как оценкой дальнейшего reward-to-go (здесь $\hat{\boldsymbol{s}}$ - симулированное состояние после выбора оцениваемого действия $\boldsymbol{a}$ в листе). Счётчики и скаляры $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})$ обновляются как раньше.

Второе изменение - в формуле TreePolicy, которая принимает следующий вид:

$$
\boldsymbol{a}:=\underset{\boldsymbol{a}}{\operatorname{argmax}}\left[\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})+\boldsymbol{C} \boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s}) \frac{\sqrt{n(\boldsymbol{\aleph})}}{n(\boldsymbol{\aleph}, \boldsymbol{a})+1}\right]
$$

Здесь $\boldsymbol{C}$ - гиперпараметр; бонус за исследования немного не похож на UCB-баидита, но имеет примерно тот же смысл (он обратно пропорционален числу выборов действия, а числитель гарантирует, что это слагаемое неограниченно растёт при фиксированном знаменателе и промотивирует выбор любого сколь угодно неоптимального действия рано или поздно). Ключевое изменение - домножение бонуса на $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$. Если действие в прошлых играх никогда не выбиралось по итогам перебора, дистилляция в нейросетку приведёт к низкой вероятности $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$, и такое плохое действие будет выбираться редко. Это позволяет существенно сократить перебор для сред с большим числом ${ }^{8}$ действий $|\mathcal{A}|$ : в новых состояниях дерева, где все оценки, условно, $\boldsymbol{Q}(\boldsymbol{\aleph}, \boldsymbol{a})=$ $=\mathbf{0}, \boldsymbol{n}(\boldsymbol{\aleph}, \boldsymbol{a})=\mathbf{0}$; мы вместо случайного тыкаиья и исследования всех вариантов будем больше опираться именно на те действия, которые показались хорошими нейросетке. Запоминание информации из предыдущих эпизодов обучения направляет раскрытие дерева «в хорошую область».

[^0]
[^0]:    ${ }^{6}$ замечание: траектории именно реальных игр, в которых каждое действие выбиралось в результате, там, 1000 шагов MCTS процедуры, на каждом из которых MCTS дельло кучу симуляций при помощи своей DefaultPolicy; эти симуляции внутри MCTS мы нигде не записываем, поскольку нас интересуют только хорошие действия по результатам перебора.
    ${ }^{7}$ AlphaZero исторически был обобщением алгоритма AlphaGo для игры в го как общей процедуры обучения стратегии для произвольной игры с доступным симулятором. Для случая игры двух игроков вроде шахмат и го, для которого она изначально и строилась, предполагается, что в симуляторе за противника играет просто недавняя версия текущей MCTS-стратегии; он также строился для детермированных сред с ненулевой наградой лишь в конце эпизода по типу «выиграл-проиграл», но мы далее опишем сразу чуть-чуть более общую схему.
    ${ }^{8}$ например, в том же го число доступных действий достигает $\mathbf{1 9}^{2}$.

---

Как и раньше, результатом работы MCTS является стратегия $\boldsymbol{\pi}^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s}) \propto \boldsymbol{n}\left(\boldsymbol{\aleph}_{0}, \boldsymbol{a}\right)^{\boldsymbol{T}}$, где $\boldsymbol{T}$ - гиперпараметр температуры, $\boldsymbol{\aleph}_{0}$ - текущий корень. В ответ на наше действие $\boldsymbol{a}$, сэмплированное из этой стратегии, получаем истинный сэмпл $s^{\prime}$ из реальной среды и оставляем от нашего дерева лишь поддерево, соответствующее $\operatorname{child}\left(\boldsymbol{\aleph}_{0}, \boldsymbol{a}\right)$.

При помощи такой MCTS-стратегии с текущими параметрами нейросети $\boldsymbol{\theta}$ играем много-много игр и сохраняем записи реальных траекторий; для каждого состояния $\boldsymbol{s}$ запомним $\boldsymbol{\pi}^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s})$ и считаем реальный reward-to-go, который обозначим за $\boldsymbol{z}$. Дальше батчами учим $\boldsymbol{V}_{\boldsymbol{\theta}}$ воспроизводить среднее $\boldsymbol{z}$, минимизируя MSE, а $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ - воспроизводить $\boldsymbol{\pi}^{\mathrm{MCTS}}$ аналогично (7.5). Ещё добавлен регуляризатор на параметры, итого:

$$
\operatorname{Loss}(\boldsymbol{\theta}):=\mathbb{E}_{s}\left[\left(z-V_{\boldsymbol{\theta}}(s)\right)^{2}-\sum_{\boldsymbol{a}} \pi(\boldsymbol{a} \mid \boldsymbol{s}) \log p_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})+\alpha\|\boldsymbol{\theta}\|^{2}\right] \rightarrow \min _{\boldsymbol{\theta}}
$$

где в мат.ожидании $\boldsymbol{s}$ берутся из буфера, $\boldsymbol{\alpha}$ - гиперпараметр для регуляризации. После многих итераций дообучения нейросети снова играем много игр при помощи MCTS с новыми параметрами нейросетки $\boldsymbol{\theta}$, снова сохраняем записи реальных игр и докидываем их в буфер, и так далее.

В буфере можно хранить в том числе довольно старые игры, поскольку $\boldsymbol{\pi}^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s})$ можно считать в некотором смысле «хорошей» стратегией даже если $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})$ выдаёт ерунду, а $\boldsymbol{p}_{\boldsymbol{\theta}}(\boldsymbol{a} \mid \boldsymbol{s})$ условно случайная. Действительно, по мере раскрытия дерева первое «перестаёт» использоваться, а второе влияет лишь на исследования внутри самого дерева. Но чем лучше будут эти нейросетки, тем лучше пройдёт MCTS-поиск. Понятно, что $\boldsymbol{V}_{\boldsymbol{\theta}}(\boldsymbol{s})$, обучаясь на reward-to-go, учит V-функцию для MCTS-стратегии, породившей сэмпл $\boldsymbol{z}$, но ей в таком алгоритме не существенно быть критиком именно самой свежей MCTS-стратегии: ведь в исходной схеме MCTS мы на этапе симуляции так вообще доигрывали игру случайной стратегией.

Пример 111 - AlphaGo Zero: Основные детали применения данного алгоритма для го можно посмотреть на этой картинке.

# 7.3.5. $\mu$-Zero 

Обобщим ${ }^{9}$ AlphaZero на случай, когда симулятор нам недоступен. Параметры всех нейросетевых моделей будем обозначать $\boldsymbol{\theta}$, в конечном счёте все части модели будут обучаться end-to-end единой функцией потерь.

На вход стратегии будет приходить реальное состояние среды $\boldsymbol{s}_{\mathbf{0}}$. Оно отправляется в кодировщик: нейросеть $\boldsymbol{h}_{\boldsymbol{\theta}}\left(\boldsymbol{s}_{\boldsymbol{0}}\right)$, которая вернёт латентное представление состояния $\boldsymbol{\aleph}_{\boldsymbol{0}}$ - некоторый компактный вещественный вектор. Далее мы для определения стратегии запускаем MCTS-процедуру, однако ей нужен симулятор. В качестве симулятора мы заведём нейросеть $\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\aleph}, \boldsymbol{a})$, которая по данному латентному представлению состояния $\boldsymbol{\aleph}$ и действию $\boldsymbol{a}$ детерминировано выдаст латентное представление следующего состояния $\boldsymbol{\aleph}^{\prime}$; а также нейросеть $\hat{\boldsymbol{r}}_{\boldsymbol{\theta}}(\boldsymbol{\aleph}, \boldsymbol{a})$ для моделирования награды за один данный шаг. Поскольку функция $\boldsymbol{g}_{\boldsymbol{\theta}}$ детерминирована, пересчитывать её значение каждый раз, когда MCTS идёт вдоль этой ветки, не понадобится. Нейросети $\boldsymbol{V}_{\boldsymbol{\theta}}$ и $\boldsymbol{p}_{\boldsymbol{\theta}}$ теперь будут тоже принимать на вход компактное латентное представление $\boldsymbol{\aleph}$; это позволит нам внутри дерева запускать их в произвольных узлах.

Осталось понять, как обучать нововведённые функции $\boldsymbol{g}_{\boldsymbol{\theta}}, \hat{\boldsymbol{r}}_{\boldsymbol{\theta}}, \boldsymbol{h}_{\boldsymbol{\theta}}$. Как и в AlphaZero, соберём датасет из нескольких игр текущей версии модели, и дальше проведём несколько итераций улучшения всех параметров $\boldsymbol{\theta}$. Мы сможем сделать это end-to-end. Возьмём некоторый момент реальной игры с состоянием $\boldsymbol{s}$, reward-to-go с этого момента $\boldsymbol{z}$ и выданной MCTS «хорошей стратегией» $\boldsymbol{\pi}^{\text {MCTS }}$. Тогда функция потерь из AlphaZero будет в том числе зависеть от параметров функции $\boldsymbol{h}_{\boldsymbol{\theta}}$ :

$$
\left(z-V_{\theta}\left(h_{\theta}(s)\right)\right)^{2}-\sum_{\boldsymbol{a}} \pi^{\mathrm{MCTS}}(\boldsymbol{a} \mid \boldsymbol{s}) \log p_{\theta}\left(\boldsymbol{a} \mid \boldsymbol{h}_{\boldsymbol{\theta}}(\boldsymbol{s})\right)+\alpha\|\boldsymbol{\theta}\|^{2} \rightarrow \min _{\boldsymbol{\theta}}
$$

Добавить слагаемое для обучения модели функции награды, которая должна воспроизводить награду $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ за один шаг, несложно:

$$
\left(r(s, a)-\hat{r}_{\theta}\left(h_{\theta}(s), a\right)\right)^{2} \rightarrow \min _{\theta}
$$

Наконец, осталось разобраться с обучением приближения динамики $\boldsymbol{g}_{\boldsymbol{\theta}}$. Тут есть первое соображение: давайте заглянем на один момент в будущее и посмотрим на реальное $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Мы могли бы воспользоваться этим сэмплом и явно попросить нашу динамику предсказывать латентное представление будущего:

$$
\frac{\left\|g_{\theta}\left(h_{\theta}(s), a\right)-\boldsymbol{h}_{\theta}\left(s^{\prime}\right)\right\|_{2}^{2} \rightarrow \min _{\theta}}{\text { предсказанное } \quad \text { будущее } \quad \text { реальное }}
$$

[^0]
[^0]:    ${ }^{9}$ для полноты описания алгоритма, в оригинальной статье введён ещё ряд непринципиальных изменений в AlphaZero: используется ещё более длинная формула для TreePolicy сразу с несколькими гиперпараметрами, а также вводится обобщение алгоритма на случай частично наблюдаемых сред (см. раздел 8.5).

---

Мы так делать не будем (хотя могли бы). Такое слагаемое функции потерь мотивирует латентное представление быть в некотором смысле «простым» для предсказания, и эта простота достигается за счёт ухудшения семантического смысла: какие-то важные фичи для $\boldsymbol{V}_{\boldsymbol{\theta}}, \boldsymbol{p}_{\boldsymbol{\theta}}$ «теряются», поскольку потеря в лоссе для них меньше, чем потеря в лоссе предсказания усложнённого представления $\boldsymbol{h}_{\boldsymbol{\theta}}(\boldsymbol{s})$. Нам же важно другое: чтобы латентное представление было «достаточным» для предсказания $\boldsymbol{V}_{\boldsymbol{\theta}}, \boldsymbol{p}_{\boldsymbol{\theta}}$, будущих наград и хорошей стратегии. Если это достигается, то мы спокойно согласимся как с «несогласованностью» латентного описания (когда результат применения модели динамики $\boldsymbol{g}_{\boldsymbol{\theta}}$ не соответствует латентному описанию реального $\boldsymbol{s}^{\prime}$, хотя всё равно позволяет предсказывать хороший reward-to-go и стратегию для этого $\boldsymbol{s}^{\prime}$ ), так и с потерей некоторой информации, несущественной для агента.

Поэтому мы будем обучать $\boldsymbol{g}_{\boldsymbol{\theta}}$ по-другому. Для $\boldsymbol{s}^{\prime}$ мы знаем целевые переменные для функций $\boldsymbol{V}_{\boldsymbol{\theta}}, \boldsymbol{p}_{\boldsymbol{\theta}}, \hat{\boldsymbol{r}}_{\boldsymbol{\theta}}$. Давайте добавим их лоссы для $\boldsymbol{s}^{\prime}$, но входное латентное представление для $\boldsymbol{s}^{\prime}$ получим не при помощи $\boldsymbol{h}_{\boldsymbol{\theta}}\left(\boldsymbol{s}^{\prime}\right)$, а при помощи $\boldsymbol{g}_{\boldsymbol{\theta}}\left(\boldsymbol{h}_{\boldsymbol{\theta}}(\boldsymbol{s}), \boldsymbol{a}\right)$. То есть пусть $\boldsymbol{z}^{\prime}$ - reward-to-go со следующего состояния в реальной игре, $\pi^{\mathrm{MCTS}}\left(\boldsymbol{a}^{\prime}\right|$ $\left|s^{\prime}\right|$ - результат работы MCTS на следующем шаге $\boldsymbol{s}^{\prime}$, тогда добавим такое слагаемое в функцию потерь:

$$
\left(z^{\prime}-V_{\theta}\left(\aleph_{\theta}^{\prime}\right)\right)^{2}-\sum_{\hat{\boldsymbol{a}}} \pi^{\mathrm{MCTS}}\left(\hat{\boldsymbol{a}}\left|s^{\prime}\right| \log p_{\theta}\left(\hat{\boldsymbol{a}} \mid \boldsymbol{\aleph}_{\theta}^{\prime}\right)+\left(r^{\prime}-\hat{r}_{\theta}\left(\boldsymbol{\aleph}_{\theta}^{\prime}, \boldsymbol{a}^{\prime}\right)\right)^{2}+\alpha \|\boldsymbol{\theta}\|^{2} \rightarrow \min _{\theta}\right.
$$

где $\boldsymbol{\aleph}_{\boldsymbol{\theta}}^{\prime}:=\boldsymbol{g}_{\boldsymbol{\theta}}\left(\boldsymbol{h}_{\boldsymbol{\theta}}(\boldsymbol{s}), \boldsymbol{a}\right)$.
Заметим, что мы можем взять из буфера целый роллаут длины $\boldsymbol{K}$, и представление для $\boldsymbol{s}^{(\boldsymbol{k})}$ получать как $\boldsymbol{k}$ преобразований нашей динамикой $\boldsymbol{g}$ представления первого состояния $\boldsymbol{h}_{\boldsymbol{\theta}}\left(\boldsymbol{s}_{\boldsymbol{0}}\right)$; аналогично, для него считать лоссы обучения $\boldsymbol{V}_{\boldsymbol{\theta}}, \boldsymbol{p}_{\boldsymbol{\theta}}$ (а также $\hat{\boldsymbol{r}}_{\boldsymbol{\theta}}$ ). Все выполненные при этом действия следует брать из роллаута, реальной записи игры. Полученные лоссы все суммируются; формальная функция потерь получается следующая:

$$
\sum_{k=0}^{K}\left[\left(z^{(k)}-V_{\theta}\left(\aleph_{\theta}^{(k)}\right)\right)^{2}-\sum_{\hat{\boldsymbol{a}}} \pi^{\mathrm{MCTS}}\left(\hat{\boldsymbol{a}}\left|s^{(k)}\right| \log p_{\theta}\left(\hat{\boldsymbol{a}}\left|\boldsymbol{\aleph}_{\theta}^{(k)}\right)+\left(r^{(k)}-\hat{r}_{\theta}\left(\boldsymbol{\aleph}_{\theta}^{(k)}, \boldsymbol{a}^{(k)}\right)\right)^{2}\right]+\alpha \|\boldsymbol{\theta}\|^{2} \rightarrow \min _{\theta}\right.
$$

где $\boldsymbol{z}^{(\boldsymbol{k})}$ - reward-to-go, начиная с $\boldsymbol{k}$-го шага после рассматриваемого момента времени $\boldsymbol{s}_{\boldsymbol{0}}$, латентное представление начального состояния определяется кодировщиком $\boldsymbol{\aleph}_{\boldsymbol{\theta}}^{\text {(II) }} \equiv \boldsymbol{\aleph}:=\boldsymbol{h}_{\boldsymbol{\theta}}(\boldsymbol{s})$, а последующие - динамикой: $\boldsymbol{\aleph}_{\boldsymbol{\theta}}^{\boldsymbol{( k )}}:=\boldsymbol{g}_{\boldsymbol{\theta}}\left(\boldsymbol{\aleph}_{\boldsymbol{\theta}}^{\boldsymbol{( k - 1})}, \boldsymbol{a}^{(\boldsymbol{k - 1})}\right)$.

Итоговую схему проще представить на картинке:


Подумаем, как полученный алгоритм $\boldsymbol{\mu}$-Дего будет обучаться, то есть «в каком порядке» будут улучшаться нейросетки. Понятно, что MCTS, в котором динамика среды заменена на необученную нейросетку, ничего разумного выдавать не будет. Однако для обучения модели награды за шаг $\boldsymbol{r}_{\boldsymbol{\theta}}$ в любом датасете у нас всегда будет ground truth, поэтому эта сетка постепенно будет обучаться, и с ходом этого обучения $\boldsymbol{h}_{\boldsymbol{\theta}}$ будет постепенно строить осмысленное латентное представление состояний. Наше приближение $\boldsymbol{V}_{\boldsymbol{\theta}}$, обучающееся на reward-to-go, тоже учится воспроизводить V-функцию «текущей MCTS-стратегии», сколь бы плохой она ни была, и мы помним, что оценочная функция любой стратегии - это путь к её улучшению. Поскольку мы учимся предсказывать будущие reward-to-go в том числе по преобразованному моделью динамики латентному представлению в слагаемом (7.6), модель $\boldsymbol{g}_{\boldsymbol{\theta}}$ научится выдавать такое представление, по которому мы умеем хорошо предсказывать будущие награды. Именно их мы и используем на этапе Simulation в листьях дерева; дерево начнёт ходить в те ветки, где reward-to-go больше, и проводить таким образом policy improvement. Дальше эта более хорошая

---

стратегия будет дистиллироваться в нейронку $\boldsymbol{p}_{\boldsymbol{\theta}}$, а для этого ещё более информативным будет становиться как латентное представление, так и его преобразование моделью динамики.

Число итераций на этапе планирования для каждого выбора действий для реальной среды здесь имеет поставить сильно меньше, чем в AlphaZero, поскольку понятно, что перебор с неидеальным симулятором менее осмысленный, чем с идеальным. Тем не менее, универсальность алгоритма $\boldsymbol{\mu}$-Zero противостоит его огромной вычислительной сложности: необходимо делать огромное количество итераций MCTS процедуры, требующей постоянных вызовов нейросетей.

# §7.4. Планирование для непрерывного управления 

### 7.4.1. Прямое дифференцирование

Построим планировщик для непрерывных пространств действий. Предположим, что модель динамики среды и награды (то есть все функции из постановки задачи) нам известны, причём даны не просто как симулятор, а как дифференцируемые и, главное, детерминированные функции. Для упрощения повествования будем считать, что эпизод состоит из $\boldsymbol{T}$ шагов, $\boldsymbol{\gamma}=\mathbf{1}$. Дополнительно рассматривая случай непрерывного пространства действий $\mathcal{A} \equiv \mathbb{R}^{d}$, мы получим ситуацию, в которой мы можем просто дифференцировать вдоль оси времени и оптимизировать награду по действиям «напрямую».

В силу детерминированности функции переходов наш выбор действий полностью определяет траекторию. То есть искать будем даже не оптимальную детерминированную стратегию, а оптимальное управление набор действий $\boldsymbol{a}_{1}, \boldsymbol{a}_{2} \ldots \boldsymbol{a}_{T}$, которые приведут к наилучшей траектории.

Поскольку мы здесь немного уходим в мир оптимального управления, для соблюдения каноничности не будем предполагать однородность: функции награды и динамики среды дополнительно зависят от дискретной переменной времени $\boldsymbol{t} \in\{\mathbf{1} \ldots \boldsymbol{T}\}$.

Итого, рассматриваем следующую задачу:

$$
\left\{\begin{array}{l}
\sum_{t}^{T} r_{t}\left(s_{t}, a_{t}\right) \rightarrow \max _{a_{1} \ldots a_{T}} \\
s_{t}=f_{t}\left(s_{t-1}, a_{t-1}\right)
\end{array}\right.
$$

Сразу заметим, что при сделанных предположениях можно попытаться решать задачу «в лоб». Промоделируем детерминированную стратегию нейросеткой $\boldsymbol{\pi}_{t}(\boldsymbol{s}, \boldsymbol{\theta})$ с параметрами $\boldsymbol{\theta}$, и рассмотрим такой вычислительный граф:


Здесь суммарная награда - дифференцируемая функция от $\boldsymbol{\theta}$, поэтому параметры стратегии можно оптимизировать напрямую. Таким образом, у нас получилась «дифференцируемая» задача динамического программирования. Однако, если $\boldsymbol{T}$ велико, то градиент вдоль такого вычислительного графа будет подвержен затуханию, а сам такой поиск в пространстве траектории будет содержать множество локальных минимумов.

Нам хочется построить, если угодно, более специализированный метод оптимизации для задач вида 7.7. Как мы обычно действуем при построении методов оптимизации: вот у нас есть некоторая сложная функция, которую мы хотим оптимизировать, возможно, с ограничениями (а аналитически ничего не считается). Заводим некоторое приближение решения, в данном случае - какой-то план $\boldsymbol{a}_{1}, \boldsymbol{a}_{2} \ldots \boldsymbol{a}_{T}$. Затем приблизим в окрестности этой точки оптимизируемую функцию какой-то простой моделью, с которой мы умеем работать, за счёт простоты модели делаем какой-то шаг и попадаем в новую точку, новый план $\boldsymbol{a}_{1}, \boldsymbol{a}_{2} \ldots \boldsymbol{a}_{T}$. Оказывается, что в контексте задачи 7.7 мы можем поступить умнее, чем просто подставить ограничения внутрь оптимизируемого функционала и приблизить всё линейной моделью, как мы делаем при градиентном спуске. Оказывается, если мы заменим функции наград на квадратичное приближение, а ограничение - функцию динамики - на линейное приближение, то мы сможем за счёт структуры задачи просто методом динамического программирования аналитически решить задачу! Тогда «проведение шага» для нас будет просто сдвиг в точку нового оптимального плана для подобной локальной аппроксимации.

---

# 7.4.2. Linear Quadratic Regulator (LQR) 

Обсудим, как можно аналитически найти решение задачи в ситуации, когда $\boldsymbol{f}, \boldsymbol{r}$ - «простые» функции, а именно линейная и квадратичная соответственно. Пусть функция $\boldsymbol{f}$ - линейна, функция $\boldsymbol{r}$ - квадратична (hence the name). Введём соответствующие обозначения:

$$
\begin{gathered}
f_{t}(s, a)=F_{t}\left[\begin{array}{l}
s \\
a
\end{array}\right]+f_{t} \\
r_{t}(s, a)=\frac{1}{2}\left[\begin{array}{l}
s \\
a
\end{array}\right]^{T} R_{t}\left[\begin{array}{l}
s \\
a
\end{array}\right]+\left[\begin{array}{l}
s \\
a
\end{array}\right]^{T} r_{t}
\end{gathered}
$$

Напоминаем, что выход функции $\boldsymbol{f}$ - следующее состояние (т.е. вектор), поэтому $\boldsymbol{F}_{\boldsymbol{t}}$ - матрица, $\boldsymbol{f}_{\boldsymbol{t}}$ - вектор. Выход функции $\boldsymbol{r}$ - скаляр, поэтому $\boldsymbol{R}_{\boldsymbol{t}}$ - матрица, $\boldsymbol{r}_{\boldsymbol{t}}$ - вектор. Свободный член квадратичной формы не пишем, потому что он не зависит от состояний и действий (траектории), поэтому при оптимизации это константная неоптимизируемая награда, и она несущественна. Матрицы $\boldsymbol{F}_{\boldsymbol{t}}, \boldsymbol{R}_{\boldsymbol{t}}$ и вектора $\boldsymbol{r}_{\boldsymbol{t}}, \boldsymbol{f}_{\boldsymbol{t}}$ считаем известными.

Нам понадобится ещё чуть-чуть обозначений. Будем считать, что блоки матрицы $\boldsymbol{R}_{\boldsymbol{t}}$ и блоки вектора $\boldsymbol{r}_{\boldsymbol{t}}$ выглядят следующим образом:

$$
R_{t}:=\left[\begin{array}{ll}
R_{t, s, s} & R_{t, s, a} \\
R_{t, a, s} & R_{t, a, a}
\end{array}\right] ; \quad r_{t}:=\left[\begin{array}{l}
r_{t, s} \\
r_{t, a}
\end{array}\right]
$$

Для упрощения выкладок также будем полагать, что $\boldsymbol{R}_{\boldsymbol{t}, \boldsymbol{s}, \boldsymbol{a}}=\boldsymbol{R}_{\boldsymbol{t}, \boldsymbol{a}, \boldsymbol{s}}^{\boldsymbol{T}}$.
Поймём, что мы можем решить задачу обычным динамическим программированием «с конца».
Теорема 82: Оптимальное действие в последний момент времени $\boldsymbol{a}_{\boldsymbol{T}}^{*}$ - линейная форма от состояния $\boldsymbol{s}_{\boldsymbol{T}}$.
Доказательство. Рассмотрим последний момент времени $\boldsymbol{T}$ и расшишем оптимальную Q-функцию (в силу отказа от однородности оценочные функции также зависят от времени):

$$
Q_{T}^{*}\left(s_{T}, a_{T}\right)=r_{T}\left(s_{T}, a_{T}\right)=\frac{1}{2}\left[\begin{array}{l}
s_{T} \\
a_{T}
\end{array}\right]^{T} R_{T}\left[\begin{array}{l}
s_{T} \\
a_{T}
\end{array}\right]+\left[\begin{array}{l}
s_{T} \\
a_{T}
\end{array}\right]^{T} r_{T}
$$

И всё, потому что на сим эпизод заканчивается и награды больше не будет. Мы легко можем найти оптимальное действие, если на последнем шаге оказались в состоянии $\boldsymbol{s}_{\boldsymbol{T}}$, промаксимизировав $\boldsymbol{Q}_{\boldsymbol{T}}^{*}$ по действию $a_{T}:$

$$
a_{T}^{*}=\underset{a_{T}}{\operatorname{argmax}} Q_{T}^{*}\left(s_{T}, a_{T}\right)
$$

Ищем оптимум квадратичной формы*, приравняв градиент $\boldsymbol{Q}_{\boldsymbol{T}}^{*}$ по действиям к нулю:

$$
\begin{gathered}
\nabla_{\alpha_{T}} Q_{T}^{*}\left(s_{T}, a_{T}\right)=R_{T, a, a} a_{T}+R_{T, a, s} s_{T}+r_{T, a}=0 \\
a_{T}^{*}=-R_{T, a, a}^{-1}\left(R_{T, a, s} s_{T}+r_{T, a}\right)
\end{gathered}
$$

Видно, что оптимальное действие - линейная форма от последнего состояния.

[^0]Видно, что придётся обращать матрицу $\boldsymbol{R}_{\boldsymbol{T}, \boldsymbol{a}, \boldsymbol{a}}$, но размерность пространства действий в задачах такой постановки имеет обычно разумные размеры (обычно не более 100), поэтому мы не боимся.

Введём обозначения, чтобы привести форму $\boldsymbol{a}_{\boldsymbol{T}}^{*}$ к стандартному виду:

$$
\begin{gathered}
\boldsymbol{K}_{\boldsymbol{T}}:=-\boldsymbol{R}_{\boldsymbol{T}, \boldsymbol{a}, \boldsymbol{a}}^{-1} \boldsymbol{R}_{\boldsymbol{T}, \boldsymbol{a}, \boldsymbol{s}}, \quad \boldsymbol{k}_{\boldsymbol{T}}:=-\boldsymbol{R}_{\boldsymbol{T}, \boldsymbol{a}, \boldsymbol{a}}^{-1} \boldsymbol{r}_{\boldsymbol{T}, \boldsymbol{a}} \\
a_{\boldsymbol{T}}^{*}=\boldsymbol{K}_{\boldsymbol{T}} s_{\boldsymbol{T}}+\boldsymbol{k}_{\boldsymbol{T}}
\end{gathered}
$$

Теорема 83: Ценность последнего состояния $\boldsymbol{V}^{*}\left(\boldsymbol{s}_{\boldsymbol{T}}\right)$ - квадратичная форма от состояния $\boldsymbol{s}_{\boldsymbol{T}}$.
Доказательство. По определению, в силу детерминированности среды, $\boldsymbol{V}^{*}\left(\boldsymbol{s}_{\boldsymbol{T}}\right)=\boldsymbol{Q}^{*}\left(\boldsymbol{s}_{\boldsymbol{T}}, \boldsymbol{a}_{\boldsymbol{T}}^{*}\right)$; осталось


[^0]:    *формально здесь нужно оговариваться, что на матрицу $\boldsymbol{R}_{\boldsymbol{t}}$ и вектор $\boldsymbol{r}_{\boldsymbol{t}}$ надо накладывать некоторые ограничения, например, что эта квадратичная форма отрицательно определена и у неё есть искомый глобальный максимум. Здесь и далее мы на условиях регулярности останавливаться не будем.

---

заметить, что подстановка линейной формы в линейную даст квадратичную:

$$
\begin{aligned}
V^{*}\left(s_{T}\right) & =Q^{*}\left(s_{T}, a_{T}^{*}\right)=\frac{1}{2}\left[\begin{array}{l}
s_{T} \\
a_{T}^{*}
\end{array}\right]^{T} R_{T}\left[\begin{array}{l}
s_{T} \\
a_{T}^{*}
\end{array}\right]+\left[\begin{array}{l}
s_{T} \\
a_{T}^{*}
\end{array}\right]^{T} r_{T}= \\
& =\frac{1}{2}\left[\begin{array}{c}
s_{T} \\
\boldsymbol{K}_{T} s_{T}+k_{T}
\end{array}\right]^{T} \boldsymbol{R}_{T}\left[\begin{array}{c}
s_{T} \\
\boldsymbol{K}_{T} s_{T}+k_{T}
\end{array}\right]+\left[\begin{array}{c}
s_{T} \\
\boldsymbol{K}_{T} s_{T}+k_{T}
\end{array}\right]^{T} r_{T}=(*)
\end{aligned}
$$

Просто перегруппируя слагаемые, видим квадратичную форму от состояний $s_{T}$. Сразу запишем её в каноничных обозначениях:

$$
\begin{gathered}
V_{T}:=R_{T, s, s}+K_{T}^{T} R_{T, a, a} K_{T}+K_{T}^{T} R_{T, a, s}+R_{T, s, a} K_{T} \\
v_{T}:=R_{T, s, a} k_{T}+r_{T}^{T} K_{T}^{T} r_{T} \\
(*)=\frac{1}{2} s_{T}^{T} V_{T} s_{T}+v_{T}^{T} s_{T}
\end{gathered}
$$

Осталось понять, что мы можем раскручивать это к началу времён, не выходя из квадратичных форм. Так и есть.

Теорема 84: Оптимальные оценочные функции $Q_{t}^{*}$ - квадратичные формы от $\left[\begin{array}{l}s_{t} \\ a_{t}\end{array}\right]$, оптимальные действия $a_{t}^{*}$ - линейные формы от $s_{t}$, а оптимальные оценочные функции $V_{t}^{*}$ - квадратичные формы от $s_{t}$.

Доказательство. Из определений и детерминированности:

$$
Q_{T-1}^{*}\left(s_{T-1}, a_{T-1}\right)=r_{T-1}\left(s_{T-1}, a_{T-1}\right)+V_{T}^{*}\left(s_{T}\right)=r_{T-1}\left(s_{T-1}, a_{T-1}\right)+V_{T}^{*}\left(F_{T}\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]+f_{T}\right)=()
$$

В последнее слагаемое подставили линейную форму динамики среды (7.8). При этом подстановка линейной формы в квадратичную оставит её квадратичной.

$$
\begin{aligned}
(*) & =r_{T-1}\left(s_{T-1}, a_{T-1}\right)+\frac{1}{2}\left[F_{T}\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]+f_{T}\right]^{T} V_{T}\left[F_{T}\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]+f_{T}\right]+v_{T}^{T}\left[F_{T}\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]+f_{T}\right]= \\
& =r_{T-1}\left(s_{T-1}, a_{T-1}\right)+\frac{1}{2}\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]^{T} F_{T}^{T} V_{T} F_{T}\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]+\left[\begin{array}{c}
s_{T-1} \\
a_{T-1}
\end{array}\right]^{T}\left(F_{T}^{T} V_{T} f_{T}+F_{T}^{T} v_{T}\right)+\operatorname{const}\left(a_{T-1}\right)
\end{aligned}
$$

Переписываем в виде квадратичной формы:

$$
\begin{gathered}
Q_{T-1}^{*}\left(s_{T-1}, a_{T-1}\right)=\frac{1}{2}\left[\begin{array}{l}
s_{T-1} \\
a_{T-1}
\end{array}\right]^{T} Q_{T-1}\left[\begin{array}{l}
s_{T-1} \\
a_{T-1}
\end{array}\right]+\left[\begin{array}{l}
s_{T-1} \\
a_{T-1}
\end{array}\right]^{T} q_{T-1}+\operatorname{const}\left(a_{T-1}\right) \\
Q_{T-1}:=R_{T-1}+F_{T}^{T} V_{T} F_{T} \\
q_{T-1}:=r_{T-1}+F_{T}^{T} V_{T} f_{T}+F_{T}^{T} v_{T}
\end{gathered}
$$

Коли это снова квадратичная формула, мы можем посчитать оптимальное действие, которое будет линейной формой:

$$
\begin{gathered}
a_{T-1}=K_{T-1} s_{T-1}+k_{t-1} \\
\boldsymbol{K}_{T-1}:=-Q_{T-1, a, a}^{-1} Q_{T-1, a, s}, \quad k_{T-1}:=-Q_{T-1, a, a}^{-1} q_{T-1, a}
\end{gathered}
$$

Подставляем её в $V_{T-1}^{*}$ и получаем квадратичную и так далее.
Собираем всё вместе в единую схему.

---

# Алгоритм 29: Обратный проход в LQR 

Вход: $\boldsymbol{F}_{\boldsymbol{t}}, \boldsymbol{f}_{\boldsymbol{t}}-$ функция динамики, $\boldsymbol{R}_{\boldsymbol{t}}, \boldsymbol{r}_{\boldsymbol{t}}-$ функция награды.
Инициализировать $\boldsymbol{V}_{\boldsymbol{T}+\mathbf{1}}=\mathbf{0}_{|\boldsymbol{\delta}| \times|\boldsymbol{\delta}|}, \boldsymbol{v}_{\boldsymbol{T}+\mathbf{1}}=\mathbf{0}_{|\boldsymbol{\delta}|}$
for $\boldsymbol{t}$ от $\boldsymbol{T}$ до 1:

1. считаем Q-функцию:

$$
\begin{gathered}
\boldsymbol{Q}_{\boldsymbol{t}}:=\boldsymbol{R}_{\boldsymbol{t}}+\boldsymbol{F}_{\boldsymbol{t}+\mathbf{1}}^{\boldsymbol{T}} \boldsymbol{V}_{\boldsymbol{t}+\mathbf{1}} \boldsymbol{F}_{\boldsymbol{t}+\mathbf{1}} \\
\boldsymbol{q}_{\boldsymbol{t}}:=\boldsymbol{r}_{\boldsymbol{t}}+\boldsymbol{F}_{\boldsymbol{t}+\mathbf{1}}^{\boldsymbol{T}} \boldsymbol{V}_{\boldsymbol{t}+\mathbf{1}} \boldsymbol{f}_{\boldsymbol{t}+\mathbf{1}}+\boldsymbol{F}_{\boldsymbol{t}+\mathbf{1}}^{\boldsymbol{T}} \boldsymbol{v}_{\boldsymbol{t}+\mathbf{1}}
\end{gathered}
$$

2. считаем оптимальную стратегию:

$$
\begin{gathered}
\boldsymbol{K}_{\boldsymbol{t}}:=-\boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{a}, \boldsymbol{a}}^{-1} \boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{a}, \boldsymbol{s}} \\
\boldsymbol{k}_{\boldsymbol{t}}:=-\boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{a}, \boldsymbol{a}}^{-1} \boldsymbol{q}_{\boldsymbol{t}, \boldsymbol{a}}
\end{gathered}
$$

3. считаем V-функцию:

$$
\begin{gathered}
\boldsymbol{V}_{\boldsymbol{t}}:=\boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{s}, \boldsymbol{s}}+\boldsymbol{K}_{\boldsymbol{t}}^{\boldsymbol{T}} \boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{a}, \boldsymbol{a}} \boldsymbol{K}_{\boldsymbol{t}}+\boldsymbol{K}_{\boldsymbol{t}}^{\boldsymbol{T}} \boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{a}, \boldsymbol{s}}+\boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{s}, \boldsymbol{a}} \boldsymbol{K}_{\boldsymbol{t}} \\
\boldsymbol{v}_{\boldsymbol{t}}:=\boldsymbol{Q}_{\boldsymbol{t}, \boldsymbol{s}, \boldsymbol{a}} \boldsymbol{k}_{\boldsymbol{T}}+\boldsymbol{q}_{\boldsymbol{t}}^{\boldsymbol{T}} \boldsymbol{K}_{\boldsymbol{t}}^{\boldsymbol{T}} \boldsymbol{q}_{\boldsymbol{t}}
\end{gathered}
$$

Выход: $\pi_{t}(s):=\boldsymbol{K}_{t} s+\boldsymbol{k}_{t}$

Поскольку в рамках сделанных предположений мы на самом деле детерминированно знаем, в каких состояниях окажемся (считаем стартовое состояние $\boldsymbol{s}_{\mathbf{1}}$ также известным), мы можем просто вывести оптимальное управление:

## Алгоритм 30: Прямой проход в LQR

Вход: $\boldsymbol{K}_{\boldsymbol{t}}, \boldsymbol{k}_{\boldsymbol{t}}$ - стратегия с прямого прохода, $\boldsymbol{f}$ - функция динамики.
for $\boldsymbol{t}$ от 1 до $\boldsymbol{T}$ :

1. $\boldsymbol{a}_{\boldsymbol{t}}=\boldsymbol{K}_{\boldsymbol{t}} s_{\boldsymbol{t}}+\boldsymbol{k}_{\boldsymbol{t}}$
2. $s_{t+1}=f\left(s_{t}, a_{t}\right)$

Выход: $a_{1}, a_{2}, \ldots a_{T}-$ план.

### 7.4.3. Случай шумной функции перехода

LQR обобщается на случай недетермнированных сред, если предположить, что динамика среды - нормальное распределение с линейной функцией от предыдущих состояний-действий и какой-то фиксированной (зависящей только от момента времени $\boldsymbol{t}$, но не состояний-действий) матрицей ковариации:

$$
\boldsymbol{p}\left(s_{t} \mid s_{t-1}, a_{t-1}\right):=\mathcal{N}\left(f_{t}\left(s_{t-1}, a_{t-1}\right), \Sigma_{t}\right)
$$

Формулу (7.9) можно интерпретировать как «зашумление сенсоров», причём зашумление не зависит от того, в какой области пространства состояний мы оказались.

Теорема 85: В предположении (7.9) схема LQR остаётся неизменной.
Доказательство. Единственное, что поменялось - это зависимость V-функции от Q-функции:

$$
Q_{t-1}^{*}\left(s_{t-1}, a_{t-1}\right)=r_{t-1}\left(s_{t-1}, a_{t-1}\right)+\mathbb{E}_{s_{t}} V_{t}^{*}\left(s_{t}\right)
$$

Покажем, что формулы для матрицы $\boldsymbol{Q}_{\boldsymbol{t}-\mathbf{1}}$ и вектора $\boldsymbol{q}_{\boldsymbol{t}-\mathbf{1}}$ не поменялись, а изменилась только константа (которая на вывод оптимальной стратегии не влияет).

$$
\mathbb{E}_{s_{t}} V_{t}^{*}\left(s_{t}\right)=\mathbb{E}_{s_{t}}\left[\frac{1}{2} s_{t}^{T} V_{t} s_{t}+v_{t}^{T} s_{t}+\operatorname{const}\left(s_{t}\right)\right]=(\)
$$

---

Итак, нужно взять мат.ожидание квадратичной формы по гауссиане. Лезем* в matrix cookbook (yp. (318)) и видим:

$$
\mathbb{E}_{s_{t} \sim \mathcal{N}\left(\mu_{t}, \Sigma_{t}\right)} s_{t}^{T} V_{t} s_{t}=\operatorname{Tr}\left(V_{t} \Sigma_{t}\right)+\mu_{t}^{T} V_{t} \mu_{t}
$$

где $\mu_{t}:=f_{t}\left(s_{t-1}, a_{t-1}\right)$ - среднее гауссианы, $\operatorname{Tr}$ - операция взятия следа.
Итого получим:

$$
(*)=\frac{1}{2} \operatorname{Tr}\left(V_{t} \Sigma_{t}\right)+\frac{1}{2} \mu_{t}^{T} V_{t} \mu_{t}+v_{t}^{T} \mu_{t}+\operatorname{const}\left(s_{t}\right)=V_{t}^{*}\left(\mu_{t}\right)+\operatorname{const}\left(s_{t}\right)
$$

то есть поменялась исключительно константа, которая на оптимальное управление не влияет.

[^0]
# 7.4.4. Iterative LQR (iLQR) 

Вернёмся к детерминированному случаю. Что делать, если функции $\boldsymbol{f}, \boldsymbol{r}$ нам известны, но не являются линейными и квадратичными соответственно? Мы хотели воспользоваться локальным приближением этих функций в окрестности некоторого текущего плана, и в качестве приближений будем использовать разложение в ряд Тейлора до первого и до второго члена соответственно.

Итак, возьмём какой-нибудь план и просчитаем честным прямым проходом точные значения состояний, в которых мы окажемся. Для полученной траектории $\hat{s}_{1}, \hat{a}_{1}, \hat{s}_{2} \ldots \hat{s}_{T}, \hat{a}_{T}$ разложим функции переходов и награды в Тейлора следующим образом:

$$
\begin{gathered}
f_{t}\left(s_{t-1}, a_{t-1}\right) \approx f_{t}\left(\hat{s}_{t-1}, \hat{a}_{t-1}\right)+\nabla_{s, a} f\left(\hat{s}_{t-1}, \hat{a}_{t-1}\right)^{T}\left[\begin{array}{c}
s_{t-1}-\hat{s}_{t-1} \\
a_{t-1}-\hat{a}_{t-1}
\end{array}\right] \\
r_{t}\left(s_{t}, a_{t}\right) \approx \frac{1}{2}\left[\begin{array}{c}
s_{t}-\hat{s}_{t} \\
a_{t}-\hat{a}_{t}
\end{array}\right]^{T} \nabla_{s, a}^{2} r_{t}\left(\hat{s}_{t}, \hat{a}_{t}\right)\left[\begin{array}{c}
s_{t}-\hat{s}_{t} \\
a_{t}-\hat{a}_{t}
\end{array}\right]+\nabla_{s, a} r\left(\hat{s}_{t}, \hat{a}_{t}\right)^{T}\left[\begin{array}{c}
s_{t}-\hat{s}_{t} \\
a_{t}-\hat{a}_{t}
\end{array}\right]
\end{gathered}
$$

Вводим обозначения аналогично LQR:

$$
\begin{gathered}
F_{t}:=\nabla_{s, a} f\left(\hat{s}_{t-1}, \hat{a}_{t-1}\right) \quad f_{t}:=f_{t}\left(\hat{s}_{t-1}, \hat{a}_{t-1}\right) \\
R_{t}:=\nabla_{s, a}^{2} r_{t}\left(\hat{s}_{t}, \hat{a}_{t}\right) \quad r_{t}:=\nabla_{s, a} r\left(\hat{s}_{t}, \hat{a}_{t}\right)
\end{gathered}
$$

Используя LQR с такой приближённой моделью динамики и награды, мы можем пересчитать оптимальную траекторию, проделав backward pass, а затем и forward pass (причём на шаге forward pass, естественно, пользуясь истинными точными функциями $\boldsymbol{f}, \boldsymbol{r}$, а не их разложениями в Тейлора). Технически, нужно только учесть, что в этой рассматриваемой задаче мы «перецентрировали» пространства состояний и действий: LQR тут на шаге $\boldsymbol{t}$ работает с «новым» пространством состояний $\boldsymbol{s}_{\boldsymbol{t}}-\hat{\boldsymbol{s}}_{\boldsymbol{t}}$ и «новым» пространством действий $\boldsymbol{a}_{\boldsymbol{t}}-\hat{\boldsymbol{a}}_{\boldsymbol{t}}$. Поэтому, чтобы окончательно получить оптимальную траекторию на этапе forward pass помимо использования истинной функции переходов нужно ещё учесть эти центрирования при вызове стратегии:

$$
a_{t}=K_{t}\left(s_{t}-\hat{s}_{t}\right)+k_{t}+\hat{a}_{t}
$$

Это соответствует простому учёту добавленных слагаемых в свободном векторе:

$$
k_{t} \leftarrow k_{t}-K_{t} \hat{s}_{t}+\hat{a}_{t}
$$

В полученной траектории снова раскладываем модели в Тейлора, и так далее до удовлетворения.

## Алгоритм 31: Iterative LQR (iLQR)

Вход: $\boldsymbol{f}$ - функция динамики, $\boldsymbol{r}$ - функция награды.
Проинициализировать траекторию $\hat{s}_{1}, \hat{a}_{1}, \hat{s}_{2} \ldots \hat{s}_{T}, \hat{a}_{T}$ при помощи случайной стратегии.
На каждом шаге:


[^0]:    *вывод не так сложен, нужно лишь применять трюк, что след скаляра равен скаляру:

    $$
    \text { затем применять свойство следв } \operatorname{Tr}(A B C)=\operatorname{Tr}(B C A) \text { и получить }
    $$
    \text { (*) }=\mathbb{E}_{s_{t} \sim \mathcal{N}\left(\mu_{t}, \Sigma_{t}\right)} \operatorname{Tr}\left(V_{t} s_{t} s_{t}^{T}\right)
    $$

---

1. Получить $\boldsymbol{F}_{\boldsymbol{t}}, \boldsymbol{f}_{\boldsymbol{t}}, \boldsymbol{R}_{\boldsymbol{t}}, \boldsymbol{r}_{\boldsymbol{t}}$ по формулам (7.10) и (7.11)
2. Получить матрицы $\boldsymbol{K}_{\boldsymbol{t}}, \boldsymbol{k}_{\boldsymbol{t}}$ при помощи алгоритма LQR с матрицами динамики $\boldsymbol{F}_{\boldsymbol{t}}, \boldsymbol{f}_{\boldsymbol{t}}$ и награды $\boldsymbol{R}_{\boldsymbol{t}}, \boldsymbol{r}_{\boldsymbol{t}}$
3. Учесть коррекцию (7.12)
4. При помощи стратегии $\boldsymbol{\pi}\left(\boldsymbol{s}_{\boldsymbol{t}}\right)=\boldsymbol{K}_{\boldsymbol{t}} \boldsymbol{s}_{\boldsymbol{t}}+\boldsymbol{k}_{\boldsymbol{t}}$ заспавиить траекторию $\hat{\boldsymbol{s}}_{\mathbf{1}}, \hat{\boldsymbol{a}}_{\mathbf{1}}, \hat{\boldsymbol{s}}_{\mathbf{2}} \ldots \hat{\boldsymbol{s}}_{\boldsymbol{T}}, \hat{\boldsymbol{a}}_{\boldsymbol{T}}$, используя честный прямой проход с использованием точных функций $\boldsymbol{f}, \boldsymbol{r}$.

Выход: $\hat{\boldsymbol{a}}_{1}, \hat{\boldsymbol{a}}_{2}, \ldots \hat{\boldsymbol{a}}_{\boldsymbol{T}}$ - план.

Мы построили планировщик для непрерывных пространств действий с произвольной (дифференцируемой) динамикой. Мы находимся в некотором состоянии, предполагаем свою будущую траекторию; рассматриваем некоторое приближение поведения среды, которое достаточно точно для предположенной траектории (это разложение в ряд Тейлора); находим оптимальную траекторию при помощи LQR; получаем новую траекторию; рассматриваем приближение поведения среды для новой траектории и так далее до сходимости.

В частности, если модель среды нам неизвестна, iLQR можно применять в качестве планировщика для обученных (например, нейросетевых) приближений динамики среды - см. общую схему model-based подхода 27 .

---

# Next Stage 

Если уж мы рассматриваем задачу RL как попытку создания алгоритма «искусственного интеллекта», то мы должны дополнительно учесть следующие три факта:

- понятно, что в одной и той же среде агент может ставить себе совершенно разные задачи; интеллектуальное обучение должно позволять обобщать решения одних задач на другие, решать сложные задачи, состоящие из составных частей, и, наконец, уметь самостоятельно ставить самому себе «промежуточные» задачи.
- в общем случае, текущее наблюдение среды не описывает её состояние полностью, и агент, во-первых, должен обладать модулем памяти для запоминания предыдущих наблюдений, во-вторых, действовать в условиях неопределённости.
- наконец, в среде могут присутствовать другие агенты, которые могут иметь как схожие, так и противоположные цели, передавать вспомогательную информацию или, в частности, играть роль эксперта, демонстрирующих оптимальное (или полезное) поведение.

В этой главе мы обсудим ряд избранных идей в направлении этих соображений, которые не вписались в предыдущее повествование.

## §8.1. Имитационное обучение

### 8.1.1. Клонирование поведения

Зачастую продемонстрировать то, как надо решать задачу, проще, чем описать её в терминах награды.
Пример 112: Чтобы обучить self-driving car, проще посадить реального водителя за руль и попросить собрать примеры траекторий, чем описать функцию награды, описывающую правила движения.

Пример 113: Чтобы обучить робота переливать воду из стакана в стакан, проще не придумать функцию награды, описывающую такую задачу, а взять руку робота и несколько раз, «держа его за ручку», перелить воду из стакана в стакан.

Допустим, некоторый эксперт $\pi^{\text {expert }}$ поязаимодействовал со средой и собрал для нас набор траекторий $(\mathcal{T})$. Если мы уверены в крутизне эксперта (то есть готовы считать его стратегию оптимальной или в достаточной степени около-оптимальной), то задача обучения собственной стратегии $\pi_{\theta}$ на первый взгляд сводится к задаче обучения с учителем:
| Определение 97: Клонированием поведения (behavioral cloning) называется обучение стратегии воспроизводить действия эксперта:

$$
\sum_{\mathcal{T}} \sum_{s, a \in \mathcal{T}} \log \pi_{\theta}(a \mid s) \rightarrow \max _{\theta}
$$

В случае успеха, мы можем надеяться на то, что наша стратегия $\pi_{\theta}$ будет вести себя не хуже эксперта. При достаточном количестве данных и достаточной крутизне эксперта такой вариант вполне может сработать. При этом мы можем даже не знать или не заниматься придумыванием функции награды: она никак не участвует в обучении.

---

метод 5). Например, в policy gradient алгоритмах можно пробовать запоминать эпизоды обучения, в которых агенту удалось набрать больше всего награды, и с некоторым весом использовать градиент (8.1), посчитанный по этим лучшим траекториям, для обучения актёра. Такой подход называется самоимитацией (self-imitation): просто учимся воспроизводить свои собственные же наилучшие попытки.

Однако, подобная «дистилляция знаний» одного актёра в другого не учитывает то, что мы работаем с последовательным принятием решений. Как только наша стратегия чуть-чуть отклонится от экспертного поведения (а это, так или иначе, неизбежно) или просто получит необычный отклик от среды, она может оказаться в той области пространства состояний, где эксперта никогда не было, и примеров правильного поведения в обучающей выборке не встречалось. Тогда модель будет предсказывать действия для состояний, которых не было при обучении (эту проблему называют covariate shift), и в такой


момент клонированное поведение может выдать сколько угодно безумные результаты.

Так или иначе, клонирование поведение может сыграть роль отличной инициализации, существенно ускоряющей процесс обучения. Если для off-policy обучения имеет смысл просто докинуть экспертные траектория в буфер, то для on-policy алгоритмов при наличии экспертных траекторий имеет смысл предобучить стратегию при помощи клонирования поведения.

Во многих задачах можно побороться с этой проблемой при помощи более «тщательного» или «хитрого» процесса сбора данных; например, специально закатываясь в те области MDP, куда может заехать «сломанная» стратегия.

Пример 114 - DAgger: Одна из универсальных идей звучит так: после клонирования поведения запустить полученную стратегию в среду, собрать набор тех состояний, которые она посетила, и попросить эксперта «разметить» их: выбрать оптимальные действия. Но такой алгоритм предполагает, что у нас есть подобное «средство разметки», за счёт которого задача и сводится к обучению с учителем.

Пример 115 - Quadcopter Navigation in the Forest: Иногда возможно извернуться и придумать какое-нибудь ухищрение, как всё-таки получить в $\overline{\text { RL }}$ «правильные ответы». Например, в этом примере квадроконтер учится лететь вдоль лесной тропинки, выбирая на каждом шаге из трёх действий: влево, вперёд или вправо. Чтобы собрать «обучающую выборку» для него, человек надел шлем с тремя камерами, смотрящими вправо, вперёд и влево, и пошёл по центру лесной тропинки. Собирается такой датасет: для камеры, смотрящей влево, правильный ответ - действие «вправо», для центральной - «вперёд», для правой - «влево». Всё свелось к обучению классификатора.

# 8.1.2. Обратное обучение с подкреплением (Inverse RL) 

Почему клонирование поведения - неидеальное решение? На самом деле, в разных областях пространства состояний у наших решений различается «критичность»: если мы ошибёмся в одном месте, наше будущее поменяется не так сильно, и эксперт в таких областях сам может выбирать довольно разнообразные действия, а в других местах от нашего решения существенно зависит дальнейшая награда, и в них крайне важно безошибочно выбрать то же действие, что и эксперт. Это означает, что функция награды как обучающий сигнал более информативен. Но зачастую функция награды нам напрямую недоступна.

Определение 98: Задачей обратного обучения с подкреплением (inverse reinforcement learning, IRL) называется задача по набору траекторий $(\mathcal{T})$ оптимального агента восстановить функцию награды, которую он максимизирует.

За счёт обратного обучения с подкреплением можно попытаться получить функцию награды, которая позволит обучить при помощи всего арсенала RL алгоритмов куда более близкую к оптимальной стратегию, чем простое клонирование поведения. Однако понятно, что задача обратного обучения с подкреплением «некорректна» в том смысле, что допускает всякие дурацкие решения: например, $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})=\mathbf{0}$ всегда «подходит» в качестве ответа. Какой критерий качества в этой задаче, то есть как понять, насколько адекватна придуманная функция награды?

Пример 116: Рассмотрим клеточный мир, в котором агент может ходить вправо-влево-вниз-вверх. Для простоты также допустим, что функция награды - детерминированная, зависит только от состояний, и на клетках одного цвета её значения совпадают. Что мы тогда можем о ней сказать, имея на руках одну экспертную траекторию, порождённую оптимальной стратегией?

---



На самом деле, не так много. Агент, видимо, стремился в зелёную клетку; наверное, за неё полагается положительная награда. Через красную клетку агент прошёл, хотя мог бы обойти за счёт более позднего попадания в зелёную клетку; видимо, это того не стоило, и за красную клетку награда, если и отрицательная, то совсем маленькая. Могла ли она быть положительной? Тогда бы эксперт, наверное, походил бы по красным клеткам; видимо, награда за зелёную сильно выгоднее. Синне клетки агент избегал; видимо, они или дают штраф, или ноль, поскольку если бы они давали бонус, то было бы выгодно добираться до зелёной клетки-цели через них. Наконец, про жёлтые клетки сказать почти ничего нельзя: агенту пришлось бы в любом случае пройти через них, чтобы добраться до зелёной клетки, и поэтому в них может быть как штраф (но не очень большой), так и бонус (но тоже не очень большой).

Ещё одна проблема задачи в том, что эксперт, обычно, не является абсолютно оптимальным. Люди не перемещаются по комнате до целей по строго прямой линии, а ходы шахматного эксперта не являются абсолютной истиной оптимальных действий.

Для упрощения формул будем везде далее полагать $\gamma=\mathbf{1}$. Введём следующее предположение: оптимальная стратегия $\boldsymbol{\pi}^{*}$ стохастична и генерирует такие траектории, что:

$$
p\left(\mathcal{T} \mid \pi^{*}\right) \propto e^{R(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right)
$$

Иначе говоря, вероятность оптимальной стратегии сгенерировать ту или иную траекторию пропорциональна суммарной кумулятивной награде. Мы считаем, что стратегия эксперта, сгенерировавшего для нас датасет, удовлетворяет этому предположению. Такое предположение позволяет нам записать правдоподобие траекторий: насколько вероятно увидеть траекторию $\mathcal{T}$, если она пришла из оптимального эксперта, максимизировавшего награду в терминах (8.2).

Откуда это предположение свалилось? На самом деле, это уже знакомый нам Maximum Entropy RL. Действительно: рассмотрим задачу поиска стратегии, которая порождает траектории из распределения (8.2):

$$
\mathbf{K L}\left(p(\mathcal{T} \mid \pi) \| p\left(\mathcal{T} \mid \pi^{*}\right)\right) \rightarrow \min _{\pi}
$$

Теорема 86: Задача (8.3) эквивалентна задаче Maximum Entropy RL (6.7).
Доказательство. Распишем (8.3):

$$
\begin{aligned}
& \log p(\mathcal{T} \mid \pi) \\
& \mathrm{KL}\left(p(\mathcal{T} \mid \pi) \| p\left(\mathcal{T} \mid \pi^{*}\right)\right)=\mathbb{E}_{\mathcal{T} \sim \pi} \overbrace{\sum_{t \geq 0} \log \pi\left(a_{t} \mid s_{t}\right)+\log p\left(s_{t+1} \mid s_{t}, a_{t}\right)}- \\
& -\mathbb{E}_{\mathcal{T} \sim \pi} \underbrace{\sum_{t \geq 0} \log p\left(s_{t+1} \mid s_{t}, a_{t}\right)-r_{t}-\operatorname{const}(\pi)}_{\log p\left(\mathcal{T} \mid \pi^{*}\right) \text { из (8.2) }}
\end{aligned}
$$

где const $(\pi)$ - нормировочная константа распределения (8.2). Убирая сокращающиеся логарифмы вероятностей переходов и домножая на минус единицу, получаем:

$$
\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0}\left[r_{t}-\log \pi\left(a_{t} \mid s_{t}\right)\right] \rightarrow \max _{\pi}
$$

что есть в точности Maximum Entropy RL.

---

Оговоримся, что на самом деле у нас нет гарантий, что мы сможем проминимизировать KL-дивергенцию до точного нуля. Другими словами, нельзя сказать, что оптимальная стратегия в Max. Entropy RL обязательно удовлетворяет свойству (8.2).

Утверждение 79: Ноль в задаче (8.3) не обязательно достижим.
Контрпример. Рассмотрим MDP с единственным состоянием, в котором от действий ничего не зависит, но с вероятностью 0.5 агент получает $+\log 2$, а с вероятностью 0.5 агент получает $+0$, после чего игра заканчивается. Распределение (8.2) говорит, что оптимальная стратегия так выбирает действия, что траектория с наградой $+\log 2$ встречает с вероятностью $\frac{2}{3}$, а с наградой $+0-$ с вероятностью $\frac{1}{3}$, однако это невозможно: любая стратегия будет получать их с вероятностями 0.5 .

Таким образом, теорема 86 говорит, что в Maximum Entropy RL можно считать, что оптимизация движет нашу стратегию в сторону (8.3), и поэтому это предположение для моделирования экспертного поведения можно считать разумным, но в строгом смысле оно для оптимальных стратегий не выполняется.

# 8.1.3. Guided Cost Learning 

Аппроксимируем функцию награды нейросетью $\boldsymbol{r}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$. Тогда правдоподобие одной траектории в предположении (8.2) равно:

$$
p_{\theta}\left(\mathcal{T} \mid \pi^{*}\right)=\frac{e^{R_{\theta}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right)}{Z(\theta)}
$$

где $\boldsymbol{R}_{\boldsymbol{\theta}}(\mathcal{T}):=\sum_{\boldsymbol{s}, \boldsymbol{a} \in \mathcal{T}} \boldsymbol{r}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})-$ текущая аппроксимация кумулятивной награды, а нормировочная константа, внимание, зависит от параметров нейросети $\boldsymbol{\theta}$ :

$$
Z(\theta):=\int_{\mathcal{T}} e^{R_{\theta}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right) \mathrm{d} \mathcal{T}
$$

Рассмотрим логарифм правдоподобия:

$$
\log p_{\theta}\left(\mathcal{T} \mid \pi^{*}\right)=R_{\theta}(\mathcal{T})+\underbrace{\sum_{t \geq 0} \log p\left(s_{t+1} \mid s_{t}, a_{t}\right)}_{\text {const }(\theta)}-\log Z(\theta)
$$

Максимизация логарифма правдоподобия имеет интересную интерпретацию: нам нужно выдавать большую награду на тех состояниях, которые эксперт посетил (первое слагаемое) и маленькую награду «всюду в остальных местах» (второе слагаемое). Тогда интеграл $\log Z(\theta)$ будет уменьшаться.

Как оптимизировать это правдоподобие? С первым слагаемым всё понятно: награда, моделируемая нейросеткой, дифференцируема по параметрам. Проблема заключается в нормировочном слагаемом.

Нам понадобится следующий фокус. Пусть у нас есть некоторая функция награды с параметрами $\boldsymbol{\theta}$. Пусть $\pi_{[\theta]}^{*}$ - стратегия, которая оптимально (в терминах Maximum Entropy фреймворка, в рамках предположения (8.2)) оптимизирует вот эту награду, которую мы предлагаем с текущими параметрами $\boldsymbol{\theta}$. Такая стратегия по определению будет в среде генерировать траектории из распределения

$$
p\left(\mathcal{T} \mid \pi_{[\theta]}^{*}\right):=\frac{e^{R_{\theta}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right)}{Z(\theta)}
$$

и это в точности есть правдоподобие траекторий эксперта при текущих значениях параметров $\boldsymbol{\theta}$.

Теорема 87 - Guided Cost Learning: Градиент для оптимизации правдоподобия (8.7) траекторий эксперта по параметрам функции награды $\boldsymbol{\theta}$ равен:

$$
\mathbb{E}_{\mathcal{T} \sim \pi^{*}} \nabla_{\theta} R_{\theta}(\mathcal{T})-\mathbb{E}_{\mathcal{T} \sim \pi_{[\theta]}^{*}} \nabla_{\theta} R_{\theta}(\mathcal{T})
$$

Доказательство. Рассмотрим градиент логарифма правдоподобия одной траектории:

$$
\nabla \log p_{\theta}\left(\mathcal{T} \mid \pi^{*}\right)=\nabla R_{\theta}(\mathcal{T})-\nabla \log Z(\theta)
$$

Мы хотим оптимизировать правдоподобие в среднем по траекториям эксперта, однако нам нужен градиент нормировочной константы (общий для всех траекторий эксперта, поэтому из мат.ожидания по ним эту константу можно вынести). Рассмотрим дифференцирование нормировочной константы отдельно:

$$
\nabla \log Z(\theta)=
$$

---

$$
\begin{aligned}
& =\{\text { градиент логарифма }\} \quad=\frac{1}{Z(\theta)} \nabla Z(\theta)= \\
& =\{\text { определение } Z(\theta)(8.6)\} \quad=\frac{1}{Z(\theta)} \int_{\mathcal{T}} \nabla_{\theta} e^{R_{\theta}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right) \mathrm{d} \mathcal{T}= \\
& =\{\text { дифференцируем экспоненту }\} \quad=\frac{1}{Z(\theta)} \int_{\mathcal{T}} e^{R_{\theta}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right) \nabla_{\theta} R_{\theta}(\mathcal{T}) \mathrm{d} \mathcal{T}= \\
& =\{\text { определение } \pi_{\mid \theta}^{*}(8.8)\}=\mathbb{E}_{\mathcal{T} \sim p\left(\mathcal{T} \mid \pi_{\mid \theta}^{*}\right)} \nabla R_{\theta}(\mathcal{T})
\end{aligned}
$$

Отсюда напрашивается такая забавная идея. С первым мат.ожиданием в формуле градиента всё понятно: мы максимизируем награду в тех парах состояние-действие, которые встречались у эксперта. Затем мы берём и при помощи любого алгоритма Maximum Entropy RL оптимизируем нашу текущую награду $\boldsymbol{r}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$, получая оптимальную относительно нашей текущей функции награды стратегию $\pi_{\mid \theta}^{*}$ (точнее, её некоторое приближение). Отправляем её в реальную среду и собираем траектории $\mathcal{T} \sim \pi_{\mid \theta}^{*}$. И чтобы получить второе слагаемое градиента, говорим, что в состояниях-действиях, которые встретились в траекториях этого «псевдо-эксперта», награду нужно минимизировать. Если наша награда стала настоящей, псевдо-эксперт сойдётся к эксперту, и градиент станет нулевым.

Конечно, мы не будем обучать RL-алгоритм для оптимизации стратегии $\pi_{\mid \theta}^{*}$ до сходимости, и вместо этого будем чередовать шаг оптимизации по параметрам $\boldsymbol{\theta}$ функции награды при помощи формулы (8.9) (где второе мат.ожидание оценено при помощи сэмплов из текущей $\pi_{\mid \theta}^{*}$ ) и шаг оптимизации параметров самой $\pi_{\mid \theta}^{*}$ при помощи RL-алгоритма. В пределе мы потенциально сойдёмся к той функции награды, которой пользовался истинный эксперт, и заодно к стратегии, её максимизирующей.

# 8.1.4. Generative Adversarial Imitation Learning (GAIL) 

Guided Cost Learning со своим чередованием двух шагов оптимизации напоминает игру двух игроков. Оптимизация по параметрам награды говорит уменьшать слагаемое, соответствующее нормировочной константе, а оптимизация стратегии - увеличивать. У этого есть важная интерпретация, которая помогает понять, как на самом деле нужно обучаться с экспертных траекторий.

Сначала мы чуть-чуть перепишем нашу задачу оптимизации по $\boldsymbol{\pi}$ и по $\boldsymbol{r}$.
Утверждение 80: Оптимизация функции награды по формуле (8.9) соответствует оптимизации следующего функционала:

$$
\mathbb{E}_{\mathcal{T} \sim \pi^{*}} R(\mathcal{T})-\max _{\boldsymbol{r}} \mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0}\left(r\left(s_{t}, a_{t}\right)+\mathcal{H}\left(\pi\left(\cdot \mid s_{t}\right)\right)\right) \rightarrow \max _{\boldsymbol{r}}
$$

Пояснение. Градиент максимума от функции есть градиент этой функции в точке максимума, а энтропия $\mathcal{H}\left(\pi\left(\cdot \mid s_{t}\right)\right)$ не зависит от $\boldsymbol{r}$, поэтому градиент второго слагаемого совпадает со вторым слагаемым (8.9).

Определение 99: Occupancy теasure для стратегии $\boldsymbol{\pi}$ будем называть

$$
\rho_{\pi}(s, a):=\pi(a \mid s) d_{\pi}(s)
$$

где $\boldsymbol{d}_{\pi}(s)$ - частоты посещения состояний (5.10).
По определению, мат.ожидания по траектории с таким обозначением можно записывать как

$$
\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0} f\left(s_{t}, a_{t}\right)=\mathbb{E}_{\rho_{\pi}} f(s, a)
$$

Давайте воспользуемся этим обозначением в (8.10). Раскрывая определение мат.ожидания и переписывая оптимизацию по $\boldsymbol{\pi}$ как минимизацию, получаем такую «игру» (читать - поиск седловой точки):

$$
\min _{\boldsymbol{r}} \max _{\boldsymbol{\pi}} \mathbb{E}_{\rho_{\pi}} \boldsymbol{r}(s, \boldsymbol{a})-\mathbb{E}_{\rho_{\pi^{*}}} \boldsymbol{r}(s, \boldsymbol{a})+\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0} \mathcal{H}\left(\pi\left(\cdot \mid s_{t}\right)\right)
$$

Сразу сделаем одно наблюдение: по определению оссирапсу пеаяиге однозначно задаёт стратегию $\boldsymbol{\pi}$ :
Утверждение 81:

$$
\pi(a \mid s)=\frac{\rho_{\pi}(s, a)}{\int \rho_{\pi}(s, a) \mathrm{d} a}
$$

---

Значит, можно поиск стратегии интерпретировать как поиск оссирансу measure: действительно, в (8.12) последнее слагаемое - суммарный энтропийный бонус стратегии $\boldsymbol{\pi}$ - тоже можно переписать в терминах $\boldsymbol{\rho}_{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. Обозначим его как

$$
\widehat{\mathcal{H}}\left(\rho_{\pi}\right):=\mathbb{E}_{\mathcal{T} \sim \pi} \sum_{t \geq 0} \mathcal{H}\left(\pi\left(\cdot \mid s_{t}\right)\right)=\mathbb{E}_{\rho_{\pi}}-\log \pi(a \mid s)=\{(8.13)\}=\mathbb{E}_{\rho_{\pi}}-\log \frac{\rho_{\pi}(s, a)}{\int_{\mathcal{A}} \rho_{\pi}(s, a) \mathrm{d} a}
$$

Получаем такой взгляд на процесс обучения:

$$
\min _{r} \max _{\rho_{\pi}} \mathbb{E}_{\rho_{\pi}} r(s, a)-\mathbb{E}_{\rho_{\pi^{*}}} r(s, a)+\widehat{\mathcal{H}}\left(\rho_{\pi}\right)
$$

Глядя на (8.14), видно, что единственная возможная седловая точка - это случай $\boldsymbol{\rho}_{\boldsymbol{\pi}} \equiv \boldsymbol{\rho}_{\boldsymbol{\pi}}^{*}$ : относительно $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ функционал внутри линеен ${ }^{1}$, как только в какой-то паре $\boldsymbol{s}, \boldsymbol{a}$ разница $\boldsymbol{\rho}_{\boldsymbol{\pi}}-\boldsymbol{\rho}_{\boldsymbol{\pi}^{*}} \neq \mathbf{0}$, функция награды может начать выдавать бесконечно большие значения соответствующего знака. На энтропийное слагаемое $\widehat{\mathcal{H}}\left(\rho_{\pi}\right)$ можно смотреть как на регуляризатор по частотам посещения пар состояния-действия. Минимизация по $\boldsymbol{\rho}_{\boldsymbol{\pi}}$ с точностью до этого энтропийного регуляризатора - это поиск такого $\boldsymbol{\rho}_{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$, который наиболее похож на $\boldsymbol{\rho}_{\boldsymbol{\pi}^{*}}(\boldsymbol{s}, \boldsymbol{a})$.

Другими словами: задача обучения по экспертным траекториям заключается не в том, чтобы матчить нашу стратегию со стратегией эксперта, а в том, чтобы заматчить оссирансу measure - частоты посещений пар состояние-действие. Это сильно разные вещи: выбор одних действий в одной области пространства состояний влияет на оссирансу measure в других! Если мы матчим стратегии, то, вероятно, мы просто во всех состояниях пытаемся сделать нашу стратегию похожей на стратегию эксперта. А на самом деле мы хотим с одинаковой с экспертом частотой во все состояния попадать! И это значит, что имитационное обучение - это матчинг распределений $\boldsymbol{p}\left(\mathcal{T} \mid \pi^{*}\right)$ и $\boldsymbol{p}(\mathcal{T} \mid \boldsymbol{\pi})$ !

Это рассуждение открывает следующую идею: давайте при обучении награды мы будем напрямую пытаться отличать распределения $\boldsymbol{\rho}_{\boldsymbol{\pi}^{*}}(\boldsymbol{s}, \boldsymbol{a})$ и $\boldsymbol{\rho}_{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. Как различать распределения? Вспоминаем генеративносостязательные сети (GAN). Там дискриминатор неявно учил различать, приходят ли сэмплы из одного класса или из другого, просто решая задачу бинарной классификации.

Но как GAN-ы связаны с обратным обучением с подкреплением, с обучением награды? Оказывается, это одно и то же. Чтобы увидеть это в чистом виде, сделаем следующий важный шаг: мы добавим регуляризатор и для оптимизации по $\boldsymbol{r}$. Это можно мотивировать тем, что, как мы обсуждали ранее, задача обратного обучения с подкреплением «некорректна» и может иметь много разных решений. Итак, добавим некоторое слагаемое $\psi(r)$, которое будет смотреть на нашу выдаваемую функцию награды и некоторым образом её дополнительно штрафовать:

$$
\min _{\boldsymbol{r}} \max _{\boldsymbol{\pi}} \psi(r)+\mathbb{E}_{\boldsymbol{\rho}_{\boldsymbol{\pi}}} r(s, a)-\mathbb{E}_{\boldsymbol{\rho}_{\boldsymbol{\pi}}^{*}} r(s, a)+\widehat{\mathcal{H}}\left(\boldsymbol{\rho}_{\boldsymbol{\pi}}\right)
$$

Оказывается, ванильный GAN, различающий сэмплы пар $\boldsymbol{s}, \boldsymbol{a}$ из распределений $\boldsymbol{p}\left(\mathcal{T} \mid \boldsymbol{\pi}^{*}\right)$ и $\boldsymbol{p}(\mathcal{T} \mid \boldsymbol{\pi})$, соответствует просто определённому выбору регуляризатора!

Теорема 88: Выберем в (8.15) следующий регуляризатор:

$$
\psi(r):=\mathbb{E}_{\rho_{\pi}} \cdot r(s, a)+\log \left(1-e^{-r(s, a)}\right)
$$

где штраф полагается бесконечно большим, если $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a}) \leq \mathbf{0}$. Тогда задача (8.15) примет вид:

$$
\min _{D} \max _{\pi}-\mathbb{E}_{\rho_{\pi}} \cdot \log (1-D(s, a))-\mathbb{E}_{\rho_{\pi}} \log D(s, a)+\widehat{\mathcal{H}}\left(\rho_{\pi}\right)
$$

где $D(s, a) \in(0,1)$.
Доказательство. Подставим в (8.15) выбранный регуляризатор:

$$
\begin{aligned}
& \min _{r} \max _{\pi} \psi(r)+\mathbb{E}_{\rho_{\pi}} r(s, a)-\mathbb{E}_{\rho_{\pi}^{*}} r(s, a)+\widehat{\mathcal{H}}\left(\rho_{\pi}\right)= \\
& =\min _{r} \max _{\pi} \mathbb{E}_{\rho_{\pi}^{*}} \cdot \log \left(1-e^{-r(s, a)}\right)+\mathbb{E}_{\rho_{\pi}} r(s, a)+\widehat{\mathcal{H}}\left(\rho_{\pi}\right)
\end{aligned}
$$

Сделаем замену переменных: вместо оптимизации по $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})>\mathbf{0}$ будем оптимизировать по $\boldsymbol{D}(\boldsymbol{s}, \boldsymbol{a})$, где $D(s, a):=e^{-r(s, a)}$ - произвольное число в диапазоне $(0,1)$. Тогда $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})=-\log (D(s, a))$.

$$
\min _{D} \max _{\pi}-\mathbb{E}_{\rho_{\pi^{*}}} \cdot \log (1-D(s, a))-\mathbb{E}_{\rho_{\pi}} \log D(s, a)+\widehat{\mathcal{H}}\left(\rho_{\pi}\right)
$$

[^0]
[^0]:    ${ }^{1}$ поскольку можно переписать мат.ожидания в следующем виде:

    $$
    \mathbb{E}_{\rho_{\pi}} r(s, a)-\mathbb{E}_{\rho_{\pi}} \cdot r(s, a)=\int_{\mathcal{S}} \int_{\mathcal{A}}\left(\rho_{\pi}(s, a)-\rho_{\pi^{*}}(s, a)\right) r(s, a) \mathrm{d} s \mathrm{~d} a
    $$

    Видно, что это фактически линейный функционал $\left\langle\rho_{\pi}-\rho_{\pi^{*}}, \boldsymbol{r}\right\rangle$ в пространстве функций $\boldsymbol{\mathcal { S }} \times \boldsymbol{\mathcal { A }} \rightarrow \mathbb{R}$.

---

Итак, что мы получили в формуле (8.17): вместо награды будем обучать дискриминатор, решающий задачу бинарной классификации, где пары $\boldsymbol{s}, \boldsymbol{a}$ из $\boldsymbol{\rho}_{\boldsymbol{\pi}}$ образуют класс 1 , а пары $\boldsymbol{s}, \boldsymbol{a}$ из $\boldsymbol{\rho}_{\boldsymbol{\pi}}$ - класс 0 . Действительно, оптимизация (8.17) по $\boldsymbol{D}$ при фиксированной $\boldsymbol{\pi}$ выглядит так:

$$
\mathbb{E}_{\rho_{\pi^{*}}} \log (1-D(s, a))+\mathbb{E}_{\rho_{\pi}} \log D(s, a) \rightarrow \max _{D} \mathbf{x}
$$

то есть мы просто учимся отличать пары $\boldsymbol{s}, \boldsymbol{a}$, порождённые (встреченные) экспертом от тех, что встречает наша текущая стратегия.

Оптимизация же по $\boldsymbol{\pi}$ (давайте вернёмся к оптимизации по стратегии) при фиксированном «дискриминатоpe» $\boldsymbol{D}$ выглядит так:

$$
\mathbb{E}_{\boldsymbol{T} \sim \pi} \sum_{t \geq 0}\left[-\log D\left(s_{t}, a_{t}\right)+\mathcal{H}\left(\pi\left(\cdot \mid s_{t}\right)\right)\right] \rightarrow \max _{\pi}
$$

То есть величина $-\log \boldsymbol{D}(\boldsymbol{s}, \boldsymbol{a})$ и будет являться наградой за шаг: именно такую кумулятивную дисконтированную награду (плюс энтропийный бонус) оптимизирует стратегия $\boldsymbol{\pi}$, она же «генератор» в этой схеме. Такой генератор учится порождать $\boldsymbol{s}, \boldsymbol{a}$, которые дискриминатор-награда не отличает от экспертных пар, только в отличие от обычного генератора здесь наши сгенерированные действия влияют на следующие состояния, и поэтому мы генерируем не «отдельные» сэмплы $\boldsymbol{s}, \boldsymbol{a}$, а целые цепочки траекторий $\boldsymbol{T} \sim \boldsymbol{\pi}$.

# 8.1.5. Generative Adversarial Imitation from Observation (GAIfO) 

Часто эксперт предоставляет нам траектории, в которых отсутствует информация о совершённых агентом действиях: есть лишь цепочки состояний $\boldsymbol{s}_{0}, \boldsymbol{s}_{1}, \boldsymbol{s}_{2}, \ldots$, которые посещал эксперт. Такая задача называется имитационным обучением по наблюдениям (imitation learning from observations).

Пример 117: Допустим, у вас есть покадровые анимации того, как персонаж делает сальто. Вы хотите научить робота с такими же конечностями делать тоже самое. В анимациях понятно, в каких координатах находились все конечности персонажа в каждый момент времени, и можно считать, что робот при выполнении задачи должен «посещать» те же цепочки состояний. Однако в анимации действий нету - вы не знаете, как нужно управлять роботом, чтобы получить ту же траекторию в реальной среде.

Оказывается, идея GAIL очень легко и просто расширяется на такую ситуацию. Мы просто хотим попадать в те же состояния, в которые попадал эксперт, поэтому дискриминатором $D(s) \in(0,1)$ теперь будем пытаться различать состояния - порождены ли они экспертом $s \sim d_{\pi^{*}}(s)$ или же нашей текущей стратегией $s \sim d_{\pi}(s)$.

$$
\min _{D} \max _{\pi}-\mathbb{E}_{d_{\pi^{*}}(s)} \log (1-D(s))-\mathbb{E}_{d_{\pi}(s)} \log D(s)+\mathbb{E}_{d_{\pi}(s)} \mathcal{H}(\pi(\cdot \mid s))
$$

В методе Generative Adversarial Imitation from Observation (GAIfO) предлагается сделать хитрость, и различать не состояния, а пары «состояние-следующее состояние» $\boldsymbol{s}, \boldsymbol{s}^{\prime}$. Другими словами, награда полагается зависящей от пары состояний $\boldsymbol{r}\left(\boldsymbol{s}, \boldsymbol{s}^{\prime}\right)$, а дискриминатор $\boldsymbol{D}\left(\boldsymbol{s}, \boldsymbol{s}^{\prime}\right)$ учится различать именно пары $\boldsymbol{s}, \boldsymbol{s}^{\prime}$ из экспертных траекторий и из порождаемых траекторий. Формально говоря, вводится альтернативное определение occupancy measure как вероятность встретить ту или иную пару $\boldsymbol{s}, \boldsymbol{s}^{\prime}$ в траекториях из стратегии $\boldsymbol{\pi}$ :

$$
\nu_{\pi}\left(s, s^{\prime}\right):=d_{\pi}(s) \int_{\mathcal{A}} p\left(s^{\prime} \mid s, a\right) \pi(a \mid s) \mathrm{d} a
$$

Тогда минимаксная задача оптимизации принимает следующий вид:

$$
\min _{D} \max _{\pi}-\mathbb{E}_{\nu_{\pi^{*}}} \log \left(1-D\left(s, s^{\prime}\right)\right)-\mathbb{E}_{\nu_{\pi}} \log D\left(s, s^{\prime}\right)+\mathbb{E}_{d_{\pi}(s)} \mathcal{H}(\pi(\cdot \mid s))
$$

## §8.2. Внутренняя мотивация

### 8.2.1. Вспомогательные задачи

В обучении с подкреплением типична ситуация разреженной функции награды, когда агенту редко поступает сигнал от среды. Например, в худшем случае функция награды представляет собой константу, имеющую смысл «штрафа за потерю времени», а в конце эпизода нам приходит условно +1 , если задача была успешно решена.

Определение 100: Задачей поиска будем называть задачу со следующей функцией награды:

$$
r(s, a)= \begin{cases}+1 & s \in \mathcal{S}^{+} \\ \text {const } & s \notin \mathcal{S}^{+}\end{cases}
$$

где $\mathcal{S}^{+}$- множество терминальных состояний, const $\leq \mathbf{0}$ - штраф за потерю времени.

---

Задача поиска сложна тем, что сигнала от среды нет. Пока мы не решим задачу, оптимизируемый функционал представляет собой плато, а наши RL алгоритмы, как и любые методы оптимизации, работают за счёт разницы в сигнале.

Что в таких ситуациях делать? Первая возможность - учить модель динамики среды, если она неизвестна. Найти сигнал от среды это скорее всего не поможет, поскольку экспоненциальный перебор всевозможных будущих траекторий не сильно лучше случайного блуждания в среде.

Мы придумаем себе другую, вспомогательную задачу, которую мы сможем решать в self-supervised режиме - режиме, не требующем никакого сигнала от среды, никакой «разметки».

Определение 101: Для данной среды $(\mathcal{S}, \mathcal{A}, \mathcal{P})$ вспомогательной задачей называется задача обучения с подкреплением для MDP $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \boldsymbol{r}^{\text {intr }})$, где $\boldsymbol{r}^{\text {intr }}-$ внутренняя мотивация (intrinsic motivation) или внутренняя награда (intrinsic reward function), определяемая самим агентом.

Слова «определяемая самим агентом» означает, что эта функция нам не дана. Исходная задача, которую мы хотим решить, состоит в оптимизации в данной среде некоторой внешней функиией награды (extrinsic reward function) $\boldsymbol{r}^{\text {extr }}$, также называемой внешней мотивацией (extrinsic motivation). Однако, если эта функция награды, например, всегда константна, как в задаче поиска (8.18), то нам необходимо откуда-то взять какой-то другой обучающий сигнал. Этот сигнал нам придётся придумать «самим себе», при помощи нового модуля в обучающейся системе.

Какой обучающий сигнал мы хотим получить? Во-первых, плотный, чтобы было, на чём обучаться базовому алгоритму. Во-вторых, осмысленный: награждающий за развитие каких-то «полезных» в среде навыков, которые могут пригодиться для взаимодействия, условно, вне зависимости от того, какой на самом деле окажется та внешне мотивированная задача, которую агент призван решать.

Пример 118: Идея, можно сказать, вдохновлена подобной «внутренней наградой» человека, поощряющей такое поведение, как игра, любопытство, стремление к познанию. В ходе игрового поведения, человек улучшает своё представление о том, как работает мир вокруг него, по каким законам он устроен и как он своими действиями может влиять на будущее состояние мира и вызывать те или иные явления. Такое более интеллектуальное «самообучение» не только ускоряет поиск сигнала от среды, но и позволяет агенту выработать широкий набор умений, которые скорее всего окажутся полезны для достижения заданной внешней наградой цели, какой бы она ни оказалась.

Придумать в общем случае такой сигнал непросто, и могут возникать ситуации, когда внутренняя мотивация «промотивирует» агента «залипнуть» в какой-то области среды.

Определение 102: Проблемой прокрастинании (procrastination) называется ситуация, когда в некоторой области в среде внутренняя мотивация выдаёт высокий (и не снижающийся с течением обучения) сигнал, перебивающий остальные мотивации агента.

# 8.2.2. Совмещение мотиваций 

По умолчанию всегда считается, что внешняя и внутренняя мотивация складываются:

$$
r(s, a):=r^{\mathrm{extr}}(s, a)+\alpha r^{\mathrm{intr}}(s, a)
$$

где $\boldsymbol{\alpha}$ - масштабирующий гиперпараметр. Для простоты далее будем считать $\boldsymbol{\alpha}=1$.
Заданная так награду можно оптимизировать любым «базовым» алгоритмом RL, ничего не подозревающим о разложении. Но понятно, что агенту доступно каждое слагаемое по отдельности (коли внешняя награда выдаётся средой, а внутренняя генерируется внутри самого алгоритма); как мы можем это использовать?

Пусть оценочные функции с индексом extr соответствуют оценочным функциям для внешней мотивации, с индексом intr - внутренней мотивации, без индекса - суммарной мотивации. Тогда:

## Утверждение 82:

$$
\begin{aligned}
Q^{\pi}(s, a) & =Q_{\text {extr }}^{\pi}(s, a)+Q_{\text {intr }}^{\pi}(s, a) \\
V^{\pi}(s) & =V_{\text {extr }}^{\pi}(s)+V_{\text {intr }}^{\pi}(s)
\end{aligned}
$$

Доказательство. По определению.

---

Итак, если в алгоритме учится оценочная функция $\boldsymbol{V}^{\boldsymbol{\pi}}$ или $\boldsymbol{Q}^{\boldsymbol{\pi}}$, то можно учить оценочные функции каждого слагаемого в награде по отдельности, например, в виде отдельной головы для каждой мотивации.

Этим фактом можно также пользоваться в ситуациях, когда награда представлена в виде суммы нескольких слагаемых (очень типичная ситуация), и агенту доступно полное разложение на эти слагаемые. Это позволит по ходу оптимизационного процесса «включать» и «отключать» мотивации, что бывает очень удобно.


Трюк можно применять даже в value-based методах, где мы учим $\boldsymbol{Q}^{*}$, поскольку тот процесс можно интерпретировать как оценивание (обучение $\boldsymbol{Q}^{\boldsymbol{\pi}}$ ) текущей стратегии $\boldsymbol{\pi}=\underset{a}{\operatorname{argmax}} Q(s, a)$, хотя формально для истинной оптимальной оценочной функции $\boldsymbol{Q}^{*}$ разложение неверно:

Утверждение 83: В общем случае аналогичное разложение для $\boldsymbol{Q}^{*}, \boldsymbol{V}^{*}$ неверно.
Доказательство. Максимум суммы может быть меньше суммы максимумов. Действительно, пусть первое действие даёт внешнюю +1 , а второе действие даёт внутреннюю +1 (иначе по нулям); тогда оптимальные оценочные функции равны $\boldsymbol{V}_{\text {int }}^{*}(\boldsymbol{s})=\boldsymbol{V}_{\text {extr }}^{*}(\boldsymbol{s})=\mathbf{+ 1}$, но оптимальная V-функция для суммы наград равна не 2 , а 1 , поскольку выбрать одновременно два действия нельзя и между мотивациями придётся выбирать.

В Distributional-подходе при использовании этой идеи придётся предполагать независимость внутренней и внешней мотивации (что практически всегда неверно).

Утверждение 84: В общем случае аналогичное разложение для $\boldsymbol{Z}^{\boldsymbol{\pi}}$ неверно.
Доказательство. Это связано с тем, что внутренняя и внешняя мотивация могут быть скоррелированы, и этой информации при доступе к отдельным $\boldsymbol{Z}_{\text {extr }}^{\boldsymbol{\pi}}$ и $\boldsymbol{Z}_{\text {int }}^{\boldsymbol{\pi}}$ у нас нет. Действительно: пусть известно, что для некоторых $\boldsymbol{s}, \boldsymbol{a}$ с вероятностью $0.5 \boldsymbol{Z}_{\text {extr }}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ принимает значение +1 , а иначе 0 , и тоже самое для $\boldsymbol{Z}_{\text {int }}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$. Мы могли бы сказать, что распределение суммы $\boldsymbol{Z}_{\text {extr }}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})+\boldsymbol{Z}_{\text {int }}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ имеет такой вид: с вероятностью 0.25 мы получим +2 , с вероятностью 0.5 получим +1 , и с вероятностью 0.25 не получим ничего. Однако, тогда мы предполагаем независимость этих случайных величин, а это может быть неправдой: например, вдруг мы получаем внешнюю +1 тогда и только тогда, когда получаем внутреннюю +1 . Тогда $\boldsymbol{Z}^{\boldsymbol{\pi}}(\boldsymbol{s}, \boldsymbol{a})$ суммы мотиваций есть на самом деле +2 с вероятностью 0.5 и +0 иначе.

Утверждение 85: В общем случае аналогичное разложение для $\boldsymbol{Z}^{*}$ неверно.
Разложение также позволяет использовать разные коэффициенты дисконтирования для разных мотиваций. Допустим, агент учит отдельно оценочную функцию будущей внутренней мотивации $\boldsymbol{V}^{\text {intr }}$ и будущей внешней мотивации $\boldsymbol{V}^{\text {extr }}$. Тогда можно использовать разное дисконтирование $\gamma^{\text {intr }}$ и $\gamma^{\text {extr }}$ при их обучении. Для примера рассмотрим одношаговые таргеты:

$$
\begin{aligned}
y^{\text {extr }} & :=\boldsymbol{r}+\gamma^{\text {extr }} \boldsymbol{V}^{\text {extr }}\left(s^{\prime}\right) \\
y^{\text {intr }} & :=\boldsymbol{r}+\gamma^{\text {intr }} \boldsymbol{V}^{\text {intr }}\left(s^{\prime}\right)
\end{aligned}
$$

Разложение также позволяет во внутренней мотивации игнорировать понятия эпизодов: это означает, что агент рассматривает весь процесс решения вспомогательной задачи как один большой никогда не заканчивающийся эпизод, не используя флаги done из среды при обучении $\boldsymbol{V}^{\text {intr }}$. Такой агент учитывает, что если он окажется в терминальном состоянии, то произойдёт сброс среды, и он окажется в стартовом состоянии $\boldsymbol{s}_{0} \sim$ $\sim \boldsymbol{p}\left(\boldsymbol{s}_{0}\right)$, для которого значение $\boldsymbol{V}^{\text {intr }}(\boldsymbol{s})$ может быть высоким. Это может мотивировать агента прерывать текущий «неудачный» эпизод ради того, чтобы начать новый, что иногда может быть полезно.

Пример 119 - Агент в яме: В среде существует некоторая труднодоступная область, которую агент внутренне мотивирован посетить. Агент предпринимает попытку добраться до области, но падает в яму. Оценочная функция для внешней мотивации знает, что из ямы уже не выбраться, и оценивает действие «закончить эпизод» как негативное («смерть»), остальные действия как безрезультатные ( +0 ). Оценочной функции для внутренней мотивации, допустим, известно, что из ямы уже не выбраться, и, в случае, если она обучается эпизодично, оценивает любые действия как безрезультатные ( +0 ). Если же оценочная функция игнорирует понятие эпизодов, агент знает, что он может произвести сброс среды и, в частности, попробовать ещё раз добраться до труднодоступной области. Это может промотивировать агента не сидеть в яме, оттягивая негативный, но неизбежный эффект смерти, а приступить к следующему эпизоду обучения.

---

# 8.2.3. Exploration Bonuses 

Как строить $\boldsymbol{r}^{\text {intr }}$ ? Что должен делать агент, который попал в среду, и не получает никакого внешнего сигнала? На этот вопрос можно ответить по-разному.

Пример 120 - Минимизация хаоса (chaos minimization): Нужно искать наиболее «безопасные», стабильные области пространства состояний, где будущее наиболее предсказуемо, «избегать сюрпризов». Интуицией в такой вспомогательной задаче является идея о том, что многие изобретения человечества были созданы для защиты от сюрпризов, которые, зачастую, неприятны. Существуют среды, например, тетрис, где задача «минимизации хаоса» коррелирует с задачей самой игры; агент, решающий такую вспомогательную задачу без доступа к внешней функции награды, «неявно» решает исходную задачу.

Мы далее рассмотрим другой ответ - заниматься исследованием. Отчасти эта задача полностью противоположна минимизации хаоса: однако задачи не противоречат друг другу, ведь чтобы найти самую «спокойную» область среды и научиться до неё добираться, агенту потребуется заняться исследованием окружения и поиском этих самых стабильных областей. В этом смысле, задача исследования, вероятно, является наиболее универсальной вспомогательной задачей, которую агент может себе поставить в среде.

Мы постоянно встречались с дилеммой исследования-использования по ходу пьесы, однако теперь, когда оптимизировать внешний сигнал у нас не получается ввиду его отсутствия, «использовать» нам абсолютно нечего, и дилемма не стоит. Итак, считаем, что нам дана среда и есть задача «заисследовать её». Как формализовать такую задачу в терминах награды?

Поиятно, что случайная стратегия, которой мы часто пользовались до этого для «исследований», является теоретическим решением (например, с точки зрения теоремы 28). Но интуитивно, куда более оптимальным поведением является некий интеллектуальный перебор состояний в среде.

Мы уже встречались с исследовательскими бонусами (exploration


bonus) в контексте UCB-бандитов (раздел 7.1.4): там мы добавляли к нашей оценке Q-функции некоторое слагаемое, имевшее смысл «награды за то, что действие редко пробовалось в прошлом». Наша внутренняя мотивация тоже есть такая добавка, только теперь она должна оценивать новизну посещаемых областей в среде.

Попробуем исходить из схожих соображений: будем награждать агента за посещения тех состояний, в которых он был редко. Мы можем это сделать двумя способами.
| Определение 103: Пусть $\boldsymbol{h}(\boldsymbol{s}): \boldsymbol{S} \rightarrow\{0,1 \ldots N\}$ - некоторая хэш-функция состояний, называемая оракулом (oracle), и $\boldsymbol{n}(\boldsymbol{i})$ - счётчик, сколько раз за время всего обучения нам встретились состояния с хэшем $\boldsymbol{i}$. Тогда

$$
r_{\text {intr }}(s, a):=\frac{1}{n(h(s))}
$$

называется нестационарным исследовательским бонусом; награда

$$
r_{\text {intr }}\left(s_{t}, a_{t}\right):=\mathbb{I}\left[\forall t^{\prime}<t: s_{t} \neq s_{t^{\prime}}\right]
$$

то есть награждение +1 , если мы попали в состояние, хэш для которого $\boldsymbol{h}\left(\boldsymbol{s}_{\boldsymbol{t}}\right)$ не встречался до этого в течение данного эпизода, называется эпизодичным исследовательским бонусом.

Нестационарные исследовательские бонусы затухают с ходом обучения; в пределе мы, надеемся, посетим все состояния достаточное число раз, внутренняя мотивация затухнет и мы переключимся на внешнюю мотивацию. Плохо это тем, что такая мотивация нарушает стационарность формализма MDP, так как с ходом обучения меняются счётчики посещения. Это довольно типично, что внутренняя мотивация нестационарна: модуль внутренней мотивации принципиально есть часть обучающейся системы, и он тоже постепенно «обучается», следовательно, меняется. Для нас это значит, что нужно будет использовать on-policy алгоритмы для обучения на такой сигнал.

Эпизодичные бонусы, конечно же, можно считать модификацией функции награды, и поэтому подобные оракулы можно считать «ручными эвристиками». Агент в том числе по итогам обучения научится в ходе одного эпизода «бегать по всему MDP». Это, однако, вполне может быть полезно в каких-нибудь лабиринтах или задачах, где агенту нужно что-то где-то найти в течение самой игры. Проблема эпизодичных бонусов в том, что они формально нарушают предположение о полной наблюдаемости пространства состояний: функция награды зависит от всей прошлой истории посещения состояний в течение эпизода, и нам по-хорошему нужно переходить в формализм PoMDP.

---

Пример 121: В табличных MDP, где $|\mathcal{S}|<+\infty$, хэш-функция по сути не нужна: $\boldsymbol{h}(\boldsymbol{s})=\boldsymbol{s}$. В ряде сред подобный оракул можно придумать эвристически; например, если у агента есть понятие «координат», можно разделить пространство сеточкой, то есть поделив его на условные блоки, и награждать агента за «посещение большого числа блоков».


# 8.2.4. Дистилляция случайной сетки (RND) 

Поиятно, что в общем случае хэш-функцию придумать сложно, и нам нужен какой-то более универсальный способ оценки новизны. Мы воспользуемся трюком из машинного обучения: известно, что если некоторая модель обучалась решать задачу регрессии, то на входных данных, не похожих на примеры из обучающей выборки, ошибка её прогнозов будет больше. Отличие предсказания модели от истинного значения целевой переменной может быть использовано как приближение оченки нонизны (novelty esimation) или аномальности данных.

Давайте возьмём какую-нибудь случайную задачу регрессии с состояниями в качестве входов. Нам важно лишь, что значение целевой переменной должно определяться по входному состоянию детерминировано и стационарно: ведь если таргет не детерминирован, то ошибка предсказывающей модели для состояния $s$ будет (если, например, модель обучается на минимизацию MSE) в среднем равна дисперсии. В такой ситуации сигнал внутренней мотивации будет выше в тех областях пространства состояний, где дисперсия целевой переменной выше. Нестационарность, очевидно, нарушает идею подхода, поскольку ошибка модели будет связана с изменением целевой переменной, а не новизной входного состояния.

Пусть $\boldsymbol{\phi}: \mathcal{S} \rightarrow \mathbb{R}^{d}$ - некоторая функция, строящая эмбеддинги для состояний; например, случайная сеть (random network) - нейросеть со случайно инициализированными весами, которые не обучаются и никак не изменяются. Пусть $\boldsymbol{f}: \mathcal{S} \rightarrow \mathbb{R}^{\boldsymbol{d}}$ учится предсказывать выход $\boldsymbol{\phi}$, используя в качестве обучающей выборки её значения на встречавшихся в ходе обучения состояниях.
| Определение 104: Задача обучения одной нейросети $\boldsymbol{f}$ на входах-выходах другой (заданной, фиксированной) нейросети $\boldsymbol{\phi}$

$$
\mathbb{E}_{s}\|\boldsymbol{f}(\boldsymbol{s})-\phi(\boldsymbol{s})\|_{2}^{2} \rightarrow \min _{\boldsymbol{f}} \mathrm{n}
$$

где мат.ожидание $\mathbb{E}_{s}$ берётся по произвольному буферу, называется дистилляиией (distillation).
Коли ошибка выше там, где новее состояния, мы можем использовать это значение в качестве обучающего сигнала:

$$
\boldsymbol{r}^{\mathrm{intr}}(\boldsymbol{s}):=\|\boldsymbol{f}(\boldsymbol{s})-\phi(\boldsymbol{s})\|_{\mathbf{2}}^{\mathbf{2}}
$$

Интуиция понятна: если мы «впервые» увидели состояние $s$, модель $f$ ещё ни разу не видела, какой эмбеддинг выдаёт на нём наша случайная сеть $\phi$, и поэтому скорее всего ошибётся. Такой сигнал будет мотивировать агента отправиться в ту область среды, где обучающаяся нейросеть плохо предсказывает выход случайно проинициализированной нейросети.

Здесь и в аналогичных местах далее использование MSE не принципиально (можно использовать любую функцию потерь для регрессии). В частности, можно одну метрику использовать для функции потерь, и другую - для расчёта внутренней мотивации.

Авторы также предложили нормировать сигнал внутренней мотивации на его бегущее среднее отклонение, то есть «в среднем» выдавать некоторую константу. Это существенно упрощает масштабирование внутренней мотивации относительно внешней, но тогда «исследование» не будет естественно затухать с ходом обучения.

---

# 8.2.5. Любопытство 

I Определение 105: Любопытством (curiosity) называется ошибка модели мира агента.
Ошибка свидетельствует о том, что агент не всё знает о той области пространства состояний, в которой был произведён неверный прогноз. Использование любопытства как внутренней мотивации приводит к тому, что агент стремится в те области среды, которые ему «непонятны», в частности те, которые для агента новы, и «дообучить» своё представление о мире. Зачастую выбор действий, максимизирующих ошибку модели мира приводит к возникновению игрового поведения (emergence of playing behavior), когда агент «играет» с доступными для взаимодействия предметами.

Любопытство и построение внутренней мотивации на основе новизны состояний немного различаются. То, что состояние ново, не означает, что оно «незаисследовано», что мы ничего о нём не знаем; мы вполне можем обобщиться за счёт прошлого опыта и спокойно ориентироваться даже в том состоянии, которое технически увидели впервые.

Пример 122 - Бесконечные двери: В ряд стоит бесконечное количество одинаковых дверей, за каждой из которых ничего нет. Агенту доступен на вход, помимо прочего, номер двери. Сигнал, построенный на основе оценки новизны состояний, будет поощрять обнаружение новых дверей, поскольку их номер «нов» по сравнению со старыми. Любопытство же через некоторое время научится предсказывать, что при движении вдоль ряда агенту будут встречаться точно такие же двери, как и раньше, и что за дверьми ничего нет; сигнал затухнет. При этом, любопытство среагирует на любое изменение в описании двери (если все двери были, например, красными, а тут вдруг бац, и дверь синяя), или если за очередной дверью окажется что-то непредсказуемое.


На самом деле, разница достаточно условна: предполагается, что модель предсказания будущего способна «обобщаться» на новые состояния, что, в целом, может оказаться верным и для модели оценки новизны. Так, в приведённом примере модель оценки новизны также может перестать оценивать увеличение номера двери как «новые» состояния, и тогда поведение этих двух видов внутренней мотивации станет схожим.

Пусть внутри агента строится модель прямой динамики:

$$
\mathbb{E}_{s, a, s^{\prime}}\left\|\boldsymbol{f}(s, a)-s^{\prime}\right\|_{2}^{2} \rightarrow \min _{f}
$$

Тогда во время очередного обучающего эпизода ошибка модели $\boldsymbol{f}$ на каждом шаге может рассматриваться как любопытство и задавать внутреннюю мотивацию агента:

$$
\boldsymbol{r}^{\text {intr }}\left(s, \boldsymbol{a}, \boldsymbol{s}^{\prime}\right) \equiv\left\|\boldsymbol{f}(\boldsymbol{s}, \boldsymbol{a})-\boldsymbol{s}^{\prime}\right\|_{2}^{2}
$$

Такой сигнал будет мотивировать агента искать не новые области, а те, в которых он не понимает, как работает среда. В некоторых средах такой сигнал, однако, может привести к прокрастинации.

Определение 106: Шумным телевизором (noisy TV) в среде называются принципиально непредсказуемые в силу стохастичности функции переходов явления.

Пример 123 - Шумный телевизор: В среде стоит сломавшийся телевизор, демонстрирующий гауссовский шум. На каждом шаге шум сэмплируется заново. Предсказывать следующее значение шума по предыдущему в силу независимости сэмплов невозможно. В рамках концепции любопытства, агент мотивирован найти подобный «шумный телевизор» в среде (что может быть непросто) и получать наслаждение от непредсказуемости своих будущих наблюдений.


---

Шумные телевизоры по определению нерелевантны: они не имеют отношения к истинной задаче, и «отвлекают» агента, когда ошибка модели мира принципиально не снижаема. Агент, мотивированный любопытством находить в среде шумные телевизоры - это типичный пример прокрастинации.

Пример 124: «Шумные телевизоры» могут принимать самые разные формы: например, у агента в некоторой области пространства состояний сломались сенсоры, и зашумляются гауссовским шумом. Или, например, среда отправляет агента в одну комнату или в другую с вероятностью 0.5 ; агент не может предсказать, куда именно его перекинет. В видеоиграх могут встречаться всякие декоративные рандомизированные спецэффекты, не имеющие отношения к самой задаче.

С одной стороны, проблема проистекает из того, что мы приближаем стохастичную динамику среды детерминированной моделью. Если стохастика среды существенно влияет на будущее агента и его путь к целевым состояниям, знание и понимание вероятностей различных исходов и их состав, очевидно, является ценным знанием об окружающем мире. Но если задуматься, строить генеративную модель сложного мира - по сути, построить модель вселенной - очень сложная задача, и шумные телевизоры скорее всего задаются сложным распределением, которое в принципе будет плохо поддаваться изучению. В совокупности с нерелевантностью шумных телевизоров, смысла заниматься этим нет, и хочется как-то избежать попыток предсказывать будущие состояния шумных телевизоров вовсе ${ }^{2}$.

Пример 125: Человек не пытается моделировать все без исключения окружающие сложные процессы, например, предсказывать поведение (траектории) всех наблюдаемых капель дождя или опадающих листьев. Вероятно, именно поэтому на них так легко «залипнуть».

# 8.2.6. Модель обратной динамики 

Определение 107: Модель, аппроксимирующая $\boldsymbol{p}\left(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{s}^{\prime}\right)$, называется моделью обратной динамики (inverse dynamics model).

Такая модель тоже представляет собой пример модели мира. Агент, делая шаг в среде, может проверить, а правда ли он может по текущему состоянию $\boldsymbol{s}^{\prime}$ и предыдущему состоянию $\boldsymbol{s}$ восстановить только что выбранное им действие. Отрицательный ответ может соответствовать ситуации, когда агент открыл новые явления в среде, попал в новую область пространства состояний. В таком случае, ошибка модели обратной динамики может быть использована в качестве любопытства.

Задача предсказывать действие по состоянию и следующему состоянию - самая обычная задача классификации для дискретных пространств действий и задача регрессии для непрерывных пространств. Преимуществом модели обратной динамики является то, что построение модели $\mathcal{S} \times \mathcal{S} \rightarrow \mathcal{A}$ сопоставимо по сложности с моделями, использующимися внутри основного RL-алгоритма: не требуется построение моделей, выдающих объекты из $\mathcal{S}$.

Для модели обратной динамики проблема «шумных телевизоров» в среде заменяется симметричной проблемой в пространстве действий $\mathcal{A}$. Из формализма MDP по формуле Байеса следует:

$$
\boldsymbol{p}\left(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{s}^{\prime}\right)=\frac{\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right) \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s})}{\int_{\mathcal{A}} \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right) \boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}) \mathrm{d} \boldsymbol{a}}
$$

Из формулы (8.22) понятно, что, во-первых, искомое распределение существенно зависит от используемой для порождения выборки стратегии $\boldsymbol{\pi}$. Во-вторых, во многих средах пространство действий для некоторых областей $\mathcal{S}$ может содержать принципиально неразличимые действия. Например, если пространство действий непрерывно, то мы скорее всего опять будем использовать детерминированную модель, а в дискретных пространствах действий, когда мы решаем задачу классификации, использование ошибки $-\log q\left(a \mid s, s^{\prime}\right)$, где $q$ - наше приближение (8.22), вовсе не будет выдавать нулевую ошибку даже если мы выучили (8.22) идеально.

Пример 126: Например, если в данном состоянии $\boldsymbol{s}$ два действия в принципе эквивалентны, то есть $\boldsymbol{p}\left(\boldsymbol{s}^{\prime}\right.$ $\left.\mid \boldsymbol{s}, \boldsymbol{a}_{1}\right) \equiv \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}_{2}\right)$, то агент будет мотивирован, находясь в $\boldsymbol{s}$, совершать действия $\boldsymbol{a}_{1}, \boldsymbol{a}_{2}$, поскольку его модель обратной динамики не сможет их различать, и классификатор будет размазывать вероятности между ними. Это типичная ситуация в видеоиграх, когда часто несколько комбинаций кнопок (считающиеся разными действиями) эквивалентны.

[^0]
[^0]:    ${ }^{2}$ поэтому считается, что сигналом любопытства должна быть не столько ошибка модели, сколько её изменение после дообучения. Иными словами, если модель продолжает ошибаться, «понимать явление» в среде не удаётся, и тогда необходимо почувствовать рассумрование (убрать мотивационный сигнал) и бросить силы на исследование других областей среды. Тогда, если возле шумного телевизора ошибка модели мира не падает, агент перестанет на него отвлекаться. Однако построить масштабируемый алгоритм, эффективно замеряющий, как много информации о модели мира предоставил очередной переход ( $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$ ), чтобы превратить это значение во внутреннюю награду, довольно сложно, и далее мы разберём модуль любопытства, который основан на определении через абсолютное значение ошибки модели.

---



Поэтому модель обратной динамики для любопытства обычно не используют. У неё, однако, есть другое, куда более интересное и полезное применение. Рассмотрим модель обратной динамики с «сиамской» архитектурой; для непрерывных пространств действий задача выглядит так:

$$
\mathbb{E}_{s, a, s^{\prime}}\left\|g\left(\phi(s), \phi\left(s^{\prime}\right)\right)-a\right\|_{2}^{2} \rightarrow \min _{g, \phi}
$$

а для классификации как

$$
\mathbb{E}_{s, a, s^{\prime}} \log g\left(a \mid \phi(s), \phi\left(s^{\prime}\right)\right) \rightarrow \max _{g, \phi}
$$

где $\phi(s): \mathcal{S} \rightarrow \mathbb{R}^{d}$ строит некоторое латентное описание состояний, а затем функция $\boldsymbol{g}$ пытается восстановить действие $\boldsymbol{a}$, случившиеся между двумя состояниями, по их латентным описаниям. Что можно сказать о том представлении состояний, которое выучит функция $\phi(s)$ ? Можно ожидать, что она будет оставлять от состояний только информацию, необходимую для предсказания промежуточных действий. Скорее всего, эта информация будет соответствовать описанию только тех объектов в среде, с которыми агент может непосредственно провзаимодействовать, по изменению состояний которых можно судить о том, какое действие совершил агент. Таким образом, $\phi(s)$ будет выдавать описание состояний, очищенное от нерелевантных для агента явлений.

Определение 108: Латентные представления состояний, которые учит функция $\phi(s)$ из задачи (8.23) или (8.24), называются контролируемым состоянием (controlable state), а сама функция $\phi(s)$ - фильтром (filter).

Пример 127: Представьте, что в следующем кадре видеоигры с вероятностью 0.1 моргает декоративное солнышко. Если его моргание никак не связано с агентом и действиями, которые агент выбирает, то это типичный пумный телевизор. Модель прямой динамики пыталась бы безуспешно предсказывать его моргание, из-за чего в предсказаниях будущих состояний всегда была бы некоторая ошибка. Модели же обратной динамики не нужно ничего знать о солнышке, чтобы предсказывать промежуточные действия, поэтому модель быстро научится игнорировать солнышко во входных данных; в описаниях $\phi(s)$ информации о солнышке не будет. Таким образом, фильтр очистит описание состояний от этого лишнего элемента.

А что, если солнышко всё-таки как-то связано с агентом? Тогда в зависимости от состояния солнца назовём его $\boldsymbol{c}$, это часть информации внутри $\boldsymbol{s}$ - при каком-то действии $\boldsymbol{a}$ функция переходов отличается от другого действия $\hat{\boldsymbol{a}}: \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{c}, \boldsymbol{a}\right) \neq \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{c}, \hat{\boldsymbol{a}}\right)$. Тогда классификатору или регрессору в модели обратной динамики будет необходимо оставить информацию $\boldsymbol{c}$ внутри латентного представления $\phi(s)$, чтобы снизить ошибку при различении $\boldsymbol{a}$ и $\hat{\boldsymbol{a}}$.

# 8.2.7. Внутренний модуль любопытства (ICM) 

Свойство модели обратной динамики наводит на мысль, как можно построить защиту от шумных телевизоров. Для этого мы возьмём описание состояний и «почистим» их от шумных телевизоров при помощи фильтра $\phi(s)$, который переведёт описание состояния в некоторое латентное пространство, хранящее лишь частичную информацию о входе. А дальше модель прямой динамики построим в таком «отфильтрованном» латентном представлении:

$$
\mathbb{E}_{s, a, s^{\prime}}\left\|\boldsymbol{f}(\phi(s), \boldsymbol{a})-\phi\left(s^{\prime}\right)\right\|_{2}^{2} \rightarrow \min _{f} \mathbf{n}
$$

Авторы алгоритма ICM предлагают обе модели обучать совместно, то есть в частности оптимизировать (8.25) по параметрам фильтра $\phi$. Итого модель ICM выглядит следующим образом (рассмотрим для примера случай непрерывных действий):

$$
\mathbb{E}_{s, a, s^{\prime}}\left[\left\|g\left(\phi(s), \phi\left(s^{\prime}\right)\right)-a\right\|_{2}^{2}+\alpha\left\|\boldsymbol{f}(\phi(s), \boldsymbol{a})-\phi\left(s^{\prime}\right)\right\|_{2}^{2}\right] \rightarrow \min _{f, g, \phi}
$$

где $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$ - произвольные тройки из любого буфера, $\boldsymbol{\alpha}$ - масштабирующий гиперпараметр.

---

В этой задаче оптимизации градиенты нигде не останавливаются: это значит, что от фильтра $\boldsymbol{\phi}$ требуется построение таких представлений, в рамках которых модели прямой динамики $\boldsymbol{f}$ «проще всего» предсказывать будущее. Понятно, что, если бы первого слагаемого (задачи обратной динамики) в таком функционале не было бы, оптимальным решением было бы $\phi(s)=$ const. Можно считать, что модель прямой динамики выступает регулиризатором для модели обратной динамики: представления $\boldsymbol{\phi}(\boldsymbol{s})$ одновременно должны содержать достаточно информации для предсказания выбранных действий и при этом быть максимально простыми.

В качестве внутренней мотивации используется только ошибка модели прямой динамики:


$$
r^{\mathrm{intr}}\left(s, a, s^{\prime}\right):=\left\|\boldsymbol{f}(\phi(s), a)-\phi\left(s^{\prime}\right)\right\|_{2}^{2}
$$

Решает ли ICM проблему шумных телевизоров? Теоретически, можно рассчитывать на то, что модель обратной динамики отфильтрует те шумные телевизоры, с которыми агент не может провзаимодействовать. Но если у агента есть «пульт» от телевизора, если случайное непредсказуемое явление можно «затриггерить» определёнными действиями, то ICM всё равно приведёт к прокрастинации.

Пример 128 - Управляемый шумный телевизор: В среде присутствует телевизор, демонстрирующий изображение (возможно, осмысленное!) из разнообразного бесконечного набора. У агента есть пульт от телевизора, то есть специальное действие $\hat{\boldsymbol{a}}$, при выборе которого изображение на телевизоре сменяется на случайное из набора. По факту смены изображения между $\boldsymbol{s}$ и $\boldsymbol{s}^{\prime}$ модель обратной динамики может сделать однозначный вывод о том, что между состояниями было выбрано именно действие $\hat{\boldsymbol{a}}$, следовательно в представлении $\boldsymbol{\phi ( s )}$ останется информация о содержимом телевизора. При этом, в силу случайности выбора изображения из набора, предсказать контент телевизора по предыдущему изображению и факту нажатия на пульт невозможно. Следовательно, модель прямой динамики в ICM будет ошибаться на тройках, содержащих $\hat{\boldsymbol{a}}$, и агент будет мотивирован бесконечно выбирать действие $\hat{\boldsymbol{a}}$.

Шумные телевизоры, с которыми агент может провзаимодействовать, представляют собой любые стохастичные явления в среде, которые агент может «запускать» своими действиями. Это не столько проблема самого алгоритма ICM, сколько концептуальная проблема любопытства. Наличие у агента возможности взаимодействовать с шумным телевизором потенциально означает, что некоторая комбинация действий приводит к решению искомой задачи в среде (высокой внешней награде), а сам шумный телевизор - связан с задачей (в примере 128 с управляемым шумным телевизором задача гипотетически могла заключаться в поиске определённого изображения из набора, или совершении определённой комбинации действий при определённых условиях на текущее отображаемое изображение). При этом любопытство, как и любая внутренняя мотивация, вводится из соображений, что априорных знаний об истинной задаче агента не дано, и произвольные явления в среде должны быть исследованы.

В большинстве традиционных задач для тестирования алгоритмов RL такие управляемые телевизоры обычно отсутствуют; обычно, для того, чтобы «сломать» ICM, нужно строить специальную среду. Но понятно, что чем сложнее и реалистичнее рассматриваются задачи, тем больше вероятность натолкнуться на такое явление.

# §8.3. Multi-task RL 

### 8.3.1. Многозадачность

Понятно, что в одной и той же среде мы можем решать много разных задач. Для одной и той же среды каждая задача - в рамках нашего формализма MDP - может быть задана при помощи своей функции награды.

Определение 109: Для данной среды $(\mathcal{S}, \mathcal{A}, \mathcal{P})$ набором подзадач будем называть множество $\mathcal{G}$, элементы $g \in \mathcal{G}$ которого имеют уникальное признаковое описание, для которых задана функция награды $r^{g}(s, a)$ (и, возможно, свой набор терминальных состояний $\mathcal{S}_{g}^{*}$ ).

Пусть наша цель - научиться в среде решать не одну задачу, а много. Сформулируем задачу так. Нам дан набор подзадач и некоторое распределение над задачами $p(g)$. Будем считать, что в начале эпизода нам сэмплируется случайная задача, и дальше в этом эпизоде мы должны решать её. Оптимизировать будем среднюю

---

награду «по задачам»:

$$
\mathbb{E}_{g \sim p(g)} \mathbb{E}_{\boldsymbol{\tau} \sim \pi} \sum_{t \geq 0} \gamma^{t} r_{t}^{g} \rightarrow \max _{\pi}
$$

Пример 129: Типичный пример $\boldsymbol{g}$ - координаты целевой точки, до которой нужно добраться, или в которую нужно что-то переместить.

Пример 130: Например, если вы учите робота ходить, то вы можете в качестве $\boldsymbol{g}$ брать направления движения. Тогда функция награды будет поощрять робота за, например, перемещение по вектору $\boldsymbol{g}$, а сам этот вектор будет генерироваться случайно в начале эпизода. Решая задачу (8.26), робот научится ходить во все стороны, и целевое направление можно будет подавать ему на вход!

Теорема 89: Задачу (8.26) можно свести к обычному MDP, добавив в состояния информацию о текущей решаемой задаче.

Доказательство. Действительно, пусть $\overline{\mathcal{S}}:=\mathcal{S} \times \mathcal{G}$. Пусть начальное состояние будет определяться стохастично как $\left(s_{0}, g\right)$, где $s_{0}$ - начальное состояние нашей исходной среды (без ограничения общности считаем его фиксированным), $\boldsymbol{g} \sim \boldsymbol{p}(\boldsymbol{g})$. В функцию переходов $\hat{\mathcal{P}}$ просто добавим информацию о текущей решаемой задачи:

$$
p\left(s^{\prime} \mid s, g, a\right):=p\left(s^{\prime} \mid s, a\right), \quad g^{\prime}:=g
$$

Функцию награды определим как $\hat{r}(s, g, a):=r^{g}(s, a)$. Наконец, терминальными определим все пары $\left\{(s, g) \mid g \in \mathcal{S}_{g}^{+}\right\}$. Тогда оптимизируемый функционал для MDP $(\overline{\mathcal{S}}, \mathcal{A}, \hat{\mathcal{P}}, \hat{r})$ в точности совпадает с (8.26).

Итак, придуманная постановка задачи просто переводит нас в другое MDP: формально «ничего не изменилось». Но мы поняли важную вещь: решение многих задач в среде «параллельно» эквивалентно хранению в состояниях информации о текущей решаемой задачи. Коли так, и если у задач есть признаковое описание (это было важное предположение), то мы можем искать оценочные функции всего набора совместно: например, моделировать $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{g}, \boldsymbol{a})$, где описание $\boldsymbol{g}$ решаемой задачи поступает модели на вход вместе с $\boldsymbol{s}$. Это в точности эквивалентно тому, что $\boldsymbol{g}$ хранилось бы как часть описание состояний; тогда оно тоже бы поступало на вход оценочным функциям.
Определение 110: Модель для аппроксимации оценочной функций сразу для набора подзадач называется универсальной оценочной функиией (universal value function). Аналогично можно рассматривать «риверсальную стратегию» $\pi(a \mid s, g)$.

# 8.3.2. Мета-контроллеры 

Обсудим, как можно формализм мультизадачности применить в обычной задаче RL. Часто в алгоритме у нас встречаются важные гиперпараметры, которые трудно заранее подобрать. Выделим два примера: коэффициент дисконтирования $\gamma$ и коэффициент масштабирования внутренней мотивации $\boldsymbol{\alpha}$ из уравнения (8.19). Вместо того, чтобы подбирать эти параметры «по сеточке», мы можем запустить multi-task RL, где $\boldsymbol{g}$ - пара $\gamma, \boldsymbol{\alpha}$. Далее, вместо оценочных функций, например, $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$, будем учить универсальные оценочные функции $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a}, \gamma, \boldsymbol{\alpha})$. Такая функция будет выдавать для данной пары $\boldsymbol{s}, \boldsymbol{a}$ будущую награду с учётом поданного на вход дисконтирования $\gamma$ и масштаба бонуса внутренней мотивации $\boldsymbol{\alpha}$. Такое обобщение хорошо и само по себе, поскольку снабдит модель оценочной функции вспомогательными градиентами.

Мета-контроллеры могут помочь автоматически определить, для каких именно значений гиперпараметров $\gamma, \boldsymbol{\alpha}$ алгоритму легче всего максимизировать среднюю внешнюю награду за эпизод. Для этого перед каждым очередным запуском эпизода обучения многорукий бандит (раздел 7.1) выбирает $\gamma, \boldsymbol{\alpha}$, которые будут использоваться в текущем эпизоде обучения для сбора данных. После окончания каждого эпизода бандит считает наградой, полученной «из данного автомата» суммарную (не дисконтированную) внешнюю награду, то есть истинное значение счёта игры.

Этот трюк может успешно применяться для автоматического подбора гиперпараметров прямо по ходу самого обучения. Можно смотреть на это так: будто в формуле (8.26) мы начинаем учить «хорошее» $\boldsymbol{p}(\boldsymbol{g})$.

---

# 8.3.3. Переразметка траекторий 

Рассмотрим обучение универсальной оптимальной Q-функции $\boldsymbol{Q}^{*}\left(\boldsymbol{s}^{\prime}, \boldsymbol{g}, \boldsymbol{a}^{\prime}\right)$ для набора подзадач $\boldsymbol{\mathcal { G }}$. Для данной тройки $\boldsymbol{s}, \boldsymbol{g}, \boldsymbol{a}$ при имеющемся сэмпле следующего состояния $\boldsymbol{s}^{\prime}$ мы можем обучаться на стандартный одношаговый таргет:

$$
y(s, g, a):=r^{g}(s, a)+\gamma \max _{\boldsymbol{a}^{\prime}} Q\left(s^{\prime}, g, a^{\prime}\right)
$$

Сделаем важное наблюдение: допустим, нам известна функция награды $\boldsymbol{r}$. Это довольно типичная ситуация в тех случаях, когда каким-то образом задан целый набор подзадач в среде, но при необходимости её также можно учить по собираемым сэмплам. Тогда заметим, что мы можем для имеющегося сэмпла $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ посчитать значение целевой переменной $\boldsymbol{y}(\boldsymbol{s}, \hat{\boldsymbol{g}}, \boldsymbol{a})$ для любых $\hat{\boldsymbol{g}} \in \mathcal{G}$. Действительно: в силу (8.27) вне зависимости от того, какую цель преследовал агент, собранший переход ( $\boldsymbol{s}, \boldsymbol{g}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{s}^{\prime}$, done), информацию о следующем состоянии $\boldsymbol{s}^{\prime}$ - сэмпл из функции переходов - можно использовать для обучения оценочной функции для любой задачи:

$$
y(s, \hat{g}, a):=r^{\hat{g}}(s, a)+\gamma \max _{\boldsymbol{a}^{\prime}} Q\left(s^{\prime}, \hat{g}, a^{\prime}\right)
$$

Пример 131: Допустим, вы стояли перед дверью ( $\boldsymbol{s}$ ), хотели пиццу ( $\boldsymbol{g}$ ), открыли дверь ( $\boldsymbol{a}$ ) и дверь открылась $\left(s^{\prime}\right)$. Тогда если бы вы, стоя перед дверью ( $\boldsymbol{s}$ ), хотели бы мороженное ( $\hat{g}$ ) и открыли дверь ( $\boldsymbol{a}$ ), дверь всё равно бы открылась ( $\boldsymbol{s}^{\prime}$ не изменился).

Это открывает путь к трансферу знаний (transfer learning): опыт, собранный при решении одной задачи, можно использовать для обучения решению других задач. Понятно, что мы так можем «переразметить» не только переход, но целую траекторию.

Определение 111: Замена в собранной траектории цели $\boldsymbol{g}$ на другую цель $\hat{g}$ и наград $\boldsymbol{r}^{g}(\boldsymbol{s}, \boldsymbol{a})$ на $\boldsymbol{r}^{\hat{g}}(\boldsymbol{s}, \boldsymbol{a})$ называется переразметкой траектории (trajectory relabeling).

В простейшем случае, если размерность пространства задач $\mathcal{G}$ не очень большое, можно «сохранить» в буфере (или использовать для on-policy обучения в зависимости от использующегося алгоритма) переразмеченные траектории для всех $\hat{g}$. Однако, если $\mathcal{G}$ континуально, или поднабор задач богатый, то необходимо выбирать, для каких $\hat{g}$ проводить переразметку. Например, можно посэмплировать $\hat{g}$ случайно; можно ли придумать что-то умнее?

### 8.3.4. Hindsight Experience Replay (HER)



Идею hindsight-переразметки сначала обсудим на примере частного случая задачи RL, задачи поиска (8.18). Мы уже обсуждали как можно справляться с этой задачей при помощи внутренней мотивации (раздел 8.2); сейчас мы сможем при помощи формализма мультизадачности придумать ещё один интересный способ.

Пусть мы предприняли попытку достичь цели и, через некоторое число шагов, остановились в состоянии $\tilde{s}$, так и не справившись с задачей. Очередной отрицательный пример с константной наградой не позволяет нам начать ничему обучаться... хотя постойте-ка. Мы же достигли состояния $\tilde{s}$ ! Давайте положим в него виртуальный тортик. За виртуальный тортик выдадим себе виртуальную +1 . Теперь у нас есть положительный пример: если бы мы хотели достичь состояния $\tilde{s}$, достичь виртуального тортика, то в ходе этой последней попытки мы всё делали правильно. Другими словами, мы говорим: я так и задумывал изначально, я с самого начала хотел добраться до тортика. В английском языке подобному «мышлению задним числом» соответствует выражение «in hindsight», и это слово часто используется для названия алгоритмов с этой идеей.

Hindsight Experience Replay может быть формализован следующим образом. Формально мы зададимся набором подзадач, и все подзадачи будут являться задачи поиска (8.18). Тогда каждая функция награды $\boldsymbol{r}^{g}$ однозначно задаётся множеством терминальных состояний - подмножеством $\mathcal{S}_{g}^{+} \subseteq \mathcal{S}$.

Определение 112: Задачей навигации (navigation) в среде с пространством состояний $\mathcal{S}$ будем называть набор подзадач $\mathcal{G} \equiv \mathcal{S}$, в котором для решения задачи $\boldsymbol{g} \in \mathcal{S}$ необходимо достичь состояния из $\mathcal{S}_{g}^{+}$- близких по некоторой метрике состояний к $\boldsymbol{g}$.


Пример 132: Самый простой и доступный всегда вариант - взять вырожденную метрику, и таким образом рассматривать $\mathcal{S}_{g}^{+} \vDash\{\boldsymbol{g}\}$. В детерминированных средах такая задача даже осмысленна, но дальнейший алгоритм сработает с такой метрикой и для стохастичных сред (поскольку in hindsight мы всегда берём

---

реально достигнутые состояния). Но во многих задачах естественным образом возникают и другие метрики. Например, если вы перемещаете шайбу, и цель задаётся координатами, то можно в качестве метрики взять расстояние от шайбы до цели.

Навигация - это в некотором смысле «полный» набор подзадач: для любого состояния $\boldsymbol{s}$ найдётся задача, для которой оно является терминальным (победным): $\Im \mathcal{S}_{g}^{+} \in \mathcal{G}: \boldsymbol{s} \in \mathcal{S}_{g}^{+}$. Это важно для нас, чтобы мы для любой траектории могли найти задачу $\boldsymbol{g} \in \mathcal{G}$, «решением» которой эта траектория является.

Допустим, обучается какой-либо off-policy алгоритм, работающий с реплей буфером. Все модели в алгоритме, принимающие на вход $s$, теперь также будут принимать описание задачи $\boldsymbol{g}$. Будем сэмплировать $\boldsymbol{g} \sim \boldsymbol{p}(\boldsymbol{g})$, как и раньше, в начале эпизода; или же, если этот вариант нам недоступен, пытаться решить исходную задачу с «истинной» целью, которую обозначим за $g^{*}$.

Допустим, нам поставили весьма конкретную задачу научиться достигать $\mathcal{S}^{+} \subseteq \mathcal{S}$. Возможно, мы не знаем даже описания целевых терминальных состояний («выйти из лабиринта», а где выход - непонятно); тогда положим описание этой «истинной» задачи каким-нибудь специальным вектором $g^{*}$.

Допустим, на очередном шаге мы не смогли решить задачу $g^{*}$, и очередной эпизод закончился в состоянии $\tilde{s}$. Собранные переходы с $\boldsymbol{r}=\mathbf{0}$ и done $=\mathbf{0}$ отправились в буфер. Но мы понимаем, что если бы мы хотели бы решить задачу $\boldsymbol{g}: \tilde{s} \in \mathcal{S}_{g}^{+}$, то данная траектория была победной. Тогда на последнем шаге, в конце эпизода, мы получили бы награду +1 и индикатор завершения эпизода done $=\mathbf{1}$. Итак, мы можем переразметить траекторию новой целью $\boldsymbol{g}$, и добавить переразмеченную траекторию в буфер.

В итоге, если раньше в буфер попадали только примеры с нулевым (константным) сигналом, и обучение было невозможно, теперь же у нас есть примеры с информативным сигналом.

Здесь важно озаботиться богатством реплей буфера для решения вспомогательных задач. Нужно иметь примеры не только правильных последних шагов, но и всех остальных; важно также не перебить вспомогательными задачами буфер и оставить примеры с целью $g^{*}$, чтобы


алгоритм в конечном счёте научился решать и её.

Пример 133: Допустим, вы хотите научиться бить по шайбе так, чтобы она попадала в ворота. Вы бьёте по шайбе, но не попадаете; информативного сигнала нет. Тем не менее, люди каким-то образом способны учиться на подобных ошибках, не имея успешных примеров. Вероятно, мы в реальности мыслим в терминах «левее-правее», которых в формализме MDP нет. С точки зрения HER, мы просто учимся попадать в ту же точку, в которую действительно попали, поскольку у нас есть пример того, как это делается; «если бы ворота были в этой точке, я бы попал». И так, обучаясь попадать в различные точки, модель может обобщится на разные $\boldsymbol{g}$, и однажды попадёт в том числе в реальные ворота $\boldsymbol{g}^{*}$.


# 8.3.5. Hindsight Relabeling 

Обобщим идею HER на произвольные multi-task задачи. Понятно, что примеры «хорошего» решения задачи нам намного ценнее примеров неудач: переразметкой мы боремся с тем, что в классическом машинном обучении назвали бы «несбалансированностью выборки», увеличивая число примеров «более редкого класса» положительного опыта, когда агент набирает больше награды, чем уже умеет набирать.

Итак, дан произвольный набор подзадач $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{g})$. Мы преследовали какую-то цель (неважно какую) и породили траекторию $\mathcal{T}$. Обозначим $\boldsymbol{R}^{g}(\mathcal{T})$ суммарную награду за траекторию с точки зрения задачи $\boldsymbol{g}$. Вопрос: для каких $\boldsymbol{g} \in \mathcal{G}$ имеет смысл переразметить эту траекторию?

Пример 134: Допустим, мы собирали орешки и породили такую траекторию:

---



Для какой задачи данная траектория является «информативной», то есть примером хорошего решения? Очевидный ответ «для пищц» внезапно неверен: да, мы собрали пищц больше чем орешков, но вдруг собрать 5 пищц - это очень мало, и такая траектория наоборот является плохой с точки зрения задачи сбора пищц?

А ещё данная траектория является успешным примером задачи «избежать львов», поскольку ни одного льва в траектории не было. То есть важно не сколько награды $R^{g}(\mathcal{T})$ мы собрали, а сравнение этого значения с тем, сколько может набрать хорошая стратегия решения задачи $\boldsymbol{g}$.

Переформулируем вопрос: для какой задачи $\boldsymbol{g}$ из нашего параметрического семейства $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{g})$ наша траектория является экспертной? Итак, ключевое наблюдение: поиск хороших $\boldsymbol{g}$ для переразметки - это задача обратного обучения с подкреплением, которую мы обсуждали в разделе 8.1.2. В идеях Maximum Entropy Inverse RL и лежит ответ на наш вопрос.

С точки зрения Maximum Entropy подхода (формулы (8.2)), траектория $\mathcal{T}$ является экспертной для задачи $\boldsymbol{g}$ с вероятностью

$$
p(\mathcal{T} \mid \boldsymbol{g}):=\frac{e^{R^{g}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right)}{Z(g)}
$$

Заметим, что нормировочная константа для каждого $\boldsymbol{g}$ своя. Рассмотрим в качестве прайора $\boldsymbol{p}(\boldsymbol{g})$, из которого нам, например, приходят задачи в начале эпизодов или возьмём какое-нибудь равномерное распределение. Тогда по формуле Байеса:

$$
p(\boldsymbol{g} \mid \mathcal{T}) \propto p(\mathcal{T} \mid \boldsymbol{g}) p(\boldsymbol{g})=\frac{p(\boldsymbol{g}) e^{R^{g}(\mathcal{T})}}{Z(\boldsymbol{g})}
$$

Часть с вероятностями переходов сократились с нормировочной константой, поскольку они общие для всех $\boldsymbol{g}$. Эта формула и говорит нам, что нужно использовать для поиска хорошей переразметки $\boldsymbol{g}$ не просто задачи, для которых мы собрали большую суммарную награду $R^{g}(\mathcal{T})$, а отнормированную на соответствующий интеграл $\boldsymbol{Z}(\boldsymbol{g})$. Именно с вероятностями (8.28) цель $\boldsymbol{g}$ можно считать «экспертной».

Формула (8.28) даёт теоретический ответ на наш вопрос, но как использовать её на практике, то есть как сэмплировать из такого распределения? Здесь придётся ограничиться эвристиками. Если $\mathcal{G}$ конечно и мало, то мы можем подсчитать для каждого $\boldsymbol{g}$ все суммарные награды $\boldsymbol{R}^{g}(\mathcal{T})$, но с нормировочной константой дела обстоят плохо:

$$
Z(g)=\int_{\mathcal{T}} e^{R^{g}(\mathcal{T})} \prod_{t \geq 0} p\left(s_{t+1} \mid s_{t}, a_{t}\right) \mathrm{d} \mathcal{T}
$$

где траектории, вообще говоря, должны быть порождены оптимальной стратегией, решающей задачу $\boldsymbol{g}$; мы можем попробовать считать, что текущая универсальная стратегия $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{g})$ и является экспертной для искомого $\boldsymbol{g}$. Однако практические эвристики здесь сводятся к ещё большим упрощениям: например, к тому, чтобы взять какие-нибудь траектории из буфера $\mathcal{T}_{1}, \mathcal{T}_{2} \ldots \mathcal{T}_{M}$ и посчитать примерно среднюю награду, которую мы набираем как-нибудь так:

$$
Z(g) \approx \frac{1}{M} \sum_{i=1}^{M} e^{R^{g}\left(\mathcal{T}_{i}\right)}
$$

Если $\mathcal{G}$ велико или, например, континуально, то просто для поиска хороших $\boldsymbol{g}$ сэмплируется случайно несколько целей-каидидатов, и поиск хороших целей для переразметки проводится среди них.

Пример 135: Продолжим пример 134 и посмотрим на несколько других встречавшихся траекторий из буфера.

---



Теперь мы видим, что собрать 5 пицц - не такое редкое явление; наша собственная стратегия когда-то уже получала даже больше. А вот до тортиков мы в засэмплированных траекториях никогда не добирались, и поэтому нашу траекторию имеет смысл переразметить для задачи «добраться до тортика». Таким образом у нас в буфере появится пример сбора целого 1 тортика, что, согласно примерам других траекторий, «много» для этой задачи.

# §8.4. Иерархическое обучение с подкреплением 

### 8.4.1. Опции

Представьте, что ваша задача - приготовить суп. Пытаться решить задачу методом проб и ошибок вам не придётся: у вас есть рецепт. Однако, беда: в рецепте первым пунктом написано «возьмите чистую кастрюлю»... И учиться брать чистую кастрюлю, видимо, придётся методом проб и ошибок.

Большинство задач в сложных средах делятся на подзадачи. «Рецепта», позволяющего разложить сложную задачу на простые, однако, в общем случае нет, и агенту нужно не просто научиться решать набор подзадач, но и определять, какие именно подзадачи требуется выполнить для успеха.

Мы можем не вводить понятие подзадач или иерархичности и пытаться оптимизировать суммарную награду напрямую, как делали до этого. Но тогда


наш рецепт для приготовления супа будет выглядеть примерно так: «сожмите вот этот перечень мышц, теперь вот этот, так-так-так, ваша рука начала подниматься...». Введение иерархий позволит агенту смотреть на задачу на более абстрактном уровне - на уровне выбора подзадач. Важно, что на этом уровне будет совершенно другой «масштаб времени»: число последовательных решений для решения всей задачи существенно сократится. Такое свойство «высокоуровневых стратегий» называется temporal abstraction. Концептуально, именно на этом уровне агент должен заботиться о награде, описывающей основную задачу. Пока что, для начала, мы ограничимся чуть менее амбициозной задачей: давайте как-нибудь сделаем нашу стратегию «многоуровневой», например, следующим образом.

Определение 113: Для данного MDP оnиией (option) $g$ называется пара* $\left(\pi_{g}, \beta_{g}\right)$ :

- $\pi_{g}$ - стратегия для исходного MDP.
- $\beta_{g}: \mathcal{S} \rightarrow[0,1]$ - политика терминальности (termination policy).
*в оригинале дополнительно рассматривалось множество $\boldsymbol{I}_{\boldsymbol{g}} \subseteq \boldsymbol{\mathcal { S }}$ тех состояний, в которых опцию можно начать выполнять, однако в дальнейшем повествовании ситуация $\boldsymbol{I}_{\boldsymbol{g}} \neq \boldsymbol{\mathcal { S }}$ нам не встретится.

Пусть у нас есть множество опций $\mathcal{G}$, то есть даны или несколько разных стратегий $\pi_{g}$, или просто универсальная стратегия $\pi(a \mid s, g)$. Эти стратегии, работающие «с исходным» MDP на уровне примитивных действий (primitive actions) (элементов $\mathcal{A}$ ), будем далее также называть рабочими (workers). Также мы заводим «высокоуровневую» стратегию, которую далее будем называть менеджером (manager) или мастерстратегией (master policy) $\hat{\pi}(g \mid s)$. Всё, что относится к менеджеру, будем помечать звёздочкой над буквами. Высокоуровневые действия также ещё называют макро-дейстөиями, а примитивные действия -микро-дейстөиями.

---

На очередном шаге менеджер, исходя из текущего состояния $\boldsymbol{s}_{\boldsymbol{t}}$, выбирает опцию, которая будет далее работать в среде: $\boldsymbol{g}_{\boldsymbol{t}} \sim \hat{\pi}\left(\boldsymbol{g}_{\boldsymbol{t}} \mid \boldsymbol{s}_{\boldsymbol{t}}\right)$. Выбранный рабочий генерирует примитивное действие: $\boldsymbol{a}_{\boldsymbol{t}} \sim \boldsymbol{\pi}\left(\boldsymbol{a}_{\boldsymbol{t}} \mid \boldsymbol{s}_{\boldsymbol{t}}, \boldsymbol{g}_{\boldsymbol{t}}\right)$. Среда переходит в новое состояние $\boldsymbol{s}_{\boldsymbol{t}+1} \sim \boldsymbol{p}\left(\boldsymbol{s}_{\boldsymbol{t}+1} \mid \boldsymbol{s}_{\boldsymbol{t}}, \boldsymbol{a}_{\boldsymbol{t}}\right)$. И в этот момент вызывается политика терминальности опции $\boldsymbol{g}$ : с вероятностью $\boldsymbol{\beta}_{\boldsymbol{t}+1} \sim \operatorname{Bernoulli}\left(\boldsymbol{\beta}_{g}\left(\boldsymbol{s}_{\boldsymbol{t}+1}\right)\right)$ рабочий завершает свою работу и снова передаёт решение менеджеру (тот снова выбирает следующего рабочего, и так далее). Если же политика терминальности не срабатывает, менеджер не вызывается, и взаимодействовать со средой продолжает рабочий $\pi_{g_{t}}$.

Для удобства будем считать, что в рамках такого фреймворка в траекториях хранятся не только примитивные действия, но и решения менеджера вместе с решениями политики терминальности:

$$
\mathcal{T}:=\left(g_{0}, a_{0}, r_{0}, s_{1}, \text { done }_{1}, \beta_{1}, g_{1}, a_{1}, r_{1}, s_{2}, \text { done }_{2}, \beta_{2} \ldots\right)
$$

причём $\boldsymbol{g}^{\prime}=\boldsymbol{g}$ с вероятностью 1 , если $\boldsymbol{\beta}^{\prime}=\mathbf{1}$, и $\boldsymbol{\beta}_{0}=\mathbf{0}$. Таким образом, вероятностная модель порождения траекторий задана так:

$$
\begin{aligned}
& \left\{\begin{array}{l}
g_{t} \sim \hat{\pi}\left(g_{t} \mid s_{t}\right) \quad \text { если } \beta_{t}=0 \\
g_{t}:=g_{t-1} \quad \text { если } \beta_{t}=1 \\
a_{t} \sim \pi\left(a_{t} \mid s_{t}, g_{t}\right) \\
s_{t+1} \sim p\left(s_{t+1} \mid s_{t}, a_{t}\right) \\
\beta_{t} \sim \operatorname{Bernoulli}\left(\beta_{g}\left(s_{t}\right)\right) \quad(t>0)
\end{array}\right.
\end{aligned}
$$

Мы поставим себе задачу обучать эту конструкцию end-to-end, то есть оптимизировать параметры стратегии менеджера, стратегий рабочих (опций) и политик терминальности опций напрямую с единственной целью максимизировать среднюю награду.

# 8.4.2. Semi-MDP 

Как нам это всё чудо обучать? Начнём с менеджера. Для простоты будем считать, что набор опций нам дан, то есть даны распределения $\pi_{g}(\boldsymbol{a} \mid \boldsymbol{s})$ и $\boldsymbol{\beta}_{g}(\boldsymbol{s})$; хотим научиться обучать $\hat{\pi}(\boldsymbol{g} \mid \boldsymbol{s})$. Попробуем понять, не живёт ли он в каком-то MDP. Поскольку мы решили, что на вход он получает текущее состояние $\mathcal{S}$, то его пространство состояний такое же, как и в исходном MDP. Его пространством действий являются макро-действия $\mathcal{G}$. На каждом шаге менеджер выбирает $\boldsymbol{g} \in \mathcal{G}$, после чего среда для менеджера работает так: стратегия-рабочий для выбранного $\boldsymbol{g}$ бегает в настоящей среде и решает поставленную подзадачу, переводя среду в новое состояние $\hat{\boldsymbol{s}}^{\prime}$. То, в каком состоянии окажется среда по итогу, полностью задано вероятностной моделью - для менеджера это можно рассматривать как функцию переходов $\hat{\boldsymbol{p}}\left(\hat{\boldsymbol{s}}^{\prime} \mid \boldsymbol{s}, \boldsymbol{g}\right)$. Наконец, награда для менеджера за этот шаг есть сумма собранной за время работы рабочего награды основной цели:

$$
\hat{\boldsymbol{r}}(\boldsymbol{s}, \boldsymbol{a}):=\sum_{t=0}^{\tau} \gamma^{t} r_{t}
$$

где $\boldsymbol{\tau}$ - число шагов, затраченных рабочим на решение задачи. И вот тут возникает нюанс.
Награду, который менеджер получит за второй шаг, необходимо дисконтировать на $\boldsymbol{\gamma}^{\boldsymbol{\tau}}$, поскольку в настоящем MDP прошло $\boldsymbol{\tau}$ шагов. Более того, это $\boldsymbol{\tau}$ - случайная величина; рабочий мог потратить на решение как 10 шагов, так и 100. А значит, менеджер живём не совсем в MDP; для него действия имеют различную продолжительность по времени. Если раньше в MDP все действия, можно считать, гарантированно «выполнялись» один шаг, то теперь, для менеджера, это не так.
| Определение 114: MDP называется полумарковскнм (Semi-Markov decision process, sMDP), если его функция перехода $\overline{\boldsymbol{p}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{\tau} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ помимо следующего состояния возвращает время $\boldsymbol{\tau}>\mathbf{0}$, «затраченное» на выполнение данного шага.

В общем случае в sMDP время шагов может быть и вещественным числом (в частности, так можно учитывать, что среда взаимодействует с агентом в режиме «реального времени»), но в нашем случае $\boldsymbol{\tau}>\mathbf{0}$ - всегда натуральное число. Итак, при работе в sMDP в траекториях дополнительно необходимо хранить время каждого шага $\boldsymbol{\tau}_{\boldsymbol{t}}$; награда за $\boldsymbol{t}$-ый шаг дисконтируется не на $\boldsymbol{\gamma}^{\boldsymbol{t}}$, а на $\boldsymbol{\gamma}^{\sum_{\boldsymbol{t}} \boldsymbol{0}}^{\boldsymbol{\tau}_{\boldsymbol{t}}}$.

Ранее рассматриваемая теория достаточно естественно обобщается на данный кейс. Например, уравнения Беллмана должны учитывать время в дисконтировании; например, для Q-функции:

$$
Q^{\pi}(s, a)=r(s, a)+\mathbb{E}_{s^{\prime}, \tau} \gamma^{\tau} \mathbb{E}_{a^{\prime}} Q^{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

Адаптация Q-learning для sMDP выглядит соответствующе: для перехода ( $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{r}, \boldsymbol{\tau}, \boldsymbol{s}^{\prime}$, done) обновление выглядит так:

$$
Q(s, a) \leftarrow(1-\alpha) Q(s, a)+\alpha\left(r+\gamma^{\tau} \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)\right)
$$

То есть по прошедшему времени $\boldsymbol{\tau}$ мы тоже стохастически аппроксимируем: используем сэмпл из буфера вместе с сэмплом $s^{\prime}$.

---

# 8.4.3. Оценочная функция по прибытию (U-функция) 

Итак, менеджер живёт в полумарковском процессе принятия решений sMDP $(\mathcal{S}, \mathcal{G}, \dot{\mathcal{P}}, \dot{\boldsymbol{r}})$, где $\dot{\boldsymbol{r}}$ определено (8.29), а функция переходов ${ }^{3} \dot{\mathcal{P}}$ задаётся процессом взаимодействия выбранного рабочего со средой с завершением по триггеру соответствующей политики терминальности. Можно обучать оценочные функции менеджера аналогом Q-learning для такого sMDP, но тогда для одного обновления понадобится получать один переход ждать, пока сработает политика терминальности выбранного рабочего. Оказывается, это необязательно.

Вместо этого удобнее работать с теорией на уровне одношаговых рекурсивных соотношений между величинами, где один шаг - это генерация одной случайной величины. Раньше в обычных MDP мы как делали: сгенерировали действие, и вся будущая награда - это Q-функция. Среда ответила нам сэмплом $s^{\prime}$ (ещё наградой за шаг и флагом done, но считаем, что это идёт «в одном комплекте») - и дальнейшая награда есть V-функция. Это было очень удобно, поскольку все эти оценочные функции легко выражались между собой. Нам надо поступить также для траектории, состоящей из случайных величин $g, a, s^{\prime}, \beta^{\prime}$ : выбор менеджера, выбор стратегии, отклик среды, выбор политики терминальности.

По определению, $\hat{Q}(s, g)$ обозначает следующее: менеджер сидел в некотором в состоянии $s$ и выбрал опцию $g$, или, что не существенно, опция $g$ уже была выбрана на предыдущем шаге, а политика терминальности не затригтерилась (в любом случае, в состоянии $s$ активировалась опция $g$ ), и функция возвращает среднюю будущую награду. Мы опустим здесь верхний индекс $\boldsymbol{\tau}$, подразумевая, что мы считаем оценочную функцию для всего комплекта подконтрольных нам распределений: стратегии менеджера, рабочего и политик терминальности. Попробуем получить одношаговое рекурсивное соотношение для $\hat{Q}(s, g)$, связав её с оценочной функцией для следующей случайной величины - выбором действия $\boldsymbol{a}$ рабочим. Каким рабочим? Ну, раз активирована опция $g$, то однозначно рабочим $\pi(a \mid s, g)$.

Мы уже поняли, что на стратегию рабочих можно смотреть как на универсальную стратегию в MDP с пространством состояний $\mathcal{S} \times \mathcal{G}$; мы можем определить $V(s, g)$ и $Q(s, g, a)$ - универсальные оценочные функции рабочих - как оценочные функции в таком MDP, как будущие награды после реализации поступающих на вход случайных величин. Тогда в силу структуры вероятностной модели:

## Утверждение 86:

$$
\hat{Q}(s, g)=V(s, g)=\mathbb{E}_{\pi_{g}(a \mid s)} Q(s, a, g)
$$

Попробуем пойти дальше и построить одношаговое соотношение для $\boldsymbol{Q}(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{g})$. Что случится после выбора действия $\boldsymbol{a}$ рабочим $\pi_{\boldsymbol{g}}$ ? Среда выдаст награду за шаг, перейдёт в следующее состояние $s^{\prime}$, и вот тут дальше случится загвоздка: будет генерироваться бернулиевская $\boldsymbol{\beta}_{\boldsymbol{g}}$. Если политика терминальности затригтерилась, то первым следующем шагом генерации траектории будет сэмплирование действия менеджером, и мы по определению получим столько, сколько выдаёт V-функция менеджера $\hat{V}\left(s^{\prime}\right)$. Здесь важно, что $\hat{V}(s)$ есть хвост награды по траектории после попадания в $s$ при условии (!) срабатывания триггера $\boldsymbol{\beta}=\mathbf{1}$, она предполагает, что первым шагом менеджер сможет выбрать новую опцию. Но если $\boldsymbol{\beta}=\mathbf{0}$, то действие выбирает текущий рабочий, и тогда будущая награда равна $\boldsymbol{V}(\boldsymbol{s}, \boldsymbol{g})$ (которое, как мы уже разобрались в (8.30), совпадает с $\hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{g})$ ). Работать с этим неудобно, поскольку мы не знаем, сработал ли триггер или нет, и поэтому для удобства вводится вспомогательная U-функция - это хвост награды по траектории после попадания в состояние $s$ без каких-либо дополнительных ограничений.

Определение 115: Для sMDP, заданного MDP с набором опций $\left\{\pi_{g}, \beta_{g}\right\}$, U-функцией или оценочной функиией по прибытию (state-value function upon arrival) называется

$$
U\left(s^{\prime}, g\right):=\left(1-\beta_{g}\left(s^{\prime}\right)\right) \hat{Q}\left(s^{\prime}, g\right)+\beta_{g}\left(s^{\prime}\right) \hat{V}\left(s^{\prime}\right)
$$

Что это за формула: на вход U-функция получает состояние, в которое «входит» агент, и текущую опцию. Дальше расписано мат.ожидание по срабатыванию триггера. Если политика терминальности не срабатывает, следующей опцией снова будет $g$, поэтому дальнейшая награда эквивалентна $\hat{Q}\left(s^{\prime}, g\right)$. Иначе выбор переходит к менеджеру и мы получим $\hat{V}\left(s^{\prime}\right)$. Таким образом, по определению:

Утверждение 87:

$$
Q(s, g, a)=r(s, a)+\gamma \mathbb{E}_{s^{\prime}} U\left(s^{\prime}, g\right)
$$

[^0]
[^0]:    ${ }^{3}$ мы, в принципе, можем полностью расписать это распределение $\hat{p}\left(\hat{s}^{\prime}, \boldsymbol{\tau} \mid s, g\right)$, но это получится громоздко: нужно выинтегрировать $\boldsymbol{a}_{0}, s_{1}, a_{1}, \ldots s_{\boldsymbol{\tau}-1}, a_{\boldsymbol{\tau}-1}$, учесть вероятность попадания в $\boldsymbol{p}\left(\hat{s}^{\prime} \mid s_{\boldsymbol{\tau}-1}, a_{\boldsymbol{\tau}-1}\right)$, срабатывание политики терминальности $\beta_{g}\left(\hat{s}^{\prime}\right)$ и несрабатывание политики терминальности для $s_{1}, s_{2} \ldots s_{\tau-1}$ внутри интеграла.

---

# 8.4.4. Intra-option обучение 

Термин «intra-option» означает, что мы можем за счёт таких соотношений обучать функции менеджера, используя информацию после выполнения каждого микро-действия (после получения информации о $s^{\prime}$ ) вне зависимости от того, запускалась ли на данном шаге в принципе стратегия-менеджер или нет. Интуитивно основное соображение выглядит так: если стратегия-рабочий сделала один шаг в среде, а политика терминальности не сработала, то это всё равно что в этот момент менеджер снова выбрал того же самого рабочего.

Допустим, мы хотим обучить $\dot{\boldsymbol{Q}}^{*}(\boldsymbol{s}, \boldsymbol{g})$ - оптимальную Q-функцию менеджера - в предположении, что политика терминальности и стратегии рабочего зафиксированы и не меняются. Попробуем составить уравнение оптимальности для неё в аналогии с обычными уравнениями.
Определение 116: Для sMDP, заданного MDP с фиксированным набором опций $\left\{\pi_{g}, \beta_{g}\right\}$, оптимальной $\boldsymbol{U}$-функиией называется

$$
U^{*}\left(s^{\prime}, g\right):=\left(1-\beta_{g}\left(s^{\prime}\right)\right) \dot{Q}^{*}\left(s^{\prime}, g\right)+\beta_{g}\left(s^{\prime}\right) \max _{g^{\prime}} \dot{Q}^{*}\left(s^{\prime}, g^{\prime}\right)
$$

В этом определении мы по сути просто взяли формулу обычной U-функции (8.31) и заменили в ней $\hat{\boldsymbol{V}}\left(s^{\prime}\right)$ на $\max _{g^{\prime}} \dot{Q}^{*}\left(s^{\prime}, g^{\prime}\right)$ в силу предположения оптимальности, что менеджер всегда выбирает наилучшую опцию $\boldsymbol{g}^{\prime}$. Давайте теперь попробуем выразить $\dot{Q}^{*}(s, g)$ через неё же саму, используя $\boldsymbol{U}^{*}\left(s^{\prime}, g\right)$.

Утверждение 88: Q-функция менеджера удовлетворяет следующему рекурсивному уравнению:

$$
\dot{Q}^{*}(s, g)=\mathbb{E}_{\pi_{g}(a \mid s)}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} U^{*}\left(s^{\prime}, g\right)\right]
$$

Доказательство. Допустим, менеджер выбрал опцию $\boldsymbol{g}$, и рабочий сделал один шаг в среде (сгенерировалось $\boldsymbol{a} \sim \boldsymbol{\pi}_{\boldsymbol{g}}(\boldsymbol{a} \mid \boldsymbol{s})$ и $\boldsymbol{s}^{\prime}$ в исходном MDP). Дисконтирование случилось только на $\boldsymbol{\gamma}$. После этого с вероятностью $\boldsymbol{\beta}_{\boldsymbol{g}}\left(\boldsymbol{s}^{\prime}\right)$ менеджер сможет выбрать новую подзадачу (это бы соответствовало обычному уравнению оптимальности Беллмана, поскольку прошёл всего один шаг), а с вероятностью $\mathbf{1}-\boldsymbol{\beta}_{\boldsymbol{g}}(\boldsymbol{s})$ менеджеру не предоставляется выбора. В такой ситуации можно считать, что менеджер просто «обязан» снова выбрать $\boldsymbol{g}$ : вероятностная модель со следующего шага выглядит в точности также.

Итак, мы можем для обучения менеджера пользоваться Q-learning-ом для sMDP: просить рабочего $\boldsymbol{g}$ решить подзадачу, дождаться $\hat{s}^{\prime}, \boldsymbol{\tau}$ и делать один шаг обновления. Но «intra-option» рекурсивное уравнение (8.34) показывает интересную альтернативу: для обучения функции ценности менеджера для перехода $\mathbb{T}:=(s, g, a, r, s^{\prime}$, done $)$ можно рассчитать целевую переменную как

$$
y(\mathbb{T}):=r(s, a)+\gamma(1-\text { done }) U^{*}\left(s^{\prime}, g\right)
$$

где U-функция вычисляется полностью по формуле (8.33) (мат.ожидание по Бернуливской $\boldsymbol{\beta}$ можем взять явно), и далее, как обычно, минимизировать MSE:

$$
\mathbb{E}_{\mathbb{T}}\left(y(\mathbb{T})-\dot{Q}^{*}(s, g)\right)^{2} \rightarrow \min _{\dot{Q}^{*}}
$$

Заметим, что переходы мы можем брать любые, в том смысле, что не обязательно, чтобы $\boldsymbol{g}$ было выбрано менеджером именно на данном шаге. Для корректного обучения мат.ожиданий нам требуется лишь, чтобы $\boldsymbol{a} \sim$ $\sim \pi_{g}(\boldsymbol{a} \mid \boldsymbol{s}), \boldsymbol{r}=\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$ и $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Обратим внимание на первое условие: сейчас мы считаем опции зафиксированным (частью стационарной среды). Если стратегии рабочих меняются (а они будут меняться, так как будут обучаться), условие стационарности нарушается, и off-policy режим обучения мы, естественно, теряем.

### 8.4.5. Обучение стратегий рабочих

Утверждение 89: В предположении оптимальности менеджера оптимальная Q-функция рабочего удовлетворяет следующему уравнению:

$$
Q^{*}(s, a, g)=\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} U^{*}\left(s^{\prime}, g\right)\right]
$$

Доказательство. Доказательство в точности повторяет теорему 88 за тем исключением, что действие $\boldsymbol{a}$ уже подано оценочной функции на вход.

Мораль отсюда простая: чтобы учить оценочные функции рабочего, мы можем использовать те же таргеты $y(\mathbb{T})$ (8.35). Только теперь для рабочих оценочная функция дополнительно обусловлена на действие $\boldsymbol{a}$.

$$
\mathbb{E}_{\mathbb{T}}\left(y(\mathbb{T})-Q^{*}(s, g, a)\right)^{2} \rightarrow \min _{Q^{*}}
$$

---

Важный момент - здесь можно применять hindsight-приём, который мы встречали в разделе 8.3.3. Действительно, поскольку единственное, что теперь требуется от переходов $-s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$, мы можем проделать переразметку имеющегося в опыте перехода $\mathbb{T} \vDash(s, g, a, r, s^{\prime}$, done $)$ на $\hat{\mathbb{T}} \vDash(s, \hat{g}, a, r, s^{\prime}$, done $)$ для любого $\hat{g}$. То есть: что было бы, если бы в состоянии $s$ мы пользовались бы опцией $\hat{g}$ и выбрали бы действие $\boldsymbol{a}$ ? Тогда мы попали бы в состояние $s^{\prime}$ и получили бы награду $r$. Другой информации для получения прецедента для обучения $Q^{*}(s, \hat{g}, a)$ нам и не нужно. Это значит, что для рабочих мы можем обучать оценочные функции сразу для всех $g \in \mathcal{G}$.

Мы обсудили обучение Q-функций менеджера и рабочего с одношаговых таргетов; конечно же, имея на руках рекурсивные соотношения, мы можем построить полные аналоги всей стандартной теории. Например, для применения Policy Gradient подхода, нам нужны не сами оценочные функции, а их несмещённые оценки; в таких ситуациях достаточно иметь лишь достаточно обучать лишь Q-функцию менеджера. Действительно: если мы знаем $\hat{Q}(s, g)$, то тогда в силу (8.32):

$$
Q(s, g, a) \approx r(s, a)+\gamma U\left(s^{\prime}, g\right), \quad s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)
$$

где $\boldsymbol{U}\left(s^{\prime}, g\right)$ выражается через Q-функцию менеджера в силу формулы (8.31) - несмещённая оценка. В качестве бэйзлайна возможно использовать $\boldsymbol{V}(s, g)$, которая совпадает с $\hat{\boldsymbol{Q}}(s, g)$.

# 8.4.6. Обучение функций терминальности 

Перейдём к оставшемся открытому вопросу: как обучать политику терминальности? Рассмотрим такую ситуацию: мы сидим в состоянии $s$ и выполняем опцию $g$. Нужно ли останавливать рабочего, полагая $\boldsymbol{\beta}(s, g)=$ $=1$, или можно продолжать действовать с его помощью? С точки зрения оптимальных оценочных функций, в первом случае мы получим $\hat{V}^{*}(s)$, а во втором $\boldsymbol{V}^{*}(s, g)$. Но очевидно, что

$$
\hat{V}^{*}(s)=\max _{\hat{g}} \hat{Q}^{*}(s, \hat{g}) \geq \hat{Q}^{*}(s, g)=V^{*}(s, g)
$$

то есть оптимально прерывать рабочего на каждом шаге.
Это, конечно, затыка в нашей теории, поскольку если рабочий прерывается на каждом шаге, никакого temporal abstraction у нас не получится. Попробуем обратится к Policy gradient подходу; мы сможем обучать функции терминальности, пользуясь формулой градиентов по их параметрам, когда сами политики терминальности будут стохастичны.

Пусть $\boldsymbol{\beta}_{\boldsymbol{\theta}}(s, g) \in[0,1]$ выдаёт вероятность бернулливской величины и дифференцируемо по параметрам $\boldsymbol{\theta}$. Пусть $\hat{\boldsymbol{A}}(s, g):=\hat{\boldsymbol{Q}}(s, g)-\hat{\boldsymbol{V}}(s)-$ Advantage-функция менеджера. Функционал, который мы оптимизируем, для начального состояния $s_{0}$, равен по определению $\hat{\boldsymbol{V}}\left(s_{0}\right)$.

## Теорема 90 - Termination Gradient Theorem:

$$
\nabla_{\theta} \hat{V}(s)=-\mathbb{E}_{T \mid s_{0}=s} \sum_{t \geq 0} \gamma^{t} \nabla_{\theta} \boldsymbol{\beta}_{\theta}\left(s_{t}, g_{t}\right) \hat{\boldsymbol{A}}\left(s_{t}, g_{t}\right)
$$

Доказательство. Используя уравнения связи (8.30) и (8.32):

$$
\nabla_{\theta} \hat{V}(s)=\nabla_{\theta} \mathbb{E}_{\theta} \mathbb{E}_{a}\left[r(s, a)+\gamma \mathbb{E}_{s^{\prime}} U\left(s^{\prime}, g\right)\right]
$$

Мат.ожидания здесь берутся по стратегиям менеджера и рабочего, не зависящих от $\boldsymbol{\theta}$, поэтому мы сразу проносим градиент вплоть до оценочной функции по прибытию, а дальше применяем стандартную для Policy Gradient технику.

$$
\begin{aligned}
\nabla_{\theta} U\left(s^{\prime}, g\right) & =\nabla_{\theta}\left[\boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right) \hat{V}\left(s^{\prime}\right)+\left(1-\boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right)\right) \hat{Q}\left(s^{\prime}, g\right)\right]= \\
& =\nabla_{\theta} \boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right)\left[\hat{V}\left(s^{\prime}\right)-\hat{Q}\left(s^{\prime}, g\right)\right]+\boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right) \nabla_{\theta} \hat{V}\left(s^{\prime}\right)+\left(1-\boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right)\right) \nabla_{\theta} \hat{Q}\left(s^{\prime}, g\right)
\end{aligned}
$$

Здесь мы хотим вернуться к мат.ожиданиям по траекториям, и в том числе к мат.ожиданиям по $\boldsymbol{\beta}$. Для этого достаточно заметить, что в полученном выражении ровно такое мат.ожидание и стоит. Действительно, убедимся, что

$$
\boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right) \nabla_{\theta} \hat{V}\left(s^{\prime}\right)+\left(1-\boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right)\right) \nabla_{\theta} \hat{Q}\left(s^{\prime}, g\right)=\mathbb{E}_{\boldsymbol{\beta}_{\theta}} \mathbb{E}_{\boldsymbol{g}^{\prime}} \nabla_{\theta} \hat{Q}\left(s^{\prime}, g^{\prime}\right)
$$

Если в выражении справа выпало $\boldsymbol{\beta}_{\boldsymbol{\theta}}=\mathbf{0}$, то дальше гарантировано $\boldsymbol{g}^{\prime}=\boldsymbol{g}$, мат.ожидание по $\boldsymbol{g}^{\prime}$ вырождается и мы получаем второе слагаемое из выражения слева. Если же в выражении справа выпало $\boldsymbol{\beta}_{\boldsymbol{\theta}}=\mathbf{1}$, то дальше $\mathbb{E}_{\boldsymbol{g}^{\prime}} \nabla_{\theta} \hat{Q}\left(s^{\prime}, g^{\prime}\right)=\nabla_{\theta} \hat{V}\left(s^{\prime}\right)$, и мы получаем первое слагаемое из выражения слева.

Собирая всё вместе, получаем рекурсивную формулу:

$$
\nabla_{\theta} \hat{V}(s)=\mathbb{E}_{\theta} \mathbb{E}_{a} \gamma \mathbb{E}_{s^{\prime}}\left[-\nabla_{\theta} \boldsymbol{\beta}_{\theta}\left(s^{\prime}, g\right) \hat{\boldsymbol{A}}\left(s^{\prime}, g\right)+\mathbb{E}_{\boldsymbol{\beta}} \boldsymbol{E}_{\boldsymbol{g}^{\prime}} \nabla_{\theta} \hat{\boldsymbol{Q}}\left(s^{\prime}, g^{\prime}\right)\right]
$$

Собирая рекурсивную формулу в мат.ожидание по всей траектории, получаем доказываемое.

---

Крайне интуитивная формула (8.36) говорит ровно то, с чего мы начали обсуждение оптимального поведения политики терминальности: если $\boldsymbol{A}(s, g)>0$, то текущая опция лучше той, которую в среднем выбрал бы текущий менеджер, и поэтому стоит продолжать её выполнять; $\boldsymbol{\beta}(s, g)$ уменьшается. Но если $\boldsymbol{A}(s, g)<0$, то менеджер сейчас может выбрать более хорошую опцию, более хорошего рабочего, и поэтому нужно передавать ему управление: $\boldsymbol{\beta}(s, g)$ надо уменьшать.

Как мы знаем, для оптимальных стратегий $\max _{g} \boldsymbol{A}(s, g)=0$, и поэтому такая градиентная оптимизация чревата вырождающими ситуациями. Типично, что менеджер начинает получать управление на каждом шаге. Бороться с этим приходится костылями: например, дополнительными регуляризациями на политику терминальности, чтобы она как можно реже передавала управление менеджеру, и менеджеры с рабочим учились в этих условиях с «неоптимальной» политикой терминальности. Популярное решение - добавить в формуле (8.36) к оценке Advantage небольшое положительное число.

# 8.4.7. Феодальный RL 

В рамках подхода с опциями мы не факт, что выучим какие-то разумные подзадачи; скорее, мы надеемся, что, возможно, рабочие как-то «распределят» между собой области среды, за которые будут ответственны. Мы понимаем, что, деля сложную задачу на последовательность более мелких, мы можем переставать думать об исходной задаче на уровне выполнения подзадач.

Пример 136: То есть: если было принято решение достать чистую кастрюлю, то истинную цель этой установки (награду за приготовление суна) можно не принимать в расчёты и оптимизировать только награду за доставание кастрюли. Такое мышление открывает возможности для переиспользования оптимальных стратегий доставания кастрюли для других задач (например, приготовления пельменей) - потенциальный путь к transfer learning.

Соображение лежит в основе принципа феодализма (feudalism), который можно сформулировать в виде следующих постулатов:

- стратегия, действующая на некотором уровне абстрактности, не должна задумываться о более высокоуровневых задачах («единственная задача вассала - выполнение задач, поставленных феодалом»).
- высокоуровневая стратегия не рассматривает примитивные действия $\mathcal{A}$ («феодала не заботит, как вассал будет достигать поставленных ему целей»).
- при наличии нескольких уровней иерархии действует принцип «вассал моего вассала не мой вассал». В дальнейшем повествовании мы ограничимся двумя уровнями иерархии, но теорию легко можно обобщить на большее число уровней.

Итак, общая схема иерархического RL с двумя уровнями, к которой мы хотим прийти, выглядит так. Откудато берётся набор подзадач; необходимо придумать, откуда его брать. Заводится стратегия-менеджер и стратегиирабочие для каждой подзадачи. Когда менеджер выбирает подзадачу $g$, запускается стратегия соответствующего рабочего $\pi_{g}$. Она делает несколько шагов в исходной среде, пока не решит задачу или не «сдастся» (кастрюли в шкафу не оказалось и нужно бежать в магазин за новой); критерий остановки процесса выполнения стратегии рабочего остаётся открытым. Далее менеджер снова принимает решение о следующей решаемой подзадаче, оптимизируя исходную награду; рабочие стремятся решить свои подзадачи, для чего им нужно как-то задать функцию награды.

Пример 137: В идеале, иерархический RL должен выглядеть как-то примерно так. Менеджер смотрит на текущее состояние и говорит: нужно добраться до банка! Откуда-то берётся функция награды, описывающая задачу $\boldsymbol{g}$ «добраться до банка», и соответствующая стратегия рабочего $\boldsymbol{\pi}(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{g})$ в течение многих шагов эту задачу решает. В банке происходит определение того, что задача решена, и управление снова передаётся менеджеру. Тот видит, что агент находится в банке, и выбирает новую задачу $g$, например, «найти кучу денег». Вызывается новая стратегия рабочего, он снова решает задачу, менеджер выбирает следующую подзадачу, и так далее.

---



Иными словами, задачи и подзадачи в концепции RL всегда задаются MDP, и для подобных иерархических RL-алгоритмов нам нужно определиться, что есть MDP для той стратегии, которая принимает решение о подзадачах, и что есть MDP подзадач.

Строить такую идеальную схему, чтобы алгоритм сам смог выучить и набор подзадач, и механизмы определения моментов передачи управления менеджеру, мы на текущий момент не умеем. В теории опций мы абстрагировались от идей подзадач и оптимизировали всю нашу иерархическую конструкцию на максимизацию исходной функции награды, а основная сложность заключалась в обучении политики терминальности, и с ней же были связаны основные причины нестабильности процесса обучения. В рамках двух следующих алгоритмов мы будем следовать заветам феодального RL и выберем какой-нибудь набор подзадач, но упростим схему, убрав политику терминальности и заменив её на эвристики.

# 8.4.8. Feudal Networks (FuN) 

Напрашивается в качестве набора подзадач брать задачу достижения некоторого состояния $\boldsymbol{g} \in \boldsymbol{\mathcal { S }}$. Основная идея феодальных сетей (feudal networks) в том, чтобы задавать рабочим не целевое состояние, а направление, в котором нам хотелось бы сдвинуться, в пространстве латентных описаний состояний. Для этого менеджер переводит текущее состояние $s_{t}$ в латентное пространство $z_{t}:=\boldsymbol{f}_{\boldsymbol{\theta}}\left(s_{t}\right) \in \mathbb{R}^{\boldsymbol{d}}$, где $\boldsymbol{\theta}$ - параметры, после чего генерирует вектор $\boldsymbol{g}_{\boldsymbol{t}} \in \mathbb{R}^{\boldsymbol{d}}$. Выданный менеджером вектор нормируется, так что $\left\|\boldsymbol{g}_{\boldsymbol{t}}\right\|_{2}=1$ : эта хитрость нужна, чтобы запрашиваемое направление не близко к тривиальному нулевому смещению и не достаточно «большое».

Вместо политик терминальности вводится следующее упрощение: менеджер всё-таки будет вызываться на каждом шаге, и для каждого $s_{t}$ будет генерироваться целевое направление $\boldsymbol{g}_{\boldsymbol{t}}$; но обучаться он будет, исходя из предположения, что рабочий действительно выполняет поставленную задачу, и через $\boldsymbol{K}$ шагов, где $\boldsymbol{K}-$ гиперпараметр, агент успешно сдвинется в указанном менеджером направлении. Иначе говоря, для функции переходов менеджера (которая составлена композицией настоящей функции перехода и стратегии рабочего) вводится следующее предположение:

$$
\mathbf{P}\left(z_{t+K}=z_{t}+g_{t}\right)=1
$$

Здесь $z_{t+K}:=\boldsymbol{f}_{\boldsymbol{\theta}}\left(s_{t+K}\right)$ - латентное описание состояния через $\boldsymbol{K}$ шагов. В жёсткой форме с подобной функцией переходов работать всё равно не столь удобно, поэтому вместо этого предположим более мягкую форму:

$$
p\left(z_{t+K} \mid z_{t}, g_{t}\right) \propto \exp \left(d\left(z_{t+K}-z_{t}, g_{t}\right)\right)
$$

где $\boldsymbol{d}$ - некоторое расстояние между векторами, например, косинусное (с учётом того, что $\left\|\boldsymbol{g}_{\boldsymbol{t}}\right\|_{2}=1$ ):

$$
d\left(z_{t+K}-z_{t}, g_{t}\right):=\frac{\left(z_{t+K}-z_{t}\right)^{T} g_{t}}{\left\|z_{t+K}-z_{t}\right\|_{2}}
$$

В таких предположениях нас волнует лишь направление $z_{t+K}-z_{t}$, в котором сдвинулось состояние; можно считать, что его мы тоже отнормировали, и тогда предположение (8.37) просто задаёт распределение на единичной сфере с центром в $z_{\boldsymbol{k}}$, у которого мода находится в $\boldsymbol{g}$, и чем менее скоррелированы выбранное направление и «цель» $\boldsymbol{g}$, тем меньше вероятность.

## Определение 117: Распределение

$$
p(d \mid g) \propto \exp \left(d^{T} g\right)
$$

где $\|\boldsymbol{g}\|=1$ и $\|\boldsymbol{d}\|=1$, называется распределением Мизес-Фииера (von Mises-Fisher distribution).

---

Утверждение 90: Нормировочная константа распределения Мизес-Фишера (8.38) не зависит от $\boldsymbol{g}$
Доказательство. Рассмотрим нормировочную константу для $\boldsymbol{g}$ :

$$
\int_{\|\boldsymbol{d}\|=1} \exp \left(d^{T} g\right) \mathrm{d} d
$$

Возьмём какое-нибудь другое единичное направление $\hat{g}$. Сделаем замену переменных: пусть $\boldsymbol{M}$ - матрица поворота, переводящая $\hat{g}$ в $\boldsymbol{g}: \boldsymbol{g}=\boldsymbol{M} \hat{g}$. Тогда сделаем замену переменных: $\hat{\boldsymbol{d}}:=\boldsymbol{M}^{\boldsymbol{T}} \boldsymbol{d}$. Если $\boldsymbol{d}$ пробегает единичную сферу, то так как $\boldsymbol{M}$ - матрица поворота, $\hat{\boldsymbol{d}}$ тоже пробегает единичную сферу; определитель матрицы поворота $\boldsymbol{M}$ также единичен, поэтому при замене переменной новых множителей не возникает. Получаем

$$
\int_{\|\boldsymbol{d}\|=1} \exp \left(d^{T} M \hat{g}\right) \mathrm{d} d=\int_{\|\hat{\boldsymbol{d}}\|=1} \exp \left(\hat{\boldsymbol{d}}^{T} \hat{g}\right) \mathrm{d} \hat{\boldsymbol{d}}
$$

что есть нормировочная константа для $\hat{g}$.
Менеджер может считать, что, выбирая действие $\boldsymbol{g}_{\boldsymbol{t}}$, он выбирает состояние, в котором окажется через $\boldsymbol{K}$ шагов, причём предлагается считать, что это распределение задако (8.37), то есть:

$$
\log \hat{\pi}\left(g_{t} \mid s_{t}\right):=\log p\left(z_{t+c} \mid z_{t}, g_{t}\right)=d\left(z_{t+c}-z_{t}, g_{t}\right)+\operatorname{const}(\theta)
$$

Подставляя это в формулу градиента для актёра, получается (для одного перехода) следующая формула:

$$
\nabla_{\theta}^{\text {manager }}=\nabla_{\theta} d\left(z_{t+K}-z_{t}, g_{t}(\theta)\right) \hat{\boldsymbol{A}}\left(s_{t}, g_{t}\right)
$$

Здесь $\hat{\boldsymbol{A}}$ - Advantage-оценка критика менеджера (оцениваемая стандартным для Policy Gradient методов через обучение единственной функции $\hat{\boldsymbol{V}}$ ), а зависимость $\boldsymbol{z}_{\boldsymbol{t}}, \boldsymbol{z}_{\boldsymbol{t}+\boldsymbol{K}}, \hat{\boldsymbol{A}}$ от параметров $\boldsymbol{\theta}$ игнорируется.

Награда для рабочего в задаче следования тем направлениям, которые указывает менеджер, может быть сформулирована так:

$$
r_{t}^{g}:=\frac{1}{K} \sum_{i=0}^{K} d\left(z_{t}-z_{t-i}, g_{t-i}\right)
$$

то есть для каждого из последних $\boldsymbol{K}$ шагов проверяется, насколько точно рабочий следует указаниям менеджера, смещается ли в латентном пространстве менеджера описание состояния в указанном им направлении. Эта награда смешивается с наградой из исходного MDP; можно считать, что $\boldsymbol{r}_{\boldsymbol{t}}^{\boldsymbol{g}}$ для рабочего - внутренняя мотивация, а истинная награда из среды - внешняя мотивация.

# 8.4.9. HIRO 

Алгоритм HIRO в целом похож на FuN, хотя общая схема иерархической стратегии выглядит чуть-чуть по-другому. В этом алгоритме авторы рассматривали задачи непрерывного управления и могли для простоты положить $\mathcal{G} \equiv \mathcal{S}$, не переводя состояния в латентное пространства. Менеджер выбирает цель $\boldsymbol{g}_{\boldsymbol{t}} \in \mathcal{S}$ раз в $\boldsymbol{K}$ шагов, где $\boldsymbol{K}$ - гиперпараметр. В последующие $\boldsymbol{K}-\mathbf{1}$ моментов времени цели определяются по предыдущей по следующему «векторному» правилу:

$$
g_{t+1}-s_{t+1}=g_{t}-s_{t} \quad \Rightarrow \quad g_{t+1}=g_{t}+s_{t+1}-s_{t}
$$

Другими словами, если на шаге $\boldsymbol{t}$ менеджер решил, что состояние $\boldsymbol{s}_{\boldsymbol{t}}$ должно меняться «в направлении» $\boldsymbol{g}_{\boldsymbol{t}}$, то дальше следующие $\boldsymbol{K}$ шагов мы считаем, что хотим этому направлению следовать. Вызов раз в $\boldsymbol{K}$ шагов кажется костылём, но позволяет избежать альтернативы с эвристикой FuN, в котором мы при обучении менеджера предполагали, что рабочий успешно выполняет возложенную на него задачу; тут же менеджера можно обучать лобовым подходом: он живёт даже не в sMDP, а обычном MDP, поскольку всегда гарантируется, что выбранное макро-действие будет выполняться в точности $\boldsymbol{K}$ шагов. Однако, поскольку рабочие будут постепенно обучаться, это MDP нестационарное.

Внутренней мотивацией рабочего, выполняющего цель $\boldsymbol{g}$, полагается расстояние от состояния, в которое попал агент, до целевого состояния $s_{t}+g_{t}$ :

$$
r^{\text {intr }}\left(s_{t}, g_{t}, s_{t+1}\right)=-\left\|s_{t+1}-\left(s_{t}+g_{t}\right)\right\|_{2}
$$

Как и в FuN, такая внутренняя мотивация смешивается со внешней мотивацией - наградой из основного MDP.
Заметим, что рабочего можно спокойно обучать в off-policy режиме (он живёт в самом обычном MDP). Хотелось бы научиться обучать в off-policy режиме и менеджера, но понятно, что пространство действий $\mathcal{G}$ для менеджера с ходом обучения меняет своё семантическое значение. Предлагается лайфхак, очень похожий на

---

идею переразметки траекторий из multi-task RL (см. раздел 8.3.3). Сохраним для менеджера в реплей буфера для $\boldsymbol{s}_{\boldsymbol{t}}, \boldsymbol{g}_{\boldsymbol{t}}$ всю последующую траекторию $\boldsymbol{a}_{\boldsymbol{t}}, \boldsymbol{s}_{\boldsymbol{t}+1} \ldots \boldsymbol{a}_{\boldsymbol{t}+\boldsymbol{K}-1}, \boldsymbol{s}_{\boldsymbol{t}+\boldsymbol{K}}$. Допустим, мы засэмплировали этот переход из буфера и хотим использовать для обучения. Проблема в том, что $\boldsymbol{\pi}^{g}$ для $\boldsymbol{g}$ из перехода уже может быть совершенно другим, и вероятность, что он сгенерировал бы такую траекторию, неприемлемо низкая. Идея: попробуем найти другое $\hat{\boldsymbol{g}} \in \mathcal{G}$, для которого эта вероятность довольно высокая. То есть, мы приблизительно попытаемся решить следующую задачу:

$$
\log p\left(a_{t}, s_{t+1} \ldots a_{t+K-1}, s_{t+K} \mid s_{t}, \hat{g}\right) \rightarrow \max _{\hat{g}}
$$

Рассмотрим, из чего состоит правдоподобие. Мы предполагаем, что менеджер делает выбор в момент времени $\boldsymbol{t}$, то есть менеджер больше на это правдоподобие не влияет. Есть логарифмы переходов $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, которые не зависят от $\hat{g}$, поэтому их можно опустить. Остаются только правдоподобия выборов действий:

$$
\sum_{\hat{t}=t}^{t+K-1} \log \pi_{\hat{g}}\left(a_{\hat{t}} \mid s_{\hat{t}}\right) \rightarrow \max _{\hat{g}}
$$

Почти всегда мы можем для данного $\hat{g}$ посчитать значение выражения. Предлагается взять несколько (штук восемь-десять) $\boldsymbol{g}$, посчитать значение выражения и выбрать наилучшее.

Какую хорошую стратегию перебора для сэмптирования кандидатов $\boldsymbol{g}$ выбрать? Поскольку агент в итоге сдвинулся на вектор $\boldsymbol{s}_{\boldsymbol{t}+\boldsymbol{K}}-\boldsymbol{s}_{\boldsymbol{t}}$, то можно использовать этот вектор в качестве центра гауссианы, из которой будет проводиться сэмплирование. Также предлагается попробовать взять $\boldsymbol{g}$ из буфера и сам центр $\boldsymbol{s}_{\boldsymbol{t}+\boldsymbol{K}}-\boldsymbol{s}_{\boldsymbol{t}}$.

# §8.5. Частично наблюдаемые среды 

### 8.5.1. Частично наблюдаемые MDP

Рассматривавшиеся MDP до этого являлись полностъю наблюдаемыми (fully observable): агенту в качестве наблюдения было доступно всё состояние целиком. Конечно, это довольно-таки существенное упрощение.

Определение 118: MDP называется частично наблюдаемым (partially observable, принятое сокращение РоMDP), если дополнительно задано множество $\mathcal{O}$, называемое пространством наблюдений (observation space), и распределение $\boldsymbol{p}(\boldsymbol{o} \mid \boldsymbol{s})$, определяющая вероятность получить то или иное наблюдение агента $\boldsymbol{o} \in \mathcal{O}$ в момент времени, когда мир находится в состоянии $\boldsymbol{s} \in \mathcal{S}$.

Если MDP - «управляемая» марковская цепь, то РоMDP можно рассматривать как «управляемую» скрытую марковскую цепь: действия влияют на то, как будут порождаться следующие состояния, но для наблюдения доступны не сами состояния, а только какие-то другие случайные величины, про которые, однако, известно, что они зависят только от текущего состояния. При этом состояния всё также удовлетворяют свойству марковости, а функция переходов и процесс генерации наблюдений по состоянию - свойству стационарности.

В РоMDP стратегия должна уметь принимать решение на основании не только текущего наблюдения, но всей истории цепочки наблюдений, так как в них может содержаться информация о текущем состоянии среды. Естественно, эта задача более приближена к реальности, но сильно сложнее. Агенту теперь нужна модель памятии (memory), способ хранения и учёта всей истории в течение эпизода.

Заметим, что награда по своему определению наблюдаема. Если раньше можно было считать, что награда - часть состояния, то теперь награда «по логике» модели есть часть наблюдений, а значит, формально может рассматриваться как функция от наблюдений. Далее будем работать в соглашении, что награда - детерминированная функция от наблюдений.

### 8.5.2. Belief MDP

Допустим, задано PoMDP, и нам доступны все распределения (вероятности переходов и вероятности наблюдений). Пусть мы знаем вероятность $\boldsymbol{p}\left(\boldsymbol{s}_{\mathbf{0}}\right)$, какое состояние генерируется изначально (будем считать, стохастично). Допустим, начался новый эпизод, и мы получили наблюдение $\boldsymbol{o}_{\mathbf{0}} \in \mathcal{O}$. Что мы можем сказать о том, в каком состоянии $\boldsymbol{s}_{\mathbf{0}}$ мы на самом деле оказались? Ответ на этот вопрос формально даёт формула Байеса:

$$
p\left(s_{0} \mid o_{0}\right)=\frac{p\left(o_{0} \mid s_{0}\right) p\left(s_{0}\right)}{\int_{\mathcal{O}} p\left(o_{0} \mid s_{0}\right) p\left(s_{0}\right) \mathrm{d} o_{0}}
$$

Здесь в числителе стоит вероятность первого наблюдения и априорное распределение на состояниях, а в знаменателе стоит нормировочная константа. Пока будем считать, что мы умеем брать любые интегралы (например, если пространства состояний и наблюдений конечны и малы).

Допустим, мы выбрали действие $\boldsymbol{a}_{\mathbf{0}}$, попали в состояние $\boldsymbol{s}_{\mathbf{1}}$ (которое мы не видим) и получили наблюдение $\boldsymbol{o}_{\mathbf{1}}$. Что мы можем сказать о том, в каком состоянии $\boldsymbol{s}_{\mathbf{1}}$ мы находимся? Опять же, пользуясь формулой Байеса и используя вероятностную модель РоMDP, мы можем получить точную формулу наших представлений о том,

---

в каком состоянии на текущий момент пребывает агент. Чтобы проделать вывод, обозначим за $\mathcal{T}_{t}$ наблюдаемую (действия, награды, наблюдения, но не состояния) траекторию до момента времени $t$, включая последнее наблюдение:

$$
\mathcal{T}_{t}:=\left(o_{0}, a_{0}, r_{0}, o_{1}, a_{1}, r_{1} \ldots o_{t-1}, a_{t-1}, r_{t-1}, o_{t}\right)
$$

Определение 119: Вероятность пребывания в том или ином состоянии при условии наблюдаемой истории $\boldsymbol{p}\left(s_{t} \mid \mathcal{T}_{t}\right)$ называется belief state и сокращённо обозначается $b_{t}$.

По определению $b_{0}\left(s_{0}\right)=p\left(s_{0} \mid o_{0}\right)$, которую мы получили в по формуле Байеса в (8.39). Как обновить belief state после совершения одного шага в среде (получения одного наблюдения и выбора одного действия)?

Теорема 91: С точностью до нормировочной константы:

$$
b_{t}\left(s_{t}\right) \propto p\left(o_{t} \mid s_{t}\right) \int_{\mathcal{S}} p\left(s_{t} \mid s_{t-1}, a_{t-1}\right) b\left(s_{t-1}\right) \mathrm{d} s_{t-1}
$$

Доказательство. Нашей целью является посчитать $b_{t}\left(s_{t}\right):=p\left(s_{t} \mid \mathcal{T}_{t}\right)=p\left(s_{t} \mid o_{t}, a_{t-1}, \mathcal{T}_{t t-1}\right)$. Применим формулу Байеса:

$$
b_{t}\left(s_{t}\right) \propto p\left(o_{t} \mid s_{t}, a_{t-1}, \mathcal{T}_{t t-1}\right) p\left(s_{t} \mid a_{t-1}, \mathcal{T}_{t t-1}\right)=()
$$

Первый множитель здесь по определению вероятностной модели есть $p\left(o_{t} \mid s_{t}\right)$, а во втором множителе, чтобы воспользоваться функцией переходов, нужно проинтегрировать по неизвестному нам предыдущему состоянию $s_{t-1}$ :

$$
\begin{aligned}
(*) & =p\left(o_{t} \mid s_{t}\right) \int_{\mathcal{S}} p\left(s_{t}, s_{t-1} \mid a_{t-1}, \mathcal{T}_{t t-1}\right) \mathrm{d} s_{t-1}= \\
& =p\left(o_{t} \mid s_{t}\right) \int_{\mathcal{S}} p\left(s_{t} \mid s_{t-1}, a_{t-1}\right) p\left(s_{t-1} \mid \mathcal{T}_{t t-1}\right) \mathrm{d} s_{t-1}
\end{aligned}
$$

Осталось заметить, что по определению $p\left(s_{t-1} \mid \mathcal{T}_{t t-1}\right)=b_{t-1}\left(s_{t-1}\right)$ - информация о текущем состоянии с прошлого шага.

Пример 138: Допустим, в PoMDP с 10 coстояниями и действиями вправо-влево получаем детерминировано в качестве наблюдения бинарный флаг: есть ли пальма или нет. Начальное состояние определяется случайно равномерно. Посмотрим, как меняется belief state, если агент заспанился в самом левом состоянии и дальше выбирает действия «вправо».


Belief state - «достаточная статистика» для описания всей предыдущей истории. Посчитав его, мы собрали абсолютно всю имеющуюся у нас информацию. Это замечание позволяет формально свести задачу в PoMDP к обычному MDP, но требует знания всех распределения и возможности пересчитывать belief state:
Определение 120: Для данного PoMDP Belief MDP называется следующее MDP:

- Пространством состояний является пространство $\mathrm{P}(\mathcal{S})$ распределений в $\mathcal{S}$;
- Пространством действий является $\mathcal{A}$;
- Функция переходов $p\left(b^{\prime} \mid b, a\right)$, где $b, b^{\prime} \in \mathrm{P}(\mathcal{S})$ устроена так: сэмплируются $s \sim b(s), s^{\prime} \sim p\left(s^{\prime} \mid s, a\right)$, $o^{\prime} \sim p\left(o^{\prime} \mid s^{\prime}\right)$, после чего новый belief state рассчитывается по формуле:

$$
b^{\prime}\left(s^{\prime}\right) \propto p\left(o^{\prime} \mid s^{\prime}\right) \int_{\mathcal{S}} p\left(s^{\prime} \mid s, a\right) b(s) \mathrm{d} s
$$

---

- Функция награды для текущего состояния $\boldsymbol{b}$ и действия $\boldsymbol{a}$ устроена так: для того же сэмпла $\boldsymbol{s}$, использованного в переходе, выдаётся награда $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$.
Теория Belief MDP позволяет предложить алгоритмы динамического программирования наподобие Value Iteration и Policy Iteration для решения задач в PoMDP. Однако, в реальности работать с Belief MDP в средах со сложным пространством состояний довольно неудобно: если все распределения неизвестны, интегралы в формуле обновления (8.40) не берутся, и даже просто хранить «распределение над состояниями» невозможно, то не совсем понятно, что можно извлечь от этой теории в сложных средах. Понятно, что при взаимодействии со средой агент должен помимо прочего стремиться выполнять те действия, которые дают наибольшую информацию o belief state, и позволяют «локализоваться» в пространстве состояний.


# 8.5.3. Рекуррентные сети в Policy Gradient 

Видимо, поэтому пока самое распространённое решение для работы в PoMDP - добавление в архитектуры моделей рекуррентных слоёв (GRU или LSTM). При использовании рекуррентных стратегий скрытое состояние инициализируется нулевым вектором в начале каждого эпизода и «хранит всю необходимую информацию» о предыдущих наблюдениях, а все модели на вход получают лишь то, что поступает с сенсоров. Де-факто все наши модели оценочных функций и стратегии становятся функциями от всей истории $\boldsymbol{o}_{\mathbf{0}}, \boldsymbol{o}_{\mathbf{1}}, \boldsymbol{o}_{\mathbf{2}} \ldots \boldsymbol{o}_{\boldsymbol{t}}$. Обсудим, как это меняет ранее встречавшиеся алгоритмы.

Чистые on-policy алгоритмы, такие как REINFORCE, A2C и мета-эвристики, практически не меняются. В A2C лишь стоит иметь в виду, что градиенты текут только по тому роллауту, который собран на текущей итерации; таким образом, агенту будет тяжело научиться запоминать информацию дольше, чем на $\boldsymbol{N}$ шагов, где $\boldsymbol{N}$ - длина роллаутов.

В РРО скрытое состояние модели сохраняется для всего собранного датасета. Далее из датасета сэмплируются не мини-батчи, а мини-батчи роллаутов (некоторой длины $\boldsymbol{N}$ ). Затем скрытое состояние инициализируется сохранённым значением при сборе датасета (поскольку стратегия «не меняется сильно», считается, что это сохранённое состояние более-менее не устарело), и модель прогоняется на засэмплированном роллауте для подсчёта всех градиентов.

### 8.5.4. R2D2

B off-policy алгоритмах сохранять значение скрытого состояния в реплей буфере на первый взгляд бессмысленно; модель может изменится сколько угодно сильно, и сохранённые значения памяти будут неактуальны. С течением обучения скрытое состояние рекуррентных сетей просто меняет своё семантическое значение: модель со свежими весами просто по-другому будет интерпретировать латентное представление.

Рассмотрим две эвристики для преодоления этой проблемы из алгоритма R2D2. Из реплей буфера генерируются роллауты некоторой длины $\boldsymbol{N}$. Скрытое состояние инициализируется сохранённым в буфере значением (которое потенциально «протухло»). Модель прогоняется заново на всём роллауте, но функция потерь и backpropagation считается лишь на последних $\boldsymbol{M}<\boldsymbol{N}$ шагов. Таким образом обучение проводится стандартным для рекуррентных сетей подходом backpropagation through time (BPTT).

Первые же $\boldsymbol{N}-\boldsymbol{M}$ шагов нужны исключительно для «прогреса» (burn-in) скрытого состояния, чтобы его семантическое значение начало более-менее соответствовать свежим весам модели. Это не так дорого вычислительно, как заново прогонять модель по всему эпизоду; так можно было бы получить «честные» актуальные значения памяти, но параллелизовать такие вычисления по батчу, в котором встречаются роллауты из разных моментов эпизодов, неудобно и достаточно дорого. Естественно, аналогичный прогон проводится для таргетсети.


Вторая эвристика заключается в том, чтобы «разогретое» состояние после $\boldsymbol{N}-\boldsymbol{M}$ шагов сохранить в буфере для того роллаута, который начинается с соответствующего перехода, и таким образом сделать сохранённое значение чуть более актуальным.

Как обычно, длина $\boldsymbol{M}$ той части траектории, вдоль которой пропускаются градиенты, и ограничивает то время, «на которое» агент может теоретически научиться что-то запоминать. Однако, увеличивать его может быть чревато не только тем, что алгоритм станет вычислительно сложным, но и тем, что в батче оказываются

---

$\boldsymbol{M}$ очень похожих друг на друга переходов, и перебивать эту скоррелированность лоссов придётся размером мини-батча.

4
Может быть удобно для ускорения сэмплирования хранить в реплей буфере эпизоды, уже разбитые на фрагменты удобного размера. Например, авторы R2D2 выбирают $\boldsymbol{N}=\mathbf{8 0}, \boldsymbol{M}=\mathbf{4 0}$, и при сборе данных собирают 40 последовательных переходов, которые упаковываются в буфер единым комплектом; из-за такой оптимизации промежуточные переходы в этом роллауте никогда не будут засэмплированы для обучения как, например, стартовые (с которых начинается роллаут из $\boldsymbol{N}$ шагов). Похоже, что это не так критично, зато сильно упрощает код и сэмплирование мини-батчей.

# 8.5.5. Neural Episodic Control (NEC) 

Эпизодичная память означает, что вместо (или дополнительно к) рекуррентных связей в качестве памяти используется набор ранее встретившихся состояний или их латентных описаний. Механизм работы с такой памятью очень схож с механизмом внимания. Рассмотрим алгоритм Neural Episodic Control в качестве примера применения такой идеи.

Для каждого действия $\boldsymbol{a}$ будем хранить словарь (dictionary), то есть некоторый набор ключ-значение $\boldsymbol{M}_{\boldsymbol{a}}:=$ $(k, v)$. Ключом (key) $\boldsymbol{k} \in \mathbb{R}^{d}$ является вектор-эмбеддинг, описывающий некоторое состояние $\boldsymbol{s}$ из опыта агента. Значением (value) $\boldsymbol{v}$ является скаляр, приближённо оценивающий $\boldsymbol{Q}^{*}(s, a)$.

При взаимодействии со средой агент действует следующим образом. Текущее состояние $\boldsymbol{s}$ подаётся на вход сети с параметрами $\boldsymbol{\theta}$, которая выдаёт его описание $\boldsymbol{q}_{\boldsymbol{\theta}}(\boldsymbol{s}) \in \mathbb{R}^{\boldsymbol{d}}$. Выходом $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$ модели является результат применения механизма внимания (attention) к словарю $\boldsymbol{M}_{\boldsymbol{a}}$ с запросом (query) $\boldsymbol{q}_{\boldsymbol{\theta}}(\boldsymbol{s})$ : для каждого ключа $\boldsymbol{k}$ в словаре определяется похожесть запроса на ключ $\boldsymbol{\rho}\left(\boldsymbol{q}_{\boldsymbol{\theta}}(\boldsymbol{s}), \boldsymbol{k}\right)$ при помощи некоторой функции близости $\boldsymbol{\rho}$, например:

$$
\rho(q, k):=\frac{1}{\|q-k\|_{2}^{2}+\delta}
$$

где $\boldsymbol{\delta}$ - небольшая константа для защиты от деления на ноль. Затем для каждой пары ключ-значение определяется его вес:

$$
w(k) \propto \rho(q, k)
$$

где нормировочная константа вычисляется из соображения $\sum_{\boldsymbol{k}, \boldsymbol{v} \in \boldsymbol{M}_{\boldsymbol{a}}} \boldsymbol{w}(\boldsymbol{k})=1$. Наконец, выход дифференцируемого чтения из словаря определяется как сумма значений с вычисленными весами:

$$
Q_{\theta}(s, a):=\sum_{k, v \in M_{a}} w(k) v
$$

Поскольку размер словаря может быть достаточно большой, а процедуру нужно проводить для каждого $\boldsymbol{a} \in \boldsymbol{\mathcal { A }}$, предлагается использовать в формуле две аппроксимации. Во-первых, учитываются только топ-50 самых значимых (имеющих наибольший вес $\boldsymbol{w}(\boldsymbol{k})$ ) пар ключей-значений. Иными словами, по расстоянию $\boldsymbol{\rho}$ находятся 50 ближайших к $\boldsymbol{q}_{\boldsymbol{\theta}}(\boldsymbol{s})$ ключей в словаре, и веса с учётом нормировочной константы вычисляются только для них; только они используются для вычисления финальной суммы. Во-вторых, поиск топ-50 ближайших соседей проводится приближённо, для ускорения вычислений.

Видно, что при фиксированном словаре $\boldsymbol{M}_{\boldsymbol{a}}$ функция $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$, вычисленная таким образом, дифференцируема по параметрам.

При помощи такой модели Q-функции агент выбирает действие $\boldsymbol{\varepsilon}$-жадно и получает для выбранного действия $\boldsymbol{a}$ следующее состояние $\boldsymbol{s}^{\prime}$ и награду за шаг $\boldsymbol{r}$. Теперь он может посчитать для примера $\boldsymbol{s}, \boldsymbol{a}$ целевую переменную (авторы используют $\boldsymbol{N}$-шаговую оценку, для чего нужно провести $\boldsymbol{N}$ шагов взаимодействия со средой; для простоты положим $\boldsymbol{N}=\mathbf{1}$ ):

$$
\hat{Q}(s, a)=r+\gamma \max _{\boldsymbol{a}^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)
$$

где значения $\boldsymbol{Q}_{\boldsymbol{\theta}}$ также получены проведением всей вышеописанной процедуры. Полученная пара $\left(\boldsymbol{q}_{\boldsymbol{\theta}}(\boldsymbol{s}), \hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{a})\right)$ добавляется в словарь $\boldsymbol{M}_{\boldsymbol{a}}$ и используется при дальнейших проходах через сеть. У словарей, естественно, есть некоторое ограничение по объёму, и при добавлении новой пары в заполненный словарь предлагается выкидывать самую редко используемую пару $(\boldsymbol{k}, \boldsymbol{v})$ : ту, для которой число попаданий в топ- 50 ближайших соседей наименьшее (для этого достаточно дополнительно хранить счётчики использований).

Тройка $(\boldsymbol{s}, \boldsymbol{a}, \hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{a}))$ добавляется в реплей буфер. На этапе обучения, который проводится после каждого шага взаимодействия со средой, сэмплируется мини-батч троек $(\boldsymbol{s}, \boldsymbol{a}, \hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{a}))$ из буфера, и модель с текущими словарями $\boldsymbol{M}_{\boldsymbol{a}}$ прогоняется на $\boldsymbol{s}, \boldsymbol{a}$ для получения оценки $\boldsymbol{Q}_{\boldsymbol{\theta}}(\boldsymbol{s}, \boldsymbol{a})$ :

$$
\left(Q_{\theta}(\boldsymbol{s}, \boldsymbol{a})-\hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{a})\right)^{2} \rightarrow \min _{\boldsymbol{\theta}, \boldsymbol{k}, \boldsymbol{v}}
$$

Запись, что минимизация ведётся как по $\boldsymbol{\theta}$, так и по $\boldsymbol{k}, \boldsymbol{v}$, означает, что минимизация ведётся не только по параметрам сети, но и по ключам и значениям, хранящимся на текущий момент в словаре $\boldsymbol{M}_{\boldsymbol{a}}$. Таким образом, эмбеддинги и значения обновляются в том числе для хранящихся в «эпизодичной памяти» состояний.

---

Заметим, что $\hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{a})$ здесь - целевая переменная, посчитанная в момент сбора перехода, и она не пересчитывается; иначе говоря, используется по сути on-policy режим обучения. Интерпретация такого хода в том, что отказ пересчитывать значение целевой переменной $\hat{\boldsymbol{Q}}(\boldsymbol{s}, \boldsymbol{a})$ по сути эквивалентен использованию замороженной таргет-сети; но, чтобы эти значения не устаревали сильно, размер реплей буфера придётся существенно сократить (авторы используют реплей буфер размера $10^{5}$, что на порядок меньше типично используемого буфера при off-policy обучении).

# §8.6. Мульти-агентное обучение с подкреплением 

### 8.6.1. Связь с теорией игр

Попробуем представить ситуацию, когда в среде действует два агента. Функция переходов теперь зависит от действий каждого из агентов $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}^{\mathbf{1}}, \boldsymbol{a}^{\mathbf{2}}\right)$, а каждый агент оптимизирует свою функцию награды $\boldsymbol{r}^{\mathbf{1}}\left(\boldsymbol{s}, \boldsymbol{a}^{\mathbf{1}}, \boldsymbol{a}^{\mathbf{2}}\right)$, где $\boldsymbol{i}$ - номер агента.

Рассмотрим сеттинг, аналогичный бандитам: игра с одним состоянием, заканчивающаяся после первого выбора. Допустим также детерминированность функций наград. Мы получим в чистом виде задачу, исследующуюся в теории игр: заданы две функции $\boldsymbol{r}^{\mathbf{1}}\left(\boldsymbol{a}^{\mathbf{1}}, \boldsymbol{a}^{\mathbf{2}}\right), \boldsymbol{r}^{\mathbf{2}}\left(\boldsymbol{a}^{\mathbf{1}}, \boldsymbol{a}^{\mathbf{2}}\right)$, зависящие от действий обоих игроков; игроки оптимизируют каждый свою функцию, при этом производя выбор действий одновременно. Отсюда видно, что любой мульти-агентный RL тесно связан с теорией игр: в частности, при обучении агенты могут застрять не только в локальных оптимумах, но и в равновесиях: Нэша (Nash equilibrium), когда ни один другой агент не может поменять свою стратегию в предположении неизменности стратегий других агентов так, чтобы увеличить свою награду.

В общем случае в среде действует $\boldsymbol{N}$ агентов. Будем обозначать как $\boldsymbol{a}^{\boldsymbol{i}}$ действие $\boldsymbol{i}$-го агента; $\overline{\boldsymbol{a}}$ - вектор из всех действий всех агентов; $\boldsymbol{a}^{-\boldsymbol{i}}$ - действия всех агентов, кроме $\boldsymbol{i}$-го. Тогда функция переходов теперь выглядит как $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \overline{\boldsymbol{a}}\right)$, а $\boldsymbol{i}$-ый агент оптимизирует свою функцию награды $\boldsymbol{r}^{\mathbf{1}}(\boldsymbol{s}, \overline{\boldsymbol{a}})$. Природа этих функций наград может быть самой разной.
| Определение 121: Если функции наград всех агентов совпадают, то задача называется кооперативной.
Определение 122: Задача, в которой сумма наград всех агентов за шаг нулевая $\sum_{i} \boldsymbol{r}^{i}(\boldsymbol{s}, \overline{\boldsymbol{a}})=\mathbf{0}$, называется антагонистической или игрой с нулевой суммой.

Это предельные случаи частных ситуаций: если $\boldsymbol{r}^{\mathbf{1}}$ в целом велико там, где велико $\boldsymbol{r}^{\mathbf{2}}$, то эти два агента, видимо, должны научиться вместе выполнять какую-то задачу. Если же большое $\boldsymbol{r}^{\mathbf{1}}$ влечёт малое $\boldsymbol{r}^{\mathbf{2}}$, то агенты противостоят друг другу. В общем случае у каждого агента может быть какая-то своя задача; этот общий случай иногда называют смешанной (mixed) задачей мульти-агентного RL. Зачастую агенты действуют в условиях частичной наблюдаемости, и тогда у каждого агента могут быть свои наблюдения; это означает, что у $\boldsymbol{i}$-го агента своя функция $\boldsymbol{p}^{\boldsymbol{i}}(\boldsymbol{o} \mid \boldsymbol{s})$, определяющая наблюдение по текущему состоянию мира. Для простоты далее частичная наблюдаемость опущена.

### 8.6.2. Централизация обучения

Децентрализованный сеттинг означает, что у агентов нет какой-либо общего «сервера» с общими данными, в том числе во время обучения. Простейший подход мульти-агентного RL в рамках такого сеттинга завести для каждого агента свою стратегию $\boldsymbol{\pi}^{i}(\boldsymbol{a} \mid \boldsymbol{s})$ и оценочную функцию $\boldsymbol{Q}^{\boldsymbol{i}}\left(\boldsymbol{s}, \boldsymbol{a}^{\boldsymbol{i}}\right)$, которая будет зависеть только от действия $\boldsymbol{i}$-го агента, и оптимизировать их с опыта данного агента любыми обычными RL алгоритмами. Тот факт, что в среде действуют другие агенты, игнорируется. Важный момент: если другие агенты не обучаются (не изменяются с течением времени), то такой подход обучения $\boldsymbol{i}$-го агента корректен.

Утверждение 91: Если все агенты, кроме $\boldsymbol{i}$-го, стационарны, то $\boldsymbol{i}$-ый агент живёт в MDP с функцией переходов

$$
\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}^{\boldsymbol{i}}\right):=\int \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}^{\boldsymbol{i}}, \boldsymbol{a}^{-\boldsymbol{i}}\right) \boldsymbol{\pi}^{-\boldsymbol{i}}\left(\boldsymbol{a}^{-\boldsymbol{i}} \mid \boldsymbol{s}\right) \mathbf{d} \boldsymbol{a}^{-\boldsymbol{i}}
$$

где $\boldsymbol{\pi}^{-\boldsymbol{i}}\left(\boldsymbol{a}^{-\boldsymbol{i}} \mid \boldsymbol{s}\right)$ - стратегия остальных агентов, в предположении их независимого выбора своих действий равная

$$
\boldsymbol{\pi}^{-\boldsymbol{i}}\left(\boldsymbol{a}^{-\boldsymbol{i}} \mid \boldsymbol{s}\right):=\prod_{j \neq i} \boldsymbol{\pi}^{j}\left(\boldsymbol{a}^{j} \mid \boldsymbol{s}\right)
$$

Это означает, что если в среде действует другой агент, использующий стационарную стратегию, среда остаётся стационарной и вся теория остаётся справедлива. Однако, если в среде действует другой агент, который обучается (и, значит, меняет свою стратегию с течением времени), то для прочих агентов это означает, что постоянно изменяются «законы физики» окружающей среды, и мы под изученную теорию подпадать перестаём.

---

В нестационарных средах естественно использовать on-policy обучение. Запуск какого-нибудь on-policy алгоритма с игнорированием факта присутствия других обучающихся агентов обычно оказывается серьёзным бэйззайном.

Полностью централизованный сеттина возможен для агентов, например, оптимизирующих одну и ту же функцию награды, находящихся в кооперации. Полная централизация означает, что для построения системы из нескольких кооперирующихся агентов все они рассматриваются как единый агент. Наблюдение есть совокупность наблюдений всех агентов, действия - набор действий для каждого из агентов, и тогда по сути задача сведена к «одноагентной» ситуации. В большинстве случаев такой вариант не годится по самой постановке задачи: наличие единого «центрального сервера», который бы принимал решения за всех агентов в среде, может быть просто-напросто очень дорогим. Хотя бы потому что агентов может быть много, и пространство состояний и, главное, действий, взорвётся. В большинстве случаев хочется, чтобы агенты принимали решения на основе только имеющейся у них информации.

Зачастую доступен интересный промежуточный вариант (частичная централизация), когда у каждого из кооперирующихся агентов хранится и обучается персональная стратегия, но, например, оценочная функция $\boldsymbol{Q}\left(\boldsymbol{s}, \boldsymbol{a}^{\mathbf{1}}, \boldsymbol{a}^{\mathbf{2}} \ldots\right)$ хранится и обучается на общем для всех агентов «сервере». Важно, что эта оценочная функция нужна в on-policy алгоритмах только для обучения, и при использовании полученных стратегий «сервер» не потребуется. Это означает, что по окончании обучения в таком сеттинге для использования стратегии никакого центрального сервера не понадобится. Такой подход называется centralised training with decentralised execution (CTDE), и многие алгоритмы для мульти-агентного RL строятся именно в рамках этой парадигмы.

В рамках парадигмы CTDE, помимо прочего, возможен parameter sharing: если задача симметрична для каких-то агентов, то веса их стратегий считаются общими. Градиенты для их обучения тогда вычисляются на общем сервере; дальше во время использования агенты используют одну и ту же модель для принятия решений. Другими словами, в рамках CTDE подхода при желании обучить колонию из 100 одинаковых муравьёв, вам понадобится лишь одна модель; без частичной централизации же, в децентрализованном сеттинге, подразумевается, что каждый из муравьёв должен как-то сам обучиться, и тогда у каждого из сотни получится в итоге своя модель.

# 8.6.3. Self-play в антагонистических играх 

В симметричных антагонистических играх обучение на опыте игр с самим собой оказалось мощным инструментом. Этот подход называют self-play, и заключается он в том, что за игроков играет одна и та же стратегия (такие игры обычно симметричны, и поэтому можно считать, что применяется parameter sharing). Собранный опыт со стороны каждого игрока можно использовать для обучения, и он обладает одним очень ценным свойством: сбалансированностью. На какой бы стадии обучения ни был агент, играет ли он на уровне гранд-мастера или бесконечно сильно тупит, при игре с самим собой в половине случаев он «выигрывает», в половине - «проигрывает». Разница в сигнале всегда даёт возможность дальнейшего улучшения.

В режиме self-play можно запустить любые алгоритмы обучения: как model-free, так и model-based. B частности, в model-based алгоритмах вроде AlphaGo и MuZero в ходе планирования можно при построении дерева ходы за оппонента проводить при помощи тех же моделей, но не максимизировать, а минимизировать награду. Понятно, что идея применима как в ситуации, когда игроки ходят одновременно, так и когда игроки ходят по очереди (что более типично для игр вроде шахмат и го).

Конечно, self-play не гарантирует, что по итогам обучения стратегия будет способна адаптироваться к произвольному оппоненту. Можно показать, что к обученным self-play методам стратегии можно применить adversarial-amaку: зафиксировать её и обучать стратегию оппонента побеждать. Такие стратегии, которые «взламывают» стратегию оппонента, зачастую ведут себя очень странно, и могут победить, ничего не делая.

Пример 139 - Adversarial Policy: По данной ссылке в примерах слева показаны игры стратегии, обученной в режиме self-play. На вид кажется, что полученная стратегия невероятно крута. Далее эта стратегия фиксировалась, и обучалась с нуля новая стратегия оппонента «побеждать» фиксированную. Примеры этих игр показаны справа: новая стратегия оппонента просто падает; сбивает с толку «крутую» стратегию, и та отправляется в какой-то нокаут.

### 8.6.4. QMix в кооперативных играх

Рассмотрим кооперативный сеттинг, где все агенты максимизируют одну и ту же функцию награды. Будущую кумулятивную награду после выполнения набора действий $\vec{a}$ в состоянии $s$ для набора политик $\pi^{1}\left(a^{i} \mid s\right)$ обозначим как $\boldsymbol{Q}^{\text {tot }}(\boldsymbol{s}, \vec{a})$, скаляр. Попробуем построить алгоритм в рамках парадигмы CTDE, то есть обучить «на центральном сервере» стратегии $\boldsymbol{\pi}^{1}\left(\boldsymbol{a}^{i} \mid \boldsymbol{s}\right)$, которые смогут дальше действовать в среде без использования подобного сервера.

Допустим, мы знаем $\boldsymbol{Q}^{\text {tot }}(\boldsymbol{s}, \vec{a})$. Тогда оптимально выбрать действия

---

однако есть две проблемы. Во-первых, если агентов много, поиск такого аргмаксимума может стать слишком сложной задачей, ровно как и моделирование такой оценочной функции. Во-вторых, в рамках CTDE в частично наблюдаемом сеттинге каждый агент на вход вместо $\boldsymbol{s}$ на самом деле получает своё собственное наблюдение $\boldsymbol{o}^{\mathbf{i}}$ и не знает наблюдения других агентов, когда $\boldsymbol{Q}^{\text {tot }}$ принимает на вход наблюдения всех агентов. С обоими проблемами можно побороться следующим образом.

Определение 123: Смешивающей сетью (mixing network) назовём моделирование $\boldsymbol{Q}^{\text {tot }}(\boldsymbol{s}, \boldsymbol{\alpha})$ в следующем виде:

$$
Q^{\mathrm{tot}}(s, \vec{a}) \approx Q_{\phi}^{\mathrm{tot}}\left(Q^{1}\left(s, a^{1}\right), Q^{2}\left(s, a^{2}\right), \ldots Q^{N}\left(s, a^{N}\right), s\right)
$$

где $\phi$ - параметры, $Q^{i}\left(s, a^{i}\right)$ - скаляры, зависящие только от той информации, которая доступна $i$-му агенту.

В частности, в частично наблюдаемом сеттинге $\boldsymbol{Q}^{\boldsymbol{i}}\left(\boldsymbol{s}, \boldsymbol{a}^{\boldsymbol{i}}\right)$ вместо $\boldsymbol{s}$ принимает на вход наблюдение $\boldsymbol{i}$-го агента $\boldsymbol{o}^{\mathbf{i}}$ и моделируется рекуррентной сетью. Сразу оговоримся, что здесь $\boldsymbol{Q}^{\mathbf{i}}\left(\boldsymbol{s}, \boldsymbol{a}^{\mathbf{i}}\right)$, несмотря на принятое обозначение, не является оценочной функцией и не имеет смысл будущей кумулятивной награды. Это лишь некоторая промежуточная вспомогательная величина, через которую по некоторым правилам выражается $Q^{\text {tot }}(s, \vec{a})$, награда «всей команды».

Пример 140 - Value decomposition network: Рассмотрим самый простой пример смешивающей сети, не использующей параметров $\phi$ вовсе:

$$
Q_{\phi}^{\mathrm{tot}}(s, \vec{a}):=\sum_{i} Q^{i}\left(s, a^{i}\right)
$$

У этой декомпозиции есть интересное свойство:

$$
\underset{\vec{a}}{\operatorname{argmax}} Q^{\mathrm{tot}}(s, \vec{a})=\left[\begin{array}{c}
\underset{a^{1}}{\operatorname{argmax}} Q^{1}\left(s, a^{1}\right) \\
\underset{a^{2}}{\operatorname{argmax}} Q^{2}\left(s, a^{2}\right) \\
\vdots \\
\underset{a^{N}}{\operatorname{argmax}} Q^{N}\left(s, a^{N}\right)
\end{array}\right]
$$

Другими словами, будущая награда команды моделируется через какие-то псевдооценочные функции каждого агента команды в отдельности. Нам крайне интересно свойство (8.42).

Определение 124: Скажем, что смешивающая сеть консистентна (consistent), если для всех состояний выполняется (8.42).

Консистентность означает, что каждому агенту вовсе не нужно знать наблюдения других агентов или обращаться к центральному серверу для выбора оптимального действия: он берёт лишь значение свой псевдооценочной функции $\boldsymbol{Q}^{\mathbf{i}}\left(\boldsymbol{s}, \boldsymbol{a}^{\mathbf{i}}\right)$ и считает аргмакс по нему. Свойство (8.42) гарантирует, что это даст аргмаксимум по всему набору действий $\vec{a}$ всей команды. Поскольку выбор декомпозиции - наш произвол, нам хочется выбрать такое параметрическое семейство смешивающих сетей, которое гарантирует это свойство. В целом, это не так сложно.

Теорема 92: Смешивающая сеть (8.41) консистентна, если

$$
\frac{\mathrm{d} Q_{\phi}^{\mathrm{tot}}}{\mathrm{~d} Q^{i}} \geq 0
$$

то есть для всех состояний $\boldsymbol{s}$ и при любых значениях параметров $\phi$ награда команды монотонно зависит от значений псевдооценочных функций агентов $\boldsymbol{Q}^{\mathbf{i}}\left(\boldsymbol{s}, \boldsymbol{a}^{\mathbf{i}}\right)$.

Доказательство. В силу монотонности, для любых действий $\vec{a}$ :

$$
Q_{\phi}^{\mathrm{tot}}\left(\max _{a^{1}} Q^{1}\left(s, a^{1}\right), \ldots \max _{a^{N}} Q^{N}\left(s, a^{N}\right), s\right) \geq Q_{\phi}^{\mathrm{tot}}\left(Q^{1}\left(s, a^{1}\right), \ldots Q^{N}\left(s, a^{N}\right), s\right)
$$

следовательно максимизация всех аргументов монотонной функции максимизирует саму функцию.
Можно ли придумать какое-нибудь богатое семейство смешивающих сетей с параметрами, для которых было бы выполнено (8.43)? Давайте возьмём нейросеть с параметрами $\phi$, которая принимает на вход скаляры $Q^{1}\left(s, a^{1}\right), \ldots Q^{N}\left(s, a^{N}\right)$ и выдаёт скаляр $Q_{\phi}^{\text {tot }}$. Тогда:

---

Утверждение 92: Пусть нейросеть полносвязная, состоит из чередования линейных слоёв и монотонных функций активаций, и все параметры нейросети $\phi_{i}$ (кроме, возможно, смещений в линейных слоях) неотрицательны. Тогда выполнено (8.43).

Доказательство. Композиция монотонных преобразований монотонно; линейная комбинация с положительными весами (и произвольными смещениями) также монотонно не убывает как функция от входов.

Итак, любая нейросеть с положительными весами подходит нам в качестве смешивающей. Сделаем ещё одно наблюдение: эта нейросетка очень маленькая, ведь на вход ей подаётся вектор размерности $\boldsymbol{N}$, где $\boldsymbol{N}$ число агентов, а на выходе скаляр. Заметим также, что веса могут зависеть от состояния: мы можем использовать разные $\phi$ для разных состояний $s$, и отсюда возникает идея, как можно повысить гибкость нашей value decomposition network.
| Определение 125: Гиперсетью (hypernetwork) называется нейросеть, выдающая веса для другой нейросети.
Мы заведём гиперсеть $\phi_{\boldsymbol{\theta}}(s)$, которая для данного состояния $s$ будет с параметрами $\boldsymbol{\theta}$ выдавать на центральном сервере веса для смешивающей сети. В частности, $\phi_{\boldsymbol{\theta}}(s)$ уже не обязано быть монотонным и может моделироваться произвольной обычной нейросетью. Мы получим, что в каждом состоянии у нас есть какое-то своё монотонное преобразование псевдооценочных функций каждого агента в оценочную функцию команды, и это очень удобно.

Итого в алгоритме QMIX оценочная функция моделируется в следующем виде:

$$
Q_{\phi_{\theta}(s)}^{\text {tot }}(s, \vec{a}, \psi):=Q_{\phi_{\theta}(s)}^{\text {tot }}\left(Q^{1}\left(s, a^{1}, \psi^{1}\right), \ldots Q^{N}\left(s, a^{N}, \psi^{N}\right)\right)
$$

где $\psi^{i}$ - параметры псевдооценочной функции $i$-го агента (возможен parameter sharing, если задача симметрична для некоторых агентов), $\boldsymbol{\theta}$ - параметры гиперсети, $\phi_{\boldsymbol{\theta}}(s)$ - получающиеся параметры смешивающей нейросети.

Процесс обучения стандартный и похож на обычный DQN. При взаимодействии со средой $i$-ый агент использует свою псевдооценочную функцию $Q^{i}\left(s, a^{i}\right)$ и ведёт себя $\varepsilon$-жадно. Собранные переходы ( $s, \vec{a}, r, s^{\prime}$ ) собираются на центральном сервере. Для перехода $\mathbb{T}:=(s, \vec{a}, r, s^{\prime})$ целевая переменная строится как

$$
y(\mathbb{T}):=r+\gamma \max _{\vec{a}^{\prime}} Q_{\phi_{\theta-1}(s)}^{\text {tot }}\left(s^{\prime}, \vec{a}^{\prime}, \psi^{-}\right)
$$

где $\boldsymbol{\theta}^{-}, \boldsymbol{\psi}^{-}-$ замороженные параметры таргет-сети. Далее с таким таргетом минимизируется MSE по всем параметрам:

$$
\mathbb{E}_{\mathbb{T}}\left(y(\mathbb{T})-Q_{\phi_{\theta}(s)}^{\text {tot }}(s, \vec{a}, \psi)\right)^{2} \rightarrow \min _{\theta, \psi}
$$

После окончания обучения центральный сервер, на котором хранится гиперсеть и смешивающая сеть, не нужны, и агенты используют лишь локальные певдооценочные функции $Q^{i}\left(s, a^{i}\right)$.

# 8.6.5. Multi-Agent DDPG (MADDPG) в смешанных играх 

Рассмотрим смешанную игру, где функции наград агентов произвольны. На «центральном сервере» будем хранить общую оценочную функцию, которая теперь должна выдавать будущие награды сразу для всех агентов.
| Определение 126: При данных политиках $\pi^{j}\left(a_{j} \mid s\right)$ центральзованной оценочной функиией (centralized state-action value function) обозначим $\overrightarrow{\boldsymbol{Q}}(s, \vec{a})$, возвращающую вектор, $\boldsymbol{i}$-ая компонента которого $Q^{i}(s, \vec{a})$ равна будущей кумулятивной награде, которую получит $\boldsymbol{i}$-ый агент после выполнения набора действий $\vec{a}$ в состоянии $s$.

На центральном сервере понятно, как учить такую Q-функцию, причём это можно делать в off-policy режиме. Для произвольного перехода из буфера $\mathbb{T}:=(s, \vec{a}, \vec{r}, s^{\prime})$ строим таргет

$$
y(\mathbb{T}):=\vec{r}+\gamma \overrightarrow{\boldsymbol{Q}}\left(s^{\prime}, \vec{a}^{\prime}\right)
$$

где $\vec{a}^{\prime}$ порождены оцениваемыми $\pi^{j}\left(a_{j}^{\prime} \mid s^{\prime}\right)$ (для стабилизации процесса - таргет-сетями для них), и минимизируем стандартное MSE:

$$
\mathbb{E}_{\mathbb{T}}\left(\overrightarrow{\boldsymbol{Q}}(s, \vec{a})-y(\mathbb{T})\right)^{2} \rightarrow \min _{\overrightarrow{\boldsymbol{Q}}}
$$

Процесс необходимо проводить на центральном сервере, поскольку для вычисления таргета необходимы текущие стратегии всех агентов. Вообще, от этого ограничения можно избавиться, если агентам в их наблюдениях доступны действия всех остальных агентов. Тогда мы сможем перейти к децентрализованному обучению следующим образом: $\boldsymbol{i}$-ый агент может локально хранить и обучать лишь интересующую его компоненту $Q^{i}(s, \vec{a})$. При построении таргета нужны стратегии опонентов; их $\boldsymbol{i}$-ый агент будет моделировать. Для моделирования собираются наблюдения ( $s, a^{-i}$ ), то есть какие действия предприняли остальные агенты в состоянии $s$, и дальше агент учится предсказывать действия других игроков по этой обучающей выборке в supervised-режиме:

$$
\mathbb{E}_{s, a^{-i}} \log \mu\left(a^{-i} \mid s\right) \rightarrow \max _{\mu}
$$

---

обычно с добавлением энтропийного регуляризатора. Имея такое приближение стратегий оппонентов на руках, таргет для обучения критика рассчитывается по формуле

$$
y^{i}(\mathbb{T}):=r^{i}+Q^{i}\left(s^{\prime}, a^{r i}, a^{r-i}\right)
$$

где $\boldsymbol{a}^{\boldsymbol{i}} \sim \boldsymbol{\pi}^{\boldsymbol{i}}\left(\boldsymbol{a}^{\boldsymbol{i}} \mid \boldsymbol{s}^{\boldsymbol{i}}\right)$, а $\boldsymbol{a}^{\boldsymbol{r}-\boldsymbol{i}} \sim \boldsymbol{\mu}\left(\boldsymbol{a}^{\boldsymbol{r}-\boldsymbol{i}} \mid \boldsymbol{s}^{\boldsymbol{r}}\right)$.
Как использовать централизованную оценочную функцию для обучения актёров? Для обучения $\boldsymbol{i}$-го актёра с параметрами $\boldsymbol{\theta}^{\boldsymbol{i}}$ можно применить формулу policy gradient:

$$
\nabla_{\theta^{i}}=\mathbb{E}_{\boldsymbol{s}} \mathbb{E}_{\boldsymbol{a}^{\boldsymbol{i}} \sim \boldsymbol{\pi}^{\boldsymbol{i}}\left(\boldsymbol{a}^{\boldsymbol{i}} \mid \boldsymbol{s}\right)} \nabla_{\boldsymbol{\theta}^{\boldsymbol{i}}} \log \boldsymbol{\pi}^{\boldsymbol{i}}\left(\boldsymbol{a}_{\boldsymbol{i}} \mid \boldsymbol{s}\right) \boldsymbol{Q}^{\boldsymbol{i}}\left(\boldsymbol{s}, \boldsymbol{a}^{\boldsymbol{i}}, \boldsymbol{a}^{-\boldsymbol{i}}\right)
$$

Действительно, для формулы градиентов по $\boldsymbol{\theta}^{\boldsymbol{i}}$ остальные стратегии можно считать фиксированными; поэтому стандартная формула применима. В ней состояния $\boldsymbol{s}$ приходят из опыта взаимодействия агентов со средой, а действия других агентов $\boldsymbol{a}^{-\boldsymbol{i}}$ должны быть порождены этими самыми стратегиями прочих агентов.

Авторы MADDPG предлагают перейти здесь от policy gradient-схемы к использованию формулы для DDPG, и забить на частоты посещения состояний. Тогда в формуле (8.44) можно брать $\boldsymbol{s}, \boldsymbol{a}^{-\boldsymbol{i}}$ - из приближения стратегий других агентов. По аналогии с DDPG также можно обучать детерминированную стратегию $\boldsymbol{\pi}^{\boldsymbol{i}}(\boldsymbol{s})$ с параметрами $\boldsymbol{\theta}^{\boldsymbol{i}}$, оптимизируя

$$
\mathbb{E}_{\boldsymbol{s}} \mathbb{E}_{\boldsymbol{a}^{i}} \boldsymbol{Q}^{\boldsymbol{i}}\left(\boldsymbol{s}, \boldsymbol{\pi}^{\boldsymbol{i}}(\boldsymbol{s}), \boldsymbol{a}^{-\boldsymbol{i}}\right) \rightarrow \max _{\boldsymbol{\theta}^{\boldsymbol{i}} \boldsymbol{\pi}}
$$

где $\boldsymbol{s}$ берутся из реплей буфера, $\boldsymbol{a}^{-\boldsymbol{i}}$ моделируются или, в случае CTDE-обучения, берутся из честных стратегий $\boldsymbol{\pi}^{\boldsymbol{j}}(\boldsymbol{s}), \boldsymbol{j} \neq \boldsymbol{i}$; в последнем случае мат.ожидание по $\boldsymbol{a}^{-\boldsymbol{i}}$ вырождается.

# 8.6.6. Системы коммуникации (DIAL) 

Пожалуй, главной особенностью взаимодействия агентов вроде людей в природе является такое явление, как язык. Давайте попробуем в парадигме RL промоделировать, как агенты могут передавать друг другу сообщения.

Определение 127: Скажем, что у агента $\boldsymbol{i}$ есть канал связи (communication channel), если на каждом шаге агент помимо действия $\boldsymbol{a}^{\boldsymbol{i}}$ выбирает некоторое сообщение (message) $\boldsymbol{m}^{\boldsymbol{i}}$, которое не влияет на функцию переходов и функции награды, и которое все агенты получают вместе со своими наблюдениями на следующем шаге.

Конечно, можно обобщить это понятие и сказать, что канал связи есть между конкретными агентами $\boldsymbol{i}, \boldsymbol{j}$, и этот канал может быть односторонним, двухсторонним, и так далее, но нам это сейчас не принципиально. Мы можем использовать эту идею в произвольных мульти-агентных средах, в том числе обучая агентов децентрализованно, рассматривая $\boldsymbol{m}^{\boldsymbol{i}}$ как часть пространства действий агентов.

В таком подходе удобно выбрать дискретное пространство сообщений, а при моделировании Q-функции вместо того, чтобы выдавать оценку для каждой пары действий ( $\boldsymbol{a}^{\boldsymbol{i}}, \boldsymbol{m}^{\boldsymbol{i}}$ ) отдельно выдаются оценки для $\boldsymbol{a}^{\boldsymbol{i}}$ и отдельно для $\boldsymbol{\mu}^{\boldsymbol{i}}$; дальше агент $\boldsymbol{\varepsilon}$-жадно выбирает действие для среды и отдельно $\boldsymbol{\varepsilon}$-жадно выбирает сообщение для отправки. Это небольшая декомпозиция упрощает пространство действий.

Рассмотрим чуть более хитрое применение канала связи под названием Differentiable Inter-Agent Learning (DIAL), применимое для кооперативных задач (с единой функцией награды) в парадигме CTDE. Здесь естественно выбрать непрерывное пространство сообщений. На центральном сервере во время обучения просто заметим, что любые модели агента $\boldsymbol{j}$, на шаге $\boldsymbol{t}$ минимизирующие свои стандартные функции потерь, дифференцируемы по входным наблюдениям и в том числе по полученным от других агентов сообщениям $\boldsymbol{\mu}_{\boldsymbol{t} \sim \boldsymbol{1}}^{\boldsymbol{i}}$. Эту информацию о градиенте можно использовать для обучения $\boldsymbol{i}$-го агента выдавать хорошие сообщения! Концептуально, схема для двух агентов в условиях частичной наблюдаемости выглядит примерно так:


---

На рисунке $\boldsymbol{o}_{k}^{1}$ - наблюдение, поступающее на вход $\boldsymbol{i}$-му агенту на шаге $\boldsymbol{t}$. Оно обрабатывается рекуррентной сетью, которое выдаёт, помимо оценочных функций и вероятностей действий, скрытое состояние $\boldsymbol{h}_{\boldsymbol{t}}^{1}$ для передачи самому себе на следующий шаг и сообщение $\boldsymbol{m}_{\boldsymbol{t}}^{1}$ для передачи другим агентам на следующий шаг (как видно на схеме, между этими двумя сущностями практически нет никакой разницы). Далее это сообщение поступает на вход моделям всех других агентов на шаге $\boldsymbol{t}+\mathbf{1}$. В конце вычислений вычисляется некоторая функция потерь в зависимости от используемого алгоритма (например, MSE для оценочных функций или суррогатная функция потерь для обучения стратегии), и этот градиент проходит не только назад по времени, но и по каналам связи в модели других агентов: как видно, вся эта схема end-to-end дифференцируема.

---

# Приложение 

## §А.1. Натуральный градиент

## А.1.1. Проблема параметризации

Стандартная градиентная оптимизация страдает от зависимости от параметризации. Допустим, мы градиентно оптимизируем

$$
f(x) \rightarrow \min _{x}
$$

и решили сменить параметризацию на $\boldsymbol{y}=\boldsymbol{y}(\boldsymbol{x})$ (допустим, даже, биективным преобразованием); задача

$$
f(y) \rightarrow \min _{y}
$$

казалось бы, эквивалентна, но траектории градиентного спуска не только будут кардинально отличаться (при эквивалентной инициализации, $\boldsymbol{x}_{\mathbf{0}}$ для первой задачи и $\boldsymbol{y}_{\mathbf{0}}=\boldsymbol{y}_{\mathbf{0}}\left(\boldsymbol{x}_{\mathbf{0}}\right)$ для второй), но и могут сильно различаться по свойствам (в одной параметризации оптимизация проходит намного успешнее другой).

Для нас обычно проблема закопана ещё чуть глубже. Оптимизируемые нами функционалы обычно имеют такой вид:

$$
f(\phi):=f(q(x \mid \phi)) \rightarrow \min _{\phi}
$$

где $\boldsymbol{q}(\boldsymbol{x} \mid \phi)$ - некоторое параметрически заданное распределение, и функционал $\boldsymbol{F}$ зависит только непосредственно от самого распределения. В качестве такого функционала обычно выступает или логарифм правдоподобия, или функция потерь для задач машинного обучения, также такой вид имеет вспомогательная функция в эволюционных стратегиях (2.5) и, самое главное, так выглядит и наш главный оптимизируемый функционал в обучении с подкреплением (1.5).

Вспомним, как устроен градиентный спуск. Мы рассматриваем некоторую окрестность точки $\phi_{0}$ и хотим, основываясь на локальных свойствах функции, выбрать направление для изменения текущих параметров. Для этого функция $\boldsymbol{f}(\phi)$ раскладывается в ряд Тейлора до первого члена с центром в точке $\phi_{0}$, и это разложение используется как приближённая модель поведения функции:

$$
\left\{\begin{array}{l}
f(\phi) \approx f\left(\phi_{0}\right)+\left\langle\nabla_{\phi} f(\phi)\right|_{\phi=\phi_{0}},\phi-\phi_{0}\right\rangle \rightarrow \min _{\phi} \\
\left\|\phi-\phi_{0}\right\|_{2}^{2} \leq \alpha
\end{array}\right.
$$

где $\boldsymbol{\alpha}$ - learning rate, а условие $\left\|\phi-\phi_{0}\right\|_{2}^{2} \leq \boldsymbol{\alpha}$ задаёт «регион доверия» (trust region) к построенному приближению. Понятие региона требует понятие расстояния: мы готовы отойти от текущей точки $\phi_{0}$ не далее чем на $\boldsymbol{\alpha}$ в смысле обычного Евклидова расстояния. Аналитическое решение задачи даёт стандартную формулу градиентного спуска:

$$
\phi-\phi_{0} \propto-\nabla_{\phi} f(\phi)\left|{ }_{\phi=\phi_{0}}\right.
$$

Однако, в случае функционала вида (А.1) параметризации $\boldsymbol{q}(\boldsymbol{x} \mid \boldsymbol{\phi})$ могут быть таковы, что небольшие изменения параметров $\phi$ могут радикально сильно поменять само распределение $\boldsymbol{q}(\boldsymbol{x} \mid \boldsymbol{\phi})$, а значит, и значение $\boldsymbol{f}(\boldsymbol{q}(\boldsymbol{x} \mid \boldsymbol{\phi}))$; и наоборот, для внесения каких-то небольших изменений в $\boldsymbol{q}(\boldsymbol{x} \mid \boldsymbol{\phi})$ необходимо поменять $\boldsymbol{\phi}$ достаточно сильно в смысле Евклидова расстояния.

Пример 141: $\mathcal{N}(0,100)$ похож $\mathcal{N}(1,100)$, когда $\mathcal{N}(0,0.1)$ совсем не похоже на $\mathcal{N}(1,0.1)$; при этом для обоих пар евклидово расстояние между значениями параметров равно единице.

---

# A.1.2. Матрица Фишера 

Оптимизация при помощи натурального градиента предлагает использовать другую метрику, которая учтёт структуру нашего функционала:

$$
\left\{\begin{array}{l}
f(\phi) \approx f\left(\phi_{0}\right)+\left\langle\nabla_{\phi} f(\phi)\right|_{\phi=\phi_{0}}, \phi-\phi_{0}\right\rangle \rightarrow \min _{\phi} \\
\mathrm{KL}\left(q\left(x \mid \phi_{0}\right) \| q(x \mid \phi)\right) \leq \alpha
\end{array}\right.
$$

Как решать такую задачу условной оптимизации? Если $\phi \approx \phi_{0}$, достаточно аппроксимировать дивергенцию $\mathbf{K L}\left(q\left(x \mid \phi_{0}\right) \| q(x \mid \phi)\right)$ при помощи разложения в ряд Тейлора до второго члена. До второго - потому что первое ноль.

## Утверждение 93:

$$
\nabla_{\phi} \mathrm{KL}\left(q\left(x \mid \phi_{0}\right) \| q(x \mid \phi)\right)\left.\right|_{\phi=\phi_{0}}=0
$$

Доказательство. KL-дивергенция в точке $\phi=\phi_{0}$ равна 0 как дивергенция между одинаковыми распределениями, следовательно как функция от $\phi$ она достигает в этой точке глобального минимума $\Rightarrow$ градиентравен нулю.

Определение 128: Для распределения $q(x \mid \phi)$ матрицей Фииера (Fisher matrix) называется

$$
F_{q}(\phi):=-\mathbb{E}_{q(x \mid \phi)} \nabla_{\phi}^{2} \log q(x \mid \phi)
$$

Теорема 93: Матрица Фишера есть гессиан KL-дивергенции:

$$
\nabla_{\phi}^{2} \mathrm{KL}\left(q\left(x \mid \phi_{0}\right) \| q(x \mid \phi)\right)\left.\right|_{\phi=\phi_{0}}=F_{q}\left(\phi_{0}\right)
$$

Доказательство.

$$
\left.\nabla_{\phi}^{2} \mathrm{KL}\left(q\left(x \mid \phi_{0}\right) \| q(x \mid \phi)\right)\right|_{\phi=\phi_{0}}=\nabla_{\phi}^{2}\left[\operatorname{const}(\phi)-\mathbb{E}_{q\left(x \mid \phi_{0}\right)} \log q(x \mid \phi)\right]\left.\right|_{\phi=\phi_{0}}=F_{q}\left(\phi_{0}\right)
$$

Прежде чем двинуться дальше, остановимся на паре важных для нас свойств матрицы Фишера.
Теорема 94 - Эквивалентное определение матрицы Фишера:

$$
F_{q}(\phi):=\mathbb{E}_{q(x \mid \phi)} \nabla_{\phi} \log q(x \mid \phi)\left(\nabla_{\phi} \log q(x \mid \phi)\right)^{T}
$$

Доказательство.

$$
\begin{aligned}
F_{q}(\phi) & =-\mathbb{E}_{q(x \mid \phi)} \nabla_{\phi} \nabla_{\phi} \log q(x \mid \phi)= \\
& =\{\text { градиент логарифма }\}=-\mathbb{E}_{q(x \mid \phi)} \nabla_{\phi} \frac{\nabla_{\phi} q(x \mid \phi)}{q(x \mid \phi)}= \\
& =\{\text { градиент отношения }\}=-\mathbb{E}_{q(x \mid \phi)} \frac{\nabla_{\phi}^{2} q(x \mid \phi)}{q(x \mid \phi)}+\mathbb{E}_{q(x \mid \phi)} \frac{\nabla_{\phi} q(x \mid \phi) \nabla_{\phi} q(x \mid \phi)^{T}}{q(x \mid \phi)^{2}}=(\star)
\end{aligned}
$$

Заметим, что первое слагаемое равно нулю. Действительно:

$$
\mathbb{E}_{q(x \mid \phi)} \frac{\nabla_{\phi}^{2} q(x \mid \phi)}{q(x \mid \phi)}=\int_{x} \nabla_{\phi}^{2} q(x \mid \phi) \mathrm{d} x=\nabla_{\phi}^{2} \int_{x} q(x \mid \phi) \mathrm{d} x=\nabla_{\phi}^{2} 1=0
$$

В оставшемся втором слагаемом перегруппируем множители (заметим, что в знаменатели дроби стоит скаляр):

$$
\begin{aligned}
(\star) & =\mathbb{E}_{q(x \mid \phi)} \frac{\nabla_{\phi} q(x \mid \phi)}{q(x \mid \phi)}\left(\frac{\nabla_{\phi} q(x \mid \phi)}{q(x \mid \phi)}\right)^{T}= \\
& =\{\text { градиент логарифма }\}=\mathbb{E}_{q(x \mid \phi)} \nabla_{\phi} \log q(x \mid \phi)\left(\nabla_{\phi} \log q(x \mid \phi)\right)^{T}
\end{aligned}
$$

---

Теорема 95 - Репараметризация матрицы Фишера: Пусть одно распределение параметризовано двумя способами, а то есть $\boldsymbol{q}_{\phi}(\boldsymbol{x} \mid \phi) \equiv \boldsymbol{q}_{\nu}(\boldsymbol{x} \mid \boldsymbol{\nu})$, где $\boldsymbol{\nu}=\boldsymbol{\nu}(\phi)-$ преобразование с якобианом* $\boldsymbol{J}$. Тогда:

$$
F_{q}(\phi)=J F_{q}(\nu) J^{T}
$$

Доказательство.

$$
\begin{aligned}
F_{q}(\phi) & =\mathbb{E}_{q_{\phi}(x \mid \phi)}\left(\nabla_{\phi} \log q(x \mid \phi)\right)\left(\nabla_{\phi} \log q(x \mid \phi)\right)^{T}= \\
=\left\{q_{\phi}(x \mid \phi) \equiv q_{\nu}(x \mid \nu)\right\} & =\mathbb{E}_{q_{\nu}(x \mid \nu)}\left(\nabla_{\phi} \log q(x \mid \nu)\right)\left(\nabla_{\phi} \log q(x \mid \nu)\right)^{T}= \\
=\{\text { chain rule }\} & =\mathbb{E}_{q_{\phi}(x \mid \phi)} J \nabla_{\nu} \log q(x \mid \nu)\left(\nabla_{\nu} \log q(x \mid \nu)\right)^{T} J^{T}= \\
& =J F_{q}(\nu) J^{T}
\end{aligned}
$$

* будем считать, что якобиан имеет размер $\boldsymbol{h}_{\phi} \times \boldsymbol{h}_{\boldsymbol{\nu}}$, где $\boldsymbol{h}_{\boldsymbol{\nu}}, \boldsymbol{h}_{\phi}$ - размерности $\boldsymbol{\nu}$ и $\phi$ соответственно.

# A.1.3. Натуральный градиент 

Итак, приближённая задача выглядит следующим образом:

$$
\left\{\begin{array}{l}
f(\phi) \approx f\left(\phi_{0}\right)+\left\langle\nabla_{\phi} f(\phi)\right|_{\phi=\phi_{0}}, \phi-\phi_{0} \rangle \rightarrow \min _{\phi} \\
\left(\phi-\phi_{0}\right)^{T} F_{q}\left(\phi_{0}\right)\left(\phi-\phi_{0}\right) \leq \alpha
\end{array}\right.
$$

По сути, матрица Фишера задаёт нам приближённо метрику ${ }^{1}$, индуцированную $\mathbf{K L}$-дивергенцией. Такая задача также решается аналитично и получается приятный ответ:

$$
\phi-\phi_{0} \propto-\left.F_{q}\left(\phi_{0}\right)^{-1} \nabla_{\phi} f(\phi)\right|_{\phi=\phi_{0}}
$$

Определение 129: Натуральным градиентом (natural gradient) функции $f(\phi):=f(q(x \mid \phi))$ называется

$$
\tilde{\nabla}_{\phi} f(\phi):=F_{q}(\phi)^{-1} \nabla_{\phi} f(\phi)
$$

Итак, натуральный градиент есть скорректированный матрицей Фишера обычный градиент. Он очень напоминает методы оптимизации второго порядка, так как происходит учёт вида оптимизируемой функции, в том числе масштабирование по осям.

Теорема 96 - Инвариантность натурального градиента относительно параметризации: Пусть одно распределение параметризовано двумя способами, а то есть $\boldsymbol{q}_{\phi}(\boldsymbol{x} \mid \phi) \equiv \boldsymbol{q}_{\nu}(\boldsymbol{x} \mid \boldsymbol{\nu})$, где $\boldsymbol{\nu}=\boldsymbol{\nu}(\phi)-$ преобразование с якобианом $J$.

Тогда натуральные градиенты указывают в одном и том же направлении:

$$
\tilde{\nabla}_{\phi} f(\nu)=J^{T} \tilde{\nabla}_{\phi} f(\phi)
$$

Доказательство.

$$
\begin{aligned}
J^{T} \tilde{\nabla}_{\phi} f(\phi) & =J^{T} F_{q}(\phi)^{-1} \nabla_{\phi} f(\phi)= \\
=\{\text { репараметризация матрицы Фишера (А.3) }\} & =J^{T}\left(J F_{q}(\nu) J^{T}\right)^{-1} \nabla_{\phi} f(\phi) \\
=\{\text { свойства обратной матрицы }\} & =J^{T} J^{-T} F_{q}(\nu)^{-1} J^{-1} \nabla_{\phi} f(\phi) \\
=\{\text { chain rule }\} & =F_{q}(\nu)^{-1} J^{-1} J \nabla_{\nu} f(\nu)= \\
& =F_{q}(\nu)^{-1} \nabla_{\nu} f(\nu)= \\
& =\tilde{\nabla}_{\nu} f(\nu)
\end{aligned}
$$

[^0]
[^0]:    ${ }^{1}$ А точнее, Римановскро метрику. В Евклидовом пространстве общей формой скалярного произведения является $(\boldsymbol{x}, \boldsymbol{y})=$ $:=x^{T} G y$, где $G$ - некоторая фиксированная положительно определённая матрица, и индуцированной метрикой соответственно является $d(x, y)^{2}:=(y-x)^{T} G(y-x)$. В Римановском пространстве $G$, называемая также метрическим тензором (metric tensor), зависит от $\boldsymbol{x}$, и относительное расстояние между точками зависит от положения в пространстве. Римановские пространства используются для описания расстояний между точками на поверхностях, а метрические тензоры для них обладают рядом приятных свойств, которыми, в частности, обладает и матрица Фишера.

---

# §А.2. Обоснование формул CMA-ES 

В данном разделе мы вычислим формулу натурального градиента для оптимизации вспомогательного функционала

$$
g(\lambda):=\mathbb{E}_{\theta \sim q(\theta \mid \lambda)} J(\theta) \rightarrow \max _{\lambda}
$$

для эволюционной стратегии $q(\theta \mid \lambda):=\mathcal{N}(\mu, \Sigma)$ с параметрами $\lambda:=(\mu, \Sigma)$.

## A.2.1. Вычисление градиента

Сначала нам придётся напрячься и продифференцировать логарифм правдоподобия по параметрам $\boldsymbol{\mu}$ и $\boldsymbol{\Sigma}$ :

$$
\log q(\theta \mid \mu, \Sigma)=\operatorname{const}(\mu, \Sigma)-\frac{1}{2} \log \operatorname{det} \Sigma-\frac{1}{2}(\theta-\mu)^{T} \Sigma^{-1}(\theta-\mu)
$$

Поскольку нам придётся дифференцировать функционал по матрице, вычислять градиенты удобно в терминах дифференциала. Будем использовать следующую нотацию: будем обозначать за $D_{x} f(x)[h]$ дифференциал функции $\boldsymbol{f}(\boldsymbol{x})$ по переменной $\boldsymbol{x}$ с приращением $\boldsymbol{h}$. Дифференциал имеет ту же размерность, что и выход функции $\boldsymbol{f}$, что и делает его использование таким удобным. В итоге мы должны получить линейную часть приращения $\boldsymbol{f}(\boldsymbol{x})$; так, в итоге вычислений мы получим представление дифференциала в виде $D_{x} f(x)[h]=\left\langle\nabla_{x} f(x), h\right\rangle$ и сможем «вытащить» градиент.

## Утверждение 95:

$$
\nabla_{\mu} \log q(\theta \mid \mu, \Sigma)=\Sigma^{-1}(\theta-\mu)
$$

Доказательство.

$$
D_{\mu} \log q(\theta \mid \mu, \Sigma)[h]=\frac{1}{2} h^{T} \Sigma^{-1}(\theta-\mu)+\frac{1}{2}(\theta-\mu)^{T} \Sigma^{-1} h=\left\langle\Sigma^{-1}(\theta-\mu), h\right\rangle
$$

Отсюда получаем градиент как вектор, скалярно перемножаемый на приращение $\boldsymbol{h}: \boldsymbol{\Sigma}^{-1}(\boldsymbol{\theta}-\boldsymbol{\mu})$.
Для $\boldsymbol{\Sigma}$ нам понадобится пара табличных формул векторного дифференцирования (здесь $\operatorname{Tr}$ - оператор взятия следа матрицы):

$$
\begin{gathered}
D_{\Sigma} \log \operatorname{det} \Sigma[H]=\operatorname{Tr}\left(\Sigma^{-1} H\right) \\
D_{\Sigma} \Sigma^{-1}[H]=-\Sigma^{-1} H \Sigma^{-1}
\end{gathered}
$$

## Утверждение 96:

$$
\nabla_{\Sigma} \log q(\theta \mid \mu, \Sigma)=-\frac{1}{2} \Sigma^{-1}+\frac{1}{2} \Sigma^{-1}(\theta-\mu)(\theta-\mu)^{T} \Sigma^{-1}
$$

Доказательство.

$$
\begin{aligned}
D_{\Sigma} \log q(\theta \mid \mu, \Sigma)[H] & =-\frac{1}{2} D_{\Sigma} \log \operatorname{det} \Sigma[H]-\frac{1}{2}(\theta-\mu)^{T} D_{\Sigma} \Sigma^{-1}[H](\theta-\mu) \\
= & \{\text { нодставляем формулы (А.6) и (А.7) }\}=-\frac{1}{2} \operatorname{Tr}\left(\Sigma^{-1} H\right)+\frac{1}{2}(\theta-\mu)^{T} \Sigma^{-1} H \Sigma^{-1}(\theta-\mu) \\
= & \{\text { фокус: если } \boldsymbol{v}-\text { скаляр, } \boldsymbol{v}=\operatorname{Tr}(\boldsymbol{v})\}=-\frac{1}{2} \operatorname{Tr}\left(\Sigma^{-1} H\right)+\frac{1}{2} \operatorname{Tr}\left((\boldsymbol{\theta}-\mu)^{T} \Sigma^{-1} H \Sigma^{-1}(\boldsymbol{\theta}-\mu)\right)= \\
= & \{\text { тождество } \operatorname{Tr}(A B C)=\operatorname{Tr}(B C A)\}=-\frac{1}{2} \operatorname{Tr}\left(\Sigma^{-1} H\right)+\frac{1}{2} \operatorname{Tr}\left(\Sigma^{-1}(\boldsymbol{\theta}-\mu)(\boldsymbol{\theta}-\mu)^{T} \Sigma^{-1} H\right)
\end{aligned}
$$

Дифференциал - это скалярное произведение градиента на приращение $\boldsymbol{H}$, которое в пространстве матриц задано оператором $\operatorname{Tr}$.

Из этих формул можно увидеть, почему градиентный подъём для оптимизации (А.5) по $\boldsymbol{\mu}, \boldsymbol{\Sigma}$ не так хорош. Допустим, матрица ковариации адаптировалась так, что вдоль оси $\boldsymbol{\theta}_{0}$ разброс большой. Пусть где-то справа нашлись особи с огромным $J(\theta)$; тогда взвешенное среднее

$$
\frac{1}{N} \sum_{\theta \in \mathscr{P}} J(\theta) \theta
$$

будет указывать примерно в центр этого скопления, будет говорить сильно увеличить компоненту $\boldsymbol{\theta}_{0}$. Однако, формула градиента говорит сделать поправку на матрицу ковариации: мол, мы генерировали вдоль $\boldsymbol{\theta}_{0}$ с большим разбросом, и поэтому вдоль этой оси градиент нужно пропорционально сбить. В итоге, к центру скопления

---

делается куда меньший шаг, чем по идее должен был бы по итогам генерации целого поколения особей. Примерно тоже самое наблюдается с матрицей ковариации: в формуле также делается поправка на текущее значение матрицы ковариации (домножение на $\boldsymbol{\Sigma}^{-\mathbf{1}}$ с двух сторон) вместо движения к эмпирической (взвешанной на $\check{\boldsymbol{J}}(\boldsymbol{\theta})$ ) матрице.

# A.2.2. Произведение Кронекера 

Нам понадобится работать с матрицей Фишера, которая имеет размерность «количество параметров на количество параметров». Иными словами, нам понадобится сравнивать попарно между собой все элементы матрицы $\boldsymbol{\Sigma}$, а это значит, нам придётся чуть-чуть залезть в алгебру Кронекера. Для этого мы введём операцию векторизации матрицы vec, вытягивающей все элементы матрицы в вектор.

Утверждение 97: Векторизация - линейная операция; поэтому, в частности:

$$
\begin{gathered}
\mathbb{E}_{\boldsymbol{\theta}} \operatorname{vec}(\boldsymbol{f}(\boldsymbol{\theta}))=\operatorname{vec}\left(\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{f}(\boldsymbol{\theta})\right) \\
\nabla_{\operatorname{vec}(\Sigma)} \boldsymbol{f}(\boldsymbol{\Sigma})=\operatorname{vec}\left(\nabla_{\Sigma} \boldsymbol{f}(\boldsymbol{\Sigma})\right)
\end{gathered}
$$

Доказательство. Можно проверить непосредственно: мы просто переписываем все элементы матрицы в другой форме, сложению и умножению на скаляры всё равно.

Определение 130: Произведением Кронекера двух матриц $\boldsymbol{A} \in \mathbb{R}^{\boldsymbol{n} \times \boldsymbol{m}}, \boldsymbol{B} \in \mathbb{R}^{\boldsymbol{p} \times \boldsymbol{q}}$ называется

$$
\boldsymbol{A} \otimes \boldsymbol{B}:=\left(\begin{array}{ccc}
a_{11} B & \ldots & a_{1 m} B \\
\vdots & \ddots & \vdots \\
a_{n 1} B & \ldots & a_{n m} B
\end{array}\right) \in \mathbb{R}^{\boldsymbol{n p} \times \boldsymbol{m} q}
$$

где $\boldsymbol{a}_{\boldsymbol{i j}}$ - элементы матрицы $\boldsymbol{A}$.
Пример 142:

$$
\begin{aligned}
& \left(\begin{array}{cc}
3 & 0 \\
1 & -2
\end{array}\right) \otimes\left(\begin{array}{ccc}
4 & 0 & -1 \\
-4 & 2 & 5
\end{array}\right)=\left(\begin{array}{c}
3\left(\begin{array}{ccc}
4 & 0 & -1 \\
-4 & 2 & 5 \\
1 & 4 & 0 & -1 \\
-4 & 2 & 5
\end{array}\right) \\
& =\left(\begin{array}{cccccc}
4 & 0 & -1 \\
-4 & 2 & 5 \\
12 & 0 & -3 & 0 & 0 & 0 \\
-12 & 6 & 15 & 0 & 0 & 0 \\
4 & 0 & -1 & -8 & 0 & 2 \\
-4 & 2 & 5 & 8 & -4 & -10
\end{array}\right) .
\end{aligned}
$$

Теорема 97: Для матриц $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}, \boldsymbol{D}$ с размерностями, для которых существует произведения $\boldsymbol{A} \boldsymbol{C}$ и $\boldsymbol{B} \boldsymbol{D}$, верно:

$$
(A \otimes B)(C \otimes D)=A C \otimes B D
$$

Доказательство. Проверяется непосредственно: пусть $\boldsymbol{a}_{\boldsymbol{i j}}$ - элементы матрицы $\boldsymbol{A} \in \mathbb{R}^{\boldsymbol{n} \times \boldsymbol{m}}, \boldsymbol{c}_{\boldsymbol{i j}}$ - элементы матрицы $\boldsymbol{C} \in \mathbb{R}^{\boldsymbol{m} \times \boldsymbol{q}}$, тогда:

$$
\begin{aligned}
(A \otimes B)(C \otimes D) & =\left(\begin{array}{ccc}
a_{11} B & \ldots & a_{1 m} B \\
\vdots & \ddots & \vdots \\
a_{n 1} B & \ldots & a_{n m} B
\end{array}\right)\left(\begin{array}{ccc}
c_{11} D & \ldots & c_{1 q} D \\
\vdots & \ddots & \vdots \\
c_{m 1} D & \ldots & c_{m q} D
\end{array}\right) \\
& =\left(\begin{array}{cccc}
\sum_{k=1}^{m} a_{1 k} B c_{k 1} D & \ldots & \sum_{k=1}^{m} a_{1 k} B c_{k q} D \\
\vdots & \ddots & \vdots \\
\sum_{k=1}^{m} a_{n k} B c_{k 1} D & \ldots & \sum_{k=1}^{m} a_{n k} B c_{k q} D
\end{array}\right)
\end{aligned}
$$

Вводя обозначение $\boldsymbol{e}_{\boldsymbol{i j}}:=\sum_{\boldsymbol{k}=0}^{\boldsymbol{m}} \boldsymbol{a}_{\boldsymbol{i k}} \boldsymbol{c}_{\boldsymbol{k} j}$, получаем Кронекерово произведение между матрицей $\boldsymbol{E}$, состоящей из этих элементов, и матрицей $\boldsymbol{B D}$. Осталось заметить, что матрица $\boldsymbol{E}=\boldsymbol{A} \boldsymbol{C}$ по определению.

---

Утверждение 98: Для обратимых матриц $\boldsymbol{A}, \boldsymbol{B}$ :

$$
(A \otimes B)^{-1}=A^{-1} \otimes B^{-1}
$$

Пояснение.

$$
\left(A^{-1} \otimes B^{-1}\right)(A \otimes B)=A^{-1} A \otimes B^{-1} B=I \otimes I=I
$$

Теорема 98: Для матриц $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}$, для которых существует произведение $\boldsymbol{A B C}$, верно:

$$
\operatorname{vec}(A B C)=\left(C^{T} \otimes A\right) \operatorname{vec}(B)
$$

Доказательство. Проверяется непосредственно. Пусть $\boldsymbol{c}_{\boldsymbol{i j}}$ - элементы матрицы $\boldsymbol{C}, \boldsymbol{b}_{\boldsymbol{i}}$ - строки матрицы B. Тогда $\operatorname{vec}(B)=\left(b_{1}, b_{2} \ldots b_{m}\right)^{T}$, и:

$$
\left(C^{T} \otimes A\right) \operatorname{vec}(B)=\left(\begin{array}{ccc}
c_{11} A & \ldots & c_{m 1} A \\
\vdots & \ddots & \vdots \\
c_{1 n} A & \ldots & c_{m n} A
\end{array}\right)\left(\begin{array}{c}
b_{1}^{T} \\
\vdots \\
b_{m}^{T}
\end{array}\right)=\left(\begin{array}{c}
\sum_{i=1}^{m} c_{i 1} A b_{i}^{T} \\
\vdots \\
\sum_{i=1}^{m} c_{i n} A b_{i}^{T}
\end{array}\right)
$$

Осталось заметить, что $\sum_{i=1}^{m} c_{i j} A b_{i}^{T}$ есть транспонированная $\boldsymbol{j}$-ая строчка матрицы $\boldsymbol{A B C}$.

# A.2.3. Вычисление матрицы Фишера 

Получение матрицы Фишера для нормального распределения представляет собой напряжное техническое упражнение. Будем далее считать, что наш набор параметров есть $\boldsymbol{\lambda}:=(\boldsymbol{\mu}, \operatorname{vec}(\Sigma))$; тогда матрица Фишера для нас будет матрицей, размера $\left(h+h^{2}\right) \times\left(h+h^{2}\right)$. Представим её в блочно-диагональном виде:

$$
F_{q}(\lambda)=\left(\begin{array}{cc}
F_{\mu \mu} & F_{\mu \text { vec } \Sigma} \\
F_{\mu \text { vec } \Sigma}^{T} & F_{\text {vec } \Sigma \text { vec } \Sigma}
\end{array}\right)
$$

где $\boldsymbol{F}_{\boldsymbol{\mu} \boldsymbol{\mu}} \in \mathbb{R}^{\boldsymbol{h} \times \boldsymbol{h}}, \boldsymbol{F}_{\text {vec } \boldsymbol{\Sigma} \text { vec } \boldsymbol{\Sigma}} \in \mathbb{R}^{\boldsymbol{h}^{2} \times \boldsymbol{h}^{2}}$. Мы воспользовались симметричностью матрицы Фишера (что было видно из её эквивалентного определения (А.2)).

Будем искать эти блоки по одному. Для удобства введём обозначение

$$
S:=(\theta-\mu)(\theta-\mu)^{T}
$$

и продублируем формулы градиентов логарифма правдоподобия, полученные в разделе А.2.1:

$$
\begin{gathered}
\nabla_{\mu} \log q(\theta \mid \mu, \Sigma)=\Sigma^{-1}(\theta-\mu) \\
\nabla_{\Sigma} \log q(\theta \mid \mu, \Sigma)=-\frac{1}{2} \Sigma^{-1}+\frac{1}{2} \Sigma^{-1} S \Sigma^{-1}
\end{gathered}
$$

Заметим, что по свойствам нормального распределения:

$$
\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{\theta}=\boldsymbol{\mu} \quad \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{S}=\boldsymbol{\Sigma}
$$

Теорема 99: Матрица Фишера для нормального распределения имеет вид:

$$
F_{q}(\lambda)=\left(\begin{array}{cc}
\boldsymbol{\Sigma}^{-1} & 0 \\
0 & \frac{1}{2}\left(\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{\Sigma}^{-1}\right)
\end{array}\right)
$$

Доказательство для $\boldsymbol{F}_{\boldsymbol{\mu} \boldsymbol{\mu}}$.

$$
F_{\mu \mu}=-\mathbb{E}_{\theta} \nabla_{\mu}^{2} \log q(\theta \mid \mu, \Sigma)=\{(\mathrm{A} .13)\}=-\mathbb{E}_{\theta} \nabla_{\mu} \Sigma^{-1}(\theta-\mu)=\mathbb{E}_{\theta} \Sigma^{-1}=\Sigma^{-1}
$$

Доказательство для $\boldsymbol{F}_{\boldsymbol{\mu} \text { vec } \boldsymbol{\Sigma}}$. Сначала посчитаем вторую производную.

$$
D_{\Sigma} \nabla_{\mu} \log q(\theta \mid \mu, \Sigma)[\boldsymbol{H}]=\{(\mathrm{A} .13)\}=-D_{\Sigma} \Sigma^{-1}(\theta-\mu)[\boldsymbol{H}]=\{(\mathrm{A} .7)\}=\Sigma^{-1} H \Sigma^{-1}(\theta-\mu)
$$

Тут дальше есть проблема: нас, вообще говоря, интересует градиент. Однако производная вектора $\nabla_{\mu} \log q(\theta \mid \mu, \Sigma)$ по матрице это трёхмерный тензор, и надо танцевать с векторизацией. Мы схитрим:

---

давайте посмотрим на мат.ожидание дифференциала по $\boldsymbol{\theta}$ :

$$
-\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}(\boldsymbol{\theta}-\boldsymbol{\mu})=-\boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1} \mathbb{E}_{\boldsymbol{\theta}}(\boldsymbol{\theta}-\boldsymbol{\mu})=\mathbf{0}
$$

Как следствие, это нулевой тензор, и матрица Фишера тоже ноль.
Доказательство для $\boldsymbol{F}_{\text {чес } \Sigma \mu}$ (Sanity check). Проверим, что и симметричный блок тоже ноль (формально это уже доказано в силу симметрии). Для этого нужно дифференцировать по $\boldsymbol{\mu}$ первую производную по $\boldsymbol{\Sigma}$ (А.14). Сразу же будем смотреть на мат.ожидание этого выражения по $\boldsymbol{\theta}$ :

$$
-\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\mu}} \boldsymbol{\nabla}_{\boldsymbol{\Sigma}} \log q(\boldsymbol{\theta} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})[\boldsymbol{h}]=\frac{1}{2} \boldsymbol{\Sigma}^{-1} \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\mu}} \boldsymbol{S}[\boldsymbol{h}] \boldsymbol{\Sigma}^{-1}
$$

При этом $\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\mu}} \boldsymbol{S}[\boldsymbol{h}]=\mathbf{0}$, поскольку:

$$
\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\mu}} \boldsymbol{S}[\boldsymbol{h}]=-\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{h}(\boldsymbol{\theta}-\boldsymbol{\mu})^{T}-\mathbb{E}_{\boldsymbol{\theta}}(\boldsymbol{\theta}-\boldsymbol{\mu}) \boldsymbol{h}^{T}=\mathbf{0}
$$

Доказательство для $\boldsymbol{F}_{\text {чес } \Sigma \text { нес } \Sigma}$ :

$$
\begin{aligned}
& -\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\Sigma}} \boldsymbol{\nabla}_{\text {vec } \boldsymbol{\Sigma}} \log q(\boldsymbol{\theta} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})[\boldsymbol{H}]= \\
= & -\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\Sigma}} \operatorname{vec}\left(\boldsymbol{\nabla}_{\boldsymbol{\Sigma}} \log q(\boldsymbol{\theta} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\right)[\boldsymbol{H}]= \\
= & \{\text { подставляем (А.14) }\}= \\
= & -\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\Sigma}} \operatorname{vec}\left(-\frac{1}{2} \boldsymbol{\Sigma}^{-1}+\frac{1}{2} \boldsymbol{\Sigma}^{-1} \boldsymbol{S} \boldsymbol{\Sigma}^{-1}\right)[\boldsymbol{H}]= \\
= & \left\{\text { вносим минус, } \boldsymbol{D}_{\boldsymbol{\Sigma}} \text { и } \mathbb{E}_{\boldsymbol{\theta}}\right\}= \\
= & \operatorname{vec}\left(\frac{1}{2} \boldsymbol{D}_{\boldsymbol{\Sigma}} \boldsymbol{\Sigma}^{-1}[\boldsymbol{H}]-\frac{1}{2} \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\boldsymbol{\Sigma}}\left(\boldsymbol{\Sigma}^{-1} \boldsymbol{S} \boldsymbol{\Sigma}^{-1}\right)[\boldsymbol{H}]\right)= \\
= & \{\text { дифф. билинейной функции }\}= \\
= & \operatorname{vec}\left(\frac{1}{2} \boldsymbol{D}_{\boldsymbol{\Sigma}} \boldsymbol{\Sigma}^{-1}[\boldsymbol{H}]-\frac{1}{2} \boldsymbol{D}_{\boldsymbol{\Sigma}} \boldsymbol{\Sigma}^{-1}[\boldsymbol{H}] \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{S} \boldsymbol{\Sigma}^{-1}-\frac{1}{2} \boldsymbol{\Sigma}^{-1} \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{S} \boldsymbol{D}_{\boldsymbol{\Sigma}} \boldsymbol{\Sigma}^{-1}[\boldsymbol{H}]\right)= \\
= & \{\text { дифф. обратной матрицы (А.7) }\}= \\
= & \operatorname{vec}\left(-\frac{1}{2} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}+\frac{1}{2} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1} \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{S} \boldsymbol{\Sigma}^{-1}+\frac{1}{2} \boldsymbol{\Sigma}^{-1} \mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{S} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}\right)= \\
= & \left\{\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{S}=\boldsymbol{\Sigma}\right\}=\operatorname{vec}\left(-\frac{1}{2} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}+\frac{1}{2} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}+\frac{1}{2} \boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}\right)= \\
= & \frac{1}{2} \operatorname{vec}\left(\boldsymbol{\Sigma}^{-1} \boldsymbol{H} \boldsymbol{\Sigma}^{-1}\right)= \\
= & \{\text { свойство (А.12) }\}=\frac{1}{2}\left(\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{\Sigma}^{-1}\right) \operatorname{vec}(\boldsymbol{H})
\end{aligned}
$$

Приращение вектора, полученное в виде умножения матрицы на векторизацию приращения, эквивалентно тому, что функция под дифференциалом рассматривалась бы как функция от vec $\boldsymbol{\Sigma}$ :

$$
\mathbb{E}_{\boldsymbol{\theta}} \boldsymbol{D}_{\text {vec } \Sigma} \boldsymbol{\nabla}_{\text {vec } \Sigma} \log q(\boldsymbol{\theta} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})[\operatorname{vec} \boldsymbol{H}]=\frac{1}{2}\left(\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{\Sigma}^{-1}\right) \operatorname{vec}(\boldsymbol{H})
$$

Отсюда $\boldsymbol{F}_{\text {vec } \Sigma \text { vec } \Sigma}=\frac{1}{2}\left(\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{\Sigma}^{-1}\right)$.

Утверждение 99: Обратная матрица Фишера для нормального распределения имеет вид:

$$
\boldsymbol{F}_{q}^{-1}(\boldsymbol{\lambda})=\left(\begin{array}{cc}
\boldsymbol{\Sigma} & 0 \\
0 & 2(\boldsymbol{\Sigma} \otimes \boldsymbol{\Sigma})
\end{array}\right)
$$

Пояснение. Для обращения нижнего блока применить формулу (А.11).

---

# A.2.4. Covariance Matrix Adaptation Evolution Strategy (CMA-ES) 

Вспомним общую формулу градиента для эволюционных стратегий:

$$
\nabla_{\lambda} g(\lambda)=\mathbb{E}_{\theta \sim q(\theta \mid \lambda)} \nabla_{\lambda} \log q(\theta \mid \lambda) J(\theta)
$$

Достаточно домножить её на обратную матрицу Фишера, чтобы получить формулу натурального градиента для обновления $\lambda=(\mu, \Sigma)$.

Теорема 100: Натуральный градиент для $\boldsymbol{\mu}$ в (А.5) выглядит так:

$$
\hat{\nabla}_{\mu} g(\mu, \Sigma)=\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta)(\theta-\mu)
$$

Доказательство.

$$
\begin{aligned}
\hat{\nabla}_{\mu} g(\mu, \Sigma) & =F_{\mu \mu}^{-1} \nabla_{\mu} g(\mu, \Sigma)= \\
=\{\text { подставляем (А.16) }\} & =\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta) F_{\mu \mu}^{-1} \nabla_{\mu} \log q(\theta \mid \mu, \Sigma)= \\
& =\left\{\text { подставляем } \boldsymbol{F}_{\mu \mu}^{-1} \text { из (А.15) и градиент из (А.13) }\right\}= \\
& =\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta) \Sigma \Sigma^{-1}(\theta-\mu)= \\
& =\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta)(\theta-\mu)
\end{aligned}
$$

Заметим, что для OpenAI-ES 2.8 при константной матрице ковариация натуральный градиент и градиент отличаются на константу (она «сокращается с learning rate»); поэтому можно считать, что OpenAI-ES есть тоже алгоритм натуральной эволюционной стратегии.

Теорема 101: Натуральный градиент для $\boldsymbol{\Sigma}$ в (А.5) выглядит так:

$$
\hat{\nabla}_{\Sigma} g(\mu, \Sigma)=\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta)(S-\Sigma)
$$

Доказательство.

$$
\begin{aligned}
\hat{\nabla}_{\mathrm{vec}(\Sigma)} g(\mu, \Sigma) & =F_{\mathrm{vec}(\Sigma) \mathrm{vec}(\Sigma)}^{-1} \nabla_{\mathrm{vec}(\Sigma)} g(\mu, \Sigma)= \\
=\{\text { подставляем (А.16) }\} & =\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta) F_{\mathrm{vec}(\Sigma) \mathrm{vec}(\Sigma)}^{-1} \mathrm{vec}\left(\nabla_{\Sigma} \log q(\theta \mid \mu, \Sigma)\right)= \\
& =\left\{\text { подставляем } \boldsymbol{F}_{\mathrm{vec}(\Sigma) \mathrm{vec}(\Sigma)}^{-1} \text { из (А.15) и градиент из (А.14) }\right\}= \\
& =\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta)\left(\Sigma \otimes \Sigma\right) \operatorname{vec}\left(\Sigma^{-1} S \Sigma^{-1}-\Sigma^{-1}\right)= \\
=\{\text { свойство (А.10) }\} & =\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta) \operatorname{vec}\left(\Sigma \Sigma^{-1} S \Sigma^{-1} \Sigma-\Sigma \Sigma^{-1} \Sigma\right)= \\
& =\operatorname{vec}\left(\frac{1}{N} \sum_{\theta \in \mathscr{P}} \hat{J}(\theta)(S-\Sigma)\right)
\end{aligned}
$$

Отсюда, убирая векторизацию, получаем формулу.

---

# §А.3. Сходимость Q-learning 

В данном разделе приведено доказательство теоремы 28. Пусть дано MDP с конечным пространством состояний $\mathcal{S}$ и действий $\mathcal{A} . Q_{0}(s, a)$ - начальное приближение, на $\boldsymbol{k}$-ом шаге $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$ строится по правилу

$$
\boldsymbol{Q}_{k+1}(s, a)=\left(1-\alpha_{k}(s, a)\right) Q_{k}(s, a)+\alpha_{k}(s, a)\left(r(s, a)+\gamma \max _{a} Q_{k}\left(s^{\prime}, a^{\prime}\right)\right)
$$

где $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, а $\boldsymbol{\alpha}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$ - случайные величины, на которые накладывается единственное требование: для всех $s, \boldsymbol{a}$ с вероятностью 1 выполнено:

$$
\sum_{k \geq 0} \alpha_{k}(s, a)=+\infty \quad \sum_{k \geq 0} \alpha_{k}(s, a)^{2}<+\infty
$$

Докажем, что $\boldsymbol{Q}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a}) \xrightarrow{n \rightarrow+\infty} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ с вероятностью 1.

## A.3.1. Action Replay Process

Для доказательства рассмотрим конструкцию под названием Action Replay Process. Давайте запустим алгоритм Q-learning (проведём, чисто теоретически, бесконечное число итераций) и запишем для каждого реализации $\boldsymbol{\alpha}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$ и $\boldsymbol{s}_{\boldsymbol{k}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})$ - те следующие состояния, которые использовались на $\boldsymbol{k}$-ом шаге для обновления $\boldsymbol{Q}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$. Будем проводить такую аналогию - запишем эту историю «на карточках»: на $\boldsymbol{k}$-ой карточке записано для каждой пары $\boldsymbol{s}, \boldsymbol{a}$ по одному сэмплу $\boldsymbol{s}_{\boldsymbol{k}}^{\prime}(\boldsymbol{s}, \boldsymbol{a}) \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, а также степень $\boldsymbol{\alpha}_{\boldsymbol{k}}(\boldsymbol{s}, \boldsymbol{a})$, с которой этот сэмпл был использован для обновления Q-функции; можно считать, что для них в случившейся реализации выполнено (А.18), поскольку это происходит с вероятностью 1. Карточки, считаем, «сложены в стопку», начиная с нулевой карточки, на которой, условно, напишем наше исходное приближение Q-функции $\boldsymbol{Q}_{0}(\boldsymbol{s}, \boldsymbol{a})$ для всех $\boldsymbol{s}, \boldsymbol{a}$. Мы сейчас будем эту стопку карт «просматривать» от конца к началу.

Определение 131: Для данной реализации алгоритма Q-learning Action Replay Process (ARP) будем называть следующее MDP:

- Пространством состояний будем считать пару $\boldsymbol{s}, \boldsymbol{n}$, где $\boldsymbol{s} \in \mathcal{S}, \boldsymbol{n}$ - номер карточки.
- Пространством действий будем считать $\mathcal{A}$.
- Процесс генерации следующего состояния по данному состоянию $\boldsymbol{s}, \boldsymbol{n}$ и действию $\boldsymbol{a}$, который мы будем обозначать как $\boldsymbol{p}_{\text {ARP }}\left(\boldsymbol{s}^{\prime}, \boldsymbol{n}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)$, задаётся следующим образом. Если номер карточки $\boldsymbol{n}$ равен нулю, то «стопка карт закончилась», и следующее состояние - терминальное. Если $\boldsymbol{n}>\mathbf{0}$, то бросаем нечестную монетку, которая с вероятностью $\boldsymbol{\alpha}_{\boldsymbol{n}-1}(\boldsymbol{s}, \boldsymbol{a})$ выдаёт результат «остановиться», а с вероятностью $1-\boldsymbol{\alpha}_{\boldsymbol{n}-1}(\boldsymbol{s}, \boldsymbol{a})$ выдаёт результат «пойти дальше». «Остановиться» означает, что мы полагаем итоговым следующим состоянием пару $\boldsymbol{s}_{\boldsymbol{n}-1}^{\prime}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{n}-\mathbf{1}$. «Пойти дальше» означает, что верхняя карточка колоды удаляется - $\boldsymbol{n}$ всё равно уменьшается на единицу, - и процедура повторяется уже для тройки $\boldsymbol{s}, \boldsymbol{n}-\mathbf{1}, \boldsymbol{a}$ : мы снова подбрасываем монетку и так далее, пока не выпадет «остановиться» или колода карт не закончится.
- Награда для тройки $\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}$ есть $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$, если $\boldsymbol{n}>\mathbf{0}$, и $\boldsymbol{Q}_{0}(\boldsymbol{s}, \boldsymbol{a})$ иначе.

Данное определение внезапно содержит в себе всю основную идею доказательства. Что тут происходит? Давайте посмотрим на первые $\boldsymbol{n}$ шагов результаты работы Q-learning-a. Попробуем построить MDP, которое было бы в некотором смысле «похожее» на исходное MDP, но использующее только собранную историю (т.к. доступа к $\boldsymbol{p}\left(\boldsymbol{s}^{\prime}\right.$ i $\boldsymbol{s}, \boldsymbol{a})$ у нас нет). Если $\boldsymbol{n}=\mathbf{0}$, и истории нет, то мы считаем, что мы бы получили $\boldsymbol{Q}_{0}(\boldsymbol{s}, \boldsymbol{a})$ в качестве награды за всю оставшуюся игру - такого наше «исходное» приближение MDP. Для больших $\boldsymbol{n}$ для данной пары состояние-действие мы в качестве следующего состояния хотели бы засэмплировать $\boldsymbol{s}^{\prime} \sim \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, но вместо этого у нас есть лишь коллекция $\boldsymbol{s}_{\boldsymbol{k}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})$. Давайте с вероятностью $\boldsymbol{\alpha}_{\boldsymbol{n}-1}(\boldsymbol{s}, \boldsymbol{a})$ возьмём в качестве сэмпла $\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})$, с вероятностью $\left(1-\boldsymbol{\alpha}_{\boldsymbol{n}-1}(\boldsymbol{s}, \boldsymbol{a})\right) \boldsymbol{\alpha}_{\boldsymbol{n}-2}(\boldsymbol{s}, \boldsymbol{a})$ возьмём в качестве сэмпла $\boldsymbol{s}_{\boldsymbol{n}-1}^{\prime}(\boldsymbol{s}, \boldsymbol{a})$, и так далее. Мы знаем, что при стремлении $\boldsymbol{n}$ к бесконечности альфы, во-первых, уходят к нулю (это гарантирует сходимость ряда из квадратов альф), а во-вторых,


сэмплов будет бесконечно много (бесконечно много альф обязано быть ненулевыми из-за расходимости ряда из альф). Однако после каждого шага в ARP, $\boldsymbol{n}$ уменьшается (как минимум на единицу), и через некоторое число шагов такая игра гарантированно завершится - вся история из первых $\boldsymbol{n}$ шагов будет «проиграна» как на повторе от $\boldsymbol{n}$-го шага до первого. Hence the name.

---

# A.3.2. Ключевые свойства ARP 

Принципиально по построению выполнен такой фокус:
Теорема 102: В любом ARP оптимальная Q-функция $\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a})$ в точности равна

$$
\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a})=\boldsymbol{Q}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})
$$

Доказательство. По индукции. Для $\boldsymbol{n}=\mathbf{0}$ по определению следующее состояние всегда будет терминальным, а наградой для $\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}$ является $\boldsymbol{Q}_{\mathbf{0}}(\boldsymbol{s}, \boldsymbol{a})$. Значит, $\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \mathbf{0}, \boldsymbol{a})=\boldsymbol{Q}_{\mathbf{0}}(\boldsymbol{s}, \boldsymbol{a})$.

Пусть выполнено $\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a})=\boldsymbol{Q}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$ для любых $\boldsymbol{s}, \boldsymbol{a}$. Тогда для любых $\boldsymbol{s}, \boldsymbol{a}$ величина $\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \boldsymbol{n}+$ $+\mathbf{1}, \boldsymbol{a})$ равна следующему: с вероятностью $\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$ после выполнения действия $\boldsymbol{a}$ в состоянии $\boldsymbol{s}, \boldsymbol{n}+\mathbf{1}$ будет получена награда $\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})$, а следующим состоянием будет $\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{n}$; тогда дальнейшее оптимальное поведение даст награду $\max _{\boldsymbol{a}^{\prime}} \boldsymbol{Q}_{\mathrm{ARP}}^{*}\left(\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{n}, \boldsymbol{a}^{\prime}\right)=\max _{\boldsymbol{a}^{\prime}} \boldsymbol{Q}_{\boldsymbol{n}}\left(\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{a}^{\prime}\right)$ по предположению индукции. А с вероятностью $\mathbf{1}-\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$ мы не остановимся на $\boldsymbol{n}$ и повторим бросок монетки для $\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}$, после которого при дальнейшем оптимальном поведении мы получаем награду $\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a})=\boldsymbol{Q}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$. Собирая это вместе, получаем:

$$
\boldsymbol{Q}_{\mathrm{ARP}}^{*}(\boldsymbol{s}, \boldsymbol{n}+\mathbf{1}, \boldsymbol{a})=\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})\left(\boldsymbol{r}(\boldsymbol{s}, \boldsymbol{a})+\gamma \max _{\boldsymbol{a}^{\prime}} \boldsymbol{Q}_{\boldsymbol{n}}\left(\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a}), \boldsymbol{a}^{\prime}\right)\right)+\left(\mathbf{1}-\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})\right) \boldsymbol{Q}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})
$$

Справа в точности стоит $\boldsymbol{Q}_{\boldsymbol{n}+\mathbf{1}}(\boldsymbol{s}, \boldsymbol{a})$ ! Иначе говоря, функция переходов в ARP специально построена так, что оценочные функции удовлетворяют формулам обновления Q-learning-a.

Доказательство сходимости Q-learning-a идейно сводится к тому, что при стремлении $\boldsymbol{n}$ к бесконечности ARP с начальным состоянием $\boldsymbol{s}_{\mathbf{0}}, \boldsymbol{n}$ (и коэф. дисконтирования $\boldsymbol{\gamma}$ ) становится всё больше похож на исходное, настоящее MDP.

Покажем, что в ARP неявно содержится информация о $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Рассмотрим следующую величину:

$$
\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)=\sum_{\boldsymbol{n}^{\prime}=1}^{n-1} \boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime}, \boldsymbol{n}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)
$$

то есть вероятность после выбора действия $\boldsymbol{a}$ из состояния $\boldsymbol{s}, \boldsymbol{n}$ оказаться в $\boldsymbol{s}^{\prime}$, если неважно, сколько карточек $\boldsymbol{n}^{\prime}$ у нас останется после одного шага.

## Теорема 103:

$$
\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right) \xrightarrow{\boldsymbol{n} \rightarrow+\infty} \boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)
$$

Доказательство. Рассмотрим $\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}+\mathbf{1}, \boldsymbol{a}\right)$ - вероятность оказаться в $\boldsymbol{s}^{\prime}$ после выполнения $\boldsymbol{a}$ из состояния $\boldsymbol{s}, \boldsymbol{n}+\mathbf{1}$ вне зависимости от $\boldsymbol{n}^{\prime}$. С вероятностью $\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$ исходом будет $\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})$ : если оно равно рассматриваемому $\boldsymbol{s}^{\prime}$, то это даёт $\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$ вероятность для $\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}+\mathbf{1}, \boldsymbol{a}\right)$, иначе 0 ; с вероятностью $\mathbf{1}-\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})$ процесс генерации следующего состояния будет повторён из $\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}$, и тогда вероятность оказаться в состоянии $\boldsymbol{s}^{\prime}$ равна $\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)$ по определению. Итого получаем:

$$
\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}+\mathbf{1}, \boldsymbol{a}\right)=\left(\mathbf{1}-\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a})\right) \boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)+\boldsymbol{\alpha}_{\boldsymbol{n}}(\boldsymbol{s}, \boldsymbol{a}) \mathbb{I}\left[\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{s}^{\prime}\right]
$$

Мы получили формулу экспоненциального сглаживания (3.27) для величин $\mathbb{I}\left[\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{s}^{\prime}\right]$. При этом альфы удовлетворяют условиям сходимости (3.26)! Значит, по теореме о сходимости экспоненциального сглаживания 27 эти величины в пределе сходятся к мат.ожиданию случайной величины $\mathbb{I}\left[\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{s}^{\prime}\right]$. Поскольку $\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})$ для любого $\boldsymbol{n}$ генерировался из $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$, то

$$
\mathbb{E} \mathbb{I}\left[\boldsymbol{s}_{\boldsymbol{n}}^{\prime}(\boldsymbol{s}, \boldsymbol{a})=\boldsymbol{s}^{\prime}\right]=\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)
$$

и, следовательно, именно к нему стремится $\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)$.
Мы показали, что процесс генерации $\boldsymbol{s}^{\prime}$ в ARP «корректно» имитирует реальную $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$ при большом числе карточек. Значит, наше ARP с большим числом карточек всё больше похоже на настоящее MDP. Коли так, то и наверняка и оптимальная Q-функция для ARP при стремлении $\boldsymbol{n}$ к бесконечности всё больше похожа на $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ исходного MDP:

$$
\lim _{n \rightarrow+\infty} Q_{\mathrm{ARP}}^{*}(s, n, a)=Q^{*}(s, a)
$$

Если это так, то в совокупности с (А.19) мы получаем доказаваемое: для любой реализации Q-learning-a

$$
\lim _{n \rightarrow+\infty} Q_{n}(s, a)=\lim _{n \rightarrow+\infty} Q_{\mathrm{ARP}}^{*}(s, n, a)=Q^{*}(s, a)
$$

---

# A.3.3. Схожесть ARP и настоящего MDP 

Нам осталось формально показать, почему «похожесть MDP» влечёт похожесть оптимальных Q-функций. Техническим препятствием для этого является то, что ARP на каждом шаге «тратит карточки» - мы, идя с конца колоды к началу, тераем какое-то случайное число карточек, а, оставшись с маленьким числом карточек, уже не умеем «хорошо имитировать» настоящую функцию переходов.

Следующее утверждение является вспомогательным для основной теоремы: оно говорит, что можно запастись достаточным количеством карточек, чтобы можно было сделать шаг и остаться всё равно со сколь угодно большим числом карточек.

Теорема 104: Для любого ARP и любого целого числа карточек $\boldsymbol{m}$ можно выбрать число карточек $\boldsymbol{n}$ так, что для всех $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$ вероятность $\boldsymbol{p}_{\text {ARP }}\left(\boldsymbol{n}^{\prime}<\boldsymbol{m} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}, \boldsymbol{s}^{\prime}\right)$ бесконечна мала.

Доказательство. Вероятность оказаться с числом карточек меньше $\boldsymbol{m}$, стартуя из уровня $\boldsymbol{n}$, не меньше чем $\prod_{\boldsymbol{i}=\boldsymbol{m}}^{\boldsymbol{n}}\left(\mathbf{1}-\boldsymbol{\alpha}_{\boldsymbol{i}}(\boldsymbol{s}, \boldsymbol{a})\right)$ - это вероятность в принципе прокрутить историю от $\boldsymbol{n}$-й карточки до $\boldsymbol{m}$-й. Воспользуемся следующим фактом: при любых $\boldsymbol{\alpha} \in[0,1]$ верно

$$
1-\alpha \leq \exp (-\alpha)
$$

Подставляя это неравенство в произведение, получаем:

$$
\prod_{i=m}^{n}\left(1-\alpha_{i}(s, a)\right) \leq \prod_{i=m}^{n} \exp \left(-\alpha_{i}(s, a)\right)=\exp \left(-\sum_{i=m}^{n} \alpha_{i}(s, a)\right) \xrightarrow{n \rightarrow+\infty} \exp (-\infty)=0
$$

В последнем переходе мы воспользовались тем, что ряд альф расходится, а значит и любой его хвост, начинающийся с любого конечного $\boldsymbol{m}$, тоже расходится.

Теперь обсудим идею основного доказательства о том, что $\boldsymbol{Q}_{\boldsymbol{A R P}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}) \xrightarrow{\boldsymbol{n} \rightarrow+\infty} \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$. Если мы в ARP сидим в состоянии с большим $\boldsymbol{n}$, то у нас есть три причины, по которым $\boldsymbol{Q}_{\boldsymbol{A R P}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a})$ отличается от $\boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})$ :

- с некоторой маленькой вероятностью мы после нескольких первых шагов окажемся в ARP в состоянии с маленьким значением $\boldsymbol{n}$, где все наши приближения уже не работают. Мы выберем $\boldsymbol{n}$ достаточно большим, чтобы эта вероятность была очень маленькой.
- наше приближение функции переходов $\boldsymbol{p}_{\text {ARP }}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)$ при больших $\boldsymbol{n}$ близка, но не точно совпадает с истинной $\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)$. Мы будем пользоваться тем, что как истинная оценочная функция, так и наша оценочная функция ограничены: $\boldsymbol{Q}_{\boldsymbol{A R P}}^{*}(\boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a})<\boldsymbol{C}, \boldsymbol{Q}^{*}(\boldsymbol{s}, \boldsymbol{a})<\boldsymbol{C}$ для некоторой константы $\boldsymbol{C}$ (это следует из наших стандартных требований регулярности к MDP, которое справедливы и для ARP), поэтому достаточно выбрать $\boldsymbol{n}$ достаточно большим, чтобы сделать эту ошибку маленькой.
- наконец, основная ошибка заключается в том, что мы принимаем решения последовательно, а значит, ошибка из-за погрешности функции переходов будет накапливаться. Очень условно, это «ошибка внутри нашей аппроксимации Q-функции». С ней мы будем бороться наиболее хитрым образом: мы рассмотрим ошибку в награде, собираемой на протяжении первых $\boldsymbol{k}$ шагов. Остальная часть этой ошибки будет домножаться на $\boldsymbol{\gamma}^{\boldsymbol{k}}$, следовательно, можно будет выбрать $\boldsymbol{k}$ достаточно большим, чтобы ошибка была меньше наперёд заданного малого числа $\boldsymbol{\epsilon}>\mathbf{0}$.

Введём следующее обозначение: «максимальная ошибка, если у нас на руках $\boldsymbol{n}$ карточек»:

$$
\nu(n):=\max _{s, a}\left|Q_{\mathrm{ARP}}^{*}(s, n, a)-Q^{*}(s, a)\right|
$$

Надо доказать, что она стремится к нулю. Можно считать, что и $\boldsymbol{\nu}(\boldsymbol{n})$ ограничено в силу ограниченности оценочных функций, чем мы будем пользоваться, когда карточек остаётся мало.

Теорема 105: Пусть $\boldsymbol{n}$ и $\boldsymbol{m}$ таковы, что для любых $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$ выполнено

$$
\begin{gathered}
\left|\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}\right)-\boldsymbol{p}\left(\boldsymbol{s}^{\prime} \mid \boldsymbol{s}, \boldsymbol{a}\right)\right|<\boldsymbol{\epsilon} \\
\boldsymbol{p}_{\mathrm{ARP}}\left(\boldsymbol{n}^{\prime}<\boldsymbol{m} \mid \boldsymbol{s}, \boldsymbol{n}, \boldsymbol{a}, \boldsymbol{s}^{\prime}\right)<\boldsymbol{\epsilon}
\end{gathered}
$$

для данного $\boldsymbol{\epsilon}$. Тогда для некоторой константы $\boldsymbol{C}$ справедливо следующее рекуррентное соотношение:

$$
\nu(n) \leq \gamma \max _{n^{\prime} \geq m} \nu\left(n^{\prime}\right)+C \epsilon
$$

Доказательство.

$$
\nu(n)=\max _{s, a}\left|Q_{\mathrm{ARP}}^{*}(s, n, a)-Q^{*}(s, a)\right|=
$$

---

$=\{$ уравнение оптимальности Беллмана (3.17); слагаемые с наградой сокращаются $\}=$

$$
\begin{aligned}
= & \gamma \max _{s, a}\left|\sum_{s^{\prime}, n^{\prime}} p_{\mathrm{ARP}}\left(s^{\prime}, n^{\prime} \mid s, n, a\right) \max _{a^{\prime}} Q_{\mathrm{ARP}}^{*}\left(s^{\prime}, n^{\prime}, a^{\prime}\right)-\sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right|= \\
= & \left\{\text { добавляем и вычитаем } \sum_{s^{\prime}, n^{\prime}} p_{\mathrm{ARP}}\left(s^{\prime}, n^{\prime} \mid s, n, a\right) \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right\}= \\
= & \gamma \max _{s, a}\left|\sum_{s^{\prime}, n^{\prime}} p_{\mathrm{ARP}}\left(s^{\prime}, n^{\prime} \mid s, n, a\right)\left(\max _{a^{\prime}} Q_{\mathrm{ARP}}^{*}\left(s^{\prime}, n^{\prime}, a^{\prime}\right)-\max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right)-\right. \\
& \left.+\sum_{s^{\prime}}\left(p_{\mathrm{ARP}}\left(s^{\prime} \mid s, n, a\right)-p\left(s^{\prime} \mid s, a\right)\right) \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \right\rvert\, \leq \\
\leq & \left\{\text { используем свойство максимумов (3.24) } \max _{x} f(x)-\max _{x} g(x) \leq \max _{x}|f(x)-g(x)|\right\} \leq \\
\leq & \gamma \max _{s, a}\left|\sum_{s^{\prime}, n^{\prime}} p_{\mathrm{ARP}}\left(s^{\prime}, n^{\prime} \mid s, n, a\right) \max _{a^{\prime}}\left|Q_{\mathrm{ARP}}^{*}\left(s^{\prime}, n^{\prime}, a^{\prime}\right)-Q^{*}\left(s^{\prime}, a^{\prime}\right)\right|+\right. \\
& \left.+\sum_{s^{\prime}}\left|p_{\mathrm{ARP}}\left(s^{\prime} \mid s, n, a\right)-p\left(s^{\prime} \mid s, a\right)\right| \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right]= \\
= & \{\text { правило произведения }\}= \\
= & \gamma \max _{s, a}\left|\sum_{s^{\prime}} p_{\mathrm{ARP}}\left(s^{\prime} \mid s, n, a\right) \sum_{n^{\prime}} p_{\mathrm{ARP}}\left(n^{\prime} \mid s, n, a, s^{\prime}\right) \max _{a^{\prime}}\left|Q_{\mathrm{ARP}}^{*}\left(s^{\prime}, n^{\prime}, a^{\prime}\right)-Q^{*}\left(s^{\prime}, a^{\prime}\right)\right|+\right. \\
& \left.+\sum_{s^{\prime}}\left|p_{\mathrm{ARP}}\left(s^{\prime} \mid s, n, a\right)-p\left(s^{\prime} \mid s, a\right)\right| \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right] \leq \\
\leq & \left\{\text { определение } \nu(n) \text { и свойство } \mathbb{E} f(x) \leq \max _{x} f(x)\right\} \leq \\
\leq & \gamma \max _{s, a}\left|\sum_{n^{\prime}} p_{\mathrm{ARP}}\left(n^{\prime} \mid s, n, a, s^{\prime}\right) \nu\left(n^{\prime}\right)+\right. \\
& \left.+\sum_{s^{\prime}}\left|p_{\mathrm{ARP}}\left(s^{\prime} \mid s, n, a\right)-p\left(s^{\prime} \mid s, a\right)\right| \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right] \leq
\end{aligned}
$$

$\leq$ \{ошибка аппроксимации функции переходов и ограниченность $Q^{*}(s, a)\} \leq$
$\leq \gamma \max _{s, a}\left[\sum_{n^{\prime}} p_{\mathrm{ARP}}\left(n^{\prime} \mid s, n, a, s^{\prime}\right) \nu\left(n^{\prime}\right)+C \epsilon\right]=$
$=\{$ рассматриваем два случая: $n^{\prime}<m$ и $n^{\prime} \geq m\}=$
$\leq \gamma \max _{s, a}\left[\sum_{n^{\prime} \geq m} p_{\mathrm{ARP}}\left(n^{\prime} \mid s, n, a, s^{\prime}\right) \nu\left(n^{\prime}\right)+\sum_{n^{\prime}<m} p_{\mathrm{ARP}}\left(n^{\prime} \mid s, n, a, s^{\prime}\right) v\left(n^{\prime}\right)+C \epsilon\right] \leq$
$\leq$ \{пользуемся условием теоремы и тем, что $\nu(n)$ ограничено $\} \leq$
$\leq \gamma \max _{s, a}\left[\sum_{n^{\prime} \geq m} p_{\mathrm{ARP}}\left(n^{\prime} \mid s, n, a, s^{\prime}\right) \nu\left(n^{\prime}\right)+C^{\prime} \epsilon\right] \leq$
$\leq$ \{свойство $\mathbb{E} f(x) \leq \max _{x} f(x)\} \leq$
$\leq \gamma \max _{n^{\prime} \geq m} \nu\left(n^{\prime}\right)+C^{\prime} \epsilon$

# Теорема 106: 

$$
\nu(n) \xrightarrow{n \rightarrow+\infty} 0
$$

Доказательство. Пусть дано $\epsilon>0$. Покажем, что начиная с некоторого номера, $\nu(n)<2 C \epsilon$. Для этого выберем целое $\boldsymbol{k}$ так, чтобы $\gamma^{\boldsymbol{k}}<\boldsymbol{\epsilon}$, и применим предыдущую теорему $\boldsymbol{k}$ раз следующим образом. Выберем какое-нибудь $\boldsymbol{m}_{\mathbf{0}}$ и подберём $\boldsymbol{m}_{\mathbf{1}}$ так, чтобы для всех $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$

$$
p_{\mathrm{ARP}}\left(n^{\prime}<m_{0} \mid s, m_{1}, a, s^{\prime}\right)<\frac{\epsilon}{k}
$$

Убедимся, что $\boldsymbol{m}_{\mathbf{1}}$ достаточно большое, что $\left|\boldsymbol{p}_{\mathrm{ARP}}\left(s^{\prime} \mid s, m_{1}, a\right)-p\left(s^{\prime} \mid s, a\right)\right|<\frac{\epsilon}{k}$ (если нет, то заменим на достаточно большое). Затем подберём $\boldsymbol{m}_{\mathbf{2}}$ так, что для всех $\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{s}^{\prime}$

$$
p_{\mathrm{ARP}}\left(n^{\prime}<m_{1} \mid s, m_{2}, a, s^{\prime}\right)<\frac{\epsilon}{k}
$$

и так далее вплоть до $\boldsymbol{m}_{\boldsymbol{k}}$.

---

Тогда для всех $n>m_{k}$ :

$$
\nu(n) \leq \gamma \max _{n^{\prime} \geq m_{k-1}} \nu\left(n^{\prime}\right)+C \frac{\epsilon}{k}
$$

Аналогично, для всех $n>m_{k-1}$ :

$$
\nu(n) \leq \gamma \max _{n^{\prime} \geq m_{k-2}} \nu\left(n^{\prime}\right)+C \frac{\epsilon}{k}
$$

Последовательно раскручивая эту цепочку $\boldsymbol{k}$ раз получим, что для всех $\boldsymbol{n}>\boldsymbol{m}_{\boldsymbol{k}}$ :

$$
\begin{aligned}
\nu(n) & \leq \gamma \max _{n^{\prime} \geq m_{k-1}} \nu\left(n^{\prime}\right)+C \frac{\epsilon}{k} \leq \gamma^{2} \max _{n^{\prime} \geq m_{k-2}} \nu\left(n^{\prime}\right)+2 C \frac{\epsilon}{k} \leq \cdots \leq \\
& \leq \gamma^{k} \max _{n^{\prime} \geq m_{0}} \nu\left(n^{\prime}\right)+k C \frac{\epsilon}{k} \leq C \epsilon+C \epsilon=2 C \epsilon
\end{aligned}
$$

Вот такие дела.

---

# Материалы 

Большая часть материалов взята из основных курсов по обучению с подкреплением:
Курс Сергея Левина;
Kypc Practical RL;
Цикл докладов Advanced RL;
Курс Дэвида Сильвера;
Kypc DeepMind;
Kypc GeorgiaTech;
В главе 2 большинство материала взято из книги [Luke, 2013]. Хороший обзор эволюционных стратегий можно найти в блоге Lil'log [Weng, 2019]. Алгоритм NEAT предложен в [Stanley and Miikkulainen, 2002]. Алгоритм WANN развил его идею в [Gaier and Ha, 2019]. Кросс-энтропийный метод как метод оптимизации и метод вычисления вероятности маловероятных событий предложен в [Botev et al., 2013]; его применение к задаче RL обычно связывают с [Szita and Lörincz, 2006], где его применили к тетрису. OpenAI-ES описана в [Salimans et al., 2017]; упомянутый алгоритм ARS, действующий примерно также, предложен в [Mania et al., 2018]. Идея адаптировать ковариационную матрицу в эволюционных стратегиях восходит корнями к [Hansen and Ostermeier, 1996]; полный технический обзор всего набора эвристик, использующихся как алгоритм CMA-ES, можно прочитать здесь [Hansen, 2016]. Доказательство того, что адаптация матрицы ковариации по сути является натуральным градиентным спуском, было независимо получено в [Akimoto et al., 2010] и [Glasmachers et al., 2010].

Больше информации по главе 3 и более подробную библиографию можно получить в классической книге Саттона-Барто [Sutton and Barto, 2018]; отмечу только некоторые дополнительные ссылки. Лемма RPI была представлена в [Kakade and Langford, 2002]. Алгоритм Q-learning, изначально придуманный в [Watkins, 1989], был придуман как эвристика, но позже авторам удалось доказать сходимость в [Watkins and Dayan, 1992]; это доказательство через ARP и приведено в приложении. Связь с теорией стохастической аппроксимации, начатой ещё в далёком 1951-ом году статьёй [Robbins and Monro, 1951], была обнаружена после этого в [Tsitsiklis, 1994], что позволило доказать более сильные утверждения вроде сходимости TD( $\mathbf{A})$. Наиболее общую форму алгоритмов off-policy оценивания стратегии и оценку Retrace представили в [Munos et al., 2016] уже в 2016-ом году.

Глава 4 основана на алгоритме DQN [Mnih et al., 2013], продемонстрировавшем потенциал совмещения глубокого обучения с классической теорией. Идея борьбы с переоценкой при помощи двух аппроксимаций Q-функций была предложена в [Hasselt, 2010] для табличного алгоритма. Twin («Clipped Double») оценка предложена была позже (в рамках алгоритма TD3) в [Fujimoto et al., 2018]. Double DQN предложен в [Van Hasselt et al., 2016]; Dueling DQN в [Wang et al., 2015]; Noisy Nets в [Fortunato et al., 2017]. Приоритизированный реплей был использован в DQN в [Schaul et al., 2015]; идею высчитывать приоритеты онлайн и добавлять в буфер уже «с правильным» приоритетом реализовали в алгоритме R2D2 [Horgan et al., 2018]. Эвристика многошагового DQN была описана в составе Rainbow [Hessel et al., 2018]. Distributional подход и алгоритм c51 был инициирован в [Bellemare et al., 2017]; идея перехода к квантильной аппроксимации и алгоритм QRDQN был описан в [Dabney et al., 2018b]; алгоритм IQN предложен в [Dabney et al., 2018a]. Эквивалентность distributional-алгоритмов с обычным подходом в табличном сеттииге была показана в [Lyle et al., 2019].

В главе 5 метод пробрасывания градиентов через стохастические узлы вычислительного графа REINFORCE и его применение к задаче RL были придуманы в [Williams, 1992]. Actor-Critic методы, в которых учится как стратегия, так и оценочная функция, позволяющая обучаться с неполных эпизодов, предложены в [Sutton et al., 2000]. Применение Policy Gradient подхода с нейросетевой аппроксимацией и алгоритм A2C предложен в [Mnih et al., 2016]. Список разных вариаций Policy Gradient алгоритмов можно найти в блоге Lil'log [Weng, 2018]. TRPO был предложен в [Schulman et al., 2015a]; обучение с длинных роллаутов привело к использованию GAE оценок, что было предложено в [Schulman et al., 2015b]. Алгоритм PPO описан в [Schulman et al., 2017], однако стандартные реализации, добившиеся высоких результатов на бенчмарках и способствовавшие распространению алгоритма, использовали дополнительные инженерные эвристики; на их существенное влияние на результаты обращено внимание в [Engstrom et al., 2019]. Позже в [Achiam et al., 2017] была представлена более точная нижняя оценка на погрешность суррогатной функции, давшая теоретическое обоснование использованию усреднённой по состояниям KL-дивергенции, которая и была представлена в тексте.

Применение идей policy gradient и value-based подхода для обучения нейросетей в задаче непрерывного управления, описанное в главе 6, предложено в [Lillicrap et al., 2015] в алгоритме DDPG. Более стабильная версия этого алгоритма TD3 описана в [Fujimoto et al., 2018]. Алгоритм Soft Q-learning, использующий теорию Maximum Entropy RL для обучения нейросетей, описан в [Haarnoja et al., 2017]; алгоритм Soft Actor-Critic, ставший практическим алгоритмом для работы с непрерывными действиями в рамках этого сеттинга, предложен в [Haarnoja et al., 2018].

---

За полным погружением в математику, стоящую за многорукими бандитами (глава 7.1), можно обратиться к книге [Lattimore and Szepesvári, 2020]. Нижняя оценка регрета (теорема Лаи-Роббинса) получена в [Lai and Robbins, 1985]. Асимптотическая оптимальность алгоритма UCB показана в [Auer et al., 2002]. Сама задача многоруких бандитов была впервые рассмотрена Томпсоном в [Thompson, 1933]; он же эвристически предложил сэмплирование Томпсона, которое позже тоже оказалось асимптотически оптимальным [Kaufmann et al., 2012]. Пример 104 с нахождением кратчайшего маршрута в графе взят из туториала по сэмплированию Томпсона [Russo et al., 2017]. Алгоритм, обучающий байесовские модели динамики и награды для табличных MDP и использующих сэмплирование Томпсона для разрешения дилеммы exploration-exploitation в MDP взят из [Osband et al., 2013].

Несмотря на то, что описанный в главе 7 model-based подход исследовался в RL всегда, применение моделей мира и концепция сновидений популяризовалась благодаря статье [Ha and Schmidhuber, 2018]. Победа алгоритма AlphaGo в го на основе совмещения MCTS и нейросетей в итоге была обобщена сначала в алгоритм AlphaZero [Silver et al., 2018], а затем и на случай неизвестной динамики в алгоритм $\boldsymbol{\mu}$-Zero [Schrittwieser et al., 2019]. Teория линейно-квадратичных регуляторов восходит ещё к оптимальному управлению; по LQR обычно ссылаются на [Bemporad et al., 2002], а по расширению iLQR - на [Li and Todorov, 2004].

Фреймворк Maximum Entropy Inverse RL, рассмотренный в главе 8.1, был предложен в [Ziebart et al., 2008]. Обобщение алгоритма из этой статьи с табличного случая на произвольный в виде процедуры Guided Cost Learning описана в [Finn et al., 2016]. Связь задачи с минимизацией расстояния между оссирансу measure была исследована в [Ho and Ermon, 2016], где был предложен алгоритм GAIL. Расширение GAIL на случай, когда в записях эксперта доступны только наблюдения (алгоритм GAIЮ) представлен в [Torabi et al., 2018]. Пример 115 со сведением к классификации задачи обучения квадрокоптера полётам по лесу описан в статье [Giusti et al., 2015].

Моделирование любопытства и скуки, описанное в главе 8.2 у агентов было описано ещё Шмидхубером в далёком 1991 году [Schmidhuber, 1991] вместе с проблемой шумных телевизоров. Эвристика RND придумана в [Burda et al., 2018]; фильтрующие свойства модели обратной динамики, понятие контролируемого состояния и алгоритм ICM описаны в [Pathak et al., 2017]. Пример минимизации хаоса 120 основан на [Berseth et al., 2019].

В главе 8.3 переразметка траекторий произвольными целями, также называемая алгоритмом IntentionalUnintentional, описана в [Cabi et al., 2017]. Идея HER придумана в [Andrychowicz et al., 2017]. Обобщение идеи для переразметки произвольных траекторий и связь этой задачи с обратным обучением с подкреплением одновременно замечена в [Eysenbach et al., 2020] и [Li et al., 2020]. Мета-контроллеры для автоматического подбора гиперпараметров использовались в алгоритме Agent57, обошедшем человека сразу во всех 57 играх Atari [Badia et al., 2020].

В иерархическом RL, описанном в главе 8.4, алгоритм Option-Critic и формулы градиентов для обучения политики терминальности представлены в [Bacon et al., 2017]. Концепция феодализма и феодальные сети FuN предложены в [Vezhnevets et al., 2017]. Обучение похожей иерархической схемы в off-policy при помощи переразметки в виде алгоритма HIRO описано в [Nachum et al., 2018].

Задача обучения в условиях частичной наблюдаемости главы 8.5 поставлена в [Smallwood and Sondik, 1973]. Обучение рекуррентных сетей в RL в рамках алгоритма DRQN описано в [Hausknecht and Stone, 2015]; эвристики разогрева и хранения скрытого состояния предложены в алгоритме R2D2 [Horgan et al., 2018]. Эпизодичная память NEC предложена в [Pritzel et al., 2017].

В главе 8.6 Adversarial-атаки на алгоритмы, обученные в режиме self-play, продемонстрированы в [Gleave et al., 2019]. Алгоритм QMix для кооперативных игр и идея моделировать смешивающую сеть в классе монотонных функций предложена в [Rashid et al., 2018]. Общий алгоритм MADDPG и идея моделирования других агентов предложены в [Lowe et al., 2017]. Идея моделировать и учить протоколы коммуникации между агентами описаны в [Foerster et al., 2016].

Изображения взяты из рассмотренных статей, книги [Sutton and Barto, 2018] и из pacпространённых сред (OpenAI Gym [Brockman et al., 2016], Mario [Kauten, 2018], Unity ML Agents [Juliani et al., 2018]). Кастомные изображения для примеров и схем были нарисованы в draw.io; изображение белки на них взято из вот этого твита; судя по всему, это незаюзанный концепт-арт для некой игры «Трагедия белок»... а вот, пригодился!


---

# Литература 

[Achiam et al., 2017] Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization. In International Conference on Machine Learning, pages 22-31. PMLR.
[Akimoto et al., 2010] Akimoto, Y., Nagata, Y., Ono, I., and Kobayashi, S. (2010). Bidirectional relation between cma evolution strategies and natural evolution strategies. In International Conference on Parallel Problem Solving from Nature, pages 154-163. Springer.
[Andrychowicz et al., 2017] Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and Zaremba, W. (2017). Hindsight experience replay. In Advances in neural information processing systems, pages 5048-5058.
[Auer et al., 2002] Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235-256.
[Bacon et al., 2017] Bacon, P.-L., Harb, J., and Precup, D. (2017). The option-critic architecture. In Thirty-First AAAI Conference on Artificial Intelligence.
[Badia et al., 2020] Badia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, D., and Blundell, C. (2020). Agent57: Outperforming the atari human benchmark. arXiv preprint arXiv:2003.13350.
[Bellemare et al., 2017] Bellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 449-458. JMLR. org.
[Bemporad et al., 2002] Bemporad, A., Morari, M., Dua, V., and Pistikopoulos, E. N. (2002). The explicit linear quadratic regulator for constrained systems. Automatica, 38(1):3-20.
[Berseth et al., 2019] Berseth, G., Geng, D., Devin, C., Finn, C., Jayaraman, D., and Levine, S. (2019). Smirl: Surprise minimizing rl in dynamic environments. arXiv preprint arXiv:1912.05510.
[Botev et al., 2013] Botev, Z. I., Kroese, D. P., Rubinstein, R. Y., and L'Ecuyer, P. (2013). The cross-entropy method for optimization. In Handbook of statistics, volume 31, pages 35-59. Elsevier.
[Brockman et al., 2016] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.
[Burda et al., 2018] Burda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.
[Cabi et al., 2017] Cabi, S., Colmenarejo, S. G., Hoffman, M. W., Denil, M., Wang, Z., and De Freitas, N. (2017). The intentional unintentional agent: Learning to solve many continuous control tasks simultaneously. arXiv preprint arXiv:1707.03300.
[Dabney et al., 2018a] Dabney, W., Ostrovski, G., Silver, D., and Munos, R. (2018a). Implicit quantile networks for distributional reinforcement learning. arXiv preprint arXiv:1806.06923.
[Dabney et al., 2018b] Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2018b). Distributional reinforcement learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence.
[Engstrom et al., 2019] Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., and Madry, A. (2019). Implementation matters in deep rl: A case study on ppo and trpo. In International Conference on Learning Representations.
[Eysenbach et al., 2020] Eysenbach, B., Geng, X., Levine, S., and Salakhutdinov, R. (2020). Rewriting history with inverse rl: Hindsight inference for policy improvement. arXiv preprint arXiv:2002.11089.
[Finn et al., 2016] Finn, C., Levine, S., and Abbeel, P. (2016). Guided cost learning: Deep inverse optimal control via policy optimization. In International conference on machine learning, pages 49-58.
[Foerster et al., 2016] Foerster, J., Assael, I. A., De Freitas, N., and Whiteson, S. (2016). Learning to communicate with deep multi-agent reinforcement learning. In Advances in neural information processing systems, pages 21372145 .
[Fortunato et al., 2017] Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295.

---

[Fujimoto et al., 2018] Fujimoto, S., Van Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477.
[Gaier and Ha, 2019] Gaier, A. and Ha, D. (2019). Weight agnostic neural networks. In Advances in Neural Information Processing Systems, pages 5364-5378.
[Giusti et al., 2015] Giusti, A., Guzzi, J., Cireşan, D. C., He, F.-L., Rodríguez, J. P., Fontana, F., Faessler, M., Forster, C., Schmidhuber, J., Di Caro, G., et al. (2015). A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters, 1(2):661-667.
[Glasmachers et al., 2010] Glasmachers, T., Schaul, T., Yi, S., Wierstra, D., and Schmidhuber, J. (2010). Exponential natural evolution strategies. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, pages $393-400$.
[Gleave et al., 2019] Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S., and Russell, S. (2019). Adversarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615.
[Ha and Schmidhuber, 2018] Ha, D. and Schmidhuber, J. (2018). World models. arXiv preprint arXiv:1803.10122.
[Haarnoja et al., 2017] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165.
[Haarnoja et al., 2018] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.
[Hansen, 2016] Hansen, N. (2016). The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772.
[Hansen and Ostermeier, 1996] Hansen, N. and Ostermeier, A. (1996). Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation. In Proceedings of IEEE international conference on evolutionary computation, pages 312-317. IEEE.
[Hasselt, 2010] Hasselt, H. V. (2010). Double q-learning. In Advances in neural information processing systems, pages $2613-2621$.
[Hausknecht and Stone, 2015] Hausknecht, M. and Stone, P. (2015). Deep recurrent q-learning for partially observable mdps. arXiv preprint arXiv:1507.06527.
[Hessel et al., 2018] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence.
[Ho and Ermon, 2016] Ho, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Advances in neural information processing systems, pages 4565-4573.
[Horgan et al., 2018] Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H., and Silver, D. (2018). Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933.
[Juliani et al., 2018] Juliani, A., Berges, V.-P., Vckay, E., Gao, Y., Henry, H., Mattar, M., and Lange, D. (2018). Unity: A general platform for intelligent agents. arXiv preprint arXiv:1809.02627.
[Kakade and Langford, 2002] Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In ICML, volume 2, pages $267-274$.
[Kaufmann et al., 2012] Kaufmann, E., Korda, N., and Munos, R. (2012). Thompson sampling: An asymptotically optimal finite-time analysis. In International conference on algorithmic learning theory, pages 199-213. Springer.
[Kauten, 2018] Kauten, C. (2018). Super Mario Bros for OpenAI Gym. GitHub.
[Lai and Robbins, 1985] Lai, T. L. and Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, $6(1): 4-22$.
[Lattimore and Szepesvári, 2020] Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press.
[Li et al., 2020] Li, A. C., Pinto, L., and Abbeel, P. (2020). Generalized hindsight for reinforcement learning. arXiv preprint arXiv:2002.11708.
[Li and Todorov, 2004] Li, W. and Todorov, E. (2004). Iterative linear quadratic regulator design for nonlinear biological movement systems. In ICINCO (1), pages 222-229.

---

[Lillicrap et al., 2015] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.
[Lowe et al., 2017] Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. (2017). Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in neural information processing systems, pages $6379-6390$.
[Luke, 2013] Luke, S. (2013). Essentials of metaheuristics, volume 2. Lulu Raleigh.
[Lyle et al., 2019] Lyle, C., Bellemare, M. G., and Castro, P. S. (2019). A comparative analysis of expected and distributional reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages $4504-4511$.
[Mania et al., 2018] Mania, H., Guy, A., and Recht, B. (2018). Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055.
[Mnih et al., 2016] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937.
[Mnih et al., 2013] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.
[Munos et al., 2016] Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and efficient off-policy reinforcement learning. arXiv preprint arXiv:1606.02647.
[Nachum et al., 2018] Nachum, O., Gu, S. S., Lee, H., and Levine, S. (2018). Data-efficient hierarchical reinforcement learning. In Advances in neural information processing systems, pages 3303-3313.
[Osband et al., 2013] Osband, I., Russo, D., and Van Roy, B. (2013). (more) efficient reinforcement learning via posterior sampling. arXiv preprint arXiv:1306.0940.
[Pathak et al., 2017] Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages $16-17$.
[Pritzel et al., 2017] Pritzel, A., Uria, B., Srinivasan, S., Puigdomenech, A., Vinyals, O., Hassabis, D., Wierstra, D., and Blundell, C. (2017). Neural episodic control. arXiv preprint arXiv:1703.01988.
[Rashid et al., 2018] Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. (2018). Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.11485.
[Robbins and Monro, 1951] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, pages $400-407$.
[Russo et al., 2017] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. (2017). A tutorial on thompson sampling. arXiv preprint arXiv:1707.02038.
[Salimans et al., 2017] Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864.
[Schaul et al., 2015] Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.
[Schmidhuber, 1991] Schmidhuber, J. (1991). A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages $222-227$.
[Schrittwieser et al., 2019] Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. (2019). Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265.
[Schulman et al., 2015a] Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and Moritz, P. (2015a). Trust region policy optimization. In Icml, volume 37, pages 1889-1897.
[Schulman et al., 2015b] Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.
[Schulman et al., 2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

---

[Silver et al., 2018] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. (2018). A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144.
[Smallwood and Sondik, 1973] Smallwood, R. D. and Sondik, E. J. (1973). The optimal control of partially observable markov processes over a finite horizon. Operations research, 21(5):1071-1088.
[Stanley and Miikkulainen, 2002] Stanley, K. O. and Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies. Evolutionary computation, 10(2):99-127.
[Sutton and Barto, 2018] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
[Sutton et al., 2000] Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages $1057-1063$.
[Szita and Lörincz, 2006] Szita, I. and Lörincz, A. (2006). Learning tetris using the noisy cross-entropy method. Neural computation, 18(12):2936-2941.
[Thompson, 1933] Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285-294.
[Torabi et al., 2018] Torabi, F., Warnell, G., and Stone, P. (2018). Generative adversarial imitation from observation. arXiv preprint arXiv:1807.06158.
[Tsitsiklis, 1994] Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and q-learning. Machine learning, $16(3): 185-202$.
[Van Hasselt et al., 2016] Van Hasselt, H., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double q-learning. In Thirtieth AAAI Conference on Artificial Intelligence.
[Vezhnevets et al., 2017] Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. (2017). Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161.
[Wang et al., 2015] Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., and De Freitas, N. (2015). Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581.
[Watkins and Dayan, 1992] Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279-292.
[Watkins, 1989] Watkins, C. J. C. H. (1989). Learning from delayed rewards.
[Weng, 2018] Weng, L. (2018). Policy gradient algorithms. lilianweng.github.io/lil-log.
[Weng, 2019] Weng, L. (2019). Evolution strategies. lilianweng.github.io/lil-log.
[Williams, 1992] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.
[Ziebart et al., 2008] Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. (2008). Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA.

---



---

