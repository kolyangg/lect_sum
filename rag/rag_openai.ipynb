{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:03:02.837274Z","iopub.execute_input":"2025-03-04T19:03:02.837483Z","iopub.status.idle":"2025-03-04T19:03:03.194358Z","shell.execute_reply.started":"2025-03-04T19:03:02.837463Z","shell.execute_reply":"2025-03-04T19:03:03.193439Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"keys = \"\"\"\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:03:03.195221Z","iopub.execute_input":"2025-03-04T19:03:03.195644Z","iopub.status.idle":"2025-03-04T19:03:03.199544Z","shell.execute_reply.started":"2025-03-04T19:03:03.195618Z","shell.execute_reply":"2025-03-04T19:03:03.198533Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"import requests\nimport time\n\ndef test_openai_key(key: str) -> bool:\n    headers = {\n        \"Authorization\": f\"Bearer {key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    try:\n        # Делаем тестовый запрос к API моделей\n        response = requests.get(\n            \"https://api.openai.com/v1/models\",\n            headers=headers,\n            timeout=10\n        )\n        \n        # Проверяем статус ответа\n        if response.status_code == 200:\n            return True\n            \n        # Логируем ошибки при необходимости\n        print(f\"Key {key[:10]}... failed: {response.status_code} {response.text[:100]}\")\n        return False\n        \n    except Exception as e:\n        print(f\"Key {key[:10]}... error: {str(e)}\")\n        return False\n\n# Разбиваем ключи на список\napi_keys = keys.split('\\n')\n\n# Предварительная фильтрация по формату\npre_validated = [key for key in api_keys if \n                key.startswith('sk-') and \n                len(key) == 51 and \n                key[3:].isalnum()]\n\n# Проверяем оставшиеся ключи через API\nvalid_keys = []\nfor idx, key in enumerate(pre_validated):\n    # Добавляем задержку между запросами\n    if idx > 0:\n        time.sleep(1)  # Избегаем rate limits\n    \n    if test_openai_key(key):\n        valid_keys.append(key)\n    print(f\"Checked {idx+1}/{len(pre_validated)} keys\", end='\\r')\n\n# Результаты\nprint(\"\\n\\nValidation complete!\")\nprint(f\"Total valid keys: {len(valid_keys)}\")\nprint(\"Valid keys list:\")\nfor key in valid_keys:\n    print(key)","metadata":{}},{"cell_type":"code","source":"## TODO ideas\n\n* текст лекции предобработать - убрать слова паразиты, поработать над структурированностью.\n* сохранить эбмединги учебника\n\n* попробать langchain","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\npdf_url = \"https://arxiv.org/pdf/2201.09746\"\n\n# Загрузите файл\nresponse = requests.get(pdf_url)\n\n# Сохраните в рабочую директорию Kaggle\nwith open(\"rl.pdf\", \"wb\") as f:\n    f.write(response.content)\n\nprint(\"PDF успешно сохранен как 'rl.pdf'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:03:03.200394Z","iopub.execute_input":"2025-03-04T19:03:03.200727Z","iopub.status.idle":"2025-03-04T19:03:05.275632Z","shell.execute_reply.started":"2025-03-04T19:03:03.200696Z","shell.execute_reply":"2025-03-04T19:03:05.274745Z"}},"outputs":[{"name":"stdout","text":"PDF успешно сохранен как 'rl.pdf'\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"l1 = \"\"\"\nпривет давайте еще минутку подождем пока у нас критическая масса наберется и приступим так я думаю можно начинать всем привет скажите пожалуйста слышно для меня да да слышно все отлично так давайте так я и демонстрирую кран и мы ступим Так, ещё раз всем привет. Давайте познакомимся. Меня зовут Сергей. Я буду вести курс по обучению с подкреплением. Сегодня у нас первое занятие. И давайте без личных промедлений приступим сначала к обзору на наш курс с точки зрения тех заданий, которые мы будем выполнять, с точки зрения количества лекций. А именно у нас будет 10 недель. Каждая неделя состоит из лекции и практического семинара, который буду вести я. Курс проходит по субботам. Начинается в 14.40, заканчивается где-то через 3 часа. Надеюсь, мы это время проведем с пользой, и я за него за эти 3 часа особо выходить не буду. В качестве домашних заданий у нас планируется пять домашек, две простые, одна из них появится уже сегодня. И там есть два дедлайна, мягкий и жесткий. Мягкий будет через неделю, жесткий будет через две недели. Правила я скажу, чем мягкий от жесткого отличается чуть позже. Соответственно, да, две домашки, две простые домашки, три сложные, в сумме пять домашек и будет составлять 65% от всей оценки курса. А потом есть такая еще активность, как презентация статьи. Раньше это была просто презентация, то есть мы выбирали статью на какую-то из тем, связанной с обучением и подкреплением, рассказывали ее. за этот рассказ, за его полноту, глубину и понимание получали какую-то оценку. В этом году я хочу сделать чуть более интереснее. По сути, вы возьмете какую-то из вам понравившихся статей и попытаетесь ее реплицировать, затем суммаризировать то, что у вас получилось. То есть можно воспринимать это как еще одну домашку, но более творческую. И перед каждым занятием у нас будут крошечные квизы по 10 вопросов по материалам предыдущего занятия, чтобы вы получали плотную награду на протяжении курса, могли каким-то образом проверять ваши знания и в целом быть готовыми к тому, что будет рассказываться на текущем занятии. Вот. Есть ли какие-то вопросы? Какие простые домашки будут отличаться от сложных? В наличии нейронки, если грубо. То есть первые две домашки, они по большей части алгоритмические. Три сложные домашки, они будут уже на реализацию каких-то алгоритмов с нейронками. Понадобится вам больше компьютера, соответственно, они будут более длительные, более сложные. Из-за них баллов, соответственно, больше можно получить. Домашки весят разное количество баллов? Да, домашки весят разное количество баллов. Первые две весят по два балла, третья весит шесть, четвёртая, пятая пять. А бонусных никаких нет? Или это они просто сомневаются? Нет, это без учета бонусов. Во первой, насколько я помню, есть бонус на 0,5 баллов, во второй бонуса, кажется, нет. В шестой бонус есть. В четвертой, пятой, кажется, бонуса нет, но если что, мы их придумаем. То есть в целом там процедуры для творчества есть, поэтому если вы делаете что-то дополнительное, то это будет ващец. То есть у нас получается, что сумма баллов 20, и это всё ещё пополам поделится и будет наш отц. Или как это работает? Нет, смотрите, это 20, я посчитаю долю того, сколько вы набрали от этих 20, и, соответственно, это умножится на 0,65. Ну, 0,65 – это вес, а просто оценка за дамажки – это вот мы будем смотреть долю сделанного от 20 баллов. Да-да-да. То есть бонусы как-то все-таки могут... Ну, бонусы, они будут линейно просто складываться. Там не будет никакого дисконта. Хорошо, спасибо. Вот. Вопрос из чата. Первый квиз через неделю, да. Насколько плотно будет... Ну в целом, наверное, не супер потно. Мне кажется, довольно комфортно, но если у вас будет промежуточный фидбэк, то пожалуйста, я очень к нему положительно отношусь, мы сможем. Преимущество этого курса в том, что веду его я, поэтому делать с ним я могу всё, что угодно. Мне не надо это ни с кем согласовывать, ну, может быть, каким-то формальным коктейлем только. Поэтому мы с вами сможем в таком формате диалога, если что, курс улучшать. Так что любой ваш фидбэк по сложности, по количеству активности приветствуется. Можно вопрос? Разбор статьи — это командная активность или как? Нет, это индивидуальная активность. Да, еще вопросы. Алгоритмы будут расписаны в течение курса. Чуть позже, про это будет прям понедельно, я расскажу, что мы будем, какой у нас план, какие алгоритмы будем рассматривать. Про квизы. Квизы нет, не перед парой. Насколько я помню, Я выкладывал их в прошлом году часов в 8 или в 9 утра. То есть у вас достаточно времени для того, чтобы их пройти. Они простые. Вам много времени на них не потребуется. Вольнослушателям сдавать квизы? Да. Я бы не знал, что у нас есть вольнослушатели, но сдавать можно. Потому что они автоматически прогоняются. Статьи? По сути, у нас есть какое-то количество ассистентов, и ассистенты будут проверять ваши домашние задания. У нас, к сожалению, из-за того, что большое количество людей, не успеем мы выступить все онлайн, поэтому надо будет записать видео, присылать код, и ассистент проверит. DZ весят 65%. Вот формулу я вам покажу. В теории можно набрать больше 10, если у вас есть какие-то бонусные активности, но все равно это будет крепиться к 10. Можно еще вопрос про презентацию статьи? Это чем-то похоже на научно-исследовательский семинар? Да, но только факт в том, что вам не просто надо будет найти, понять, что происходит в статье, а ещё попытаться её реплицировать. Но там у нас и есть как раз репликация статьи. Окей, тогда да. Там она, мне кажется, довольно специфическая. Понятно, не нужно создавать результаты статьи целиком, но на маленьких датасетах, например, можно. Здесь то же самое? Да. Я еще какие-то более четкие требования потом скажу, но в целом не требуется, не знаю, если вы взяли по какой-нибудь LLM-ке, не надо будет обучать многомиллиардную модель. чтобы реплицировать. Просто можно взять какую-то RL-ную часть, взять маленький датасет, показать, что это работает, но полностью стопроцентно реплицировать не надо, но в презентации, понятно, нужно будет рассказать, что вообще происходит. Спасибо. Спасибо большое. Опять же, это такая тестовая активность, потому что в прошлый год была только презентация, Поэтому, опять же, фидбэк очень приветствуется, и посмотрим, как это получится. Я думаю, к середине курса мы поймем вообще, в каком формате это будет финально. Но не переживайте, жестить в плане оценивания я и ассистента точно не буду. Поэтому все должно быть хорошо. Так, кажется, Статьи сами выбираете? Все, вроде вопросов нет. Давайте приступать. И первое у нас это видение. Я иногда, может быть, буду останавливаться, буду пытаться писать на планшете, если нужны какие-то дополнительные пояснения. Но пока всего этого не будет. И давайте вспомним много ассистентов. 7 у нас ассистентов. По курсу. Продолжаем. Итак, давайте вспомним, что мы вообще знаем и умеем, а именно наше любимое – обучение с учителем. Вспомним, что в рамках обучения с учителем мы оперируем над парами иксов и игреков, где иксы – это некоторые фичи, игреки – это некоторые ответы в случае бинарной классификации. Просто классификация – это метки класса в случае регрессии задачи. Это какие-то вещественные числа. Важно, чтобы вот эти пары, весь датасет у нас был так называемый IAZ, то есть независимые, одинаково распределенные данные, и цель всего этого мероприятия — построить некоторую апоксимацию для нашего таргета Y с крышкой с помощью функций из некоторого семейства алгоритмов. Мы часто рассматриваем параметрические семейства алгоритмов. Ну и, соответственно, задача обучения с учителем формулируется в терминах минимизации некоторого loss. Опять же, в случае задачи регрессии, это MSE, mean average, mean absolute error. В случае задачи классификации, это log loss. Соответственно, задача понятна. Вот тут есть пример. Отмечаем кошечек от собачек. Параметрическое семейство алгоритмов у нас — это некоторая сверточная нейросетка или vision transformer, в зависимости от того, что вам больше нравится. На выходе получаем вероятность того, что на картинке изображена, например, кошка. Соответственно, ну, фичи у нас получаются как раз с этой картинки, и наше предсказание — это вероятность того, что на картинке кошка. А в чем особенность этой задачи? В том, что, во-первых, кто-то уже за вас собрал эти лейблы. Возможно, это, кстати, были вы, если у вас какая-то специфичная задача, на которую нужен кастомный разметок. То есть у вас есть какой-то супервайзер. Во-вторых, задача заканчивается после первого шага. То есть у вас, по сути, есть один шаг вашего взаимодействия, где вы возвращаете в качестве ответа вероятность того, что на картинке изображена кошка. Задача очень прикладная, много где используется, ну и в целом хорошо обобщаемая на другие домейны, но Если вы подумаете о том, какие задачи вообще встречаются в реальной жизни, какие задачи мы с вами когда-то выполняли или выполняем сейчас, то они немного отличаются. Они более сложные по своей структуре, они многошаговые, они требуют от вас какой-то взаимодействия с окружающей средой или с каким-то симулятором, который вы придумали, и у вас часто нет правильных ответов. У вас есть только некоторый сигнал, который говорит о том, насколько вообще вы хорошо выполняете эту задачу. Ну вот давайте рассмотрим пример с... тем, как ребенок учится ходить, в целом его можно обобщить и на обучение машины, чтобы она самостоятельно ездила, никого не сбивала, соблюдала правила дорожного движения. Вот ребенок не умеет ходить, ему нужно каким-то образом ориентироваться в пространстве, выбирать направление, потом передавать импульсы в в конечности, чтобы каким-то образом передвигаться. Соответственно, ситуации могут быть разные. С первого раза может не получиться, ребенок упал, он получил какой-то негативный фидбэк, ему больно, или он не дошел до своей любимой игрушки. В общем, задача до конца не выполнена, и там дальше есть развилка. Он может попытаться еще на основании уже своего опыта, либо же может бросить эту затею и заняться чем-то еще. Фан-факт того, что у нас есть некоторая такая эфемерная среда, с которой наш агент, в данном случае ребенок, взаимодействует, получает от него фидбэк, ну и на основании этого фидбэка пытается каким-то образом улучшить свое поведение. Так, давайте... Я просто буду проверять периодический чат, что ничего нового не появилось. Какие вообще, ну давайте попробуем, я сейчас сказал много слов, давайте попробуем это каким-то образом формализовать. Первое, с чего я начал, у вас нет супервайзера. Никто вам правильные ответы не собрал предварительно. У вас есть только некоторый скалярный сигнал, который сигнализирует вам о том, насколько хорошо вы вашу задачу выполняете сейчас. Более того, сигнал может быть отложенным, то есть вы можете получить награду за то, что вы сделали сейчас через много-много-много шагов, это вообще отдельная проблема в Reinforced Learning, а какое именно из действий привело вас к финальной награде, так называемый Credit Assignment. В-третьих, время начинает иметь значение. То есть то, как вы взаимодействуете сейчас, влияет на то, как вы будете взаимодействовать в будущем. Более того, если вы соберете все данные, состояние действия, которое вы пронаблюдали, которое вы выполнили, чем-то похоже на эту на эти пары иксов и игреков, только здесь индекс будет завиваться по времени, то вы с удивлением обнаружите, что из-за предыдущей причины эти данные больше не IID, и там могут возникать чисто теоретические и технические проблемы с тем, чтобы обучаться на таких данных. Вот, соответственно, задача последовательная, некоторый такой тайм-сириас, ваши текущие решения влияют на ваше будущее, на то, куда вы придете. Вот, но это в целом и к четвертому пункту относится. Как раз то, что вы делаете сейчас, будет влиять на то, какие данные вы вообще получите от среды. Какие примеры того, что я описал, вообще задач, помимо обучения ребёнка ходить или обучения машиной ездить без водителя. Самый такой, наверное, классический пример – это настольные игры. Вот здесь вы можете наблюдать эволюцию алгоритмов вместе с тем, на какие игры они могут быть применены. Начинали с игры Go, дошли до Go, шахмата, сёги и вот даже, если вы помните, такой... терминал, точнее автомат для того, чтобы играть в какие-то пиксельные старые игры. Если не помните, то скоро мы с этим познакомимся, потому что будем учить агента как раз в такие игры играть. Важно, вот, что тут я чуть-чуть прокомментирую эту эволюцию, в том, что начиная, когда все это зарождалось, какая была цель? цель была сделать какой-то алгоритм, который мог бы в такие игры играть на высоком уровне. Изначально, если у нас есть данные игры других людей, давайте попробуем дистиллировать эти данные в наш алгоритм. Потом, если мы имеем какое-то представление о том вообще, как выглядит наша игра, у нас есть какие-то domain knowledge, например, Go, если я не ошибаюсь, Daskav, симметрично относительно поворотов и отражений. Вроде какая-то такая тема там была, что... Так, на вопрос отвечу чуть-чуть позже. Да, соответственно, вот у вас есть симметрия, вы про нее знаете. И третье — это то, что вы знаете правила. Это... На этом давайте чуть-чуть остановимся, мы будем об этом говорить на последующих неделях, но, зная правила, вы, по сути, находясь в текущем состоянии, можете прикинуть, а куда вы попадете, если совершите определенные действия. Вы знаете, как игра будет развиваться, можете пропланировать внутри игры. Как вы видите, от первых двух отказались буквально на следующую итерацию алгоритма, от всех трех уже отказались при переходе от AlphaZero к MuZero. Мюзиру буквально ничего не знает про... не использует никакие human data для среды, не знает ничего про то вообще с какой игрой, с каким средой она взаимодействует изначально, и она не знает правила. Там это интересно построено, что у вас есть по сути нейронка, которая внутри себя симулирует игру. Вы можете с помощью неё пропланировать игру на несколько шагов вперёд, но это такая чистая симуляция. Буквально дистиллировали вашу следу в нейрон. Но это игры с полной информацией, вы видите кады соперника, есть ещё обобщения, игры с неполной информацией, когда в целом вещи, какие-то Все состояние игры, его часть, может быть от вас скрыта. Тоже интересно почитать, как в таких ситуациях можно обучать алгоритмы, и оказывается, что можно обучить алгоритм, который находит наш эквилибриум в данной игре. Что еще? Чему еще можно применить? Такая мета-задача, давайте попробуем с помощью Reinforced Learning быстрее умножать матрицы. Мы не будем сейчас рассматривать, как именно задача ставится с точки зрения, какие там пространства действий, какое там пространство состояния, но сам факт того, что вы пытаетесь оптимизировать Оптимизировать нейронку, которая внутри этого перемножения матриц, саму операцию перемножения матриц, тоже довольно эффективно. Там есть такое же По-моему, называется AlphaDef, вот это называется AlphaTensor. Есть AlphaDef, по-моему, это для того, чтобы быстрее сортировать алгоритмы сортировки. И тоже интересный пример с, не помню, как называется, AlphaChip, может быть. То есть как с помощью обучения с подкреплением более оптимально дизайнить чипы, для вычислений в более оптимальным образом. Следующее. Ну, мы живем с вами в эпоху уже больших языковых моделей, поэтому грех про это не поговорить. Ну вот Reinforced Learning или некоторое его подобие используется как раз в то есть в попытке модель, большую языковую, заставить действовать в соответствии с человеческими предпочтениями. Так, я вижу поднятые руки. Да, есть вопрос, а почему задача перемножения матриц — это reinforcement learning, а не просто обычная, типа там же есть, ну, можно легко сделать истинную разметку, просто перемножить матрицы. Да, но само пространство у вас, количество матриц, которые нужно перемножить, оно очень большое. То есть вы действительно можете обучить алгоритм, который бы, по сути, предсказывал вам, как умножать, собрав данные. Тут важный вопрос. То есть у вас есть задача, независимая от фреймворка, которым вы будете решать. Вы хотите быстрее умножать матриц за меньшее число операций. Вы можете ее поставить в терминах того, что давайте я соберу большой датасет. Давайте я закончу эту мысль. Можете собрать большой датасет и дистиллировать его в нейронку. Так, например, в шахматы сделали. Но проблема в том, что когда у вас задача с открытым концом, то есть, по сути, вам нужно создать датасет, где у вас матрицы умножались за минимальное число шагов. Но поскольку вы не знаете этот алгоритм, вы не можете создать такой датасет. Здесь смысл в том, чтобы каким-то образом обучить агента, который бы не просто правильно перемножал матрицы, а еще и использовал для этого наименьшее количество операций. Если я не ошибаюсь, там реворд состоял из двух частей. Смогли ли вы правильно умножить матрицы и сколько шагов вам для этого потребовалось. Надеюсь, я ответил на вопрос. Ну и важно, что у вас может быть какая-то задача, вам не обязательно использовать для нее обучение. Это как раз пример того, как задачу, которая казалось бы... которая просто существует, вот ее натянули на этот фреймворд. Мы сейчас поговорим про это. Да, alignment... Как я уже сказал, давайте попытаемся заставить нейронку следовать человеческим предпочтениям. Тут много шагов, на самом деле, если у вас есть уже... То есть вам нужно reward каким-то образом получить, reward model. Потом с помощью reward model мы запускаем какое-то PPO, о котором тоже будем говорить в курсе. И вот получаем модель, которая с человеческими предпочтениями вырвана. Недавняя статья от «Антропика» — alignment faking, как-то так называется. В общем, как нейронка может притворяться, что выровнена с человеческим предпочтением. Плюс последние вести с полей — это ризнинг. Опять же, поскольку это OpenAI, который то никакой конкретики здесь нет, но утверждается, что O1 по сравнению с GPT-4 была дообучена с помощью обучения с подкреплением, но тут как раз левый график говорит о том, какое качество вы получаете при увеличении количества преувеличение времени, которое вы с помощью Reinforced Learning обучаете вашей модели. Довольно такой нетривиальный пример. Я о нем, наверное, узнал года полтора назад. А именно была статья от DeepMind и EPFL, как они пытались Reinforced Learning заменить более привычные системы управления даже не на самом реакте, а тут прототип термоядерного реактора. Как заменить вот эти control system на этом реакторе с помощью Reinforced Learning. Если вкратце, нужно удерживать с помощью магнитного поля поток плазмы в каком-то стабильном состоянии, может быть, его изменять, чтобы энергия выделялась. Но это задача управления. Вы подаете ток ток или напряжение на катушку, я не физик, и за счет этого у вас магнитное поле изменяется. Но вот задача непрерывного управления, для нее, кажется, хорошо как раз подошел Reinforced Learning Framework. И последний пример – это трейдинг. Тут комментировать особо нечего. Проблема в том, что задача действительно интересная, задача сложная, и мало кто из представителей индустрии действительно за нее берется. Проблема в том, что если даже что-то получается, Мы об этом не узнаем, плюс вся литература по этой теме, которую вы найдёте в открытом доступе, по большей части бесполезна абсолютно. Поэтому такой довольно анекдотичный пример, что в целом такое направление существует, но насколько оно успешно – непонятно. Так, есть ли какие-то вопросы, прежде чем мы пойдем уже в более формальную зону. Простите, я пропустил вопрос. Одна модель может сразу в нескольких доменах работать. Насколько я помню, это был вопрос про вот это. как бы одна и та же архитектура, одни и те же гиперпараметры, да, могут работать с разными играми. Что касается того, что это одна модель, которая может играть во всё, кажется, конкретно в этом случае это не так, но есть такая модель, может быть, кстати, у неё есть развитие, можем посмотреть, по-моему, она называется Gato, Gato от DeepMind. Она мультимодальная. То есть она тут и чатиться может, и в игры играть, и видимо имидж капшенинг делать. Кажется, по-моему, у неё есть какое-то развитие после этого. Короче, вот эта модель, она прям обучена, чтобы быть мультимодальной. Что касается вот этого алгоритма, здесь, скорее, просто одна и та же теория, одни и те же какие-то параметры, но под каждую задачу обучается своя модель. Так. Где мы? Сейчас мы здесь. А reward? Reward — это вообще основополагающая концепция в Reinforced Learning, потому что это, по сути, чуть ли не единственная ниточка, которая у вас есть, чтобы двигаться к своей цели. То есть буквально у вас есть некоторый скалярный сигнал, который Я буду много раз повторять, как некоторые заклинания, которые гайдят вашего агента двигаться вперед, чтобы он не сдавался, чтобы добивался своей цели. Соответственно, задача агента это максимизировать свой кумулятивный реворд, то есть реворд, который был получен как сумма за все его взаимодействия, поскольку у нас, опять же, взаимодействие многошаговое, вы можете получать реворд в течение всего этого процесса, и задача как раз его максимизировать в полном объеме. Ну вот Ричард Саттон — это один из начальников Reinforced Learning, у него есть с коллегой книжка, по такому более классическому обучению с подкреплением. Вот сформулировал он такую reward hypothesis, то все задачи достижения каких-то целей можно описать в виде задачи максимизации ожидаемой кумулятивной, на самом деле еще дисконтированной награды по дисконту. Через один-два слова. Вот. Это как бы довольно старая вещь. Вот в 20, кажется, не вижу год. А, вот, в 21 году, когда как раз заговорили про intelligence и про то, как можно заставить текущие модели быть каким-то образом развить способность к мышлению, к языку, к генерализации за свой обученный домейн. Ну и в целом, короче, чтобы стать таким AGI, утверждается, что и для этого достаточно просто награды. которая бы как раз гайдила вас к тому, чтобы эти способности у вас развивались. На самом деле, реворд — это сложная тема, потому что буквально от того, как вы задизайните вашу награду, будет зависеть то, какую к какому состоянию ваш агент сойдет, как он вообще будет действовать в вашей среде. Ну вот здесь пример, то есть просто гонки, вы лодочка, пытаетесь прийти первым к финишу, еще получаете какую-то промежуточную награду за то, что собираете что-то в процессе. Проблема в том, что если вы дадите вот этой промежуточной награде за процесс слишком большой вес, то в какой-то момент, как здесь, агент может решить, что ему в целом эта гонка больше не интересна, и начать В общем, он нашел какое-то стабильное состояние, когда он движется по кругу и просто с какой-то периодичностью, в которой спаунятся как раз вот эти штуки, и просто их собирает. То есть гонка для него уже перестала его перестала интересовать. Он взломал награду. Реворд хайкинг тоже такой довольно распространенный термин в обучении с потреблением. То есть ваш агент просто понял, как можно как можно взломать награду, получать ее все больше и больше, но при этом основной цели, а именно прихода в эти гонки первым, он не достиг. Я думаю, у кого-то может быть голова закружилась от этого вращения. Поэтому давайте двигаться дальше. В общем, что хочу сказать. Reward is all you need, но в целом буквально с чего вы начинаете постановку вашей задачи, а именно какой реворд я буду давать агенту. Как я буду его мотивировать двигаться дальше. И насколько это соответствует тому, чего я хочу в итоге от агента. Какую задачу он должен выполнять. Кстати, простой пример. Награды, которые вы можете давать для достижения цели, это просто нолик, в общем, единичка, если ваш агент достигает какой-то цели, то есть какого-то терминального состояния, или он приходит первым, и нолик в противном случае. То есть вы даете вашему агенту награду только в самом конце, Что в целом соответствует тому, чего вы хотите добиться. Действительно, агент, который мог бы максимизировать такую награду, мог бы и решать вашу задачу. Проблема в том, что такая награда очень разрежена. Буквально, если у вас тысячи шагов взаимодействия, то представьте, что вы, находясь в начальной точке, должны понять, что через тысячу шагов вы получите какую-то награду. Пока довольно сложно обучать такие алгоритмы, которые смогли бы как раз такой кредит-ассаймент проводить. Потому что, опять же, проблема в том, что если у вас много промежуточных взаимодействий, много действий, которые вы совершили, вы не понимаете, какое же действие действительно привело к финальной награде. Опять же про Sparks, Dance, Reward мы тоже в течение курса будем говорить. А плюс у нас есть еще дискаунт фактор. В целом люди с психологической точки зрения ценят награду, которая получена сейчас, сильнее, чем награду, которая будет получена через nShaggo. Ну и в целом есть некоторая неопределенность относительно будущего, поэтому, возможно, стоит аккумулировать вот эту награду сейчас, нежели ждать, когда вы в будущем получите больше. Ну и даже если... простой пример, это если вы в итоге получите одинаковую награду, то награда, полученная за n шагов, должна еще умножаться на некоторый дискаунт-фактор гамма в степени t. Какие для этого есть предпосылки? Чисто с математической точки зрения, если у вас как-то так случилось, что процесс взаимодействия потенциально бесконечный и Каждый раз, когда вы получаете какую-то одинаковую награду, то вы хотите избежать вот этих бесконечных сумм. Вот бесконечные суммы, они всё сломают, мы такого не хотим. С точки зрения, опять же, некоторой психологии, будущее не определено, будущее довольно туманно, есть некоторые риски, поэтому вы бы хотели получить с большей вероятностью награду сейчас, нежели чем потом. Ну и в целом это наблюдается, опять же, у животных, у людей, такое поведение, что они более склонны аккумулированию награды, которую они получат раньше, нежели чем позже. Вот еще одно, если в экономике есть всякие типа risk-lover, risk-averse, то есть насколько у вас вообще отвращение к риску, соответственно, если вы избегаете риска, то дискаунт-фактор у вас будет меньше единицы и сильно меньше единицы. Можно ли использовать недисконтированные награды? В целом, да, можно, но тогда вы должны гарантировать, что у вас все взаимодействия через какое-то фиксированное количество шагов заканчиваются, чтобы опять же избегать этих бесконечных сумм. Можно еще это воспринимать так, что с вероятностью 1 минус гамма у вас взаимодействие заканчивается. Да, теперь как бы поговорили про дискаунт, поговорили про награды, теперь давайте поговорим про наших двух главных героев, а именно агент и среда. Как происходит взаимодействие между ними? В момент времени Т агент находится в некотором состоянии или наблюдении, не совсем одно и то же, поговорим позже про отличия, находится в некотором состоянии от ОУТ, наблюдения, И исполняет какое-то действие АТ. То есть посылает его буквально в среду. Среда каким-то образом это действие обрабатывает. Возвращает ему награду РТ. И возвращает ему следующее дублюзение. Олд Т плюс один. В общем-то, такой цикл взаимодействия между агентом и средой. Что касается состояния. Тут немного хита с этим, потому что в целом состояние среды, внутреннее состояние среды, наблюдение, которое среда возвращает, и внутреннее состояние агента могут отличаться друг от друга. Все три друг от друга могут отличаться. Вот. А в целом... Наверное, можно это воспринимать так, что если у вас среда — это всё, что вас окружает вообще в мире, то состояние среды — это все возможные конфигурации атомов во Вселенной, что в целом довольно тяжело представить. поместить куда-то, чтобы обработать. А наблюдение, которое вам всегда показывают, это... Это текущее, то, что вас окружает в какой-то локальной окрестности, а ваши внутренние наблюдения — это то, как вы воспринимаете мир. Например, сейчас мой мир ограничен, как минимум с визуальной точки зрения, ограничен только моим поле зрения, я не вижу, что происходит за мной, соответственно, моё Моё внутреннее состояние как агента, оно уже, чем то наблюдение, которое мне среда возвращает. При этом я могу превратить головой, и я буду видеть какие-то более расширенные... расширенную часть этого наблюдения, но, опять же, оно будет ограничено моим... полем моего видимости. Соответственно, давайте просто договоримся, что у нас будет два вида сред. Мы в основном будем работать вот в этой парадигме, но такие тоже будут встречаться, частично наблюдаемые среды. То есть будем говорить, что среда частично наблюдаемая, если ее внутреннее состояние среды не равно внутреннему состоянию агента. И будем называть ее полу. просто полностью наблюдаемый. Если у нас как раз все три наблюдения, внутреннее состояние среды, внутреннее состояние агента равны между собой. То есть агент действительно наблюдает то, то в себе, внутри среда содержимого. Политика. Политика – это некоторый синоним к агенту. Агент – это, по сути, робот, человек, который со средой взаимодействия, а политика – это попытка формализовать действия агента, подчинить их какому-то более понятному алгоритм, скажем так. В простом случае можно воспринимать политику как просто отображение из состояния в действие. Бывают два вида политик. Это детерминированная политика, которая, по сути, полностью соответствует определению. Вы детерминированным образом по состоянию получаете действия, которые вам в этом состоянии нужно сделать. И стокхастическая политика представляет из себя вероятность распределения То есть буквально вы для каждого состояния имеете целое распределение, из которого вы можете засамплировать засемплировать ваше действие. В качестве примера, представьте, что вы меняете скорость, и скорость вы можете менять непрерывно, просто каким-то образом добавляя некоторую дельту вашей текущей скорости. Это ваше действие. Соответственно, в данном случае можно представить, что действие приходит из нормального распределения, стандартного нормального распределения, И вот как раз в зависимости от того, положительно или отрицательно, вы либо увеличиваете скорость, либо уменьшаете. И вероятность того, что вы ее не измените, равна нулю в данном случае. Как формально... формальное определение для нашей задачи, какую задачу оптимизации мы пытаемся решить? У нас есть тут некоторое мотожидание по... тут это называется EP. На самом деле это чуть более хитрая конструкция, но давайте считать, что это некоторое мотожидание по нашей политике P текущей. У нас есть вот такая дисконтированная сумма, γt на rt, взаимодействие начинается в нулевой момент времени и заканчивается через T большой шагов. Т большое может быть бесконечно. И задача стоит в том, чтобы максимизировать как раз вот эту ожидаемую дисконтированную награду по нашей политике. Вы пытаетесь, по сути, придумать какой-то алгоритм действий в каждом состоянии, который бы такую награду вам максимизировал. Да, только вот сейчас как раз мы подошли к тому моменту, когда мы уже с вами знаем примерно с какими задачами будем работать, знаем некоторое базовое определение, и поэтому мы можем поговорить про то, о каком вообще у нас план на курс. План следующий. Интро у нас уже было. Дальше мы поговорим про марковский процесс принятия решения и познакомимся с нашими первыми алгоритмами, которые попадают в парадигму динамического программирования. Это policy evaluation для табличных сред, у которых пространство состояния и пространство действий конечно. которых можно представить как табличку. Соответственно, будет первый домашний, как раз на имплементацию этих алгоритмов. Затем мы перейдем в более сложную парадигму, когда по среду мы будем знать все меньше и меньше. Скоро поймем, что это значит. В целом, когда среда для нас уже станет черным ящиком, с которым мы можем просто взаимодействовать, но при этом сама сложность среды с точки зрения того, табличка это не табличка, она не изменится. Будет вторая Дома. Третье. Здесь начинаются нейросетки. Поговорим про Deep RL в общем случае, познакомимся с алгоритмом, базовый алгоритм DQN, потом про его различные модификации и как их все вместе скомпоновать, получить алгоритм Rainbow, ну и там на самом деле. Есть о чем поговорить дальше, потому что на Rainbow развитие конкретно этой ветки алгоритмов не закончилось. Входит третья домашка. Кажется, самая непростая, потому что в целом здесь происходит некоторый скачок с базовых алгоритмов к нейронкам, ну и там на самом деле много-много компонентов, Я думаю, что все справятся. В любом случае. На четвертой неделе поговорим с вами про другую попытку формализовать задачу с точки зрения deep RL, а именно policy-based алгоритмы, где мы будем непосредственно политику учить. Представьте, что вы политику параметризовали в виде нейросетки и теперь ее учите. Поговорим про алгоритмы reinforce. Возможно, вы встречались с этим алгоритмом вне контекста обучения с подкреплением. И поговорим про Actor-Critic Framework. У вас там появятся уже две вспомогательных сущности, которые друг другу помогают. Дальше на пятой неделе поговорим про продвинутые policy-based алгоритмы. Тут мы начнем, на самом деле, с TRPO, который более теоретически обоснован. Но вообще все на ваше внимание будет фокусировано вокруг PPO, потому что, несмотря на то, что алгоритм вышел в 2015 году, что, с точки зрения RL довольно архаический век. ППО все еще активно используется и в целом такая рабочая лошадка. Некоторые бейзлайн, с которого вы будете часто начинать, когда у вас буквально есть только следа. Первый подход к снаряду, давайте применим ППО. И четвертый домашний как раз на реализацию ППО. Шестое. Continuous Control. Опять же, когда действие у вас становится непрерывным. На самом деле непрерывные действия появятся уже здесь, в четвертой-пятой неделе, но под непрерывные действия есть прям отдельный набор алгоритмов, под которые делается работа. Седьмая неделя — Offline RL. Мы с вами все до этого будем жить в парадигме Online RL, то есть когда агенты буквально бросаются в среду и говорят «давай взаимодействуй, собирай опыт, потом обучишься на нем». Offline RL — это как если бы вы собрали а если вы хотите обучить, например, агента для того, чтобы он играл в, не знаю, в го, вы собрали исторические данные, как люди играли в го и попытались дистиллировать это в вашего агента сначала, чтобы у него была какая-то, не знаю, отправная точка, ну или в целом на этом закончить, и вот вы попытались научить вашего агента на демонстрациях, которые собрали люди, и вот у вас новый алгоритм. Это, на самом деле, близко к стоперевать лернингу, но там есть свои особенности. Там есть свои особенности, там очень часто out of distribution может возникнуть, потому что у вас, у агента, когда он учится по траекториям, которые кто-то собрал, может возникнуть желание попробовать действия, которые в траектории нет. И в этот момент вы выходите за пределы вашего распределения, которое над данными есть, и могут быть разные неприятные спецэффекты. Пятая домашка будет по Continuous Control, но вы увидите, что Continuous Control и Offline RL во многом связаны друг с другом. Об Offline RL вам не обязательно решать задачу Continuous Control, но связь, тем не менее, довольно сильная. А дальше model-based RL, как раз все то, что связано с обучением агентов на играть в шахматы, в бо, то есть когда каким-то образом, у вас нет знания о среде, но вы пытаетесь каким-то образом планировать в этой среде, либо же зная правила, либо же дистиллируя вашу среду в какую-то новую нейронку. Дальше Multi-Armed Bandits – такой урезанный framework RL, но с теоретическим гарантием. Поэтому, возможно, красиво с математической точки зрения и довольно много где используется. Ну и десятая, заключительная неделя нашего курса будет посвящена обучению с подкреплением в контексте LLM. Посмотрим про то, как можно делать alignment, немного поговорим про reasoning, попытаемся пошупать это руками, понять вообще какие там алгоритмы и чем это различается от классического RL, в котором мы все эти 9 недель предыдущих жили. Так, вопрос. Будет ли DPO? Да, DPO будет на 10-й неделе. Так, наш план... – Если можно, есть небольшой практический вопрос. Будут ли ресурсы дополнительные? В целом коллаба должна хватить. Пульс построен из расчета того, что коллаба должна хватить. По ресурсу сейчас затрудняюсь ответить, потому что у меня... Коллаба бесплатного или оплатного? Да-да-да, бесплатного. А, всё, супер, спасибо большое. Ну, коллап, кегл еще есть. Так что, ну, вот это значит, что бесплатно должно хватить. Так. И... Да, маркерский процесс принятия решения. Давайте посмотрим. Так, мы с вами уже сейчас разговариваем. Это значит, что нам нужно ускоряться, но посмотрим. Мне кажется, мы залезем на семинар. Возможно, какая-то часть семинара пойдет на самостоятельное ознакомление. Посмотрим по состоянию. Давайте говорить про MDP, про Марвский процесс принятия решения. Это, по сути, формализация как раз вашего взаимодействия агента с средой и самой среды, накладывая на нее какие-то дополнительные условий, чтобы можно было вокруг этого какую-то развивать теорию. Соответственно, что у нас тут есть? У нас есть вот это А красиво, это наш action space, то есть, по сути, те действия, которые мы можем выполнять в нашей среде. Пространство состояний – это то, каким состоянием ваша среда оперирует. Еще может быть изначальное распределение над тем, в какое состояние вы можете попасть, когда начинаете взаимодействие. Четвертое – это вот такая условная вероятность. Это вероятность того, что, находясь в состоянии S и выполнив действие A, вы получите награду R и перейдете в состояние S3. Ну и дискаунт фактор. На самом деле, вот в этом как раз скрыто так называемый Mark of Property, то есть будущее в данном случае у вас не зависит от прошлого, если преизвестно настоящее. То есть вам для того, чтобы знать вот это распределение, вам достаточно знать текущее состояние и текущее действие. Вам не обязательно обуславливаться на всю историю взаимодействия. Тут еще важный момент, помимо того, что у вас здесь зашито марковское свойство, у вас еще здесь зашита стационарность. Обратите внимание, что вот эта вероятность не зависит от времени. она для всех пар, ну, по сути, для всех четверых, она всегда одинаковая. Это важный момент. Опять же, получается, вы можете воспринимать вашу следу как некоторую самоподобную систему, где, когда вы переходите в следующее состояние, у вас взаимодействие, ну, не сначала начинается, но у вас игра в этот момент, подобно тому, по что вы играли в предыдущие шаги. А мы время закладываем в reward? В расчет reward? Если нам это важно. Если нам важно время каким-то образом, то мы можем его заложить в state. Есть такая тема time-aware RL. То есть вы можете... Обычно так делается. Считается, если вы знаете горизонт, когда вы закончите, вы вычитаете из этого горизонта ваше текущее время и делите на весь горизонт. Вы получаете число от нуля до единицы, которое сигнализирует о том, в какой момент взаимодействия вы сейчас находитесь. ответил ли я на вопрос? да, спасибо. вот примеры немарковских сред. если вы посмотрите на вот эту картинку слева, это стрим игры Breakdown, если я не ошибаюсь. Или... Да, Breakdown. Когда у вас ездит платформа, есть вот этот мячик, вы его отбиваете, попадаете в эти кирпичики, зарабатываете очки. Если это ваше состояние, один скрин, то вы не можете сказать, в какое направление, по какой траектории ваш мячик полетит. Вам нужно как минимум два скрина для этого. И поэтому если использовать один скрин, то среда не марковская. Тут пример с роботом-пылесосом. Здесь, поскольку он пытается обойти всю квартиру, ему важно знать, где он уже был. Если вы каким-то образом не закладываете эту информацию в ваш стейт, то среда опять перестает быть марковской. Ну и третий пример. Кажется, здесь у вас есть дверь и у вас есть ключ, который вам нужно подобрать, чтобы эту дверь открыть. Если вы придете к этой двери без ключа, то ничего не произойдет. И вот если в state не закладывать, есть ли у вас ключ или нет, то опять же среда будет не марковской. Хорошо или плохо, вся теория, которую мы сейчас будем рассматривать, теоретическая гарантия, она построена вокруг того, что среда марковская. Как это можно обойти? Самый простой совет, ну это вредный совет, но просто что можно сделать. Можно взять и положить в ваш стейт это по сути все то, что вы, все стейты, которые вы наблюдали до этого. Буквально в этот момент вы смотрите на все кадры, которые появились у вас в процессе взаимодействия, понятно, что здесь очень быстро можно выйти out of memory, если особенно какие-то тяжелые картинки хранить. Поэтому часто вводят понятие памяти агента, Если вам каким-то образом важно, чтобы агент обуславливался на свою историю, то как раз в частично наблюдаемых средах вводится понятие памяти. Но, опять же, пока это далекий концепт, мы поговорим об этом позже. Важный момент, который стоит вынести. Типа, хотите делать среду марковской? Просто положите в ваш стейт все предыдущие Задефайните новый стейт, который является конкретинацией всех предыдущих. То есть у вас, по сути, каждый раз конкретинируется последнее наблюдение, но вот это, понятно, не очень эффективно, но решает проблему макроэсти. Мы будем пользоваться, на самом деле, немного другим определением, оно в целом эквивалентно, но чуть попроще, нам там с мотожиданиями будет полегче работать после этого. Ну и в целом оно покрывает многие, если не большинство реальных реальных задач, поэтому остановимся на нем. А именно, будем говорить, что MDP — это вот такой tuple, состоящий из четырех элементов, action space, state space, вот такая условная вероятность того, что вы перейдете в состояние S-штрих при условии того, что вы сейчас находитесь в состоянии S и совершаете действие A. И считаем, что награда — это детерминированная функция от состояния и действия. Весь процесс взаимодействия с точкой зрения этого формализма представлен на картинке. Вот у вас агенда-то. Марио действия... какие у него действия? По-моему, он назад не мог ходить, если я не ошибаюсь. Поэтому давайте представим, что он может только ходить... нет, он мог, но неважно. может ходить, может прыгать. Вот, соответственно, его пространство действий, пространство состояний. Это, допустим, скрин вашего экрана, ну или несколько скринов предыдущего экрана. То есть, скрин с текущего взаимодействия, скрин с предыдущего взаимодействия и так далее. Вот. Реворд, ну там реворд какой-то кастомный в зависимости от того, добрались ли вы до конца уровня, смогли ли разбить ящики. Но, в целом, да. Такой пример. И мы будем пользоваться. Опять же, пример, чуть более приближенный к студенческой жизни, это student MDP. Здесь оно представляется в виде графа, соответственно, Давайте представим... Попробую порисовать. Получилось. Давайте представим, что мы находимся в этом состоянии. И дальше у нас есть выбор. в некой соцсети, либо же пойти учиться. Соответственно, если вы пошли сидеть в соцсетях, то велика вероятность, что вы приедете сюда, и дальше Дальше у вас может подойти такой бесконечный цикл, когда вы никак не можете прекратить там сидеть, листаете ленту, смотрите тиктоки. Соответственно, с точки зрения формализма этой среды вы получаете минус единичку все время. Ну и так можно до бесконечности. В какой-то момент вы можете остановиться и вернуться вот в изначальное состояние. Дальше Обратите внимание, что вы можете пойти по другому пути, можете начать ботать, но в начале этого пути может так случиться, что вы получите отрицательную награду, просто потому что результат вашего обучения может быть сильно отложен. Вы, конечно, можете получать удовольствие от процесса, но сам результат может произойти через несколько лет. Соответственно, вы в моменте получаете отрицательную награду, но если продолжите учиться, то в какой-то момент, это если что не агитация, это просто пример, в какой-то момент вот можете получить награду плюс 10. Я не знаю, что там произошло, может быть какой-то жирный оффер пришел. И вот, соответственно, здесь мы попадаем в терминальное состояние наше. Я еще сейчас скажу, что значит терминальное состояние. Будем считать, что на этом взаимодействие закончится. То есть мы дальше не будем ничего делать. А еще тут есть такая развилка — это пойти в какой-то момент в паб. Соответственно, там есть уже вот стахастика в том плане, что непонятно, куда вас эта дорога приведет. обуславливается на количество выпитого, соответственно, вот тут как раз проявляется стахастичная природа нашей ряды, что вы с какой-то вероятностью можете попасть в разные состояния. До этого у нас все стрелочки были детерминированы. Сейчас я удалю это. Что касается терминального состояния, в терминальном состоянии вы не можете из него выйти, у вас как будто петля остановится. То есть вероятность того, что вы попадете в это состояние при любом действии равна единице. И вы получаете нулевую награду. Здесь мы рассмотрели пока примеры так называемой tabular MDP. Это значит, что пространство состояния и пространство действий конечны. Можно их представить в виде таблички, то есть для каждого состояния. Вы их можете пересчитать, во-первых, но при этом это какое-то конечное множество. Ну и для каждого конечное множество у вас есть какие-то действия, которые вы в этом Для каждого элемента такое нечто многое, что у вас есть какие-то действия, которые вы в нем можете совершить. Тут еще важный момент, что в целом мы рассматриваем такую общую структуру, когда у вас все действия Для всех состояний доступны все действия. На самом деле, конечно, в реальной жизни это не так, вы даже можете здесь увидеть, что у вас в разных состояниях доступны разные действия. Ну и в целом можно, я не знаю, смогу ли я реплицировать эту замечательную букву, но в целом можно рассматривать множество а красивое, которое зависит от вашего состояния. В практических имплементациях это делается через action masking, то есть по сути вы запрещаете как бы если у вас, не знаю, алгоритм предсказывает какие-то, не знаю, логиты или вероятности, например, четыре действия, то вы либо... ну и вот эти действия, они забанены, например, вы их не можете выполнять. Вы либо даете какую-то большую отрицательную награду за то, что агент выбирает эти действия, потому что в конечном итоге он будет саплировать из этого распределения, то есть... Вы даете какую-то отрицательную награду за то, что агент выполняет эти действия, либо вообще блокируете эти действия и распределяете вероятностную массу между действиями, которые вам доступны. Но, опять же, пока об этом думать не надо, пока мы не пришли к более практическим вещам. Пока это так. Да, поговорим, что у нас есть какой-то горизонт взаимодействия, это большое. Если этот горизонт, он конечный, то среда называется эпизодической. То есть, по сути, все взаимодействия у вас в какой-то момент заканчиваются, ну и вы можете сказать, что каждое взаимодействие Каждое не взаимодействие, а последовательное взаимодействие до окончания всего, это будет называться эпизодом. А какие у нас действующие лица здесь важны? Мы будем называть кумулятивную награду, полученную с момента времени t, return, или reward to go, и вычислять ее по следующей формуле. то есть вот у нас в момент времени t мы совершили какое-то действие и получили награду rt. здесь какое-то действие at привело к награде rt. а дальше мы перешли в состояние... Вот так, давайте так. СТ, мы были в состоянии СТ, сделали действие АТ, получили награду РТ. Дальше перешли в состояние СТ плюс один, совершили действие АТ плюс один, получили награду РТ плюс один. Ну и так далее до конца эпизода. Эпизод закончился через Т-шагов. Зачем нужен дискаунт-фактор? Представьте, что у вас t чисто с математической точки зрения, t бесконечность и наградой какой-то, не знаю, расходящийся ряд. 1 делить на n, например. Соответственно, у вас вы суммируете вот такую... вы суммируете без дисконтирования 1 делить на t. и получаете бесконечную награду. Максимизация вот такой награды – она плохо обусловлена задачей. Вы ожидаете, что получите в итоге какое-то конечное число, из которого вы можете переигрывать. то бесконечно, то начинаются всякие сложности. Это чисто с математического точки зрения. С точки зрения как бы в реальном мире у вас есть некоторая неопределенность относительно того, какую награду вы получите в будущем. Ну, например, вы находитесь, двигаетесь по такому коридорчику, в целом все понятно, но вот здесь И здесь у вас лава. И типа, если вы в какой-то момент пойдете влево или вправо, то вы упадете, и взаимодействие закончится. Допустим, мы за каждое взаимодействие получаем какую-то награду, соответственно, вы должны каким-то образом сейчас учитывать, что в целом в будущем вы можете попасть в плохую ситуацию, попасть в лаву, и взаимодействие закончится, то есть вы недополучите свою награду, которую вы через n шагов хотите получить. Поэтому, чисто с психологической точки зрения, награды, которые приходят раньше, более ценные, чем награды, которые приходят позже. Ну и вот вы это правило, по сути, каким-то образом вставляете в вашу математическую модель. Discount factor, immediate reward. В целом, reward to goal – новая концепция, то есть кумулятивная награда, которую вы получаете с текущего момента. Важно, что на самом деле reward to goal подчиняется следующему рекуррентному соотношению. просто небольшой математический вывод, если вы вынесете гамму из ваших следующих immediate Reward, то увидите, что на самом деле это гамма, умноженная на Reward угол в момент времени t плюс 1, и можно будет записать такое рекуррентное соотношение. Да, вообще мы считаем, что Defcon Factor фиксирован. Обычно мы считаем, что он фиксирован. То есть мы не пытаемся... Это не то, что гиперпараметр, который мы пытаемся каким-то настраивать во время игры, это скорее некоторые характеристики вашей среды. Хотя вы, конечно, можете его ставить разным, ну и смотреть, насколько ваш агент хорошо себя вообще чувствует, сколько наград он получает. но обычно у нас нет скеду на дискомфактор. есть какой-то ресёрч, который говорит о том, а что будет, если дискомфактор увеличить на сколько-то, то есть, например, было 0,99, 0,995 вроде бы, давайте посмотрим, что будет, если мы Будем учиться 0999. Вот. Давайте более подробно глянем, что мы максимизируем и что вообще происходит. Опять же, агент взаимодействует со средой и сэмплирует действия из какой-то политики P. Тут важно, что у нас есть два источника статистичности. Первый источник – это сама политика. Политика может быть статистичной, каково это вероятность распределения. Соответственно, действие, которое мы из него пресамплируем, это какая-то случайная личина. И второе – это статистичность в среде. Как раз с какой вероятностью, если мы совершим действие совершим действие А в состоянии С, с какой вероятностью в какие состояния мы перейдем. То есть вот у нас два источника стахастики в среде, и тут опять же новый термин — это траектория нашего взаимодействия. Это, по сути, перемежающаяся последовательность состояния действия в состоянии действия. А граду можно восстановить награду можно восстановить детерминированным образом из пары состояния действия. давайте поймем вообще что мы максимизируем. я как говорил, мы максимизируем на самом деле вот такую штуку erp от суммы t от нуля до t большого гамма t rt. Но это не что иное, как G0, что, в общем-то, здесь и написано. А давайте поймем вообще, что за матожидание. Пока это просто какой-то символ, не совсем понятно, как говорить о матожидании. Перепишем его в таком виде. перепишем его в таком виде, как вероятность траектории при условии вашей политики. Потому что, опять же, политика, те действия, которые вы выбираете, влияют на ту траекторию, которую вы получите. Соответственно, почему я говорил про два источника стахастики? Потому что мы вот это мотожидание можем разбить на цепочку вложенных мотожиданий. Следите за руками. Свет другой. Можно? Да, пожалуйста. Стохастика среды у нас, наверное, всегда есть? Или нет? Нет, не всегда. Типа, когда ты со студентом, там не было, да? Нет. Ну, там есть стохастика при походе в бар, но в целом, да, там много детерминированных переходов из одного состояния в другое. Окей, спасибо. Спасибо за вопрос. Интересный пример с игрой двух агентов в какой-нибудь шахмате. То есть, если вы играете с человеком, про которого вы, например, ничего не знаете, то его действия для вас неизвестны, у вас среда стахастическая. При этом есть такой трюк, что если вы играете с самим собой, вы ставите двух агентов одинаково, И получается, на самом деле, в тот момент, когда у вас переходит ход, у вас как бы меняется точка зрения. Короче, агент один и тот же играет с самим собой. Это попытка обойти вот эту дистахастическую среду, когда вы играете с каким-то старым игроком. Наверное, не супер наглядно. Давайте у меня будет целых семь недель или восемь, когда мы подойдем к Model-based, чтобы придумать более наглядное объяснение. В общем, интересно, как в таких ситуациях, когда у вас игра двух агентов, когда у вас среда по дефолту стокастична, обходить эту стокастичность через различные трюки. Давайте пока сфокусируемся на наших вложенных надлежданиях. Соответственно, да, первая стахастика – это сэмплирование изначального распределения, то есть в каком состоянии вы окажетесь при начале взаимодействия. Второе – это как раз вы в этом состоянии выбираете действие и переходите в какое-то новое состояние с 1. Ну и в с1 у вас сэмплируется как раз из стахастики среды, ну и и так далее. процесс повторяется дальше, но поскольку R1, ой, R0 у вас зависит только от S0 и A0, то его можно вытащить за мотожидание, вот, то есть вот это мотожидание, оно уже не будет третье. она уже не будет влиять на R0, она уже будет влиять только на то, что находится под гамма-фактором. Для краткости будем просто писать мотожидание по S0, мотожидание по A0, мотожидание по S1 и так далее. Вот такую цепочку вложенных мотожиданий нам нужно максимизировать по нашей политике. Пока Не очень понятно, мы как раз на четвертой неделе поймем, как можно от этого взять, если у вас политика какая-то была бы параметризованная параметром θ, мы могли бы пытаться взять градиент. И как раз на пятой, сейчас, третья, четвертая, На четвертой неделе мы поймем, как это можно сделать, но пока нам такая магия недоступна. Поэтому будем пытаться по-другому. Так. Да, еще давайте поймем вообще, как нам оценивать... Можно ли придумать некоторую оценку для нашей политики как функцию от состояния? То есть, буквально, если вы находитесь в состоянии С, сколько награды вы можете получить, действуя политикой И? И да, это как раз вот оценка того, насколько хорошо вы можете играть, начиная из состояния С. Определение вы видите на слайде. Важно вот что. Во-первых, в терминальных состояниях, поскольку у вас там нет никакой награды, всегда vp от s, value function, в состоянии s равно 0. Можете, пожалуйста, пояснить, что вы имеете в виду? под интегралом. ну если мы сэмплируем каждый следующий ход, то мы берем интеграл этой функции. ну по всем пробегаемся, условно говоря, по всем вероятностям. я понял, но интеграл мотожидания в данном случае, мотожидание это интеграл, поэтому да. Важно, что в терминальном состоянии vp от s равна 0, и jp можно получить, если просто по начальному состоянию с 0, то можно отожидать vp. А почему? У нас же конечное количество экшенов. сейчас да, но в целом никто не мешает вам осматривать, когда у вас мощность множества бесконечно. сейчас да, сейчас мы будем считать, что у нас конечная, поэтому мы будем распивать мод ожидания как сумму. но в целом это все одно и то же, просто общий ключ интеграл сумма получается, когда у вас вероятность массы сосредоточена в каких-то конкретных точках, поэтому это все двойственная вещь. не запутались с этим? еще вопрос. в равно мотожиданию равно мотожиданию суммы. там от k равно 0 или от k равно t? то есть мы же уже в каком-то состоянии в моменте t? а, там t плюс k, все, я вижу. да, там t плюс k. ну и дисконтирование начинается сначала. тут важное. Я думаю, у вас в какой-то момент появится интуиция, но тут важно, что из-за того, что у вас задача самоподобна, из-за социальности и марковости, вам в целом не так... Я понимаю, что сейчас это будет звучать довольно странно, но вам не так важно, из какого состояния вы начинаете. Потому что игра у вас такая. Самоподобная. То есть вы, по сути, считаете, что здесь вы начинаете игру из какого-то произвольного состояния S, и получаете какую-то награду. Дальше value function не будет достаточно, у нас пока никак не участвуют действия, а нам же на самом деле хочется оценивать действия. Как оценивать действие, появляется Q-фанкшн, который зависит от состояния и от действия. То есть это та награда, которую вы можете получить, если в состоянии S совершите действие A и затем будете действовать в соответствии с политикой P. логика точно такая же, то есть в терминальном состоянии u,p равно 0 и v,p можно получить, если промот ожидать u, а u,p должно быть. по вашим действиям. то есть по вашей политике, по стахастике вашей политики. тут еще появляется такое ружье, которое выстрелят потом, а именно advantage. advantage вам говорит о том, насколько действия Насколько в среднем действие принесет награды больше, чем в среднем в этом состоянии? То есть у вас есть какая-то награда, которую вы получите в среднем из этого состояния? Это как раз ваше... в функции. И у вас есть награда, которую вы получите, если в состоянии S делаете конкретное действие A. Вот как раз разность говорит вам о том, насколько действие A в состоянии S лучше, чем в среднем. мы в состоянии, ну политика сама ограничена, она может, если политика статистическая, то она может, например, у нас три действия, она может этому действию дать 1, 2, этому 1, 2, этому 0, поэтому тогда мы не можем никуда пойти, если будем действовать этой политикой из этого состояния. Так, action value function, state value function и равнение Bellman. Равнение Беллмана. Да, тут сейчас будет страшно, мы не будем подробно этому останавливаться. Важность в том, что опять же из-за того, что у вас из-за марковости, из-за национальности вы можете расписать в функцию и ку функцию через как бы в функцию состояние в которое вы попадете следующий раз давайте здесь просто пробежимся в кратко что происходит кстати вот здесь вот вот здесь момент мы здесь предполагаем что у нас количество то у нас множество действий конечны и множество состояний конечны. Поэтому как раз здесь мы заменяем мотожидание на сумму. Если бы это было не так, у нас бы вместо суммы были бы интегралы. Так, раскрываем мотожидание, обуславливаемся на всю нашу историю, но из-за марковости из-за марковости у нас эта вероятность просто сводится к условной вероятности перейти в S3 при условии S и A. Так, какой-то вопрос. Отличный вопрос. Вся базовая теория будет строиться вокруг марковости. В какой-то момент мы просто Ну не то что уйдем, мы скорее забьем на это, потому что у нас там появятся более серьезные проблемы, у нас появятся универсальные аппоксиматоры нейронки, и в целом у нас уже и так не будет никаких теоретических гарантий, поэтому марковостью Мы не то что откажемся от неё совсем, мы попытаемся каким-то образом её поддерживать. Например, мы будем в какой-то момент играть в игры, в компьютерные, Atari, и там как бы есть скрины игр, и вот если, как мы уже говорили, если вы будете использовать один скрин, один скриншот, то среда перестанет дать марковской, и в целом у этого будут последствия неприятные. Но вы можете взять какую-то историю скриншотов, не всю, как я говорил, а взять, например, четыре скриншота и сказать, что это новое состояние. И как бы вы, с одной стороны, С одной стороны, как бы у вас по-прежнему теоретических гарантий нет, но вы чуть больше приблизились к тому, к тем foundations, которые теоретически обоснованы, с которых мы начинали. Вот. Да. Что-то еще хотел сказать? А, ну ещё можно обходить это с помощью памяти. Опять же, вместо того, чтобы стакать четыре последних скриншота, четыре последних скриншота вы можете ввести понятие памяти, ну или рекуррентную сетку. В рекуррентной сетке есть hidden state, которая будет аналогом некоторой памяти. Соответственно, когда вы будете применять сетку на последовательность, у вас как раз есть текущий сприншот и есть память, которая каким-то образом аккумулирует в себе информацию про прошлое взаимодействие. Тоже как бы попытка как раз работать вне марковских средок. Важно то, что… А, вот, я вспомнил, что хотел сказать, что мы не будем развивать теорию для не марковских. Это останется за кадром, потому что сложность экспоненциально увеличивается. Следующий вопрос. Что означает, что... Можете пояснить более подробно? То есть, типа, мы садим компьютер, ну, какую-то сетку, которую мы обучили, играть с человеком, ну, а дальше смотрим WinRate. И получается, что у сетки он выше. Ну, или как, например, когда посадили играть с альфа-зирус, я забыл как его зовут, Ли Сёгун, нет? В общем, чемпиона мира по го. И там есть какие-то правила, там несколько игр, но получается, что по итогам этих игр сетка обыграла чемпиона мира по го. Ответил ли я на вопрос? да так там со временем прошу прощения 23 почти почти выходим за границы так давайте я вот что сделала так я сейчас ненадолго прекращу трансляцию но пока нормально. так снова возвращаю атмосферацию. так чуть нам надо ускориться конечно. чтобы в независимом времени нас с вами не не разъединило вот если у вас есть какие-то вопросы то у нас такая рекламная пауза можете и задать все сюда так вопросов нет значит так скажите пожалуйста вы видите мойка да Да, я почему-то прервался на одном из самых сложных вещей. Короче, здесь мы просто пытаемся раскрыть мотожидание через следующий шаг. Соответственно, первое, что произошло, мы расписали это как вероятность, обусловленную на всю траекторию, используя закон повторного мотожидания. Затем, поскольку у нас следа марковская и стационарная, мы заменяем как раз вот эту вероятность на всю территорию вероятностью, которая обусловлена только на предыдущее состояние действия. Затем как раз мы увидим, что внутри у нас появляется величина, очень похожая по структуре на на V-функцию. На самом деле, определение V-функции в состоянии S3. Опять же, из-за того, что у нас такая рекурсивная среда, где у нас, по сути, взаимодействие с каждым моментом может начинаться сначала, в каждое время, то здесь содержится как раз определение Vp в состоянии S3. Вот. vp в состоянии штрих действует от ожидания. Давайте его просто сплотнем обратно и получим, что v функция в состоянии s выражается через награду, которую мы получим. Ожидаем награду, которую мы можем получить из этого состояния, плюс гамма, умноженная на от ожидания v-функции в состоянии h3. То есть на самом деле вот эта часть, вторая часть не зависит от состояния, то есть вы можете ее так написать, типа от ожидания награды, которую вы можете получить, плюс гамма умноженная на vp от h3. Вот. И это как бы наше первое уравнение Bellman, которое связывает который связывает значение v-функцию в состоянии s со значением функции в состоянии h3. еще у нас таких равнений будет 4. здесь это просто вывод, очень похожий вывод, но но для Q-функции. Обратите внимание, что здесь у нас получается почти то же самое, только нет мотожидания по политике. Напомню, что только что мы увидели, что чтобы получить V-функцию, нужно промотожидать еще по политике. Да. что мы хотим сделать? мы хотим выразить vp и qp в состоянии s через vp и qp в состоянии s'. соответственно, что мы тут видим? во-первых, мы видим, что мы уже вывели это для vp. для qp мы вывели почти то же самое, только у нас вместо Вместо QP от S3 у нас здесь VP. Но мы знаем, это я уже провозглашал, здесь по сути строгое доказательство того, что VP это мотожидание QP по политике. и второе это если вы воспользуетесь как раз с этим соотношением то вы сможете вот эту вайпи заменить на вот такое вот ожидание. то есть по сути по сути опять же выразить купи в состоянии s через купи в состоянии s3 как это как это стоит воспринимать. да, вот так выглядит уравнение Беллмана, они называются Bellman Expectation Equations. узнаем, почему... есть еще Bellman Opt... забыл, как называется, сейчас увидим. Bellman Optimality Equations. поймем, в чем разница чуть позже. важно, что у нас получаются вот такие, по сути, регулярные соотношения, но уже на S и S-штрих, и S и A, и S-штрих, H-штрих. Как? Тут просто картинка, как это можно воспринимать, как это можно запомнить. Вот можно представить игру, поскольку мы все еще находимся в состоянии, в ситуации, когда у нас конечное число состояний действий, можем воспринимать это как дерево, и вот если у вас нода этого дерева это s, вы дальше... дальше вы совершаете какое-то действие a, которое сэмплируется из политики, дальше из среды у вас сэмплируется следующее состояние h3, ну вот следующее состояние, оно на самом деле может быть разным. этот штрих может принимать такие значения. но это утверждается, что чтобы получить значение в состоянии s, вам нужно как раз по этому дереву спуститься, взять все все значения, которые у вас могут быть в следующих состояниях, умножить их на соответствующую вероятность, ну и вот вы получите как раз регулянтное соотношение между VP в состоянии S и VP в состоянии штык. Вот, что надо про награды не забыть. Для купи все то же самое, только вместо... здесь уже нода соответствует не одному состоянию, а паре состояния и действия. Но еще важно, что на самом деле здесь у нас уже действие фиксировано, мы поэтому сэмплируем следующее состояние из-за нашей среды. А потом, находясь уже в новом состоянии, мы сэмплируем действия из этого состояния. То есть мы идем еще на один шаг глубже и смотрим, какую награду мы можем получить как раз из новой пары h3, h3. Давайте введем еще одно определение, так называемое optimal value function. Вообще мы хотим найти политики, которые максимизируют нашу ожидаемую дисконтированную кумулятивную награду. Давайте поэтому введем следующее определение. Скажем, что Vs Zvezda – это максимум в IP по политике P. С Куца-Звездой сделаем так же. Важно вот что. Это нам поможет, это некоторый проксик того, какую награду мы можем получить, если у нас политика была бы оптимальна. Соответственно, мы пока это не доказали строго, но мы можем использовать эти ВС со звездой, чтобы, зная их, определять оптимальную политику. Давайте еще введем частичный порядок на... наших политиках, а именно политика пи будет не хуже политики пи без штрих, если значение v функции политики пи будет не меньше, чем значение v функции политики пи с штрих во всех состояниях. Вот утверждается, что для любого тут важно добавить, что для его конечного MDP существует оптимальная политика P со звездой, которая не хуже, чем все остальные политики. И что у всех, если вот у нас есть P со звездой 1, например, и P со звездой 2, то есть две оптимальные политики, то V-функции у них будут ровными. Мы это доказывать не будем, просто будем пользоваться этим как фактом. Поэтому если мы знаем вкус со звездой, на самом деле мы можем восстановить оптимальную политику по следующему правилу. Давайте всю вероятность массу сложим в действие, которое доставляет наибольшую награду. то есть буквально если у нас есть функция, она дает ответ сколько награды можно получить из текущего состояния, применив текущее действие. Если у нас есть курсы с звездой, это как бы сколько максимум награды мы можем получить. Нам достаточно в текущем состоянии взять действие жадно, в тот момент, когда жадность не порох. Можем действительно выгнать это действие жадно, получить вид оптимальной политики, в этом состоянии. Если у вас есть несколько действий, которые максимизируют эту кофункцию, то вы можете вероятность массы между ними как угодно распределить. То есть на самом деле здесь есть несколько утверждений. Во-первых, в том, что у вас есть детерминированная политика, оптимальная политика. А во-вторых, если у вас есть хотя бы две политики, которые оптимальны, то у вас есть континиум политик. Вы можете между ними распределять вероятность на массу, перекладывая ее из одного действия в другой. Вот это как бы основполагающее, то есть зная куса-звездой, можно установить оптимальную политику. Ну и для ВС-звездой и куса-звездой существует тоже уравнение Беллвина. В чем разница? Смотрите, раньше у нас для ВП вместо максимума было отожидание по то есть буквально vp от s, у нас было мат ожидания по p, кстати не по p, ну да, это одно и то же, по действию сэмплированной из политики от rsa плюс гамма и штрих vp от h' Это для вайпи. Здесь у нас получается, для вэса звездой у нас мотожидание заменяется на максимум. Соответственно, что мы получаем? Мы опять получаем связь вэса звездой в состоянии s через вэса звездой в состоянии штрих. для курса звездок все то же самое, только обратите внимание, что максимум переезжает под ожидание. вот у нас есть два таких уравнения. важно вот что. типа какая связь между этим, то что раньше мы действие сэмплировали из политики, а сейчас мы выбираем действие жадно. Выбираем действие жадно, как бы оптимальное действие. То же самое для кофункции, но здесь, поскольку действие у нас фиксировано, мы действие жадно выбираем уже в следующей очереди, когда у нас S-штрих нужно выбрать в следующей очереди. Поэтому максимум у нас появляется вот здесь. Да, как я уже говорил... Да, да, абсолютно верно. Жадность в данном случае оптимальна, и это как раз гарантируется. Именно потому что для следующих шагов вы уже знаете оптимальную кофункцию, и поэтому вам достаточно действовать жадно в текущий момент, чтобы получить оптимальную политику. а как восстанавливается оптимальная политика? ну как я уже говорил, если куса звездой известна, то давайте просто возьмем arg максимум по действиям для куса звездой. если веса звездой известна, то давайте восстанавливаем из нее куса звездой. вот это это просто уравнение для куса звездой. я напомню, что у нас есть у нас есть следующее соотношение, что q vp vp от s равняется мат ожидания, мат ожидания по, а нет, sorry, это не оно. это просто соотношение между у и в это не имеет отношения на чем мы сейчас просто за замкнемся вот это просто это можно воспринимать как некоторые некоторые вариации уравнения Беллмана. но в целом, если мы знаем весу звездой, мы сначала восстанавливаем кусу звездой, а затем берем от максимума. есть проблема у этого подхода. И это причина, по которой мы с вами кофункции будем любить больше. Да, в этом случае политика выражена в распределении. А так, вторую... Прошу прощения, вторую часть вопроса я не зачитал. Так, вторая вопроса. Это либо выраженное распределение, либо вы можете воспринимать это как просто мэп, который однозначным образом сопоставляет состояние действия. Тут опять же вопрос того, что вам больше нравится. Почему второй вариант хуже? И почему мы к Q-функции будем любить их больше, чем в F-функции? Потому что на самом деле в реальных средах вот эта штука вам будет недоступна. Это на самом деле динамика среды вашей. Она определяет то, с какой вероятностью вы переходите из одного состояния в другое при совершении действия. В реальных средах вам это не доступно, соответственно, вы не сможете так просто взять междужитание. И поэтому вы сфокусируетесь на обучении Q-функции, нежели на обучении V-функции. Еще можно пристально посмотреть на... уравнение Беллмана для V-функции и увидеть, что это на самом деле система из N линейных уравнений. Из мощности множества состояний. И значит, что можно пытаться решить эту систему линейных уравнений. Вот решение находится по такой формуле. Проблема в том, что Если вы так сделаете, то сложность этого решения будет пропорциональна по VU от количества состояний. С другой стороны, вы получаете аналитическое решение для VFUX. Если вы знаете динамику среды, вы можете аналитически найти в функцию для вашей политики. Правда, за довольно большую сложность. можно ли сделать лучше, может быть, мы можем попробовать итеративные методы, методы простой итерации. Ответ – да. Почему мы можем? Нужно обосновать все, почему мы вообще можем это сделать. Потому что это просто Еще одна терминология. Смотрите, тут важно определять вот что. Если вы знаете V со звездой, Q со звездой, то считайте его задачей рифтуна. Вы можете восстановить однозначно оптимальную политику. Проблема в том, что вы их не знаете и Поэтому нужно будет каким-то образом их узнать. Если вы просто для текущей политики, для какой-то фиксированной политики P, хотите понять, сколько награды она может получить, вы как раз находите в A легкофункцию, и этот процесс называется policy evaluation. Довольно логично. У вас есть политика, вы хотите понять, сколько награды можно благодаря ей получить. Соответственно, в Policy Optimization вы хотите найти VA и Q-функцию для оптимальной политики. Соответственно, если вы знаете VA и Q-функцию для оптимальной политики, вы можете саму политику восстановить, как мы уже с вами разобрали. Bellman Equations, вот они все слева направо, четыре уравнения, которые связывают VA и Q-функции между собой. vp и qp, v со звездой и q со звездой. На самом деле их можно представить в виде операторов, которые действуют на некотором банах в пространстве, которые переводят, по сути, одни функции в другие. Да, если вы это сделаете, то и эти операторы действуют на некотором банах в пространстве. Можно показать, что они все так называемые сжатия, операторы сжатия, то есть операторы, которые две точки переводят в более близкие точки. То есть, по сути, поскольку это отображение в себя, то есть вы взяли две точки, подействовали на обе оператором, померили расстояние между ними, и оказалось, что они ближе чем были, чем были их образом. Позволь еще сказать, что все эти операторы сжатия с коэффициентом гамма относительно субнормы, то есть супрэмум по OS от нормы ну да. v s все таки вот так лучше лучше вот так supreme по s вот вы и это с минус в 1 в 2 вот соответственно да такая у вас норма, такие пространства, оператор сжатия замечательная, banner fixed point theorem, который утверждает о том, что у вас существует единственная неподвижная точка. единственная точка, которая при действии... вот, оператор b красивая, который действует на функцию v и получается сама функция. это замечательно, потому что это именно то, что мы ищем. вот если вы посмотрите на уравнение bellman, это буквально уравнение, которое говорит о том, что мы под действием оператора b должны получить саму функцию v. Соответственно, это теорема нам гарантирует, во-первых, существование, во-вторых, единственность. Причем до всех четырех операторов. Это значит, что после этого мы можем... А, вот. Более того, там еще есть пункт. Это первый пункт, что у нас есть существование и единственность. Второе, то, что с какой бы вы в 0 не начали, вот такая последовательность Vec k плюс 1 которая равняется b от v к t, она сходится к вот именно тому v, который неподвижная точка. Более того, вы можете с любой точки начинать метод простой итерации, вы сойдетесь к неподвижной точке. И именно на этом и будут построены наши методы. То есть у нас раньше была система линейных уравнений, давайте превратим ее просто в последовательную. Метод простой итерации. Будем каждый раз получать новый элемент vk плюс один для некоторой фиксированной политики. Алгоритм на экране, вот мы используем по сути Bellman Expectation Operator для функции VP, ну и у нас есть какой-то критерий того, когда мы остановимся. Соответственно, критерий у нас такой, типа мы смотрим норму разности между нашим текущим решением и новым решением, соответственно, мы должны стабилизироваться, когда эти две величины равны. Вот мы его используем в качестве критерия остановки. После какого-то количества итераций мы стабилизировались, нашли... Опять же, у нас есть проблемы с численным приближением, и поскольку это итеративный метод, 100% совпадения мы не получим, но с какой-то точностью мы смогли получить истинное значение для v-функции. Дальше. Policy evaluation — это одно. У нас есть значение v-функции. Что мы можем сделать с этим? Давайте попробуем действовать жадно по... Прошу прощения. Давайте попробуем действовать жадно по отношению к нашей кофункции. Если мы так сделаем, мы получаем новую политику P''. утверждается, что вот эта новая политика, она не хуже, чем политика P. Вот здесь как бы доказательство этого, но вот важно в том, что если вы проделаете эти шаги, то вы получите, что ваша новая политика P' она не хуже, чем политика VP. То есть таким образом вы привели improvement в вашу политику. Получили политику, которая не хуже текущей. Это хорошо. Это значит, что в целом мы можем... А, но еще важно, что если вы не можете провести improvement, значит, вы нашли оптимальную политику. Потому что, по сути, в этот момент из-за построения, из-за того, как вы строите политику, вы получаете, что у вас... вайпи удовлетворяет уравнению бэлбона. это значит, что как раз теорема существования единственности, вы нашли ту единственную vs звездой, которая удовлетворяет уравнению бэлбона, это значит, что у вас процесс сошелся, вы теперь знаете оптимальную вайфункцию, значит, вы знаете оптимальную политику. И из этого можно сделать следующий алгоритм, который называется Policy Iteration. Он, по сути, состоит из двух частей. Первый – это Policy Evaluation. Вы начинаете с какой-то случайной V-функции, с какой-то случайной политики. Дальше вы для этой политики оцениваете VP. Дальше вы с помощью этой VP улучшаете, делаете шаг в полюсе импрувмента. Если вы не стабилизировались, если ваше новое WSS-звездой, не WSS-звездой, а WSS-1, не удовлетворяет уравнению Белкона, то повторяете шаги. Типа для новой политики оцениваете VP, дальше делаете шаг policy pool. Гарантируется, что в какой-то момент процесс стабилизируется, и вы садитесь к оптимальной V-функции, оптимальной политике. Ну, условно это выглядит так. Здесь мы использовали пока только Bellman Expectation Equations, но поскольку у нас Bellman Optimality Equations тоже сжатие, мы можем устроить метод простой итерации для них. Соответственно, когда мы это сделаем, у нас тоже есть гарантия, что мы сойдемся к VSA звездой. В чем разница? На самом деле это все тоже очень двойственные вещи. Смотрите, здесь мы сначала сходимся к VP, затем улучшаем политику, переходим от P к P'. Для того, чтобы оценить VP, нам нужно сделать n шагов, где n — это количество этих итераций policy evaluation внутри цикла, чтобы как раз к этой VP сойтись. На самом деле мы можем сделать n, равный одному шагу, вот этот, а потом, если вы посмотрите, вот эта сумма, вот это и вот это это одно и то же. только здесь еще максимум появляется. поэтому Давайте заменим двухшаговый алгоритм на одношаговый алгоритм, который будет носить название value iteration и отличаться от нашего алгоритма только в том, что мы сливаем по сути шаги policy evaluation и policy improvement, С одной стороны. С другой стороны, как на это еще можно смотреть? На том, что мы просто напрямую применяем Bellman Optimality оператор к нашей текущей вайпи. Но здесь, как бы в Value Iteration у нас политика появляется только в самый последний момент. То есть, когда у нас алгоритм заканчивается, мы восстанавливаем, опять же, P со звездой восстанавливаем как ARC-максимум от U со звездой SA. Промежуточные у нас не происходят восстановления политики. Да, то же самое. Вот здесь хорошо будет понятно в чем разница. Общий алгоритм такой. Давайте сделаем n шагов в policy evaluation, поймем какая оценка для текущей политики сейчас. Потом сделаем 1 шаг в policy provenance. Соответственно, если n равно 1, то это value iteration. Если n равно бесконечности, то это policy iteration, потому что мы каждый раз доходим до конца. Возможны промежуточные варианты. Типа мы можем сделать два шага policy evaluation, остановиться на них и сделать шаг policy improvement. И в целом тут вот что важно понять. Большинство наших алгоритмов, которые мы будем разрабатывать на основе сеток, когда уже никаких теоретических гарантий нет, будут подчиняться этому двухшаговому алгоритму. Делаем policy evaluation, оцениваем качество текущей политики. С помощью этих чисел, с помощью этих оценок делаем какой-то policy. как мы увидим потом, когда мы будем использовать как раз стокастический градиентный спуск, нам важно, чтобы мы в среднем двигались в правильном направлении. То есть, в целом, у нас не будет гарантии сходимости, понятно, но, опять же, если в среднем мы будем двигаться туда, куда надо, то в итоге мы к чему-то хорошему сойдемся. Хотя бы эмпирически. Последний аспект – сравнение между алгоритмами. Здесь включают policy evaluation, причем в самом простом случае, когда мы просто решаем систему линейных выравнений. Плюс восстановление оптимальной политики. Policy value iteration у него, соответственно, не тратит ресурсов на то, чтобы делать policy evaluation, а просто сразу оценивает vk plus 1 со звездой. Так, какой-то вопрос. V мы не максимизируем, потому что V не... короче, мы максимизируем обычно по действиям, а V не зависит от действий, мы по функциям максимизируем. Вот. То есть, опять же, к этому надо привыкнуть. надо привыкнуть что на самом деле вот это вот это это наша у к плюс один с а вот но поэтому да как бы последовательно в цикле вычисляем и максимизируем текущая текущую оценку для у функции ни капли садина к Ответил ли я на вопрос? Примерно да. Спасибо. Так. Это да, сравнение. Проблема в том, что опять же метафора от меня. В этой постановке задачи мы смотрим на среду свысока, в том плане, что мы сразу рассматриваем все множество состояний, но в реальной среде вам же с ней нужно будет взаимодействовать, будете с ней играть, двигаться дальше по этому лабиринту, и поэтому имеет смысл придумать какой-то онлайн алгоритм, который бы по ходу взаимодействия и обновлял, во-первых, v-функцию, v, q-функцию. На самом деле, все то, что мы с вами рассмотрели, это была больше специфично v-функция, просто потому что у нас пока есть возможность из v-функции восстанавливать q-функцию. Но в общем случае, когда у нас этого нет, то мы, конечно, будем учить q-функцию, нежели чем v-функцию. Да, хочется придумать какой-то алгоритм, который бы онлайн мог обновлять VR-функцию, и нам не надо было бы каждый раз проходить по всему пространству состояний, которых в целом может быть довольно много. Сейчас мы рассматриваем какие-то более простые случаи, но вот, например, для Go это, скорее, неприменимо, потому что просто невозможно столько вычисления выполнить, потому что в постоянном состоянии оно невообразимо. Можно почитать, но представить его в этом мире, а тем более в вашем компьютере, будет довольно проблематично. Поэтому хочется такой асинхронный алгоритм, который работал только с одной парой, с одним состоянием в текущий момент, но при этом не потерял свои гарантии на сходимость. Поэтому простые идеи. Во-первых, вот здесь обратите внимание, каждый раз у нас было две у нас было две копии vf-функции, то есть vk и vk++. Соответственно, когда мы для первого состояния обновили vf-функцию, мы эту информацию никак не использовали. сначала первое обновляется, потом второе, потом третье, мы используем старый вектор ВК. На самом деле можно сделать так, чтобы мы каждый раз использовали уже новую информацию, то есть буквально вы, когда у вас начался этот процесс, и вы обновили первое состояние, Вы потом будете использовать это же состояние для обновления следующих, но давайте использовать новое значение его функции. Делать такие in-place апдейты. Это во-первых. Во-вторых, второе тоже можно делать. Давайте каким-то образом выбирать, какие состояния мы будем апдейтить. Не все состояния обдейтить, а только какие-то хорошие. И третье, это давайте, то есть буквально у нас игра начинается в каком-то состоянии С0, мы из него делаем действие А0, приходим в какое-то новое состояние С1, А1. Ну, давайте попробуем обновлять только те состояния, которые у нас на пути встретились. А те, которые нам не встретились, обновлять не будем. И на самом деле, такие скетчи, идеи, они будут суперполезны нам в будущем. Потому что они, на самом деле, заиграют новыми красками, когда мы когда мы будем говорить про алгоритмы, связанные с нейронками. Просто потому что там вам важно, что вы провзаимодействовали какое-то количество шагов. Дальше эту информацию каким-то образом нужно использовать, чтобы в нейронку ее дистиллировать. Но у вас нет доступа ко всем возможным состояниям, ко всевозможным парам состояния действия, поэтому нужно будет выкручиваться. Вот, на самом деле, на этом все. Ссылки я оставлю здесь. Давайте... Сколько у нас времени? Да, мы, к сожалению, сильно вышли за границу, но нам хватит времени на как минимум одну часть семинара, поэтому давайте минут 10 отдохнем. и вернёмся на семинар. Если у вас есть какие-то вопросы, давайте обсудим. А у нас семинар будет до 17.40? Да, я постараюсь, может быть, до 17.45, но постараюсь не задерживать вас. Тогда, я думаю, может быть, смысла нет отдыхать сразу, добить всё до конца. Давайте тогда я хотя бы три минутки попью, может быть, пообщаюсь с вами. И в целом, если... Я не против. Можем сразу к сценарию приступить. Не знаю, насколько мое мнение учитывается, но лично мне хотелось бы перерыв. Да, мне тоже. Мои мозги здесь плавятся к концу. Давайте компромиссно, давайте до... 7 минут отдохнем. До 15. До 17-15. Вот. И вернемся. Я думаю мы все равно не успеем вторую часть. Я отдам вам на самостоятельное ознакомление. Поэтому в целом времени у нас достаточно. Можете, пожалуйста, еще раз вернуть на слайд с самими алгоритмами? С обоими алгоритмами или с каждым? Да, ну желательно с обоими. Ну вот если честно, просто я, может быть, недостаточно внимательно следил, но непонятно, зачем мы сначала считаем v, а потом все равно максимизируем q. То есть у нас там эти вообще... Ну то есть даже в данном случае мы... Вот тут как бы находим V примерно, то есть итеративным каким-то способом. Потом output, нашу политику мы считаем как argmax. И хотелось посмотреть просто, откуда вообще информация из первого итеративного метода, то есть V приходит вот в этот argmax. то есть у нас буквально там вот v от s штрих – это наша функция v, которую мы нашли в оперативном методе, да? конкретно у нас здесь value iteration немного по-другому, потому что у нас нет процесса ну точнее, ладно, он есть. короче, policy improvement в данном случае. ой, policy... policy evaluation. у нас здесь одна итерация. вот она. вот она здесь вшита. вот она. это policy improvement. а вот это policy... ой, сорри, простите. это policy evaluation. одна итерация, policy evaluation. вот это policy improvement. здесь они просто слиты воедино. Вопрос, почему V, а не Q. Потому что, на самом деле, если вы будете учить Q, то вам нужно Q-функцию учить для состояний и для действий, для всех. Что может быть довольно дорого, дороже, чем просто выучить V, потом из нее восстановить Q один раз, и потом максимизировать. по действиям. но если у вас нет вот этого вот этого вот ожидания вот этого вот этой динамики среды то вы будете учить Q-функцию потому что просто вы не сможете вот это если у вас нет знания о динамике среды то вот этот шаг вы не сможете сделать просто вы не сможете из V восстановить Q из Q в V можно восстановить из V в Q нельзя так у вас какие-то еще вопросы были? можете повторить? у меня вот в самом деле просто вопросы про вот это понимание были, а понимание больше стало. спасибо. то есть динамика среды это подразумевается вот эта стахастичность среды? это подразумевается вот это по сути вы находитесь в состоянии s, делаете действие а, а дальше вас с какими-то вероятностями перекидывает в разные состояния. вот если вы не знаете эти вероятности, то вы не сможете посчитать вот это мотожидание. я понял, спасибо большое. Ну и в целом, в следующей неделе мы откажемся от этого, то есть у нас этого знания не будет доступно, и мы будем именно учиться по нашим же взаимодействиям. Опять же, это моя метафора, что сейчас мы смотрим на среду как бы из третьего измерения. Она для нас полностью понятна, мы про нее все знаем. в какой-то момент мы приземлимся прямо в нее, будем прямо по ней ходить и собирать наши опыты S0, A0, S1, A1 и так далее. И это как бы будут наши данные, по которым мы будем учиться. Мы не сможем вот так вот посмотреть, откуда бы мы могли пойти из S0 и промоторжидать по всему. Так что, да, там много будет приколов, но вот важно, что мы пока в таком довольно приятном сеттинге, когда все известно, все прекрасно. Еще какие-то вопросы? Пока. 14.16, давайте продолжать. Не появилось? За такой короткий промежуток времени не появилось еще вопросов? Да, кстати. Я не знаю, здесь это я не указал, но есть... Если вы зайдете на страницу курса, то там как раз есть эта ссылка на конспект на русском языке от коллеги из шада вот довольно подробный довольно с одной стороны формально с другой стороны с большим количеством примеров так что если вы бы хотели посмотреть на какую-то альтернативу, даже не альтернативу, а скорее более полную, полный рассказ того, что мы с вами сегодня прошли, то как раз вот рекомендую получается глава 3, разделы 3.1, 3.2 и 3.3. Я это добавлю еще на слайд. Но мне кажется, что если вы прочитаете, тут не так много страниц, то станет намного, может быть, понятнее и легче в будущем. А пока давайте приступать тогда к семинару. А можете, пожалуйста, поделиться этой ссылкой потом в Телеграме? Да, конечно. Если что, она есть на странице курса... Курс. Ладно, давайте не будем тратить время. Да, я поделюсь обязательно. Да, что касается... семинара. Это суперпростая часть, суперботная. Просто поговорим про библиотеку, которая нам позволит запускать наши любимые эксперименты, наши любимые среды, чтобы затем мы могли полностью сфокусироваться на разработке агента. Тут, да, еще важный момент, которого я забыл упомянуть. У вас всегда В RL у вас всегда есть двое. Это среда и агент. И вот часто забывают про среду, а все внимание уделяется агентам. Ну и мы в целом с вами будем по большей части фокусироваться на агентстве в состоянии вопроса. Если у вас есть какая-то задача практического толка, для которой нет симулятора, то первое, что вам нужно будет сделать, это симулятор придумать. Будь то это какие-то эвристики. Кстати, почему, например, тяжело применять RL в трейдинге, потому что там часто все упирается в том, что у вас нет достаточно reliable и точного симулятора, который бы смог в какие-то какую-то динамику и ваш импакт учитывать. А если у вас есть такой симулятор, вы можете сделать много более простых вещей, чтобы заработать денег. Соответственно, не забывайте, что если вы приступаете к какой-то практической задаче, то в первую очередь нужно будет озаботиться, конечно, о качестве вашего симулятора, насколько Насколько он вообще соответствует тому, что будет происходить в реальной жизни, насколько он реагирует на действия агента, насколько импакт, который агент носит, учитывался. Но благо в наших полуигрушечных примерах все среды уже готовы, кто-то их за нас написал. Можем этим пользоваться, и давайте приступим. Наш First Class Citizen – это Gymnasium библиотека. Раньше это был OpenAI Gym. OpenAI ее поддерживал, но в какой-то момент, по-моему, По-моему, это было в ноябре 22 года, я тогда читал первую итерацию курса, и резко прекратилась поддержка OpenAI Gym. Появился Gymnasium, и пришлось на него переезжать, но сейчас мы уже заимелы даже в версию 1.0. так что он в целом довольно stable. а что из себя представляет библиотека? можно на самом деле просто сходить в документацию и Немного посмотреть, это просто API для того, чтобы взаимодействовать с нашими любимыми средами. Здесь Lunar Lander, среда, где вам нужно посадить как раз этот замечательный луноход на посадочную площадку. Обратите внимание, что среда каждый раз меняет свой ландшафт, то есть у вас тоже есть такая статистичность. В целом, здесь много всего доступно. Здесь доступен Classic Control, где нужно управлять вот этими сочленениями или тележку двигать, чтобы палочка не упала. Здесь есть Atari. у них появилась новая страничка, неважно. Atari как раз из игрового автомата, среды очень много, 57, если я не ошибаюсь. Здесь есть Mojoca, это как раз уже более продвинутые среды для Stochastic Control, где вот чуть ли не какого-то а-ля гуманоида можно похожего на человека пытаться выучить. в общем, много всего, ну и у них, понятно, есть всякие расширения, поэтому довольно хороший инструмент. external environment. Можно здесь тоже посмотреть. Какой у него интерфейс? Интерфейс мы создаем в среду с помощью функции GMake. Вот здесь у нас Mountain Car. Надо тележку завести на эту горку. Action Space у вас дискретный. если я не ошибаюсь, вы можете посмотреть, давайте посмотрим mountain car, то есть увеличить скорость, уменьшить скорость и наверное я предполагаю ничего не делать да не в смысле уменьшить скорость, скорость она со знаком, то есть в зависимости от того, какой вы знак, вы либо едете влево, либо вправо, но, соответственно, одно действие — это ничего не делать. Чего не делать? Вот как среда представляется когда вы вызываете функцию рендер, то вам, по сути, показан такой кинематографичный вид среды. На самом деле, что видит нас агент, это два числа. Первое в диапазоне от минус 1,2 до 0,6. Второе вот в этом диапазоне. Первое – это критината, второе – это скорость. Но агент видится только в два этих числа. Какой, в целом, интерфейс у Gymnasium? У него есть функция Reset. Это функция, которая вам возвращает начальное состояние. То есть, если вы хотите спросить среду до ее начального состояния, то вызывайте функцию Reset. Рендер мы как раз используем для того, чтобы получить кинематографичное изображение среды. И функция Step. Функция Step — это функция, которая по действию возвращает вам следующее состояние, награду, и два флажка. В смысле, два буллинг-флага. Первый отвечает за то, пришли ли вы в терминальное состояние, а второй, закончилось ли время взаимодействия. Часто бывает такое, что вы ограничиваете, то есть потенциально можно кататься бесконечно по этой среде, никто вам этого не запретит, но вы каким-то образом ограничиваете ограничиваете, сколько агент может находиться в этой среде. После того, как этот горизонт заканчивается, то взаимодействие заканчивается. Вы считаете, что агенту мало того, что нужно забраться на эту горку, так ему еще надо сделать это за ограниченное время? Ну и тут есть принципиальная разница, потому что terminated подразумевает, что вы попали в терминальное состояние. Терминальное состояние здесь — это забраться на горку, достичь цели. Если мы рассматриваем какой-то другой пример, то терминальное состояние может быть либо добраться до цели, либо упасть в лаву, например, если вы идете по какому-то узкому коридору. Вот это терминальное состояние. Итог один – взаимодействие заканчивается. Но разница есть. Ранкейд – это именно обрубает ваше взаимодействие, но вы не попали в терминальное состояние. Пока нам это не важно, но в будущем, когда мы будем как раз обучать сетки, это может играть свое значение. Можно почитать, почему как раз здесь написано «handling time limits», почему это важно, но пока я объяснять не буду. Узнаем об этом. совсем скоро. Важно еще, что в функцию reset можно передать seed, то есть в целом среда может тоже сэмплировать из начального распределения, поэтому в зависимости от того, хотите ли вы всегда начать с одного и того же, что в целом не всегда хорошо, потому что это потенциальный путь к overfeed под фиксированное начальное состояние. тем не менее такая возможность у вас есть. Соответственно вот, это reset, степ возвращает 5 чисел. Не помню, когда я последний раз пользовался тем, что лежит в инфо, но может быть полезно знать, что оно вообще есть. Так, запустим увидим. Увидим, что после того, как мы увеличили скорость, в этой среде мы получаем минус единицу за каждое время, проведенное в этой среде. То есть, если у вас, допустим, 200 шагов, то минимальная награда, которую вы можете получить, равна минус Минус 200. Соответственно, чем быстрее вы придете, тем больше награды вы получите. Что дальше? Давайте поиграем. Вот здесь поставим таймлимит равный 250. полисе. полисе у нас будет супер простая. давайте попробуем всегда ехать вправо. всегда попробуем увеличить скорость. еще как бы есть зависимость от времени, но политика она в целом может зависеть от времени. то есть в разное время вы можете захотеть в одном и том же состоянии делать разные действия, но в этом нет особого смысла. На самом деле, вот здесь в конспекте хорошо написано отказ от… вот здесь вот. То есть рассматривается общий вид политик, зависящий от времени, ну и доказывается, что в целом в этом особо нет смысла. ну соответственно да здесь мы в любой момент времени едем направо давайте посмотрим к чему нас это приведет spoiler ни к чему хорошему то есть мы пытаемся поехать направо но нам не хватит импульса нам не хватит чтобы забраться сюда и поэтому вот мы так вот в вечном цикле будем пытаться будем пытаться, пытаться забраться, но ничего хорошего не получится. Как вы думаете, какая может быть оптимальная стратегия, глядя на вот эту картинку? Пока мы едем вправо, продолжать ускоряться вправо, потом нас начнёт стягивать вниз, в этот момент начать ехать влево. ну и наоборот. короче, если мы едем вправо, то ускоряемся, если едем влево, то ускоряемся влево. да. можно так? можно вообще как бы с самого начала попытаться проехать? а нет, я думаю, вы правы. давайте попробуем это записать. то есть смотрите, у нас есть preposition, есть velocity. и вот то, что было озвучено. если velocity отрицательная, то давайте двигаться влево. ну а соответственно, если велосипед положительный, то будем двигаться вправо. и да, это совершенно верно, что здесь произойдет. мы попробуем ехать направо, здесь мы остановимся, начнем ехать влево, наберем как раз импульс и уже нам хватит энергии, чтобы забраться вверх. поздравляем поздравляем мы прошли эту игру проблема в том что мы с вами воспользовались некоторой дополнительной информацией мы с вами видим игру с дополнительного измерения и в целом как бы у нас с вами опыта намного больше, мы знаем, как работают физические явления, поэтому можем предположить, какая есть, какая может быть оптимальная стратегия. Агент уже, вот агент, эта тележка, она будет наблюдать только два числа — это текущее свое положение и свою скорость. И в процессе этого взаимодействия она каким-то образом должна понять, А какая же все-таки оптимальная политика? То есть, возможно, если бы мы находились в такой очень узкой ситуации, то мы бы оптимальную политику восстанавливали тоже очень долго, но поскольку мы с вами имеем очень богатый опыт за плечами и привилегированную информацию, то мы очень быстро решили эту задачу. Тут еще вот важно, что? У нас есть привлекательная информация, у нас с вами есть какой-то прошлый опыт, у нас буквально память довольно хорошо агрегирует прошедшие события, а ген же, ну давайте представим, что это какая-то нейросетка, оно буквально инициализируется какими-то случайными числами, то есть у него нет никакого прошлого опыта, и поэтому у него задача еще сложнее. давайте у нас осталось на самом деле не очень много времени давайте мы уже не будем писать код мы с вами посмотрим на на мою реализацию вот чего. Я вам ноутбук скину, в целом про кросс-энтропийный метод. Вообще, в чем смысл этого ноутбука? Вот если вы прослушали первую лекцию, вам показалось, что ну все это, конечно, интересно, круто, но что-то мне не хочется это изучать, а задачи такие решать хочется. Они там на практике возникают или или просто какой-то праздный интерес, но вот все, весь этот формалист Арелла, он такой, довольно... ну, не лежит к нему душа, например. Вот как быть? Неужели тогда нельзя будет решать задачи? Но вот ответ... В целом, можно. Придется, конечно, пожертвовать эффективностью, потому что мы с вами чем больше inductive bias сделаем, ведем, тем задача у нас будет легче решаться, данных нам потребуется меньше. А вот если мы, например, попробуем забрутфорсить нашу нашу задачу, что будет. Ну и как бы приходит на ум следующий метод. Типа давайте попробуем просто собрать данные и обучиться Обучиться на хороших данных, на плохих обучаться не будем. Очень общее утверждение. Давайте делать хорошо, а плохо не делать. Но, как ни странно, оно работает неплохо. То есть, что мы делаем? Это на самом деле способ, вдохновлен как раз эволюционными стратегиями. Давайте сделаем популяцию, популяция каким-то образом, каждый элемент популяции можно замапить в некоторые типа фитнес-фанкшн, то есть по сути значения, насколько много награды мы получаем. Ну давайте выберем элитных представителей популяции, а именно точки, в которых награды больше всего. Ну и попробуем обучиться под эти элитные точки. Ну вот, приходим к методу кроссантропии. Алгоритм я, по сути, уже описал. Общая популяция, выбираем точки, которые нам кажутся элитными. Часто используется такой критерий, типа, важно тут вот что, что конкретно для нашей задачи точки популяции — это целые территории. То есть мы выбираем... мы N раз взаимодействуем со средой с помощью какого-то алгоритма, какой-то случайно нейросетки. Дальше у великой силы рандома у нас есть... мы надеемся на то, что у нас есть какое-то распределение наград, которые мы набрали благодаря этому. Благодаря этому То есть это не все, не коллапсирует в одну точку в какое-то значение, а у нас есть какое-то распределение, поэтому мы можем это распределение отсечь по определенному квантилю, выбрать только траектории, которые попали как раз справа от этого квантиля. обучиться на них. Ну и, соответственно, если все сделать правильно, то, как видно на картинке, тут, конечно, суперпростой пример, то мы постепенно… А, мы обучились, вот. Дальше у нас как бы есть… Мы пытаемся сэмплировать точки в окрестности элитных точек и обучиться на еще более элитных точках. У нас такой самоулучшающий процесс. Если верить этой картинке, то за 5 итераций мы можем сойтись к какому-то глобальному оптимуму. Опять же, здесь довольно простой пример. С точки зрения формализации, нам важно придумать критерии, по которым мы будем выбирать эти элитные точки, и нам важно придумать критерии, по которым мы будем обучаться под эти как раз элитные точки. Да, да, ну как бы байские подборки параметров, если... то он же тоже на эволюционных стратегиях построен. Последнее... называется, в Аптуне. По-моему, так вот, если я не ошибаюсь. Ну да, вот. Картинка похожа. Вот только там, если я не ошибаюсь, вы видите как раз среднее гауссиана и ее матрицскую вариацию. Но аналогия абсолютно верна. Это все на самом деле. Ноги растут из одного и того же места. Это эволюционный алгоритм. Вот. Здесь мы подробно останавливаться не будем. Тут вводится новая среда. Я хочу вам показать пример, который мне понравился. Опять же, тут есть расстреление, про которое я говорил, расстреление наград. Вот мы отсекаем либо по медиане, либо по девяностому квантире и пытаемся обучиться на этой траектории. Это довольно старый метод. И он хорошо общается на нейронке в том плане, что вы можете попытаться по состоянию предсказывать вероятность распределения. Если у вас дискретная задача, то это какой-то softmax над логитами. Если у вас continuous задача, то вы можете возвращать, например, среднее и стандартное отклонение для гусяны. Опять же, зависит от того, как вы хотите вашу параметризовать ваше распределение. Можно использовать распределение студента, если у вас хвосты тяжелые, но it's up to you. Статья от OpenAI, название говорит само за себя. Здесь вкратце в чем смысл, мы на этом не будем устанавливаться, потому что есть еще более… я не знаю, на самом деле разницы особой прям вот колоссальной нет, но… Давайте остановимся на Augmented Random Search. Видимо, потому что для него я написал код, и для него есть эксперименты, которые я могу показать. Augmented Random Search тоже про попытку использовать революционные стратегии для обучения с подкреплением, но за счет… у вас sample efficiency меньше, то есть ценность одной конкретной траектории, одного конкретного она небольшая, поэтому вы пытаетесь залить это большим количеством компьютер. Но при этом вам не надо никаких Vecu функций, Bellman equations и тому подобное. Вы просто... Что вы делаете? Что вы делаете? У вас есть такая задача. Мы ее уже где-то видели. Мы пытаемся максимизировать вот это ожидание, здесь нет дисконтирования, это неважно, по нашей политике. Мы бы хотели, может быть, взять градиент, ну потому что целом это первая идея. если это дифференциальная функция, давайте попробуем взять от нее градиент. дифференцируемая или не дифференцируемая? мы узнаем, что она дифференцируемая на четвертой неделе и сможем взять от нее градиент. но пока давайте забудем про это. давайте сделаем кое-что другое. во-первых, давайте скажем, что политика это просто линейная модель. буквально какая-то линейная комбинация ваших состояний. Тут важно, что мы работаем в сеттинге continuous control, где часто у вас просто состояние – это один вектор. Вектор каких-то наблюдений, скорость, текущая позиция, ускорение, еще какие-то вещи. Соответственно, давайте просто возьмем ленину комбинацию, вот наша политика получится. Задача, опять же, статистического контроля, поэтому на выходе у вас вещественное число. Всё. Супер простой сеттинг. Даже не нейросетка. Точнее, нейросетка, но самый простой её способ. Просто линейная функция. Давайте вместо того, чтобы брать этот градиент точно, аналитически, попробуем аппроксивировать его с помощью, как называется, numerical derivative. То есть просто по сути как-то работает. я думаю все знают про вот это в одномерном случае. типа fx плюс h минус fx минус h делить на 2h. попытка апоксимировать производную функцию в точке x. довольно неплохо. вот можно смотреть, там какая-то абсолютно точка. в многомерном случае давайте попробуем апоксимировать градиент вот так вот. вот типа находимся в точке текущих параметров θ. дальше сэмплируем какой-то галсовский вектор. на самом деле много галсовских вектор. отступаем от отступаем на этот дельту в одну сторону, отступаем на этот дельту в другую сторону, считаем значение нашего функционала здесь, значение нашего функционала здесь, ну и оцениваем с помощью этих значений, как раз вот gθ плюс дельта, gθ минус дельта, оцениваем с помощью разности значения нашего градиента. еще умноженный на величину шага. вот. так делал много раз, то есть дельтаитов у нас будет много, но вот получаем какую-то оценку градиента. дальше будем делать градиентный подъем с помощью этой оценки. тут еще есть Трюки. Типа, нейросетки любят, когда у вас входы сцентрированы относительно нуля и с единичным стандартным отклонением, поэтому можно считать в онлайне статистики по вашим наблюдениям и нормализовывать их на эти роллинг-статистики. Чем-то похоже на бач-шнор. Вот, это первое. Второе, то что у вас еще на самом деле между разными траекториями может быть разная дисперсия нашей награды, поэтому давайте вот здесь вот посчитаем. Сори, я, наверное, не супер хорошо видно. Так, все уехало, ну ладно. Давайте посчитаем еще стандарты отклонения ваших наград, поделим на него, чтобы как-то стабилизировать их. А, ну еще, на самом деле, вместо того, чтобы брать все пертубации ваших исходных параметров, взять только b топовых. И, да, соответственно, использовать только их для оценки. Вот такой незамысловатый алгоритм. Численная оценка градиентов, только линейные политики. и там пару хаков с тем, чтобы нормализовывать, стабилизировать обучение в процессе. Давайте посмотрим, что получается. На код смотреть не будем, представляю это питателю в качестве упражнения. Вот такая среда, по сути, палочка с несколькими сгибами, как змея, чтобы двигаться. Вот можно посмотреть, это сам алгоритм, вот можно посмотреть награду в зависимости от номера итерации. Можно еще посмотреть, как обученный алгоритм двигает эту палочку. Работает достаточно неплохо. Более сложный пример. Здесь, по-моему, даже окрестность 350 – это максимум, чего можно достичь. Здесь награда эволюции намного более дисперсная, но и среда более сложная. То есть у вас такой, это называется half cheetah, половина гепарда. Надо его заставить ходить. Подавая какой-то момент силы на сочинение. Ну вот. то есть уже чуть ближе к реальности, ходит достаточно бодро. Реально тут прошло буквально 200 итераций алгоритма. И мы увидим, кстати, когда будем проходить Stochastic Control, то в целом это довольно сложная задача. И тот факт, что настолько простой алгоритм может их решить, у меня, честно говоря, впервые, когда я это увидел, вызвал изумление. ну и еще это тоже такую лицехвата такого нужно обучать ходить кто-то по моему если у хавчита был по моему 2, размерность пространства действия была равна 2, то у этого, по-моему, 4, если я не ошибаюсь. То есть это еще более сложная среда. Вот, так что ответ на ваш вопрос, можно ли решать RL задачи без RL, без всей этой теории. Вы не задавали этот вопрос, я знаю, что оно вертелось у вас в голове. Какие у этого есть побочные эффекты? Предлагаю вас как раз почитать статью. Если не ошибаюсь, какое-то колоссальное количество ТПУ нужно, чтобы оно собирало данные. Ну потому что каждый раз вам нужно сэмплировать большое количество как раз этих векторов и каждый из новых хеток по сути играть снова. То есть если вы просэмплировали n вот этих векторов, то вам нужно 2n раз сыграть игру. Ну конечно вы это распределяете как-то между цифрушками. Да. И таким образом собираете данные в brain. Вот. На этом все. Я надеюсь, я вас не испугал до первой лекции. И мы с вами пройдем этот путь вместе до конца. Любые вопросы, которые у вас есть, смело задавайте в чате. да, на этом. если у вас есть какие-то вопросы сейчас, можем обсудить. если нет, то до встречи через неделю. всем хороших выходных. спасибо. А там вопрос в чате вроде бы насчет просадок ревордов на поздних итерациях. На гепарде был. Так. Рассказано просадки низких ревордов на поздних итерациях. Хороший вопрос. Я предполагаю... Это мое предположение, я не уверен что это так, но короче у этого гепарда есть особенности. что в целом есть... его не штрафуют за то, что он падает на спину, и у него есть стабильное состояние, когда он лежа на спине, двигает своими конечностями и двигается вперед. Ну, соответственно, поскольку у вас награда эта в первую очередь состоит из того, насколько вы двигаетесь вперед, и у вас не терминируется среда, если вы падаете на спину, то вы можете прийти в таком стабильном состоянии. Я предполагаю, опять же, это предположение, что у него есть хорошее стабильное состояние такое, и он иногда падает на спину, И там как бы все плохо, но за то, что его не штрафуют за это. Он все равно иногда попадает в это состояние, может там даже подвигаться, но при этом награду он там получает не очень высокую, но потом в какой-то момент он из-за стахастики всего процесса из этого состояния выпрыгивает, и все снова становится хорошо. Вообще, это, кстати, хороший вопрос, потому что часто мы сейчас с вами наблюдаем только одну траекторию награды в хороших статьях по RL. Из-за того, что у RL такая довольно сложная парадигма с большим количеством компонентов, часто одни сиды удачнее других. поэтому хорошим тоном считается репортить вот такую траекторию по нескольким седам, считать среднее, считать плюс-минус стандартное отклонение. но вот у некоторых сред дисперсия награды может быть намного выше, чем у других. вот, мое предположение. давайте давайте расходиться спасибо до свидания счастливо пока пока\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:03:05.278102Z","iopub.execute_input":"2025-03-04T19:03:05.278355Z","iopub.status.idle":"2025-03-04T19:03:05.309937Z","shell.execute_reply.started":"2025-03-04T19:03:05.278333Z","shell.execute_reply":"2025-03-04T19:03:05.308942Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#print(l1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U langchain-community\n!pip install langchain-openai\n!pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:05:37.790721Z","iopub.execute_input":"2025-03-04T19:05:37.791089Z","iopub.status.idle":"2025-03-04T19:05:48.993073Z","shell.execute_reply.started":"2025-03-04T19:05:37.791060Z","shell.execute_reply":"2025-03-04T19:05:48.992049Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.19)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.41)\nRequirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.20)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.8.1)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (2.11.0a2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (2.29.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain-community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\nRequirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.3.7)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.39 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.41)\nRequirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.65.2)\nRequirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.9.0)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.2.3)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (9.0.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (4.12.2)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (2.11.0a2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.39->langchain-openai) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (1.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain-openai) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\nRequirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings  # Импорт из langchain_openai\nfrom langchain.vectorstores import FAISS\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain_community.chat_models import ChatOpenAI\n\napi_key = \"\"\n# 1. Загрузка учебника по RL (PDF)\npdf_path = \"/kaggle/working/rl.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# 2. Разбиение учебника на фрагменты (чанки)\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(docs)\n\n# 3. Создание векторного хранилища с эмбеддингами\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-ada-002\",\n    openai_api_key=api_key\n)\nvectorstore = FAISS.from_documents(chunks, embeddings)\n\n# 4. Настройка ретривера для извлечения 5 наиболее релевантных фрагментов\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n\n# Предполагается, что переменная l1 содержит текст лекции на русском языке\n# 5. Извлечение релевантных фрагментов учебника по тексту лекции\nrelevant_docs = retriever.get_relevant_documents(l1)\nretrieved_context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:06:02.634148Z","iopub.execute_input":"2025-03-04T19:06:02.634478Z","iopub.status.idle":"2025-03-04T19:06:29.971078Z","shell.execute_reply.started":"2025-03-04T19:06:02.634453Z","shell.execute_reply":"2025-03-04T19:06:29.970347Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-f8eabf52a7e9>:30: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  relevant_docs = retriever.get_relevant_documents(l1)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 6. Формирование промпта для генерации итоговой сводки (на русском)\nprompt_template = \"\"\"\nТы эксперт в области обучения с подкреплением (RL). \nСделай подробную суммаризацию лекции, представленной ниже, \nдополняя её информацией из учебника по RL, используя только следующие фрагменты:\n\n{retrieved_context}\n\nЛекция:\n{lecture_text}\n\nПожалуйста, выведи структурированный итог.\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"retrieved_context\", \"lecture_text\"],\n    template=prompt_template\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Инициализация LLM через ChatOpenAI и создание цепочки для генерации итогового ответа\nllm = ChatOpenAI(\n    model_name=\"gpt-4o-mini\",\n    temperature=0,\n    openai_api_key=api_key\n)\nchain = LLMChain(llm=llm, prompt=prompt)\n\nfinal_summary = chain.run(\n    retrieved_context=retrieved_context,\n    lecture_text=l1\n)\n\nprint(\"Суммаризация лекции с данными из релевантных фрагментов учебника:\")\nprint(final_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:14:43.929479Z","iopub.execute_input":"2025-03-04T19:14:43.929877Z","iopub.status.idle":"2025-03-04T19:14:57.485153Z","shell.execute_reply.started":"2025-03-04T19:14:43.929850Z","shell.execute_reply":"2025-03-04T19:14:57.484350Z"}},"outputs":[{"name":"stdout","text":"Суммаризация лекции с данными из релевантных фрагментов учебника:\n### Итог лекции по обучению с подкреплением (RL)\n\n#### 1. Введение в курс\n- Курс состоит из 10 недель, включает лекции и практические семинары.\n- Пять домашних заданий: две простые и три сложные.\n- Презентация статьи как творческое задание.\n- Квизы по материалам предыдущих занятий.\n\n#### 2. Основные концепции RL\n- **Обучение с подкреплением (RL)**: агент взаимодействует со средой, получая награды за свои действия.\n- **Марковское свойство**: текущее состояние содержит всю необходимую информацию для принятия решения.\n- **Награда**: скалярный сигнал, который агент стремится максимизировать. Важно учитывать, что награды могут быть отложенными.\n\n#### 3. Модели мира (World Models)\n- В реальных задачах функция переходов и награды неизвестна, но можно попытаться ее выучить.\n- Использование байесовского вывода для оценки параметров модели.\n\n#### 4. Алгоритмы RL\n- **Policy Evaluation**: оценка текущей политики для определения ее эффективности.\n- **Policy Improvement**: улучшение политики на основе оценок.\n- **Value Iteration**: объединение шагов оценки и улучшения политики в один процесс.\n\n#### 5. Структура MDP (Марковский процесс принятия решения)\n- Определяется как кортеж, состоящий из:\n  - Пространства состояний.\n  - Пространства действий.\n  - Условной вероятности перехода между состояниями.\n  - Награды.\n\n#### 6. Взаимодействие агента и среды\n- Агент принимает действия, получает награды и переходит в новые состояния.\n- Важно учитывать, что состояние может быть частично наблюдаемым.\n\n#### 7. Политики\n- Политика может быть детерминированной или стохастической.\n- Оценка политики через функции ценности (V-функция и Q-функция).\n\n#### 8. Проблемы и решения в RL\n- **Credit Assignment Problem**: сложность в определении, какое действие привело к полученной награде.\n- **Exploration vs. Exploitation**: баланс между исследованием новых действий и использованием известных.\n\n#### 9. Применение RL\n- Примеры применения RL: игры (шахматы, Go), управление роботами, трейдинг.\n- Использование RL в контексте больших языковых моделей и других современных задач.\n\n#### 10. Библиотеки и инструменты\n- **Gymnasium**: библиотека для работы с RL-средами, позволяющая легко взаимодействовать с различными задачами.\n- Примеры сред: Lunar Lander, Mountain Car, Atari.\n\n#### 11. Заключение\n- RL — это мощный инструмент для решения сложных задач, требующий понимания как теоретических основ, так и практических аспектов.\n- Важно учитывать как агента, так и среду при разработке RL-решений.\n","output_type":"stream"}],"execution_count":15}]}